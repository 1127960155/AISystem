# AI集群通信软硬件

## 介绍

随着大模型的出现，它所需的算力和参数存储并非有限单卡能力能够满足的。比如：A100 的单卡算力只有 312 TFLOPS，而 GPT-3 则需要 314 ZFLOPs 的总算力，两者相差了 9 个数量级。GPT-3 的 1750 亿参数本身就需要 700 GB 的显存空间（每个参数按照 4 个字节计算），而 NVIDIA A100 GPU 只有 80 GB 显存。

有限的单卡能力和模型的巨大的存储、计算需求之间的矛盾，是可以通过分布式训练的方法解决，但是分布式训练又会遇到集群通信的性能问题。分布式训练过程中需要在多个gpu之间或者多台机器之间传输大量的网络模型参数进行梯度同步或者参数更新等操作，协同完成训练任务。因此一旦某一个环节通信存在性能瓶颈或问题，将影响全局的分布式训练。因此AI集群通信是分布式训练的重要部分。

AI集群通信的具体实现方式分为**机器内通讯**和**机器间通讯**两种类型
**机器内通信**一般分为共享内存、PCIE、NVLink三种方式；**机器间通信**一般分为TCP/IP网络、RDMA网络两种方式。

### 机器内通信

**共享内存**

早期GPU需要将数据从显存复制到GPU驱动在系统内存中pin住的Memory 1，再从Memory 1复制到Memory 2，之后才能再做进一步传输。然而使用了GPUDirect Shared Memory之后，内存空间实现了共享，减少了一次数据复制，降低了数据交换延迟。多个GPU，或者GPU与网卡，或者GPU与其他设备之间可以通过访问GPUDirect Shared Memory方式访问**同一台**主机上的内存资源。注意：共享内存方式，设备双方须满足在同一台机器上的条件。

![GPUDirect Shared Memory](images/01Cluster_network.png)

**PCIE**
PCIE 全称“Peripheral Component Interconnect Express”，即“高速外围组件互联接口”。它是一种串行总线接口标准，用于连接计算机系统中的各种设备。它可拓展性强，可以支持的设备有：显卡、固态硬盘、无线网卡、有线网卡、声卡、视频采集卡、PCIe转接M.2接口、PCIe转USB接口、PCIe转接Tpye-C接口等。总的来说，PCIE 是现代计算机系统中不可或缺的一部分，它为各种设备提供了高速、灵活和可靠的连接方式。

![](images\02Cluster_network.png)

目前PCIE已经迭代更新过多个版本，每个版本的带宽数据如下：

![](images/03Cluster_network.png)

PCIe的设计旨在使所有硬件设备能够高效地与计算机的CPU进行通信。然而，尽管最新的PCIe版本（比如PCIe 4.0和5.0)提供了非常高的数据传输速度，但它们仍然不能满足一些特别高性能应用的需求。

NVIDIA推出了能够提升GPU通信性能的技术——GPUDirect P2P技术，使GPU可以通过 PCI Express 直接访问目标GPU的显存，避免了通过拷贝到CPU host memory作为中转，大大降低了数据交换的延迟，但受限于PCI Express总线协议以及拓扑结构的一些限制，依旧无法做到更高的带宽。

![](images/04cluster_network.png)

**NVLink**

在大模型训练过程中，GPU 间通信的带宽通常在数百GB/s以上，PCIe总线的数据传输速率容易成为瓶颈，且GPU发出的信号需要先传递到PCIe Switch, PCIe Switch中涉及到数据的处理，CPU会对数据进行分发调度，这些都会引入额外的网络延迟，影响GPU并行计算的效率和性能。为此，此后，NVIDIA 提出了 NVLink 总线协议。

NVLink 是一种高速互连技术，旨在加快 CPU 与 GPU、GPU 与 GPU 之间的数据传输速度，提高系统性能。NVLink通过GPU之间的直接互联，可扩展服务器内的多GPU I/O，相较于传统PCIe总线可提供更高效、低延迟的互联解决方案。

![](images/05cluster_network.png)

NVLink的首个版本于2014年发布，首次引入了高速GPU互连。2016年发布的P100搭载了第一代NVLink，提供 160GB/s 的带宽，相当于当时 PCIe 3.0 x16 带宽（双向）的 5 倍。之后陆续发布了很多新版本，V100搭载的 NVLink2 将带宽提升到300GB/s ，A100搭载了NVLink3带宽为600GB/s。H100中包含18条第四代NVLink链路，总带宽（双向）达到 900 GB/s，是PCIe 5.0 x16带宽（双向）的7倍。日前Nvidia在2024GTC大会上发布了最新的Blackwell结构的B200芯片，搭载NVLink5链路可达到双向带宽1800GB/s。几代的NVLink性能对比如表格   

|                          | NVLink 5      | NVLink 4   | NVLink 3   | NVLink 2   | NVLink 1   |
| ------------------------ | ------------- | ---------- | ---------- | ---------- | ---------- |
| Signaling Rate           | 200 Gbps      | 100 Gbps   | 50 Gbps    | 25 Gbps    | 20 Gbps    |
| Lanes/Link               | 2             | 2          | 4          | 8          | 8          |
| Bandwidth/Direction/Link | 50 GB/sec     | 25 GB/sec  | 25 GB/sec  | 25 GB/sec  | 20 GB/sec  |
| Total Bandwidth/Link     | 100 GB/sec    | 50 GB/sec  | 50 GB/sec  | 50 GB/sec  | 40 GB/sec  |
| Links/Chip               | 18(Blackwell) | 18 (H100)  | 12 (A100)  | 6 (V100)   | 4 (P100)   |
| Bandwidth/Chip           | 1800 GB/sec   | 900 GB/sec | 600 GB/sec | 300 GB/sec | 160 GB/sec |

NVLink高速互联主要有两种：

第一种是以桥接器的形式实现。可通过桥接器将两张卡的nvlink接口连接在一起。

![](images/06Cluster_network.png)

另一种是在主板上集成 NVLink 接口。

![](images/07Cluster_network.png)

**NVSwitch**
为了解决GPU之间通讯不均衡问题，NVIDIA引入NVSwitch。NVSwitch芯片是一种类似交换机的物理芯片（ASIC），通过NVLink接口可以将多个GPU高速互联到一起，可创建无缝、高带宽的多节点GPU集群，实现所有GPU在一个具有全带宽连接的集群中协同工作，从而提升服务器内部多个GPU之间的通讯效率和带宽。NVLink和NVSwitch的结合使NVIDIA得以高效地将AI性能扩展到多个GPU。

![](images/08Cluster_network.png)

NVSwitch是英伟达公司提供的产品，目前不单独出售，只和英伟达自家的硬件设备打包一起卖，所以外界很少见过它，下图为Nvidia的DGX A100机器。

![](images/09Cluster_network.png)



### 机器间通信

**TCP/IP**

由于计算数量巨大，多机多卡的计算/训练已经是一个常态，多台机器间的通信是影响分布式训练行的一个重要指标。在传统的 TCP/IP 网络通信中，如下图所示，数据发送方需要将数据进行多次内存拷贝，并经过一系列的网络协议的数据包处理工作；数据接收方在应用程序中处理数据前，也需要经过多次内存拷贝和一系列的网络协议的数据包处理工作。经过这一系列的内存拷贝、数据包处理以及网络传输延时等，server 间的通信时延往往在毫秒级别，不能够满足多机多卡场景对于网络通信的高带宽、低延迟的需求。

当然在无RDMA网卡时，想要实现机器之间通信，也可以通过普通网卡进行TCP/IP通信，只是这样传输带宽将大大降低，延迟也会增加。

![](images/12Cluster_network.png)

**RDMA(Remote Direct Memory Access)**

RDMA是为了解决网络传输中数据处理延迟而产生的一种远端内存直接访问技术，主要有三点特性：

**CPU Offload**：无需CPU干预，应用程序可以访问远程主机内存而不消耗远程主机中的任何CPU。远程主机内存能够被读取而不需要远程主机上的进程（或CPU)参与。

**Kernel Bypass**：RDMA 提供一个专有的 Verbs interface 而不是传统的TCP/IP Socket interface。应用程序可以直接在用户态执行数据传输，不需要在内核态与用户态之间做上下文切换。

**Zero Copy**：每个应用程序都能直接访问集群中的设备的虚拟内存，这意味着应用程序能够直接执行数据传输，在不涉及到网络软件栈的情况下，数据能够被直接发送到缓冲区或者能够直接从缓冲区里接收，而不需要被复制到网络层。

![](images/10cluster_network.png)

目前有三种RDMA相关协议分别Infiniband、RoCE和iWARP。
**Infiniband（IB）**：专为 RDMA 设计的网络，从硬件层面保证可靠传输，同时由于这是一种新的网络技术，需要支持该技术的 NIC 和交换机，价格高昂。

**RoCE(RDMA over Converged Ethernet)**：RoCE 是一种允许通过以太网进行 RDMA 的网络协议。RoCE 有 2 个版本：RoCE v1 和 RoCE v2。RoCE v1 是一种链路层协议，允许在同一个广播域下的任意两台主机直接访问；RoCE v2 基于 UDP 层协议，实现了路由功能，RoCE v2 针对 RoCE v1 进行了一些改进，如引入 IP 解决扩展性问题，可以跨二层组网等。可以使用普通的以太网交换机，但是需要支持RoCE的网卡。

**iWARP（internet Wide Area RDMA Protocol）**：允许在 TCP 上执行 RDMA 的网络协议。在大型组网的情况下，iWARP 的大量 TCP 连接会占用大量的内存资源，对系统规格要求较高。可以使用普通的以太网交换机，但是需要支持iWARP的网卡。
介绍下硬件网卡信息

通常AI集群跨机通信，一般采用**GPUDirect RDMA**技术，实现高速网络传输。所谓GPUDirect RDMA，就是server1的GPU可以通过**同一 root complex** 下的RDMA网卡直接访问server2的GPU内存。而在没有这项技术之前，GPU需要先将数据从GPU内存搬移到系统内存，然后再利用RDMA传输到server2，server2的GPU还要做一次数据从系统内存到GPU内存的搬移动作。GPUDirect RDMA技术使得进一步减少了GPU通信的数据复制次数，通信延迟进一步降低。

![](images/11cluster_network.png)





## 软件篇
通信软件指用于分布式训练时，多个计算设备之间的集合通信。在分布式系统中，各个节点间往往存在大量的集合通信需求，而我们可以用消息传递接口 (Message Passing Interface，MPI，一套集合通信相关的接口标准) 来定义一些比较底层的消息通信行为。譬如 Reduce、AllReduce、Scatter、Gather、AllGather 等。

常见的集合通信库（如：Open MPI、Gloo、NCCL等）都在 MPI 的基础上，对各种集合通信的模式和算法作了各自的实现。

**MPI(Message Passing Interface)**: 定义了多个原语的消息传递接口，这一接口主要还用于多进程的通信。MPI系统通信方式是建立在点对点通信之上。而集合通讯是建立在端到端通信的基础上在一组进程内的通讯原语。对于目前大型的AI集群，一般利用MPI进行跨机器多进程通信。成熟的软件库有Open-MPI、Intel-MPI等等

**XCCL(X Collective Communication Library)**泛指一类用于集合通信的软件库，包含NCCL、HCCL、 BCCL和Byte CCL等，它们的功能都是类似，一般都是基于开源NCCL进行优化或者改造，使得适用于自研的AI芯片架构，实现设备之间的集合通信。这里主要介绍下NCCL库。NVIDIA 的 NCCL 提供编程抽象，通过高级拓扑检测、通用路径搜索和针对 NVIDIA 架构优化的算法，针对每个平台和拓扑进行高度调整。 NCCL API 从 CPU 启动，GPU 执行，在 GPU 内存之间移动或交换数据。最后利用 NVLink 聚合多个高速 NIC 的带宽，实现多设备多机器多集群之间的集合通信。

![](images/13Cluster_network.png)

**GLoo**是一个用于高性能集合通信的库，特别适用于分布式深度学习和大规模模型训练。它是 Facebook 开发的一个开源项目，旨在提供高效、可扩展的通信基础设施，以支持在分布式环境下进行模型训练和推理。

如下是三种常见的集合通信库在CPU和GPU设备支持集合原语的情况，不难看出，mpi主要用于cpu间的通信，NCCL主要用于GPU间的通信

![](images/14Cluster_network.png)

