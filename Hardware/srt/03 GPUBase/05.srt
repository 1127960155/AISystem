1
00:00:00,000 --> 00:00:06,560
哈喽大家好

2
00:00:06,600 --> 00:00:09,680
我是唯一终于有空过来更新的终米了

3
00:00:09,720 --> 00:00:13,240
现在我们还是在AI芯片里面的GPU详解

4
00:00:13,280 --> 00:00:16,760
这里面的GPU主要是特子英伟达的GPU

5
00:00:16,800 --> 00:00:18,200
其实已经给大家汇报了

6
00:00:18,200 --> 00:00:21,480
从非米到Word架构的具体的详细的内容

7
00:00:21,520 --> 00:00:23,920
今天我们来从Turing到Hub架构

8
00:00:23,960 --> 00:00:26,320
去看看具体有什么不一样

9
00:00:26,520 --> 00:00:29,760
今天我们主要去分开几个内容给大家介绍的

10
00:00:29,880 --> 00:00:31,480
第一个就是从图灵的架构

11
00:00:31,520 --> 00:00:33,320
安博的架构到Hope的架构

12
00:00:33,360 --> 00:00:35,200
主要讲解这三个架构

13
00:00:35,240 --> 00:00:39,720
这三个架构也是近5年来才慢慢的出现的

14
00:00:39,760 --> 00:00:43,200
而图灵架构更多的适用在游戏

15
00:00:43,880 --> 00:00:47,920
对于AI训练或者AI的从业者来说用的最多的

16
00:00:47,960 --> 00:00:50,520
包括现在很重要的有几个内容

17
00:00:50,520 --> 00:00:54,200
就是Tensor Core迎来了3.0就第三代了

18
00:00:54,240 --> 00:00:57,040
而NVLink也迎来了第三代了

19
00:00:57,080 --> 00:01:00,000
最后我至少我现在还来不到货的

20
00:01:00,400 --> 00:01:01,320
Hope的架构

21
00:01:01,800 --> 00:01:03,600
Hub架构是非常的惊艳

22
00:01:03,600 --> 00:01:06,720
除了用了4纳米的制程以外

23
00:01:06,760 --> 00:01:10,720
里面的晶体管对比起安博架构翻了三倍

24
00:01:10,760 --> 00:01:13,560
而整个的设计也是非常惊艳

25
00:01:13,560 --> 00:01:16,240
我们后面会给大家去介绍展开的

26
00:01:16,440 --> 00:01:17,560
回到我们今天内容

27
00:01:17,600 --> 00:01:19,960
今天我们主要是回顾一下Word架构

28
00:01:19,960 --> 00:01:23,200
Word架构是第一代的Tensor Core

29
00:01:23,320 --> 00:01:26,600
然后Tensor Core更好的支持了我们AI的运算

30
00:01:26,600 --> 00:01:30,280
后面每个架构都会对Tensor Core进行迭代

31
00:01:30,280 --> 00:01:31,200
到了Turing架构

32
00:01:31,200 --> 00:01:35,000
Turing架构更多的是一个RT Core光线追踪的核心

33
00:01:35,000 --> 00:01:38,120
所以我说它主要是用在游戏领域

34
00:01:38,120 --> 00:01:40,200
后来有了Ampere架构和Hope架构

35
00:01:40,200 --> 00:01:41,920
今天我们来详细的看看

36
00:01:41,960 --> 00:01:45,440
这4个架构或者后面三个架构有什么不一样

37
00:01:47,800 --> 00:01:50,800
黄元勋教授就发布了Turing架构

38
00:01:50,960 --> 00:01:53,320
这一代架构确实也是非常的惊艳

39
00:01:53,320 --> 00:01:56,640
我们看到里面有两个主要的更新

40
00:01:56,640 --> 00:01:58,840
第一个更新就是Tensor Core

41
00:01:58,840 --> 00:02:00,000
新一代的Tensor Core

42
00:02:00,000 --> 00:02:03,720
第二个更新就是引入了RT Core光线追踪

43
00:02:03,720 --> 00:02:06,280
而这里面这两代的架构很有意思的

44
00:02:06,280 --> 00:02:10,320
基本上都引入了到我们的消费级的显卡

45
00:02:10,320 --> 00:02:13,080
也就是GTX系列里面

46
00:02:13,320 --> 00:02:15,280
第一个内容就是我们刚才说到的

47
00:02:15,280 --> 00:02:17,320
Turing架构引入了Tensor Core

48
00:02:17,600 --> 00:02:21,880
Tensor Core新增了int8到int4的支持

49
00:02:21,920 --> 00:02:25,920
更好的深度学习的推理引擎进行加速

50
00:02:26,840 --> 00:02:28,760
RT就是RT Core了

51
00:02:28,760 --> 00:02:32,480
RT就是ray traced光线追踪的核心

52
00:02:32,480 --> 00:02:35,960
主要是用来做一些三角形和光线的求交

53
00:02:36,800 --> 00:02:38,760
那是在Bork之外的

54
00:02:38,760 --> 00:02:42,080
所以相对于PU,int32,fp32这种计算来说

55
00:02:42,400 --> 00:02:44,640
两者之间是异步的关系

56
00:02:45,560 --> 00:02:46,440
来看看RT Core

57
00:02:46,440 --> 00:02:49,760
RT Core分为左边的框框和右边的框框

58
00:02:49,840 --> 00:02:52,600
左边的框框一部分是用来做碰撞的检测

59
00:02:52,600 --> 00:02:55,560
另外一部分是真正的去求交面的

60
00:02:55,840 --> 00:02:57,200
下面我们来看一下

61
00:02:57,200 --> 00:03:00,920
有RT Core和没有RT Core的一个不同点

62
00:03:01,120 --> 00:03:02,880
上面就是没有开RT Core的

63
00:03:02,880 --> 00:03:04,400
下面就是开了RT Core

64
00:03:04,400 --> 00:03:08,400
可以看到这里面的人物的光线是更进一步的

65
00:03:08,400 --> 00:03:11,920
就看出来更加右边的没有RT Core的时候

66
00:03:12,040 --> 00:03:13,240
就没有做光线追踪

67
00:03:13,240 --> 00:03:15,760
我们这里有fire有火焰的时候

68
00:03:16,000 --> 00:03:18,440
其实在车上是没有反应出来的

69
00:03:18,480 --> 00:03:19,600
而开了RT Core之后

70
00:03:19,880 --> 00:03:23,760
整个光线的反照是变得非常之有意思的

71
00:03:23,760 --> 00:03:25,200
而非常之真实逼真

72
00:03:25,200 --> 00:03:29,640
而这里面连车头车灯这些反光全都出来

73
00:03:29,640 --> 00:03:32,760
这就是光线追踪非常有意思的一个话题点

74
00:03:32,760 --> 00:03:36,520
这个也是因为他最擅长的事情了

75
00:03:36,680 --> 00:03:37,680
下面我们看一看

76
00:03:37,680 --> 00:03:42,160
其实突然出现在RTX2090,3090到40X系列

77
00:03:42,160 --> 00:03:44,640
就是40系列到现在能够卖的

78
00:03:44,640 --> 00:03:47,040
整体来说我个人是非常喜欢的

79
00:03:47,040 --> 00:03:50,480
因为真正的把AI变成一个消费级的显卡

80
00:03:50,480 --> 00:03:53,440
让我们能下巴就没有更好的卡去用的时候

81
00:03:53,760 --> 00:03:55,960
我在家里还可以去玩一玩

82
00:03:56,640 --> 00:04:00,040
下面我们来到了AMP架构安倍架构

83
00:04:00,040 --> 00:04:02,560
安倍架构也是2020年推出的

84
00:04:02,560 --> 00:04:04,080
因为三年前的一个架构

85
00:04:04,080 --> 00:04:07,280
其实现在在市场上还是抢断货的

86
00:04:07,360 --> 00:04:09,720
一个安倍架构在AI领域

87
00:04:09,720 --> 00:04:11,800
还是非常精的做考验的

88
00:04:11,920 --> 00:04:15,000
现在我们看看安倍架构有哪些主要的特点

89
00:04:15,000 --> 00:04:20,480
首先第1个特点就是有超过541个晶体管所组成

90
00:04:20,480 --> 00:04:23,360
也就是当时候应该是2020年的时候

91
00:04:23,360 --> 00:04:26,840
世界上最大的7纳米的处理器

92
00:04:26,840 --> 00:04:29,280
就是我们的英伟达的A100

93
00:04:29,280 --> 00:04:32,840
而第2个就是新增了第3代的Tensor Core

94
00:04:32,840 --> 00:04:36,120
这里面包括新增了一个特别有意思的

95
00:04:36,720 --> 00:04:39,880
F30专门针对AI进行加速

96
00:04:40,120 --> 00:04:41,600
接着就我觉得很重要

97
00:04:41,600 --> 00:04:43,000
简单的列一列

98
00:04:43,240 --> 00:04:46,120
这个就MIG Multi Instance GPU

99
00:04:46,120 --> 00:04:47,120
多实力的GPU

100
00:04:47,120 --> 00:04:48,760
将单个的A100 GPU

101
00:04:48,840 --> 00:04:51,040
划分成为多个独立的GPU

102
00:04:51,040 --> 00:04:52,040
提供不同的用户

103
00:04:52,040 --> 00:04:53,400
来提供不同的算力

104
00:04:53,400 --> 00:04:55,040
这也是针对我觉得更多的

105
00:04:55,040 --> 00:04:59,320
是为云服务器厂商提供一种更好的算力切分的方案

106
00:04:59,640 --> 00:05:02,200
接着用了第3代的MV-Link和MV-Switch

107
00:05:02,200 --> 00:05:03,400
MV-Switch很有意思

108
00:05:03,400 --> 00:05:07,320
就把多台机器通过MV-Switch进行互联

109
00:05:07,320 --> 00:05:09,880
单卡之间也就单机多卡之间

110
00:05:09,960 --> 00:05:12,120
通过MV-Link进行互联

111
00:05:12,160 --> 00:05:15,280
最后还有一个明示就是西苏信的加速

112
00:05:15,280 --> 00:05:18,840
通过西苏信对AI的矩阵层进行加速

113
00:05:19,480 --> 00:05:22,000
下面我们看看整体的安倍架构

114
00:05:22,320 --> 00:05:24,280
右边就是安倍架构的架构图

115
00:05:24,280 --> 00:05:26,160
整体基于720年的时候

116
00:05:26,160 --> 00:05:29,280
720里面最大的一个管的一款芯片

117
00:05:29,280 --> 00:05:33,040
里面就有6912个CUDA的内核

118
00:05:33,040 --> 00:05:35,120
和4300个Tensor Core

119
00:05:35,120 --> 00:05:36,840
Tensor Core非常多

120
00:05:36,840 --> 00:05:40,000
所以A版是非常善于深度学习的内容了

121
00:05:40,000 --> 00:05:44,240
整体的晶体管数已经到了541个晶体管

122
00:05:44,240 --> 00:05:48,280
108个SM SBM非常多

123
00:05:48,280 --> 00:05:50,440
而采用了第3代的MV-Link

124
00:05:50,440 --> 00:05:51,880
我们看看MV-Link在哪里

125
00:05:51,880 --> 00:05:53,920
对MV-Link在下面

126
00:05:53,920 --> 00:05:55,720
第3代的MV-Link

127
00:05:55,720 --> 00:05:59,720
GPU和服务器之间的双向带宽是4.8TB每秒

128
00:05:59,720 --> 00:06:03,480
而GPU跟GPU之间是600GB每秒

129
00:06:03,480 --> 00:06:05,520
所以说他遇到很多友商

130
00:06:05,800 --> 00:06:07,520
他们在训练大模型的时候

131
00:06:07,840 --> 00:06:10,200
用的更多的都是NPIO

132
00:06:11,200 --> 00:06:14,240
里面很重要的就是Tensor Core的新一代

133
00:06:14,240 --> 00:06:16,160
引入了TF32 BF16

134
00:06:16,160 --> 00:06:18,640
还有FP64的支持

135
00:06:18,640 --> 00:06:23,440
在Tensor Core里面很重要的就是BF16和TF32

136
00:06:23,440 --> 00:06:24,560
这两个很有意思

137
00:06:24,560 --> 00:06:28,120
我们平时用的更多的是FP32和FP16

138
00:06:28,120 --> 00:06:30,000
FP32有8个

139
00:06:30,000 --> 00:06:32,320
FP16这只所谓有5个

140
00:06:32,320 --> 00:06:34,400
整体来说FP32的位宽

141
00:06:34,640 --> 00:06:37,840
也就是它的range会比FP16更多

142
00:06:37,840 --> 00:06:40,080
而精度数位小数位决定精度

143
00:06:40,360 --> 00:06:42,600
小数位在FP32有32个

144
00:06:42,600 --> 00:06:43,760
而整体来说

145
00:06:43,760 --> 00:06:45,240
FP16只有10个

146
00:06:45,240 --> 00:06:46,880
这个时候我们在训练AI的时候

147
00:06:46,880 --> 00:06:49,440
其实发现FP16很多时候是够用的

148
00:06:49,440 --> 00:06:50,880
但是遇到部分情况下

149
00:06:51,000 --> 00:06:53,360
动态范围其实表示的不是很大

150
00:06:53,360 --> 00:06:55,840
于是就推出了TF32

151
00:06:55,840 --> 00:06:59,680
TF32就是子数位保持跟FP32相同

152
00:06:59,680 --> 00:07:02,480
而小数位也就是我们后面的小数位

153
00:07:02,480 --> 00:07:04,320
跟FP16相同

154
00:07:04,320 --> 00:07:06,600
后来又出现了BF16

155
00:07:06,600 --> 00:07:08,840
也就是从我们安排架构去引入的

156
00:07:08,840 --> 00:07:10,600
BF16用的子数位

157
00:07:10,600 --> 00:07:12,840
其实跟我们的FP32

158
00:07:12,840 --> 00:07:14,520
还有TF32相同的

159
00:07:14,520 --> 00:07:17,560
但是小数位小了三位

160
00:07:17,560 --> 00:07:20,440
把三位让给了我们的range

161
00:07:20,440 --> 00:07:22,960
这个也是放进了很多传言说

162
00:07:22,960 --> 00:07:26,240
FP16在模型的时候不够用

163
00:07:26,240 --> 00:07:27,840
更多的是用BF16

164
00:07:27,840 --> 00:07:30,040
其实我在训练大模型的时候

165
00:07:30,400 --> 00:07:32,840
用的很多FP16是够用的

166
00:07:32,840 --> 00:07:36,160
如果可以肯定是用TF32更好

167
00:07:36,160 --> 00:07:38,200
但是其实发现用FP16

168
00:07:38,200 --> 00:07:39,880
在训练的LAMA

169
00:07:39,880 --> 00:07:40,680
还有GPT-3

170
00:07:40,960 --> 00:07:43,440
是没有遇到精度不收敛的问题

171
00:07:43,440 --> 00:07:46,120
或许我的大模型还没训练完

172
00:07:47,400 --> 00:07:49,160
下面我们看看安培架构的

173
00:07:49,160 --> 00:07:50,960
一个稀疏化的情况

174
00:07:51,120 --> 00:07:53,880
后面右面图就是安培架构的

175
00:07:53,880 --> 00:07:56,600
一个细腻度的稀疏化情况

176
00:07:56,600 --> 00:07:57,560
或者Tensor Core

177
00:07:57,640 --> 00:07:59,520
除了执行层架的操作以外

178
00:07:59,680 --> 00:08:00,720
它还可以支持

179
00:08:00,720 --> 00:08:03,240
稀疏的结构化的矩阵

180
00:08:04,080 --> 00:08:05,760
现在有一个稠密的矩阵

181
00:08:05,760 --> 00:08:06,400
稠密的矩阵

182
00:08:06,520 --> 00:08:08,160
是在训练的时候得到的

183
00:08:08,160 --> 00:08:09,520
但是我真正定义的时候

184
00:08:09,680 --> 00:08:11,480
做了一个简单的减资

185
00:08:11,680 --> 00:08:12,960
减资它是有比例的

186
00:08:12,960 --> 00:08:13,720
减资完之后

187
00:08:13,800 --> 00:08:15,440
我会做一个factorial的减资

188
00:08:15,440 --> 00:08:17,760
然后得到一个稀疏的矩阵

189
00:08:17,760 --> 00:08:18,920
或者稀疏的权重

190
00:08:18,920 --> 00:08:20,560
接着在英伟达架构里面

191
00:08:20,680 --> 00:08:23,120
就会对矩阵进行压缩

192
00:08:23,120 --> 00:08:24,800
变成一个稠密的矩阵

193
00:08:24,800 --> 00:08:25,760
稠密的矩阵之后

194
00:08:25,760 --> 00:08:26,880
有一个很有意思的点

195
00:08:26,880 --> 00:08:29,240
就是除了矩阵的数据之外

196
00:08:29,240 --> 00:08:31,040
它还有一个indexed

197
00:08:31,040 --> 00:08:33,400
所以那些我们压缩过的数据

198
00:08:33,720 --> 00:08:35,440
进行检索记录

199
00:08:35,440 --> 00:08:37,360
最后进行一个矩阵层

200
00:08:37,360 --> 00:08:38,480
这个是activation

201
00:08:38,480 --> 00:08:39,240
矩阵层之后

202
00:08:39,360 --> 00:08:41,120
得到我们的output的activation

203
00:08:41,120 --> 00:08:43,320
整体的那就是右边的图

204
00:08:43,320 --> 00:08:44,040
而打开

205
00:08:44,040 --> 00:08:46,440
在我们来到了安培架构里面

206
00:08:46,440 --> 00:08:48,280
比较后期的一些内容了

207
00:08:48,280 --> 00:08:48,960
可以看到了

208
00:08:48,960 --> 00:08:51,520
这里面是多实力分割

209
00:08:51,520 --> 00:08:53,120
也就是我们提到的MIG

210
00:08:53,120 --> 00:08:55,480
每个100可以分为7个

211
00:08:55,480 --> 00:08:56,880
不同的GPU实力

212
00:08:56,880 --> 00:08:58,840
被不同的任务所执行的

213
00:08:58,840 --> 00:08:59,800
所以我们可以看到

214
00:08:59,800 --> 00:09:01,960
有很多不同的user

215
00:09:01,960 --> 00:09:03,480
分为7个实力

216
00:09:03,480 --> 00:09:04,240
这些用户

217
00:09:04,400 --> 00:09:06,080
可以将这些虚拟化的GPU

218
00:09:06,080 --> 00:09:08,200
当成实际的GPU去执行

219
00:09:08,200 --> 00:09:10,200
所以说觉得A100架构

220
00:09:10,400 --> 00:09:12,760
就是为很多云计算的厂商

221
00:09:12,760 --> 00:09:14,120
提供了算力切分

222
00:09:14,120 --> 00:09:16,520
和多用户的租赁的任务

223
00:09:16,520 --> 00:09:18,520
这个也是安培架构

224
00:09:18,520 --> 00:09:20,040
被更多的人去用到

225
00:09:20,040 --> 00:09:22,000
因为大家去租用服务器厂商

226
00:09:22,000 --> 00:09:23,200
或者用云的时候

227
00:09:23,600 --> 00:09:25,720
用的很多的安培架构

228
00:09:25,720 --> 00:09:27,400
或者A100的服务器

229
00:09:28,400 --> 00:09:30,480
那就整个英伟达的

230
00:09:30,480 --> 00:09:33,120
A100的整体的硬件的规格

231
00:09:33,360 --> 00:09:35,040
放开硬件左边的硬件规格

232
00:09:35,040 --> 00:09:37,120
我们看看右边的图

233
00:09:37,120 --> 00:09:38,200
很有意思的

234
00:09:38,200 --> 00:09:40,320
就是现在它整块

235
00:09:40,640 --> 00:09:42,880
上面的这些上面这一坨

236
00:09:42,880 --> 00:09:44,440
都是散热板

237
00:09:44,440 --> 00:09:45,720
而真正的A100

238
00:09:45,840 --> 00:09:47,840
是我们下面的贴片

239
00:09:47,840 --> 00:09:49,120
下面的芯片

240
00:09:49,120 --> 00:09:52,200
这里面不再是通过PCIE插进去

241
00:09:52,200 --> 00:09:55,360
而是直接焊在我们的主板上面

242
00:09:55,360 --> 00:09:58,440
英伟达卖是卖整一个节点

243
00:09:58,440 --> 00:10:02,600
一个节点就有8个A100的芯片了

244
00:10:03,600 --> 00:10:04,280
大模型的时候

245
00:10:04,400 --> 00:10:06,560
大家会感受的特别深刻

246
00:10:06,560 --> 00:10:07,960
我在同一个节点里面

247
00:10:08,120 --> 00:10:09,600
进行模型并行的时候

248
00:10:09,840 --> 00:10:10,920
是非常方便的

249
00:10:10,920 --> 00:10:11,920
但是跨节点

250
00:10:11,920 --> 00:10:13,320
跨机器之间训练大模型

251
00:10:13,640 --> 00:10:14,640
我们的带宽

252
00:10:14,640 --> 00:10:17,440
就会成为整个网络制约的瓶颈

253
00:10:17,440 --> 00:10:19,640
或者整个大模型训练的瓶颈

254
00:10:21,000 --> 00:10:23,000
里面我发现有个很有意思的点

255
00:10:23,000 --> 00:10:24,920
就是整一个整机

256
00:10:25,040 --> 00:10:27,800
它的内存高达1TB

257
00:10:28,000 --> 00:10:30,840
或者2TB是非常多

258
00:10:30,840 --> 00:10:31,560
我很多时候

259
00:10:31,720 --> 00:10:34,640
直接把数据全部都加载在CPU里面

260
00:10:34,640 --> 00:10:37,320
然后再不断的回传到我们的GPU里面

261
00:10:37,320 --> 00:10:39,000
这样可以很好的去加速

262
00:10:39,000 --> 00:10:40,400
我们大模型的训练

263
00:10:41,680 --> 00:10:44,240
接下来就是英伟达

264
00:10:44,240 --> 00:10:46,760
2022年发布的hopper架构

265
00:10:47,000 --> 00:10:49,320
hopper它不是一个漏斗的意思

266
00:10:49,320 --> 00:10:51,120
因为它英文叫hopper

267
00:10:51,120 --> 00:10:53,520
实际上hopper是耶鲁大学的第一位

268
00:10:53,560 --> 00:10:56,880
李博士是一个杰出的计算机学家

269
00:10:57,560 --> 00:10:58,920
原发明字母

270
00:10:59,120 --> 00:11:01,600
现在我们看看hopper架构有什么不一样

271
00:11:01,840 --> 00:11:04,360
hopper架构非常的惊艳

272
00:11:04,360 --> 00:11:04,920
聪明

273
00:11:04,920 --> 00:11:06,240
因为整个hopper架构

274
00:11:06,400 --> 00:11:09,760
它除了提出了整个hopper的GPU以外

275
00:11:10,040 --> 00:11:12,880
它还提出了一个graced CPU

276
00:11:13,440 --> 00:11:16,280
它叫做graced hopper superchip

277
00:11:16,400 --> 00:11:19,280
一个将英伟达的hopper的GPU的突破性

278
00:11:19,280 --> 00:11:22,560
跟英伟达的graced CPU连在一起

279
00:11:22,640 --> 00:11:23,840
在单个芯片里面

280
00:11:24,080 --> 00:11:27,480
CPU跟GPU之间通过NVLink进行连接

281
00:11:27,480 --> 00:11:31,360
GPU跟GPU之间也是通过NVLink进行连接

282
00:11:31,360 --> 00:11:34,440
而夸机之间通过PCIe5进行连接

283
00:11:34,440 --> 00:11:36,520
可以看到CPU跟GPU之间

284
00:11:36,520 --> 00:11:38,640
以前是通过PCIe进行连接的

285
00:11:38,640 --> 00:11:41,560
现在直接通过NVLink进行传输

286
00:11:41,560 --> 00:11:44,600
数据的传输速率高达900GB每秒

287
00:11:44,600 --> 00:11:48,400
而GPU跟GPU之间传输速率也高达900GB每秒

288
00:11:48,400 --> 00:11:49,440
这个进行工作

289
00:11:49,600 --> 00:11:51,880
使得GPU跟CPU之间的数据传输

290
00:11:51,880 --> 00:11:54,480
实验和搬运不再是问题

291
00:11:54,480 --> 00:11:55,960
变成了一个C to C

292
00:11:55,960 --> 00:11:58,520
也就是chip to chip互联

293
00:11:58,520 --> 00:12:00,880
所以总理觉得是不是所有用户都用到

294
00:12:01,040 --> 00:12:01,920
我觉得不一定

295
00:12:02,240 --> 00:12:05,960
因为训练大模型的用户确实没有那么多

296
00:12:05,960 --> 00:12:07,160
没有传说中的那么多

297
00:12:07,160 --> 00:12:10,760
更多的人是用大模型进行一个下游任务的微调

298
00:12:10,760 --> 00:12:12,760
然后识别到它具体的任务

299
00:12:13,000 --> 00:12:16,320
右边这个就是具体的graced hopper superchip

300
00:12:16,320 --> 00:12:18,320
也就是这一款的渲染图

301
00:12:19,000 --> 00:12:20,520
继续的展开整个hopper架构

302
00:12:20,840 --> 00:12:22,280
有哪些不一样的点

303
00:12:22,520 --> 00:12:25,120
这里面周敏就总结了4个点

304
00:12:25,120 --> 00:12:26,080
首先第一句话

305
00:12:26,880 --> 00:12:29,440
它是真正异构的一个加速平台

306
00:12:29,440 --> 00:12:32,000
适用于高性能计算机和AI的工作负载

307
00:12:32,320 --> 00:12:35,640
我觉得我更关心的是整个AI的工作负载

308
00:12:35,640 --> 00:12:38,720
里面还非常重要的4个提出新的点

309
00:12:38,720 --> 00:12:40,520
就是graced CPU

310
00:12:40,520 --> 00:12:42,800
这是非常基于2021年的时候

311
00:12:42,920 --> 00:12:43,680
想收购ARM的

312
00:12:43,680 --> 00:12:44,960
但是没有收购成功

313
00:12:44,960 --> 00:12:46,360
这是另外一个故事了

314
00:12:46,360 --> 00:12:49,920
但是英伟达基于ARM提出了自己的graced CPU

315
00:12:50,000 --> 00:12:52,200
另外我觉得更关心的是hopper GPU

316
00:12:52,200 --> 00:12:53,800
也就是我们今天的主角

317
00:12:53,800 --> 00:12:56,520
这里面就更新了第4代的tensor core

318
00:12:56,520 --> 00:12:59,080
里面就引出了transformer engine

319
00:12:59,360 --> 00:13:02,880
然后transformer engine专门针对我们的大模型进行加速的

320
00:13:02,880 --> 00:13:03,600
另外的话

321
00:13:03,600 --> 00:13:07,200
GPU里面的内存已经高达了300GB每秒的速度

322
00:13:07,200 --> 00:13:09,200
然后缓存还有的提升

323
00:13:09,200 --> 00:13:10,240
而整体来说

324
00:13:10,440 --> 00:13:12,800
它的互联是envlink的c2c

325
00:13:12,800 --> 00:13:16,920
把CPU跟GPU之间通过envlink进行连接

326
00:13:16,920 --> 00:13:19,160
而envlink是看看整体的hopper架构

327
00:13:19,160 --> 00:13:22,560
hopper架构这里面有8组GPC

328
00:13:22,560 --> 00:13:24,120
也就是我们图像处理处

329
00:13:24,120 --> 00:13:27,480
然后一共有66组TCP

330
00:13:27,480 --> 00:13:30,400
TCP里面又有132组RSM

331
00:13:30,400 --> 00:13:34,280
总共有16896个CUDA的核心

332
00:13:34,280 --> 00:13:35,880
528个tensor core

333
00:13:35,880 --> 00:13:37,280
之前在A100里面

334
00:13:37,400 --> 00:13:39,120
其实它的tensor core只有400多个

335
00:13:39,120 --> 00:13:40,880
现在已经多到500多个了

336
00:13:40,880 --> 00:13:44,440
而在里面出现了50MB的二级缓存

337
00:13:44,440 --> 00:13:47,320
整个显存是新一代的HBM3容量了

338
00:13:47,320 --> 00:13:48,600
跟A100相同

339
00:13:48,600 --> 00:13:49,760
保持在80G

340
00:13:49,760 --> 00:13:52,280
位宽是5120bit的带宽

341
00:13:52,280 --> 00:13:54,240
高达3TB每秒

342
00:13:54,240 --> 00:13:56,840
整体的性能非常的夸张

343
00:13:56,840 --> 00:13:57,840
可以用猛说来

344
00:13:57,840 --> 00:14:00,320
hopper架构其实一点都不夸张

345
00:14:00,880 --> 00:14:03,120
现在我们打开看看

346
00:14:03,120 --> 00:14:05,320
hopper架构里面的SM

347
00:14:05,320 --> 00:14:06,960
SM里面其实

348
00:14:06,960 --> 00:14:07,960
总体来说

349
00:14:07,960 --> 00:14:09,480
我们看一下跟A100比

350
00:14:09,480 --> 00:14:11,520
后面我们会有A100的一些参数

351
00:14:11,520 --> 00:14:12,360
那4个web schedule

352
00:14:12,360 --> 00:14:14,080
我们可以看到一个SM里面

353
00:14:14,240 --> 00:14:15,720
有4个web schedule

354
00:14:15,720 --> 00:14:18,200
1 2 3 4

355
00:14:18,200 --> 00:14:19,080
4个web schedule

356
00:14:19,080 --> 00:14:20,880
然后4个dispatch unit

357
00:14:20,880 --> 00:14:22,680
就是对我们的指令进行分发

358
00:14:22,680 --> 00:14:24,680
我们在应该上上节课里面

359
00:14:24,680 --> 00:14:25,880
去给大家讲过

360
00:14:25,880 --> 00:14:28,040
web schedule和dispatch unit

361
00:14:28,040 --> 00:14:29,040
有什么不一样

362
00:14:29,040 --> 00:14:30,000
接着我们看一下

363
00:14:30,000 --> 00:14:31,640
里面的essential的core

364
00:14:31,640 --> 00:14:32,840
就这里面就变成

365
00:14:32,840 --> 00:14:33,320
intessential

366
00:14:33,320 --> 00:14:35,720
essential和essential的core

367
00:14:35,720 --> 00:14:37,160
而不再是基本上

368
00:14:37,160 --> 00:14:37,800
essential

369
00:14:37,800 --> 00:14:39,640
intessential都是翻了一倍

370
00:14:39,640 --> 00:14:42,480
而里面有4个tensor core

371
00:14:43,480 --> 00:14:46,360
这一点就是下面这条绿色的

372
00:14:46,360 --> 00:14:47,200
相比A100

373
00:14:47,200 --> 00:14:49,440
A100多了一个tensor memory

374
00:14:49,440 --> 00:14:50,520
selerator

375
00:14:50,520 --> 00:14:52,440
专门针对张量

376
00:14:52,480 --> 00:14:55,000
进行数据的传输的

377
00:14:55,000 --> 00:14:56,440
所以说以前我们的张量

378
00:14:56,440 --> 00:14:58,680
都是放在L2或者L1的cache

379
00:14:58,680 --> 00:14:59,440
会更多的

380
00:14:59,440 --> 00:15:00,480
可能有些数据

381
00:15:00,640 --> 00:15:01,720
放在为just file

382
00:15:01,720 --> 00:15:02,840
就我们的继承器

383
00:15:02,840 --> 00:15:04,600
现在有了张量memory

384
00:15:04,600 --> 00:15:05,160
selerator

385
00:15:05,440 --> 00:15:07,120
更好的对我们的大矩阵

386
00:15:07,120 --> 00:15:08,720
大模型进行加速的

387
00:15:09,360 --> 00:15:10,040
往下看

388
00:15:10,200 --> 00:15:11,960
这个就是英伟达官方的一个图

389
00:15:11,960 --> 00:15:12,680
我简单看看

390
00:15:12,680 --> 00:15:14,920
其实去买到一个A100的

391
00:15:14,920 --> 00:15:16,400
单款的异构的芯片

392
00:15:16,400 --> 00:15:17,720
当然我们也可买回来

393
00:15:17,720 --> 00:15:19,320
同时插在我们的插槽里面

394
00:15:19,320 --> 00:15:20,600
更多的刚才讲到的

395
00:15:20,600 --> 00:15:21,600
很多的收发区

396
00:15:21,600 --> 00:15:22,280
或者NVLink

397
00:15:22,600 --> 00:15:23,840
是通过整机

398
00:15:23,840 --> 00:15:25,280
或者整个节点去卖的

399
00:15:25,280 --> 00:15:27,080
同时英伟达还提出了

400
00:15:27,080 --> 00:15:29,080
非常之变态的售卖方式

401
00:15:29,080 --> 00:15:30,600
就直接卖一台port

402
00:15:30,720 --> 00:15:31,560
把网

403
00:15:33,200 --> 00:15:34,560
都给你组好了

404
00:15:34,560 --> 00:15:36,240
整一台port卖给你

405
00:15:36,240 --> 00:15:37,560
这是多么的变态

406
00:15:37,840 --> 00:15:39,680
这收钱收的真的是手软

407
00:15:39,680 --> 00:15:41,280
怪不得他的市值能这么高

408
00:15:41,280 --> 00:15:41,720
好了

409
00:15:41,720 --> 00:15:43,640
今天的内容就到这里为止

410
00:15:43,640 --> 00:15:44,920
我们简单的总结

411
00:15:44,920 --> 00:15:45,760
或者回顾一下

412
00:15:45,760 --> 00:15:47,520
GPU的架构的发展

413
00:15:47,520 --> 00:15:48,960
每一代GPU的架构

414
00:15:49,080 --> 00:15:51,200
都是以科学家进行命名的

415
00:15:51,200 --> 00:15:53,360
从2010年的菲米架构开始

416
00:15:53,360 --> 00:15:56,280
它是作为一个GPU的计算架构提出

417
00:15:56,280 --> 00:15:58,000
接着到了vote架构

418
00:15:58,000 --> 00:15:59,160
或者巴斯卡架构里面

419
00:15:59,440 --> 00:16:02,000
提出了第一代的NVLink双向的互联

420
00:16:02,840 --> 00:16:03,120
好的

421
00:16:03,120 --> 00:16:05,480
在HPC场景里面去发挥它的作用

422
00:16:05,480 --> 00:16:07,320
接着应该是2017年的时候

423
00:16:07,320 --> 00:16:08,240
AI非常火

424
00:16:08,240 --> 00:16:10,440
于是顺势的推出了vote架构

425
00:16:10,520 --> 00:16:12,720
提出了第一代的Tensor Core

426
00:16:12,720 --> 00:16:15,240
而后来应该是在2018年的时候

427
00:16:15,560 --> 00:16:18,480
在消费级的显卡里面提出了RT Core

428
00:16:18,480 --> 00:16:21,440
实现了光线追踪都非常的惊艳

429
00:16:21,440 --> 00:16:24,000
然后在服务用厂商里面安排架构

430
00:16:24,000 --> 00:16:25,200
是卖的最好的

431
00:16:25,200 --> 00:16:27,400
因为这里面除了有Tensor Vlink

432
00:16:27,400 --> 00:16:28,800
还有RT Core的更新之外

433
00:16:29,000 --> 00:16:30,880
还提出了MIG

434
00:16:31,640 --> 00:16:32,320
U实例

435
00:16:32,320 --> 00:16:34,240
最后到最近的应该是

436
00:16:34,240 --> 00:16:36,560
现在你还买不到货的hopper架构

437
00:16:36,560 --> 00:16:37,920
也就是我们赫博架构

438
00:16:37,920 --> 00:16:39,680
里面的进行出了CPU跟GPU

439
00:16:39,680 --> 00:16:41,600
一个架构变成一个superchip

440
00:16:41,600 --> 00:16:42,920
去对外去销售

441
00:16:42,920 --> 00:16:45,400
当然你也可以买一个简单的GPU

442
00:16:45,400 --> 00:16:46,760
或者简单的H100

443
00:16:46,760 --> 00:16:49,480
它的销售方式就变得非常多了

444
00:16:49,480 --> 00:16:52,080
或者它的解决方案就变得非常的多

445
00:16:52,080 --> 00:16:54,000
那今天的内容就到这里为止

446
00:16:54,000 --> 00:16:54,680
谢谢各位

447
00:16:54,680 --> 00:16:55,600
拜了个拜

448
00:16:56,920 --> 00:16:58,600
卷的不行了

449
00:16:58,600 --> 00:17:00,000
记得一键三连加关注

450
00:17:00,400 --> 00:17:01,760
所有的内容都会开源

451
00:17:01,760 --> 00:17:03,600
在下面这条链接里面

452
00:17:03,960 --> 00:17:04,920
拜拜

