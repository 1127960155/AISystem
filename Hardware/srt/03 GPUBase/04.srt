1
00:00:00,000 --> 00:00:07,000
大家好,一直偷庚的中米又回来了

2
00:00:07,000 --> 00:00:09,000
今天我们还是在GPU详解

3
00:00:09,000 --> 00:00:14,800
我们看看英伟达GPU的不同年代不同型号的架构有什么区别

4
00:00:14,800 --> 00:00:17,800
今天主要给大家去分享的一个主要的内容

5
00:00:17,800 --> 00:00:20,800
就是从费米到帕斯卡架构

6
00:00:20,800 --> 00:00:24,200
对英伟达GPU的基础概念不太了解的话

7
00:00:24,200 --> 00:00:27,400
可以看回我们上一节对大家汇报和分享

8
00:00:27,400 --> 00:00:30,800
今天我们主要我们从2010年的费米架构开始

9
00:00:30,800 --> 00:00:35,200
去看看英伟达GPU每一代的架构有什么不一样

10
00:00:35,200 --> 00:00:39,800
实际上在这一节里面我给大家去汇报的内容

11
00:00:39,800 --> 00:00:40,800
主要分开几个

12
00:00:40,800 --> 00:00:42,800
第一个是总体概览一下

13
00:00:42,800 --> 00:00:45,800
GPU历史架构的总体情况

14
00:00:45,800 --> 00:00:47,400
接着我们看看费米架构

15
00:00:47,400 --> 00:00:49,600
然后了解一下开普勒架构

16
00:00:49,600 --> 00:00:52,800
最后我们Maxwell 麦克思维架构

17
00:00:52,800 --> 00:00:56,200
在结束之前我们看一个非常重要的内容

18
00:00:56,200 --> 00:00:59,600
就是帕斯卡架构也是开启了

19
00:00:59,600 --> 00:01:03,000
英伟达的一个AI星辰之旅

20
00:01:06,200 --> 00:01:11,000
下面我们来看看英伟达GPU的架构发展的总体情况

21
00:01:11,000 --> 00:01:12,200
几个比较重要的年份

22
00:01:12,200 --> 00:01:16,000
就是2010年的费米架构的提出

23
00:01:16,600 --> 00:01:19,800
GPU架构演进的十几年

24
00:01:19,800 --> 00:01:22,600
然后里面拥有比较重要的历史突破

25
00:01:22,600 --> 00:01:24,600
或者历史性意义的里程碑的架构

26
00:01:25,000 --> 00:01:30,200
就是从2016年的帕斯卡架构开始

27
00:01:30,800 --> 00:01:33,200
或者深度学习有比较重要历史意义的

28
00:01:33,400 --> 00:01:35,800
就是我们的伏特架构

29
00:01:35,800 --> 00:01:39,800
然后针对产品对AI产品卖的最好的

30
00:01:39,800 --> 00:01:42,800
就是这里面的安倍架构

31
00:01:42,800 --> 00:01:45,400
这里面是一个整体的情况

32
00:01:45,400 --> 00:01:48,000
我们今天主要是聚焦于费米开普勒

33
00:01:48,000 --> 00:01:49,400
Maxwell 还有帕斯卡

34
00:01:49,400 --> 00:01:52,400
还有沃特架构这5个主要的类

35
00:01:52,400 --> 00:01:54,000
有几个历史性的意义

36
00:01:54,000 --> 00:01:57,200
就是费米架构是首个完整的

37
00:01:57,200 --> 00:01:59,200
GPU的计算的架构

38
00:01:59,200 --> 00:02:01,600
里面就提出了非常多新的概念

39
00:02:01,600 --> 00:02:04,400
这些新的概念很多都沿用到至今

40
00:02:04,400 --> 00:02:06,600
而Maxwell里面比较重要的一个提出

41
00:02:06,600 --> 00:02:09,200
就是NVLink提出了第一代NVLink

42
00:02:09,200 --> 00:02:14,600
当时候双向的互联贷款是160GB每秒

43
00:02:14,600 --> 00:02:17,400
而沃特架构最重要的一个提出

44
00:02:17,400 --> 00:02:18,800
就是Tensor Core

45
00:02:18,800 --> 00:02:22,200
专门针对神经网络矩阵卷机

46
00:02:22,200 --> 00:02:25,000
进行加速的第一代核心

47
00:02:27,400 --> 00:02:29,200
现在我们来看看第一个重要的内容

48
00:02:29,200 --> 00:02:30,600
就是费米架构

49
00:02:30,600 --> 00:02:33,000
费米是英伟达费米的一个架构图

50
00:02:33,000 --> 00:02:34,600
最大可以支持16个SM

51
00:02:34,600 --> 00:02:37,400
每个SM有32个CUDA核

52
00:02:38,200 --> 00:02:40,400
架构开始取消了SP这个概念

53
00:02:40,400 --> 00:02:42,200
引用了CUDA Core的概念

54
00:02:42,200 --> 00:02:45,400
在整体里面一共有512个CUDA Core

55
00:02:45,400 --> 00:02:48,200
也就是16个SM乘以32个CUDA Core

56
00:02:48,400 --> 00:02:50,800
就一共得到我们512个

57
00:02:51,400 --> 00:02:53,800
而每个CUDA的核心就是实际的计算单元

58
00:02:54,600 --> 00:02:57,200
架构当时还是以游戏作为主打

59
00:02:57,200 --> 00:02:59,200
所以这里面会有多个GPC

60
00:02:59,200 --> 00:03:01,600
也就是我们刚刚提到的图形图像处理处

61
00:03:01,600 --> 00:03:03,800
在每个图形图像处理处里面

62
00:03:03,800 --> 00:03:05,600
又有一个光栅引擎

63
00:03:05,600 --> 00:03:07,200
也就是我们的Wester Engine

64
00:03:08,400 --> 00:03:09,000
太多了

65
00:03:09,000 --> 00:03:10,800
所以这里面的L2 Cache

66
00:03:10,800 --> 00:03:12,000
会放在中间

67
00:03:12,000 --> 00:03:13,000
给到上面

68
00:03:13,000 --> 00:03:14,400
快速的传输到上面的CUDA Core

69
00:03:14,400 --> 00:03:16,000
还有下面的CUDA Core

70
00:03:16,800 --> 00:03:18,800
而费米架构的SM右边

71
00:03:18,800 --> 00:03:20,400
这个就是整体的SM

72
00:03:20,400 --> 00:03:22,400
每个SM有32个CUDA Core

73
00:03:22,400 --> 00:03:24,000
每排有8个

74
00:03:24,000 --> 00:03:25,200
一共有两排

75
00:03:25,200 --> 00:03:26,600
这里面SFU

76
00:03:26,600 --> 00:03:29,000
就是我们之前提到的特殊处理单元

77
00:03:29,000 --> 00:03:29,800
可以看到

78
00:03:29,800 --> 00:03:31,800
算的都是发生在我们的CUDA Core里面

79
00:03:31,800 --> 00:03:33,400
进行处理的

80
00:03:33,400 --> 00:03:35,200
而打开每一个CUDA Core

81
00:03:35,200 --> 00:03:36,800
我们可以看到中间这个图

82
00:03:37,000 --> 00:03:38,600
首先我们的指令或者现场

83
00:03:38,800 --> 00:03:41,000
会给到CUDA Core里面去真正的执行

84
00:03:41,000 --> 00:03:42,000
而这里面执行

85
00:03:42,000 --> 00:03:43,800
我们可以同时Punit

86
00:03:43,800 --> 00:03:44,800
这里面具体的

87
00:03:44,800 --> 00:03:46,600
是指FPS去执行

88
00:03:46,600 --> 00:03:48,800
也可以选择Int8去执行

89
00:03:48,800 --> 00:03:50,000
但这里面的执行

90
00:03:50,200 --> 00:03:51,400
不能是并行的

91
00:03:51,400 --> 00:03:52,400
它在CUDA Core里面

92
00:03:52,600 --> 00:03:53,200
只能说

93
00:03:53,200 --> 00:03:54,200
Unit去执行

94
00:03:54,200 --> 00:03:56,200
或者选择Unit去执行

95
00:03:57,200 --> 00:03:59,000
硬件肯定会有对应的软件

96
00:03:59,000 --> 00:04:00,400
而英伟达里面对应的软件

97
00:04:00,400 --> 00:04:01,400
就是CUDA

98
00:04:01,400 --> 00:04:02,800
CUDA里面就分层

99
00:04:02,800 --> 00:04:04,000
之前提到了分层

100
00:04:04,000 --> 00:04:06,200
线程Block线程快

101
00:04:06,200 --> 00:04:07,400
还有Grid网格

102
00:04:07,400 --> 00:04:08,400
三个层次

103
00:04:08,400 --> 00:04:09,000
每个层次

104
00:04:09,200 --> 00:04:10,800
这都有对应不同的硬件

105
00:04:10,800 --> 00:04:11,400
例如Flat

106
00:04:11,600 --> 00:04:14,200
它可以共享局部的Memory

107
00:04:14,200 --> 00:04:16,600
而Block用的是Shared Memory

108
00:04:16,600 --> 00:04:17,600
共享内存

109
00:04:17,600 --> 00:04:19,200
最后到我们的Grid也是

110
00:04:19,200 --> 00:04:19,800
SM之间

111
00:04:20,000 --> 00:04:23,000
可以共享我们的全局内存

112
00:04:25,200 --> 00:04:26,600
在两年之后

113
00:04:26,600 --> 00:04:27,600
2012年的时候

114
00:04:27,800 --> 00:04:30,000
英伟达就发布了下一代的架构

115
00:04:30,000 --> 00:04:31,000
叫做Kepler

116
00:04:31,000 --> 00:04:32,400
Kepler架构

117
00:04:32,600 --> 00:04:33,600
Kepler架构里面

118
00:04:33,800 --> 00:04:35,000
SM改变了

119
00:04:35,000 --> 00:04:37,400
这里面SM就变成了SM-ext

120
00:04:37,400 --> 00:04:38,800
但是所代表的意义

121
00:04:38,800 --> 00:04:40,000
其实没有太多的变化

122
00:04:40,000 --> 00:04:41,400
它还是Streaming

123
00:04:42,000 --> 00:04:43,000
的意义

124
00:04:43,000 --> 00:04:44,600
在整个Kepler架构里面

125
00:04:44,800 --> 00:04:46,200
硬件上面最直接的

126
00:04:46,200 --> 00:04:49,200
就是拥有了双精度的运算单元

127
00:04:49,200 --> 00:04:51,200
也就是从Kepler架构开始

128
00:04:51,200 --> 00:04:52,400
英伟达GPU

129
00:04:52,400 --> 00:04:55,600
慢慢的进入到了HPC这个领域

130
00:04:55,600 --> 00:04:58,000
就是我们的高性能机这个领域

131
00:04:58,000 --> 00:05:00,000
例如我国就有泰国之光

132
00:05:00,200 --> 00:05:01,000
天和这种

133
00:05:01,000 --> 00:05:04,000
就属于我们高性能计算机里面的领域

134
00:05:04,000 --> 00:05:05,600
而现在基本上

135
00:05:05,600 --> 00:05:07,400
每80的高性能计算机集群

136
00:05:07,600 --> 00:05:10,400
都会带有GPU进行加速

137
00:05:10,400 --> 00:05:12,400
也是因为这里面拥有了

138
00:05:12,400 --> 00:05:13,800
双精度浮点运算

139
00:05:14,400 --> 00:05:15,000
它的改进

140
00:05:15,200 --> 00:05:17,000
就是在Kepler架构里面

141
00:05:17,200 --> 00:05:19,000
就提出了GPU Direct

142
00:05:19,000 --> 00:05:20,600
这个GPU数据处理

143
00:05:20,800 --> 00:05:23,800
需要多次经过CPU的内存拷贝

144
00:05:24,000 --> 00:05:25,800
为了降低仿存的延迟

145
00:05:25,800 --> 00:05:27,400
还有数据重新搬运的问题

146
00:05:27,400 --> 00:05:29,200
就出现了GPU Direct

147
00:05:29,200 --> 00:05:31,600
通过GPU或者我们的host里面的

148
00:05:31,600 --> 00:05:32,400
System Memory

149
00:05:32,400 --> 00:05:34,800
就是必须里面的内存进行交互

150
00:05:34,800 --> 00:05:37,600
而是直接通过GPU跟GPU之间

151
00:05:37,600 --> 00:05:39,200
进行直接的数据交互

152
00:05:39,200 --> 00:05:40,200
进一步的提升了

153
00:05:40,200 --> 00:05:42,600
我们数据的处理和数据的带宽

154
00:05:43,400 --> 00:05:44,800
回顾一下非米架构里面

155
00:05:44,800 --> 00:05:47,200
SM一共有32个

156
00:05:47,200 --> 00:05:48,800
在Kepler架构里面

157
00:05:48,800 --> 00:05:52,200
SM就高达了192个

158
00:05:52,200 --> 00:05:55,200
整体的核心多了非常多

159
00:05:55,200 --> 00:05:56,600
它的并行处理的能力

160
00:05:56,600 --> 00:05:58,400
也是多了很多很多

161
00:06:00,200 --> 00:06:01,000
过了两年之后

162
00:06:01,200 --> 00:06:02,600
也就是2014年

163
00:06:02,600 --> 00:06:03,600
英伟达又提出了

164
00:06:03,600 --> 00:06:04,800
下一代的GPU架构

165
00:06:04,800 --> 00:06:07,400
Maxwell 麦克思维架构

166
00:06:07,400 --> 00:06:08,600
那在麦克思维架构

167
00:06:08,600 --> 00:06:10,400
其实没有太多的变化

168
00:06:11,200 --> 00:06:12,600
在里面具体的看一下

169
00:06:12,600 --> 00:06:13,600
在非米架构里面

170
00:06:13,600 --> 00:06:15,600
SM的核心有32个

171
00:06:15,600 --> 00:06:16,600
在Kepler架构里面

172
00:06:16,600 --> 00:06:18,200
SM的核心有192个

173
00:06:18,200 --> 00:06:19,800
而在Maxwell架构里面

174
00:06:19,800 --> 00:06:21,600
SMX这个名字

175
00:06:21,800 --> 00:06:24,600
又回归到SM这个名字里面了

176
00:06:24,600 --> 00:06:26,000
而整体的核心棍数

177
00:06:26,000 --> 00:06:27,800
就变回了128个

178
00:06:27,800 --> 00:06:29,200
甚至英伟达其实发现了

179
00:06:29,200 --> 00:06:30,800
核心数它没必要太多

180
00:06:30,800 --> 00:06:31,800
但是我们的线程数

181
00:06:32,200 --> 00:06:33,800
可以超配可以更多

182
00:06:34,600 --> 00:06:35,800
到了2016年

183
00:06:35,800 --> 00:06:37,800
我们来到了PASCO架构

184
00:06:39,000 --> 00:06:41,600
那是法国的物理学家和数学家

185
00:06:41,600 --> 00:06:43,200
里面的学到的

186
00:06:43,800 --> 00:06:45,200
大解释以PASCO

187
00:06:46,200 --> 00:06:47,800
整体的里面的SM

188
00:06:47,800 --> 00:06:49,400
进行了一个精简

189
00:06:50,200 --> 00:06:51,400
里面包含的内容

190
00:06:51,600 --> 00:06:52,600
也是越来越少

191
00:06:52,600 --> 00:06:53,600
但是整体上

192
00:06:53,800 --> 00:06:55,800
因为大米的制程提升了

193
00:06:55,800 --> 00:06:57,600
所以它里面包含的SM

194
00:06:57,600 --> 00:06:58,600
也会增加

195
00:06:58,600 --> 00:07:01,400
对比起打开PASCO架构里面的

196
00:07:01,400 --> 00:07:02,000
SM

197
00:07:02,800 --> 00:07:04,400
里面就有64个

198
00:07:04,400 --> 00:07:06,000
IP32的CUDA core

199
00:07:06,000 --> 00:07:08,200
这里面CUDA core非常非常的多

200
00:07:08,200 --> 00:07:09,800
相比起MAXWELL里面的

201
00:07:09,800 --> 00:07:11,000
128个CUDA core

202
00:07:11,000 --> 00:07:13,200
还有KIP192个

203
00:07:13,800 --> 00:07:15,000
确实TAGO里面的

204
00:07:15,000 --> 00:07:16,400
CUDA core少了很多

205
00:07:16,400 --> 00:07:18,600
而且这里面分开两个区域

206
00:07:18,600 --> 00:07:20,800
一个是左边的区域

207
00:07:20,800 --> 00:07:22,800
一个是右边的区域

208
00:07:22,800 --> 00:07:25,800
每个区域有32个CUDA core

209
00:07:26,800 --> 00:07:28,200
我们再往下看看

210
00:07:28,200 --> 00:07:30,600
为什么要区分成两块

211
00:07:30,800 --> 00:07:32,000
是因为这里面看到

212
00:07:32,000 --> 00:07:32,800
with just file

213
00:07:32,800 --> 00:07:34,200
就是寄存器的

214
00:07:34,200 --> 00:07:35,200
这样的保持不变

215
00:07:35,200 --> 00:07:36,200
分开两个

216
00:07:36,200 --> 00:07:37,200
这样的话就可以保证

217
00:07:37,200 --> 00:07:38,000
我们的CUDA core

218
00:07:38,000 --> 00:07:39,600
每个执行具体的线程

219
00:07:39,600 --> 00:07:41,600
可以使用更多的寄存器

220
00:07:41,600 --> 00:07:43,200
而SM单个SM

221
00:07:43,200 --> 00:07:44,600
可以并发的去执行

222
00:07:44,600 --> 00:07:45,600
更多的线程

223
00:07:45,600 --> 00:07:46,200
更多的web

224
00:07:46,200 --> 00:07:47,200
更多的block

225
00:07:47,800 --> 00:07:49,200
然后英伟达GPU的

226
00:07:49,200 --> 00:07:50,600
并行处理能力

227
00:07:52,000 --> 00:07:53,000
PASCO架构里面

228
00:07:53,000 --> 00:07:54,400
非常重要的一点

229
00:07:54,400 --> 00:07:55,800
就是右边的图

230
00:07:55,800 --> 00:07:57,400
里面就提出了

231
00:07:57,400 --> 00:07:59,600
第一代的NV LINK

232
00:07:59,600 --> 00:08:02,800
带宽高达了160GB每秒

233
00:08:02,800 --> 00:08:06,200
相当于当时PCIe里面的三倍

234
00:08:07,400 --> 00:08:08,200
单个节点

235
00:08:08,200 --> 00:08:10,600
也就是单台服务器里面的GPU

236
00:08:10,600 --> 00:08:12,800
可以进行数据的互联

237
00:08:13,600 --> 00:08:15,200
减少了数据传输的延迟

238
00:08:15,200 --> 00:08:16,600
也减少了我们数据之间的

239
00:08:16,600 --> 00:08:18,000
大量的通过PCIe

240
00:08:18,000 --> 00:08:19,800
回传到CPU的内存里面

241
00:08:19,800 --> 00:08:22,000
进行重复的搬运性的工作

242
00:08:22,000 --> 00:08:24,800
实现了整个网络的拓扑互联

243
00:08:24,800 --> 00:08:27,800
这个建树也是非常非常的重要

244
00:08:27,800 --> 00:08:29,400
很多现在训练大模型的

245
00:08:29,400 --> 00:08:31,600
过程当中就发现了

246
00:08:31,600 --> 00:08:32,800
那会成为我们整个

247
00:08:32,800 --> 00:08:34,400
分布式训练系统里面的

248
00:08:34,400 --> 00:08:35,400
主要的瓶颈

249
00:08:38,800 --> 00:08:39,600
一年之后

250
00:08:39,600 --> 00:08:41,200
也就是在2017年

251
00:08:41,200 --> 00:08:43,400
英伟达又提出了新一代的架构

252
00:08:43,400 --> 00:08:45,000
叫做Voltage

253
00:08:45,000 --> 00:08:46,600
叫做Voltage架构

254
00:08:46,600 --> 00:08:48,000
Voltage是

255
00:08:48,000 --> 00:08:49,600
第一个发现用化学的方式

256
00:08:49,600 --> 00:08:51,200
产生电流原理的

257
00:08:51,200 --> 00:08:52,600
意大利科学家

258
00:08:52,600 --> 00:08:55,000
福特进行命名的

259
00:08:55,000 --> 00:08:55,800
而这一代架构

260
00:08:55,800 --> 00:08:57,600
也叫做福特架构

261
00:08:57,600 --> 00:08:59,000
在整体福特架构里面

262
00:08:59,000 --> 00:09:00,600
就引用了非常非常多的

263
00:09:00,600 --> 00:09:01,600
新鲜的玩意

264
00:09:01,600 --> 00:09:02,400
那首先

265
00:09:02,400 --> 00:09:04,600
里面将CUDA Core进行拆分

266
00:09:04,600 --> 00:09:07,000
把FPU和ALU拆分

267
00:09:07,000 --> 00:09:08,200
取消了CUDA Core

268
00:09:08,200 --> 00:09:09,400
就是以后就没有

269
00:09:09,400 --> 00:09:10,200
CUDA Core

270
00:09:10,200 --> 00:09:12,200
这个整体的硬件的概念

271
00:09:12,200 --> 00:09:14,000
但是CUDA Core这个软件的概念

272
00:09:14,000 --> 00:09:15,400
都可以保留下来

273
00:09:15,400 --> 00:09:16,200
而一条指令

274
00:09:16,200 --> 00:09:17,400
就实现我们一条指令

275
00:09:17,400 --> 00:09:20,000
可以同时执行不同的计算

276
00:09:20,000 --> 00:09:20,800
而第二点

277
00:09:20,800 --> 00:09:22,200
就是提出了独立的

278
00:09:22,200 --> 00:09:23,200
线程的调度

279
00:09:23,200 --> 00:09:24,800
改进了整个SIMT的模型

280
00:09:24,800 --> 00:09:26,200
也就是我们单指令

281
00:09:26,200 --> 00:09:28,000
多线程的架构

282
00:09:28,000 --> 00:09:29,000
使得每个线程

283
00:09:29,000 --> 00:09:30,600
都有自己的独立的

284
00:09:30,600 --> 00:09:31,600
Programming Counter

285
00:09:31,600 --> 00:09:33,000
就是我们的独立的PC

286
00:09:33,000 --> 00:09:34,000
还有自己的Ethic

287
00:09:34,000 --> 00:09:35,400
自己的Zen

288
00:09:35,400 --> 00:09:36,600
第三个就是

289
00:09:36,600 --> 00:09:38,600
针对AI提出来的

290
00:09:38,600 --> 00:09:41,400
Tensor Core第一代的张亮核心

291
00:09:41,400 --> 00:09:42,800
针对我们深度学习

292
00:09:42,800 --> 00:09:44,800
提供了专门针对卷集

293
00:09:44,800 --> 00:09:45,800
或者矩阵层

294
00:09:45,800 --> 00:09:48,000
进行计算加速

295
00:09:48,000 --> 00:09:49,000
还有其他黑科技

296
00:09:49,000 --> 00:09:50,600
例如MV-Link的第二代

297
00:09:50,600 --> 00:09:53,600
还有提出了MPS这个概念

298
00:09:53,600 --> 00:09:55,000
也就是更好地

299
00:09:55,000 --> 00:09:57,000
去适配到云厂商里面

300
00:09:57,000 --> 00:09:58,600
进行多用户的租赁

301
00:09:58,600 --> 00:09:59,600
或者多用户的处理

302
00:09:59,600 --> 00:10:01,400
和多用户的排队

303
00:10:01,400 --> 00:10:02,600
接下来我们看看

304
00:10:02,600 --> 00:10:05,200
Vote架构里面具体的SM

305
00:10:05,200 --> 00:10:08,000
首先这里面的SM的内容

306
00:10:08,000 --> 00:10:10,000
就比之前多了非常多

307
00:10:10,000 --> 00:10:12,000
因为把CUDA Core拆分出来了

308
00:10:12,000 --> 00:10:13,000
这里面就有了

309
00:10:13,000 --> 00:10:14,400
就有4个Web Schedule

310
00:10:14,400 --> 00:10:16,000
1234

311
00:10:16,000 --> 00:10:17,800
刚好上下两个

312
00:10:17,800 --> 00:10:20,000
里面有64个IP32的

313
00:10:20,000 --> 00:10:22,000
和每一组有16个

314
00:10:22,000 --> 00:10:24,000
64个IP32的和

315
00:10:24,000 --> 00:10:26,000
每一组也是共有16个

316
00:10:26,000 --> 00:10:27,400
然后整体64

317
00:10:27,400 --> 00:10:30,000
一共有32个IP64的和

318
00:10:30,000 --> 00:10:32,400
也就是每一组有8个

319
00:10:32,400 --> 00:10:33,800
当然了这里面很重要的

320
00:10:33,800 --> 00:10:36,000
就是Tensor Core的提出

321
00:10:36,000 --> 00:10:37,800
Tensor Core里面的共有8个

322
00:10:37,800 --> 00:10:39,000
当然还有其他的

323
00:10:39,000 --> 00:10:40,400
32个LDST Unit

324
00:10:40,400 --> 00:10:42,000
还有4个SFU

325
00:10:42,000 --> 00:10:45,600
就是我们特殊的数据处理单元

326
00:10:45,600 --> 00:10:48,000
把IP32和INT两组

327
00:10:48,000 --> 00:10:49,200
运车单元独立出来

328
00:10:49,200 --> 00:10:51,200
出现在我们整个流水线里面

329
00:10:51,400 --> 00:10:53,600
就使得SM可以同时

330
00:10:53,600 --> 00:10:54,800
支流同时执行INT

331
00:10:54,800 --> 00:10:56,400
同时执行IP32

332
00:10:56,400 --> 00:10:57,600
还有Tensor Core

333
00:10:57,600 --> 00:11:00,200
使得吞吐进一步的增加

334
00:11:00,200 --> 00:11:02,600
每个cycle都可以同时执行

335
00:11:02,600 --> 00:11:04,400
上面的这些指令

336
00:11:04,400 --> 00:11:06,000
也就是每个十字路由器

337
00:11:06,000 --> 00:11:08,000
可以执行的数量更多了

338
00:11:08,000 --> 00:11:10,600
可以执行我们的计算量更大了

339
00:11:10,600 --> 00:11:11,200
那在

340
00:11:12,400 --> 00:11:13,800
我们打开Water架构里面

341
00:11:13,800 --> 00:11:16,200
最重要的一个单元Tensor Core

342
00:11:17,000 --> 00:11:19,300
计算里面最常见的是

343
00:11:19,300 --> 00:11:21,900
卷积或者矩阵乘的方式

344
00:11:21,900 --> 00:11:24,300
其实在之前的GPU里面

345
00:11:24,300 --> 00:11:26,300
我们需要编码成FMA

346
00:11:26,300 --> 00:11:27,700
也就是FuseNet

347
00:11:27,700 --> 00:11:29,700
也就是把运算合并起来

348
00:11:29,700 --> 00:11:30,300
那这个时候

349
00:11:30,300 --> 00:11:31,700
就其实硬件层面

350
00:11:31,700 --> 00:11:33,500
需要把数据来回的搬运

351
00:11:33,500 --> 00:11:35,300
从寄存器搬到ALU

352
00:11:35,300 --> 00:11:36,300
执行一个乘法

353
00:11:36,300 --> 00:11:38,500
然后再从寄存器搬到ALU

354
00:11:38,500 --> 00:11:39,700
执行一个加法

355
00:11:39,700 --> 00:11:41,700
然后再放回我们的寄存器里面

356
00:11:41,700 --> 00:11:43,900
整个数据是来回的搬运的

357
00:11:43,900 --> 00:11:45,500
那现在一个Tensor Core

358
00:11:45,500 --> 00:11:46,300
一个指令

359
00:11:46,300 --> 00:11:49,300
就可以执行4×4×4的一个GMM

360
00:11:49,300 --> 00:11:51,500
也就是64个FMA

361
00:11:51,500 --> 00:11:54,700
极大地减少了系统内存的开销

362
00:11:54,700 --> 00:11:55,700
硬件的开销

363
00:11:56,700 --> 00:11:58,500
分数是fp16

364
00:11:58,500 --> 00:12:00,900
但是输出可以是fp32

365
00:12:00,900 --> 00:12:01,900
相当于就提供了

366
00:12:01,900 --> 00:12:04,300
64个fp32的ALU的计算能力

367
00:12:04,300 --> 00:12:07,100
能耗上也是非常的具有优势

368
00:12:07,100 --> 00:12:08,100
一个时钟周期内

369
00:12:08,100 --> 00:12:10,900
可以执行更多的矩阵的运算了

370
00:12:10,900 --> 00:12:13,100
而在整个福特架构的硬件里面

371
00:12:13,100 --> 00:12:13,900
我们可以看到

372
00:12:13,900 --> 00:12:14,900
后来卖的

373
00:12:14,900 --> 00:12:18,300
就不仅仅是我们经常理解到的一张卡了

374
00:12:18,300 --> 00:12:20,900
而是卖的一个DGX的Station

375
00:12:20,900 --> 00:12:22,100
一个工作站

376
00:12:22,100 --> 00:12:23,300
每个工作站上面

377
00:12:23,300 --> 00:12:24,700
就可以贴合8块

378
00:12:24,700 --> 00:12:25,900
或者4块

379
00:12:25,900 --> 00:12:27,700
Volted架构的芯片

380
00:12:29,300 --> 00:12:30,900
而在整机一块芯片

381
00:12:30,900 --> 00:12:32,700
就是通过这种方式

382
00:12:32,700 --> 00:12:34,900
贴合在我们整个机柜上面

383
00:12:34,900 --> 00:12:37,100
而整个机柜上面也可以累加起来

384
00:12:37,100 --> 00:12:39,500
后面有非常多的风口

385
00:12:39,500 --> 00:12:40,100
而这里面

386
00:12:40,100 --> 00:12:44,700
又有很多CPU贴合在我们整个机器上面

387
00:12:44,700 --> 00:12:46,700
这个就是具体的形态

388
00:12:46,700 --> 00:12:47,300
好了

389
00:12:47,300 --> 00:12:50,500
今天的内容就到这里为止

390
00:12:50,500 --> 00:12:53,300
我们回顾了英伟达从2010年

391
00:12:53,300 --> 00:12:55,500
到2017年里面的

392
00:12:55,500 --> 00:12:57,500
主要的GPU架构的发展

393
00:12:57,500 --> 00:12:59,300
从菲米发展到Volt

394
00:12:59,300 --> 00:13:00,700
而这里面有架构

395
00:13:00,700 --> 00:13:02,100
经历了7年

396
00:13:02,100 --> 00:13:03,500
而这7年里面

397
00:13:03,500 --> 00:13:05,100
中米觉得最重要的几个概念

398
00:13:05,100 --> 00:13:06,900
就是从菲米架构提出了

399
00:13:06,900 --> 00:13:09,100
首个完整的GPU计算架构

400
00:13:09,100 --> 00:13:11,300
接着到2016年的Pascal

401
00:13:11,300 --> 00:13:12,900
提出了第一代的NVLink

402
00:13:12,900 --> 00:13:16,900
到2017年提出的第一代的Tensor Core

403
00:13:16,900 --> 00:13:18,700
非常重要的一些概念

404
00:13:18,700 --> 00:13:20,900
也就是因为从2016年的

405
00:13:20,900 --> 00:13:22,500
NVLink和Tensor Core开始

406
00:13:22,500 --> 00:13:23,500
英伟达开发

407
00:13:23,500 --> 00:13:26,700
在AI的道路上越走越强

408
00:13:26,700 --> 00:13:28,100
内容就到这里为止

409
00:13:28,100 --> 00:13:28,900
谢谢各位

410
00:13:28,900 --> 00:13:30,700
拜了个拜

