1
00:00:00,000 --> 00:00:07,120
大家好,磨羊回首,肚子依然在抖

2
00:00:07,120 --> 00:00:09,280
我是终年发福的终米

3
00:00:09,280 --> 00:00:17,320
今天我们还是在整个AI芯片系列里面的GPU详解

4
00:00:17,320 --> 00:00:20,440
去看看GPU里面的AI编程的本质

5
00:00:20,440 --> 00:00:23,760
说白了英伟达的GPU它整体架构

6
00:00:23,760 --> 00:00:27,640
其实在近10年来已经发展了非常多代

7
00:00:27,640 --> 00:00:29,560
但是我们有一个最大的问题

8
00:00:29,560 --> 00:00:32,960
也就是为什么GPU适用于整个AI的计算

9
00:00:32,960 --> 00:00:35,280
或者换一个方式来去问

10
00:00:35,280 --> 00:00:38,160
为什么AI的训练我们要使用GPU

11
00:00:38,160 --> 00:00:39,920
而不是使用CPU呢

12
00:00:39,920 --> 00:00:43,520
这个就是我们今天要给大家去汇报的一个内容

13
00:00:43,520 --> 00:00:45,720
看看GPU的AI编程的本质

14
00:00:45,720 --> 00:00:50,240
首先今天我们会分开4个内容给大家去汇报

15
00:00:50,240 --> 00:00:52,040
首先第一个就是我们来看看

16
00:00:52,040 --> 00:00:55,640
回顾一下卷机的计算到底是怎么卷的

17
00:00:55,640 --> 00:00:56,640
怎么算的

18
00:00:56,640 --> 00:01:01,080
接着我们看一看就是来到了GPU的线程分歧

19
00:01:01,080 --> 00:01:03,120
那第三个就是回顾一下

20
00:01:03,120 --> 00:01:06,120
整个AI的计算模式和线程之间的关系

21
00:01:06,120 --> 00:01:08,520
把前面两个内容结合起来

22
00:01:08,520 --> 00:01:10,120
最后我们来看内容就是

23
00:01:10,120 --> 00:01:12,920
GPU既然能够对AI进行编程

24
00:01:12,920 --> 00:01:14,400
对AI进行加速

25
00:01:14,400 --> 00:01:17,040
那我们看看AI里面最重要的一个运算

26
00:01:17,040 --> 00:01:19,920
就是矩阵层这个运算GMM

27
00:01:19,920 --> 00:01:22,200
怎么去提升我们的算力利用率

28
00:01:22,200 --> 00:01:24,200
或者提升我们的算法利用率的

29
00:01:25,200 --> 00:01:27,680
正式进入到第一个小内容里面

30
00:01:27,680 --> 00:01:29,480
我们回顾一下在之前的章节里面

31
00:01:29,480 --> 00:01:31,080
我们给大家去汇报的

32
00:01:31,080 --> 00:01:33,280
就是GPU的线程的机制

33
00:01:33,280 --> 00:01:37,040
那左边的这个就是A100的一个简单的架构图

34
00:01:37,040 --> 00:01:39,000
下面有分层的一些cache

35
00:01:39,000 --> 00:01:40,840
或者分层的缓存

36
00:01:40,840 --> 00:01:44,560
针对每一个SM里面的具体的计算单元

37
00:01:44,560 --> 00:01:47,920
A100的SM里面就有非常多的web

38
00:01:47,920 --> 00:01:50,400
那这些web就是我们的线程处

39
00:01:50,400 --> 00:01:52,680
通过不同的SM里面有大量的web

40
00:01:52,680 --> 00:01:56,720
而每个web可以同时去并行执行多个线程

41
00:01:56,720 --> 00:01:59,640
这里面就有非常多的寄存器

42
00:01:59,640 --> 00:02:01,680
还有现成的调度器

43
00:02:01,680 --> 00:02:04,680
整体提供大量的线程和多级的缓存

44
00:02:04,680 --> 00:02:07,840
使得GPU整体的吞吐率变得非常的高

45
00:02:07,840 --> 00:02:10,400
有了大量的非常夸张并行的任务

46
00:02:10,400 --> 00:02:11,600
那了解到这点之后

47
00:02:11,720 --> 00:02:14,240
我们就知道GPU的本质

48
00:02:14,840 --> 00:02:15,560
行

49
00:02:16,200 --> 00:02:18,280
是希望尽可能的去加快

50
00:02:18,280 --> 00:02:20,520
我们每一个指令的运算

51
00:02:21,080 --> 00:02:23,320
我们之间的最大的一个不同

52
00:02:23,320 --> 00:02:24,480
下面我们来到

53
00:02:24,680 --> 00:02:26,520
在之前推的引擎系列里面

54
00:02:26,520 --> 00:02:29,440
其实钟明老师已经给大家去汇报过

55
00:02:29,440 --> 00:02:32,280
一些卷机计算的详细的内容

56
00:02:32,280 --> 00:02:34,200
今天我们简单的去回顾一下

57
00:02:34,200 --> 00:02:36,000
卷机是怎么卷的

58
00:02:36,000 --> 00:02:38,480
首先我们看一下下面的这个图

59
00:02:38,480 --> 00:02:40,440
就是我们的卷这一块内容

60
00:02:40,680 --> 00:02:43,840
就是一些卷机和我们的模板

61
00:02:43,840 --> 00:02:45,120
或者我们的kernel

62
00:02:45,120 --> 00:02:47,400
右边这一个就是我们的图片

63
00:02:47,400 --> 00:02:50,040
需要处理需要带卷机的图片

64
00:02:50,040 --> 00:02:51,520
最后模板里面的内容

65
00:02:51,720 --> 00:02:54,280
就是我们的卷机和每一个元素

66
00:02:54,280 --> 00:02:56,040
跟图片里面的每一个元素

67
00:02:56,200 --> 00:02:58,080
主元素相层再相加

68
00:02:58,080 --> 00:03:00,640
就得到我们最终的一个输出的

69
00:03:00,640 --> 00:03:01,320
feature map

70
00:03:01,320 --> 00:03:02,600
或者我们的特征图

71
00:03:02,600 --> 00:03:03,480
这个就是卷机

72
00:03:03,480 --> 00:03:05,240
我们的NPUGPUCPU里面

73
00:03:05,440 --> 00:03:07,680
真正去执行卷机运算的时候

74
00:03:07,840 --> 00:03:10,040
不会通过刚才那种划窗的方式

75
00:03:10,160 --> 00:03:11,240
对我们的图片

76
00:03:11,240 --> 00:03:13,080
对我们的数据进行卷机

77
00:03:13,080 --> 00:03:15,920
而是把卷机变成image to clone

78
00:03:15,920 --> 00:03:19,040
把我们的图片变成一个矩阵的向量

79
00:03:19,200 --> 00:03:22,000
这个时候就恢复我们的卷机的运算

80
00:03:22,200 --> 00:03:23,560
这里面我们图片

81
00:03:23,560 --> 00:03:25,600
我们会把它进行一个重排

82
00:03:25,600 --> 00:03:27,120
把图片里面的一个窗口

83
00:03:27,120 --> 00:03:28,400
12345679

84
00:03:28,400 --> 00:03:29,760
往后排成一排

85
00:03:29,760 --> 00:03:31,960
然后再按kernel数往后排

86
00:03:31,960 --> 00:03:32,920
通过这种方式

87
00:03:33,080 --> 00:03:34,440
对我们的图像

88
00:03:34,440 --> 00:03:36,400
进行一个矩阵的重排

89
00:03:36,520 --> 00:03:39,240
接着我们有第二把我们的frict

90
00:03:39,240 --> 00:03:41,480
把卷机盒进行重排

91
00:03:41,640 --> 00:03:42,440
卷机盒的重排

92
00:03:42,440 --> 00:03:43,440
其实跟刚才一样

93
00:03:43,440 --> 00:03:45,960
把每一个卷机和123456789

94
00:03:45,960 --> 00:03:48,920
然后重新的展开成为一行

95
00:03:48,920 --> 00:03:51,640
接着不断堆叠我们的圈量

96
00:03:51,640 --> 00:03:54,680
然后每一列就是代表有多少个n

97
00:03:54,680 --> 00:03:56,400
我们有多少个卷机盒

98
00:03:56,400 --> 00:03:57,200
通过这种方式

99
00:03:57,400 --> 00:03:58,600
就把我们的图片

100
00:03:58,600 --> 00:03:59,040
feature map

101
00:03:59,040 --> 00:03:59,960
把我们的kernel

102
00:04:00,120 --> 00:04:02,240
变成了两个大的矩阵

103
00:04:02,240 --> 00:04:04,040
变成两个大的相乘的方式

104
00:04:04,040 --> 00:04:06,200
得到我们最终的feature map

105
00:04:06,200 --> 00:04:07,080
或者我们的特征

106
00:04:07,080 --> 00:04:09,400
去模拟我们的卷机的运算

107
00:04:10,240 --> 00:04:11,080
一句话说明

108
00:04:11,200 --> 00:04:12,480
就是卷机的运算

109
00:04:12,480 --> 00:04:15,160
我们转换成为两个矩阵相乘

110
00:04:15,400 --> 00:04:16,760
的一个求解

111
00:04:16,760 --> 00:04:18,760
最终得到卷机的输出

112
00:04:18,960 --> 00:04:19,960
上面这个卷机

113
00:04:20,080 --> 00:04:22,120
原来这个是输的图片

114
00:04:22,120 --> 00:04:23,920
这个右边的绿色

115
00:04:23,920 --> 00:04:25,360
我们的卷机盒

116
00:04:25,360 --> 00:04:26,680
通过两个相乘

117
00:04:26,680 --> 00:04:27,640
或者滑窗的方式

118
00:04:27,800 --> 00:04:29,160
得到我们的feature map

119
00:04:29,160 --> 00:04:30,880
实际上我们在运行的时候

120
00:04:31,120 --> 00:04:32,840
会把卷片进行展开

121
00:04:32,840 --> 00:04:34,360
把卷机盒进行展开

122
00:04:34,360 --> 00:04:36,040
然后通过矩阵相乘的方式

123
00:04:36,040 --> 00:04:38,040
得到我们足clump to image

124
00:04:38,040 --> 00:04:39,360
逆变的过程

125
00:04:39,360 --> 00:04:41,320
恢复成为我们的feature map

126
00:04:41,320 --> 00:04:43,120
这个就是我或者NPU里面

127
00:04:43,120 --> 00:04:46,280
真正执行卷机的计算求解的方式

128
00:04:46,960 --> 00:04:48,560
卷机的运算之后

129
00:04:48,680 --> 00:04:49,320
我们就知道

130
00:04:49,320 --> 00:04:50,640
AI的计算的本质

131
00:04:50,640 --> 00:04:52,080
其实就是矩阵层

132
00:04:52,080 --> 00:04:53,160
那接着我们看看

133
00:04:53,160 --> 00:04:55,080
GPU的现成的分级

134
00:04:55,080 --> 00:04:56,960
现成的等级制度

135
00:04:56,960 --> 00:05:00,360
首先我们在整个AI的计算模式里面

136
00:05:00,360 --> 00:05:01,600
并不是所有的计算

137
00:05:01,600 --> 00:05:03,040
都会现成独立的

138
00:05:03,040 --> 00:05:05,240
我简单的举几个例子

139
00:05:05,440 --> 00:05:06,280
首先是

140
00:05:06,560 --> 00:05:08,040
就是足元数相加

141
00:05:08,040 --> 00:05:10,320
或足元数相乘的这种操作

142
00:05:10,320 --> 00:05:11,360
我们之前的例子

143
00:05:11,360 --> 00:05:13,560
ax加y简单的线性操作

144
00:05:13,840 --> 00:05:16,560
其实是足元进行一个简单的处理

145
00:05:16,560 --> 00:05:18,000
得到另外一个元数

146
00:05:18,000 --> 00:05:19,920
但是在真正卷机

147
00:05:19,920 --> 00:05:21,680
或者真正AI运算过程当中

148
00:05:21,800 --> 00:05:23,160
会有大量的卷机

149
00:05:23,160 --> 00:05:24,000
大量的卷机

150
00:05:24,120 --> 00:05:26,680
就涉及到我们的元数之间

151
00:05:26,680 --> 00:05:27,880
其实是有交互的

152
00:05:27,880 --> 00:05:30,040
我们的数据之间是有交互的

153
00:05:30,040 --> 00:05:31,440
我要算一个元数

154
00:05:31,440 --> 00:05:33,320
可能就需要周边的其他元数

155
00:05:33,320 --> 00:05:35,320
周边的数据进行配合

156
00:05:35,680 --> 00:05:37,960
第三种就是all to all的方式

157
00:05:37,960 --> 00:05:38,640
例如matmul

158
00:05:38,640 --> 00:05:40,200
或者一项复利变换这种

159
00:05:40,200 --> 00:05:42,120
我们元数的求解

160
00:05:42,120 --> 00:05:43,480
得到另外一个元数

161
00:05:43,480 --> 00:05:44,720
是由其他数据之间

162
00:05:44,920 --> 00:05:47,240
并不能够做到完全的相乘

163
00:05:47,240 --> 00:05:51,160
下面我们以中间卷机作为例子

164
00:05:51,160 --> 00:05:52,240
看看local memory

165
00:05:52,240 --> 00:05:53,080
在GPU里面

166
00:05:53,280 --> 00:05:54,040
跟我们的thread

167
00:05:54,040 --> 00:05:54,960
跟我们的线程

168
00:05:54,960 --> 00:05:56,600
是怎么去配合工作的

169
00:05:57,280 --> 00:05:59,480
我们现在有一个图片

170
00:05:59,600 --> 00:06:01,160
这个图片是一只猫猫

171
00:06:01,160 --> 00:06:02,040
在GPU里面

172
00:06:02,040 --> 00:06:03,800
利用网格就是grid

173
00:06:03,800 --> 00:06:06,400
对我们的整个图片进行覆盖

174
00:06:06,400 --> 00:06:08,000
切分成一个块

175
00:06:08,240 --> 00:06:09,800
其中我们就拿出来

176
00:06:09,800 --> 00:06:11,880
中间的一个模块进行

177
00:06:12,160 --> 00:06:13,200
每一个grid里面

178
00:06:13,360 --> 00:06:16,040
实际上还会分开不同的block

179
00:06:16,040 --> 00:06:18,600
每个block就有可能会重叠的

180
00:06:18,600 --> 00:06:19,440
而在GPU里面

181
00:06:19,440 --> 00:06:21,560
block会去独立的执行的

182
00:06:21,720 --> 00:06:23,520
接着三步的时候

183
00:06:23,880 --> 00:06:26,120
block里面会有大量的线程

184
00:06:26,120 --> 00:06:27,400
我们的大量的thread

185
00:06:27,480 --> 00:06:29,400
通过本地的数据共享

186
00:06:29,640 --> 00:06:31,880
或者我们叫做local data memory

187
00:06:32,240 --> 00:06:33,720
来去进行计算

188
00:06:33,720 --> 00:06:34,800
每个元数

189
00:06:34,800 --> 00:06:36,200
或者每一个像素点

190
00:06:36,480 --> 00:06:39,000
都会给一个线程进行计算

191
00:06:39,200 --> 00:06:41,800
这个时候就变成了整个线程

192
00:06:41,800 --> 00:06:43,480
是分层分级的

193
00:06:43,480 --> 00:06:45,280
我们再简单的看看

194
00:06:45,280 --> 00:06:47,360
刚才的讲到的一些概念

195
00:06:47,400 --> 00:06:49,680
首先我们的网格

196
00:06:49,680 --> 00:06:50,480
就是grid

197
00:06:50,480 --> 00:06:53,120
会表示所有需要执行的任务

198
00:06:53,120 --> 00:06:55,000
这里面会有大量的线程

199
00:06:55,000 --> 00:06:57,240
但实际上最高层的概念

200
00:06:57,240 --> 00:06:58,120
网格之下

201
00:06:58,440 --> 00:07:01,360
就会分成非常多的block

202
00:07:01,640 --> 00:07:02,640
每一个block里面

203
00:07:02,800 --> 00:07:05,520
又有非常多的这种线程

204
00:07:05,520 --> 00:07:07,640
快速我们的block之间的线程

205
00:07:07,840 --> 00:07:09,280
是互相独立的

206
00:07:09,280 --> 00:07:10,680
也就是block a

207
00:07:10,680 --> 00:07:11,480
跟block b

208
00:07:11,480 --> 00:07:12,760
之间里面的线程

209
00:07:12,760 --> 00:07:14,200
是独立执行的

210
00:07:14,200 --> 00:07:16,240
而block里面的线程

211
00:07:16,240 --> 00:07:19,160
他们是共享本地内存数据的

212
00:07:19,160 --> 00:07:21,560
就共享我们的local memory

213
00:07:21,760 --> 00:07:22,200
这个时候

214
00:07:22,320 --> 00:07:24,360
我们就在每一个block里面

215
00:07:24,360 --> 00:07:25,680
每一个快里面

216
00:07:25,680 --> 00:07:28,640
就可以执行相同相关的操作

217
00:07:28,640 --> 00:07:29,680
通过本地时候

218
00:07:29,680 --> 00:07:31,160
我们忽略了一个点

219
00:07:31,160 --> 00:07:34,360
就是网格里面的block里面的快

220
00:07:34,480 --> 00:07:37,280
其实在GPU里面是超额分配的

221
00:07:37,280 --> 00:07:38,560
通过超额的分配

222
00:07:38,560 --> 00:07:39,760
大量的这些block

223
00:07:39,760 --> 00:07:42,000
而block里面又有大量的线程

224
00:07:42,000 --> 00:07:42,680
每个线程

225
00:07:42,800 --> 00:07:44,120
就可以对我们的像素

226
00:07:44,120 --> 00:07:45,560
进行大量的操作

227
00:07:45,560 --> 00:07:47,080
而block的超量的分配

228
00:07:47,360 --> 00:07:49,000
就可以通过我们的计算

229
00:07:49,000 --> 00:07:51,480
去掩盖我们的延时的问题

230
00:07:52,120 --> 00:07:52,640
也好

231
00:07:52,640 --> 00:07:53,920
不管是线程也好

232
00:07:53,920 --> 00:07:55,480
都会在我们的block里面

233
00:07:55,480 --> 00:07:57,040
真正的去执行

234
00:07:58,120 --> 00:07:59,040
我们的这个问题

235
00:07:59,040 --> 00:08:00,880
因为我们在之前的课程里面

236
00:08:00,880 --> 00:08:03,200
其实给大家大量的去强调过

237
00:08:03,200 --> 00:08:04,880
并行的能力是最重要的

238
00:08:04,880 --> 00:08:06,400
并行的其实是为了解决

239
00:08:06,400 --> 00:08:07,640
我们带宽慢

240
00:08:07,640 --> 00:08:09,880
带宽时延长的问题

241
00:08:09,880 --> 00:08:10,520
多少线程

242
00:08:10,680 --> 00:08:12,200
是由我们的计算的复杂度

243
00:08:12,200 --> 00:08:13,120
来去决定的

244
00:08:13,120 --> 00:08:14,080
而计算复杂度

245
00:08:14,080 --> 00:08:15,040
这个操作来说

246
00:08:15,160 --> 00:08:16,440
我们每增加一个线程

247
00:08:16,440 --> 00:08:17,920
这意味着我们需要

248
00:08:18,440 --> 00:08:21,320
对数据进行一次新的加载

249
00:08:22,040 --> 00:08:24,200
GPU里面是并行的去加载

250
00:08:24,200 --> 00:08:26,600
或者并行的去提供我们的线程的

251
00:08:26,600 --> 00:08:28,840
于是我们增加线程的数量

252
00:08:28,840 --> 00:08:31,000
并不会对我们实际的运算

253
00:08:31,000 --> 00:08:32,520
或者对我们的实际的时延

254
00:08:32,520 --> 00:08:34,200
产生任何的影响

255
00:08:34,880 --> 00:08:36,600
规模在合理的范围内增大

256
00:08:36,600 --> 00:08:37,600
或者我们的数据的

257
00:08:37,600 --> 00:08:39,240
实际算法的效率

258
00:08:39,240 --> 00:08:40,120
并不会提升

259
00:08:40,120 --> 00:08:41,080
计算的过程当中

260
00:08:41,240 --> 00:08:42,880
觉得它会变慢了

261
00:08:43,840 --> 00:08:45,320
因为我们的线程是分级的

262
00:08:45,320 --> 00:08:47,760
在对于卷积的这一类运算的时候

263
00:08:47,920 --> 00:08:49,760
我们每增加一次超线程

264
00:08:49,880 --> 00:08:51,400
对于我们的数据的读取

265
00:08:51,400 --> 00:08:52,200
因为是并行的

266
00:08:52,200 --> 00:08:53,840
它的影响也不会太大

267
00:08:54,720 --> 00:08:55,640
GPU的执行效率

268
00:08:55,920 --> 00:08:58,080
跟AI的计算模式之间

269
00:08:58,080 --> 00:08:59,920
就非常的匹配了

270
00:08:59,920 --> 00:09:02,400
可以看到里面的算法的复杂度

271
00:09:02,400 --> 00:09:03,360
或者算法的规模

272
00:09:03,360 --> 00:09:04,720
或者我们的强度的规模

273
00:09:04,720 --> 00:09:06,000
都是O的E

274
00:09:06,000 --> 00:09:07,840
非常的和谐

275
00:09:08,600 --> 00:09:10,760
因为里面通过线程的分层分级

276
00:09:10,760 --> 00:09:12,040
能够很好的匹配到

277
00:09:12,040 --> 00:09:14,600
我们的算法的一些计算的强度

278
00:09:16,160 --> 00:09:18,000
现在我们来到了第三个内容

279
00:09:18,000 --> 00:09:19,120
去详细的打开

280
00:09:19,120 --> 00:09:20,120
AI的计算模式

281
00:09:20,120 --> 00:09:22,080
跟线程之间的关系

282
00:09:22,080 --> 00:09:22,920
第一个小内容里面

283
00:09:23,040 --> 00:09:23,880
给大家汇报过

284
00:09:23,880 --> 00:09:25,520
实际上卷积的计算

285
00:09:25,520 --> 00:09:27,240
我们可以用矩阵层来代替

286
00:09:27,240 --> 00:09:28,560
那现在这个概念

287
00:09:28,560 --> 00:09:29,920
真正的在我们的kernel

288
00:09:29,920 --> 00:09:31,440
在GPU执行的阶段下

289
00:09:31,440 --> 00:09:33,040
看看矩阵层

290
00:09:33,160 --> 00:09:34,600
跟我们的AI计算模式

291
00:09:34,600 --> 00:09:35,640
或者矩阵层

292
00:09:35,640 --> 00:09:36,600
跟GPU之间

293
00:09:36,600 --> 00:09:37,960
是怎么去进行更好的

294
00:09:37,960 --> 00:09:39,280
去融合和交互的

295
00:09:39,280 --> 00:09:41,200
首先这里面每一行

296
00:09:41,200 --> 00:09:42,240
跟蓝色的矩阵

297
00:09:42,240 --> 00:09:43,160
每一列

298
00:09:43,160 --> 00:09:44,840
进行一个乘加的操作之后

299
00:09:45,000 --> 00:09:46,400
就得到输出的层

300
00:09:46,400 --> 00:09:48,840
是在这块里面的一个元素

301
00:09:49,440 --> 00:09:51,400
要提取红色矩阵里面的

302
00:09:51,400 --> 00:09:52,880
第一行的元素

303
00:09:52,880 --> 00:09:54,320
还有蓝色矩阵

304
00:09:54,320 --> 00:09:55,840
第一列的元素

305
00:09:56,320 --> 00:09:59,240
是每一个元素进行相乘

306
00:09:59,240 --> 00:10:00,160
相乘完之后

307
00:10:00,360 --> 00:10:03,000
再逐个元素进行相加

308
00:10:03,560 --> 00:10:06,280
到最终的层次的值

309
00:10:07,200 --> 00:10:08,920
因为会提供一个FFM

310
00:10:08,920 --> 00:10:10,040
或者MAC的操作

311
00:10:10,040 --> 00:10:11,160
就是把我们的层加

312
00:10:11,160 --> 00:10:12,480
变成一个具体的指令

313
00:10:12,480 --> 00:10:14,880
所以每一次我们乘法和加法

314
00:10:14,880 --> 00:10:16,400
是一起去执行的

315
00:10:16,600 --> 00:10:17,840
这个时候矩阵层里面

316
00:10:17,960 --> 00:10:19,960
我们不仅是要算一行一列

317
00:10:19,960 --> 00:10:21,760
我们可能以这一行

318
00:10:21,760 --> 00:10:22,680
算第二列

319
00:10:22,680 --> 00:10:23,280
第三列

320
00:10:23,280 --> 00:10:23,960
第四列

321
00:10:23,960 --> 00:10:24,800
第五列

322
00:10:24,800 --> 00:10:26,640
然后得出第一个元素

323
00:10:26,640 --> 00:10:27,320
第二个元素

324
00:10:27,320 --> 00:10:28,360
第三个元素

325
00:10:28,360 --> 00:10:29,440
第四个元素

326
00:10:29,440 --> 00:10:31,360
第五个元素的值

327
00:10:32,000 --> 00:10:34,240
而矩阵层里面的红色的这一行

328
00:10:34,240 --> 00:10:35,920
加载了一次

329
00:10:35,920 --> 00:10:38,480
但是我们的次去加载了5次

330
00:10:38,480 --> 00:10:40,080
每一次都有5个元素

331
00:10:40,080 --> 00:10:42,360
因此它执行了25次计算

332
00:10:43,080 --> 00:10:45,040
通过矩阵的大小的提高了

333
00:10:45,040 --> 00:10:47,160
我们的算力的需求

334
00:10:47,160 --> 00:10:48,520
就不断的提高了

335
00:10:49,160 --> 00:10:50,480
就知道矩阵的运算

336
00:10:50,600 --> 00:10:53,160
其实还是有一个算术的强度的

337
00:10:53,160 --> 00:10:53,960
算术的强度

338
00:10:53,960 --> 00:10:54,920
越矩阵越大

339
00:10:54,920 --> 00:10:56,480
算术的强度就越大

340
00:10:56,480 --> 00:10:58,480
我们需要搬去的量就会增大

341
00:10:58,600 --> 00:11:00,520
这个时候算术的强度的比

342
00:11:00,840 --> 00:11:02,520
也会相对应的增大

343
00:11:03,760 --> 00:11:05,080
我们来到最后一个内容

344
00:11:05,080 --> 00:11:06,600
去看看算术的强度

345
00:11:06,600 --> 00:11:07,800
去对比一下表格

346
00:11:07,800 --> 00:11:08,880
我们在矩阵层里面

347
00:11:08,880 --> 00:11:10,160
整体的算力的利用率

348
00:11:10,160 --> 00:11:12,040
或者我们的算术的强度

349
00:11:12,720 --> 00:11:13,680
里面的横坐标

350
00:11:13,680 --> 00:11:16,560
就是我们矩阵的一个大小的增加

351
00:11:16,560 --> 00:11:18,840
纵坐标就是算术的强度

352
00:11:18,840 --> 00:11:20,880
或者我们的计算的强度

353
00:11:20,960 --> 00:11:21,800
那么可以看到

354
00:11:21,800 --> 00:11:22,560
我们可以看到

355
00:11:22,560 --> 00:11:23,360
矩阵的大小

356
00:11:23,360 --> 00:11:24,960
从1到64

357
00:11:24,960 --> 00:11:26,160
我们的矩阵的增大

358
00:11:26,400 --> 00:11:27,440
我们的计算强度

359
00:11:27,440 --> 00:11:29,200
也是层次性的增加

360
00:11:30,000 --> 00:11:32,480
能打横画一条层次的线

361
00:11:33,120 --> 00:11:35,280
就代表GPU里面的浮点运算的

362
00:11:35,280 --> 00:11:36,160
计算的强度

363
00:11:36,160 --> 00:11:38,360
假设在和蓝色这条线

364
00:11:38,440 --> 00:11:39,160
我们可以看到

365
00:11:39,160 --> 00:11:41,120
中间有一个交叉点

366
00:11:41,120 --> 00:11:42,040
交叉点的位置

367
00:11:42,200 --> 00:11:44,200
就是我们的GPU的浮点的运算

368
00:11:44,200 --> 00:11:47,560
跟矩阵的计算强度的一个交点

369
00:11:48,400 --> 00:11:49,720
整个GPU的计算强度

370
00:11:49,720 --> 00:11:51,120
也就是让我们的sm

371
00:11:51,120 --> 00:11:52,800
或者让我们的计算单元

372
00:11:52,800 --> 00:11:54,160
不断的去执行的时候

373
00:11:54,280 --> 00:11:56,360
我们要求整个矩阵的大小

374
00:11:56,480 --> 00:11:58,320
大概是在50左右

375
00:11:58,480 --> 00:11:59,840
这个时候就会满足

376
00:11:59,840 --> 00:12:02,240
我们整个计算的强度

377
00:12:02,240 --> 00:12:04,320
我们的硬就会忙碌起来

378
00:12:05,000 --> 00:12:06,720
情况下为了让我们的机器

379
00:12:06,720 --> 00:12:07,640
或者我们的GPU

380
00:12:07,800 --> 00:12:10,160
在运算跟搬运数据的同时

381
00:12:10,320 --> 00:12:11,640
保持平衡

382
00:12:11,640 --> 00:12:12,880
就是百分百的速度

383
00:12:12,880 --> 00:12:13,960
去整体的运算

384
00:12:13,960 --> 00:12:15,640
这个就是我们所说的

385
00:12:15,640 --> 00:12:17,320
GPU它是个吞吐机的意义

386
00:12:17,320 --> 00:12:18,200
这一个点

387
00:12:18,520 --> 00:12:20,240
继续把蓝色的这条线

388
00:12:20,240 --> 00:12:21,760
不断的扩大

389
00:12:21,760 --> 00:12:22,680
看一下成什么

390
00:12:22,680 --> 00:12:24,240
我们还是在这个点里面

391
00:12:24,240 --> 00:12:25,840
就是我们的交集是50

392
00:12:25,840 --> 00:12:26,480
这个时候

393
00:12:26,560 --> 00:12:27,560
在我们的矩阵大小

394
00:12:27,560 --> 00:12:28,520
为50的时候

395
00:12:28,840 --> 00:12:30,720
能够充分的发挥GPU

396
00:12:30,720 --> 00:12:33,040
在FP32的计算的强度

397
00:12:33,800 --> 00:12:35,880
小不断的增加的时候

398
00:12:35,880 --> 00:12:37,000
GPU里面的内存

399
00:12:37,120 --> 00:12:38,360
就会空闲下来了

400
00:12:38,360 --> 00:12:39,480
所谓空闲

401
00:12:39,480 --> 00:12:42,400
不是指它的内存容量降低了

402
00:12:42,400 --> 00:12:44,160
或者它不需要那么多内存容量了

403
00:12:44,160 --> 00:12:46,560
而是指我们的内存的搬运

404
00:12:46,560 --> 00:12:47,480
越来越慢了

405
00:12:47,480 --> 00:12:48,600
内存的数据发信

406
00:12:48,600 --> 00:12:49,840
变得越来越慢了

407
00:12:50,520 --> 00:12:52,280
PU里面的计算单元里面

408
00:12:52,280 --> 00:12:54,800
需要花费更多的时间

409
00:12:54,840 --> 00:12:56,440
去对我们的矩阵

410
00:12:56,440 --> 00:12:58,480
进行执行和运算

411
00:12:59,200 --> 00:13:01,720
我们需要找到一个更好的平衡点

412
00:13:02,360 --> 00:13:03,320
大的矩阵运算

413
00:13:03,320 --> 00:13:05,560
去找到更好的计算的强度

414
00:13:06,200 --> 00:13:08,520
条线就是英伟达GPU里面

415
00:13:08,520 --> 00:13:10,320
后来提出了探测扩

416
00:13:10,320 --> 00:13:13,080
专门针对我们的矩阵进行运算

417
00:13:13,080 --> 00:13:15,040
去提高我们的计算的强度

418
00:13:15,040 --> 00:13:16,640
使得我们的内存的搬运

419
00:13:16,640 --> 00:13:19,240
跟得上我们的数据的运算的速度

420
00:13:19,240 --> 00:13:22,000
而这个时候用于AI加速的GPU

421
00:13:22,000 --> 00:13:24,440
还是用于AI加速的NPU

422
00:13:24,480 --> 00:13:25,360
TPU也好

423
00:13:25,360 --> 00:13:29,160
我们都在寻找中间最好的平衡点

424
00:13:29,880 --> 00:13:31,600
计算强度的提高的同时

425
00:13:31,800 --> 00:13:33,520
还能够让我们计算的越大越好

426
00:13:33,520 --> 00:13:34,640
因为在神经网络里面

427
00:13:34,760 --> 00:13:37,360
我们处理的是非常大的一个矩阵的运算

428
00:13:37,360 --> 00:13:39,160
而不是小矩阵的运算

429
00:13:40,440 --> 00:13:43,000
它延伸右边的表格里面

430
00:13:43,240 --> 00:13:45,920
我们看到现在其实多了一个功能

431
00:13:45,920 --> 00:13:47,360
或者多了一个模块

432
00:13:47,360 --> 00:13:48,600
叫做探测扩

433
00:13:48,600 --> 00:13:51,640
里面就是矩阵去提出的一个DSA

434
00:13:52,440 --> 00:13:53,640
使用LE的款存

435
00:13:53,800 --> 00:13:55,880
整个计算的强度是32

436
00:13:55,880 --> 00:13:56,720
使用LE的款存

437
00:13:56,840 --> 00:13:58,560
计算的强度是156

438
00:13:59,200 --> 00:14:00,840
就要根据多级的缓存

439
00:14:00,880 --> 00:14:02,800
到底在哪里去搭配

440
00:14:02,800 --> 00:14:04,120
我们的张亮的核心

441
00:14:04,120 --> 00:14:05,320
使得整个张亮

442
00:14:05,440 --> 00:14:07,280
在小矩阵或者大矩阵里面

443
00:14:07,280 --> 00:14:09,760
都能够更加高效的去执行运算

444
00:14:10,480 --> 00:14:11,480
我们看一个图

445
00:14:11,480 --> 00:14:12,600
这里面我们就表明

446
00:14:12,600 --> 00:14:13,920
我们在LE的款存

447
00:14:13,920 --> 00:14:14,720
LE的款存

448
00:14:14,720 --> 00:14:16,520
还有HBM的款存里面

449
00:14:16,560 --> 00:14:18,360
都针对矩阵的运算的强度

450
00:14:18,640 --> 00:14:20,040
都有不同差点

451
00:14:20,240 --> 00:14:21,440
这个时候我们就明白

452
00:14:21,480 --> 00:14:23,800
为什么说数据在哪里

453
00:14:23,840 --> 00:14:25,120
非常的重要

454
00:14:25,120 --> 00:14:26,640
比我们的算力的强度

455
00:14:26,640 --> 00:14:28,960
或者我们的复数更加重要

456
00:14:30,120 --> 00:14:31,040
就是我们的数据

457
00:14:31,040 --> 00:14:32,960
假设已经搬运到LE的款存里面

458
00:14:33,160 --> 00:14:34,600
这个时候我们可以执行一些

459
00:14:34,600 --> 00:14:36,520
更小规模的矩阵的运算

460
00:14:36,520 --> 00:14:38,240
例如是小的卷积核

461
00:14:38,440 --> 00:14:39,800
对于NLP这种大的

462
00:14:39,800 --> 00:14:41,000
transformer的结构

463
00:14:41,280 --> 00:14:43,360
我可能会把数据搬运到L2

464
00:14:43,360 --> 00:14:45,440
然后通过L2的cache去读取

465
00:14:45,440 --> 00:14:46,160
然后去执行

466
00:14:46,160 --> 00:14:48,320
因为数据跟读取之间

467
00:14:48,320 --> 00:14:50,200
是有一个关系比例的

468
00:14:50,200 --> 00:14:52,320
我们数据如果大量都在搬运

469
00:14:52,320 --> 00:14:54,760
我们大量都在等待计算的时候

470
00:14:54,760 --> 00:14:57,080
就会导致计算跟通讯之间

471
00:14:57,760 --> 00:15:01,160
因此找到计算的强度的点

472
00:15:01,200 --> 00:15:02,880
对于我们系统的优化

473
00:15:02,880 --> 00:15:04,840
变得非常重要

474
00:15:06,320 --> 00:15:06,880
好

475
00:15:06,880 --> 00:15:08,080
我们现在来总结一下

476
00:15:08,080 --> 00:15:10,680
为什么GPU更适用于AI计算

477
00:15:10,880 --> 00:15:11,760
是在乎算力

478
00:15:12,040 --> 00:15:13,240
可能我们真的不是说

479
00:15:13,240 --> 00:15:14,080
非常在乎

480
00:15:14,080 --> 00:15:15,480
我们的算力到底有多少

481
00:15:15,480 --> 00:15:16,480
而更应该关注

482
00:15:16,480 --> 00:15:18,440
我们的内存带宽跟时延

483
00:15:18,480 --> 00:15:20,960
之间跟算力的一个匹配度

484
00:15:21,680 --> 00:15:23,200
说算力不是最重要的

485
00:15:23,200 --> 00:15:24,920
是因为我们有一个计算强度

486
00:15:24,920 --> 00:15:25,360
在

487
00:15:25,360 --> 00:15:26,840
不管我们的算的更坏

488
00:15:26,840 --> 00:15:28,560
但是我们的内存来不及搬运

489
00:15:28,760 --> 00:15:31,280
所有的算力都是徒劳无功的

490
00:15:32,280 --> 00:15:35,240
另外一块并没有时延那么重要

491
00:15:35,240 --> 00:15:38,160
因为在真正延会非常非常的长

492
00:15:38,160 --> 00:15:39,760
为了解决时延的问题

493
00:15:39,880 --> 00:15:41,560
GPU提供了大量的线程

494
00:15:41,560 --> 00:15:44,000
通过线程去都是超配的

495
00:15:44,840 --> 00:15:47,360
给我们提供了非常多的线程

496
00:15:47,360 --> 00:15:49,360
但这些线程其实不是

497
00:15:49,360 --> 00:15:51,800
所有都能够独立的去运作的

498
00:15:51,800 --> 00:15:53,720
它们之间需要有一个配合

499
00:15:54,520 --> 00:15:56,320
那数据有不同的线程

500
00:15:56,320 --> 00:15:58,240
对它进行一个读写的操作

501
00:15:58,880 --> 00:16:02,320
对线程进行了一个分层分级的概念

502
00:16:02,320 --> 00:16:03,320
通过分层分级

503
00:16:03,440 --> 00:16:05,880
把线程跟线程之间划分出来

504
00:16:05,880 --> 00:16:07,560
哪一些可以独立的操作

505
00:16:07,560 --> 00:16:08,920
哪一些可以独立的运行

506
00:16:08,920 --> 00:16:11,240
哪一些需要相互进行配合

507
00:16:12,160 --> 00:16:13,200
增加大量的线程

508
00:16:13,320 --> 00:16:14,720
去解决时延的问题之后

509
00:16:14,960 --> 00:16:16,720
现在我们需要去解决

510
00:16:16,720 --> 00:16:19,680
计算跟带宽之间的一个平衡点

511
00:16:20,520 --> 00:16:22,760
小型或者计算密集型的任务里面

512
00:16:22,760 --> 00:16:24,960
我们为了获得更高的效率

513
00:16:24,960 --> 00:16:26,600
或者更高的计算的方式

514
00:16:26,600 --> 00:16:28,120
于是就对于存

515
00:16:28,800 --> 00:16:29,560
需求

516
00:16:29,560 --> 00:16:32,080
也就是我们的分层分级的内存

517
00:16:32,080 --> 00:16:34,120
从L1 L2到HBM

518
00:16:35,000 --> 00:16:37,000
提升我们单芯片里面的

519
00:16:37,000 --> 00:16:38,520
浮点运算的能力

520
00:16:38,520 --> 00:16:40,400
于是GPU GI计算

521
00:16:40,400 --> 00:16:42,120
就提出了Tensor Core了

522
00:16:42,880 --> 00:16:44,080
就能够最大限度的

523
00:16:44,120 --> 00:16:46,880
去提升整体的系统线程

524
00:16:46,880 --> 00:16:47,720
内存

525
00:16:47,720 --> 00:16:50,080
还有所有组件的效率问题

526
00:16:50,080 --> 00:16:51,640
而这些所有的问题

527
00:16:51,640 --> 00:16:53,400
都取决于我们的数据

528
00:16:53,400 --> 00:16:54,320
从哪里来

529
00:16:54,320 --> 00:16:56,800
我们的数据到底是怎么运算的

530
00:16:58,160 --> 00:17:00,360
我们今天的内容就到这里为止

531
00:17:00,360 --> 00:17:01,400
今天我们回答了

532
00:17:01,400 --> 00:17:03,600
为什么GPU适用于AI计算

533
00:17:03,600 --> 00:17:05,200
是因为在GPU里面

534
00:17:05,360 --> 00:17:06,960
通过超配的线程

535
00:17:06,960 --> 00:17:08,560
来掩盖整体的失言

536
00:17:08,560 --> 00:17:09,880
还有多级的缓存

537
00:17:09,880 --> 00:17:11,040
多级的cache

538
00:17:11,360 --> 00:17:13,320
去平衡计算带宽的cap

539
00:17:13,640 --> 00:17:15,240
然后提出了Tensor Core

540
00:17:15,240 --> 00:17:17,280
来增加峰值的算力

541
00:17:17,280 --> 00:17:18,800
通过这三个手段

542
00:17:18,800 --> 00:17:19,920
使得整个GPU

543
00:17:20,280 --> 00:17:22,600
非常适用于AI的计算

544
00:17:22,760 --> 00:17:23,560
谢谢各位

545
00:17:23,560 --> 00:17:24,560
拜拜

546
00:17:24,800 --> 00:17:25,640
卷的不行了

547
00:17:25,640 --> 00:17:26,480
卷的不行了

548
00:17:26,480 --> 00:17:27,920
记得一键三连加关注

549
00:17:28,280 --> 00:17:29,640
所有的内容都会开源

550
00:17:29,640 --> 00:17:31,480
在下面这条链接里面

551
00:17:31,840 --> 00:17:32,800
拜拜

