1
00:00:00,000 --> 00:00:11,500
Hello,大家好,我是你们投油秃要用肥的宗敏

2
00:00:11,500 --> 00:00:16,500
今天我们来到一个新的系列里面,主要是GPU的详解

3
00:00:16,500 --> 00:00:23,000
今天我们要分享给大家或者给大家汇报的内容就是GPU的工作原理

4
00:00:24,000 --> 00:00:28,000
我们在之前已经讲了AI的计算体系

5
00:00:28,000 --> 00:00:30,000
还有AI芯片的整体的基础

6
00:00:30,000 --> 00:00:32,000
而这支里面我们埋了一个雷

7
00:00:32,000 --> 00:00:35,000
就是通用图形处理器GPU

8
00:00:35,000 --> 00:00:39,000
简单的去讲了一些它的主要的概念

9
00:00:39,000 --> 00:00:43,000
今天我们会详细的去打开GPU里面的更多的内容

10
00:00:43,000 --> 00:00:46,000
那可能我们会分两个章节来聊

11
00:00:46,000 --> 00:00:49,000
第一个就是硬件的基础

12
00:00:49,000 --> 00:00:50,000
GPU的工作原理

13
00:00:50,000 --> 00:00:51,000
还有它的编程本质

14
00:00:51,000 --> 00:00:54,000
第二个就是英伟达的GPU的架构

15
00:00:54,000 --> 00:00:56,000
英伟达其实发展的非常久了

16
00:00:56,000 --> 00:00:59,000
它整体架构也演变了非常多代

17
00:00:59,000 --> 00:01:03,000
那从female到hopper架构它也引进了非常多

18
00:01:03,000 --> 00:01:06,000
其中我觉得跟AI特别相关的就有Tensor Core

19
00:01:06,000 --> 00:01:08,000
还有NVLink两个内容

20
00:01:08,000 --> 00:01:11,000
那在最后我们可能会单独分一节出来

21
00:01:11,000 --> 00:01:14,000
去聊一聊GPU的图形图像处理

22
00:01:14,000 --> 00:01:16,000
它最原始的一些内容

23
00:01:16,000 --> 00:01:20,000
今天我们来到我们的重点就是GPU的工作原理

24
00:01:20,000 --> 00:01:22,000
它的Working Principle

25
00:01:22,000 --> 00:01:24,000
在GPU的工作原理里面

26
00:01:24,000 --> 00:01:28,000
我们主要今天我们会给大家汇报几个内容

27
00:01:28,000 --> 00:01:30,000
第一个我们以AX加Y这个例子

28
00:01:30,000 --> 00:01:33,000
去看看GPU是怎么做并行的

29
00:01:33,000 --> 00:01:36,000
而谈到并行我们有两个概念要澄清的

30
00:01:36,000 --> 00:01:39,000
就是并发跟并行两个概念是不一样的

31
00:01:39,000 --> 00:01:41,000
而并发更多的是CPU

32
00:01:41,000 --> 00:01:44,000
并行更多的是GPU里面所处的

33
00:01:44,000 --> 00:01:46,000
或者所处理的一些任务

34
00:01:46,000 --> 00:01:49,000
然后我们看看GPU的缓存机制

35
00:01:49,000 --> 00:01:52,000
它的Cache跟我们的线程是分不开的

36
00:01:52,000 --> 00:01:54,000
因为GPU它是一个SIMT

37
00:01:54,000 --> 00:01:56,000
T就是我们的线程的意思

38
00:01:56,000 --> 00:02:00,000
所以今天我们会分开四个内容给大家去汇报

39
00:02:01,000 --> 00:02:03,000
还没有进入第一个内容的时候

40
00:02:03,000 --> 00:02:05,000
我们看看什么是GPU

41
00:02:05,000 --> 00:02:07,000
为什么GPU这么独特

42
00:02:07,000 --> 00:02:12,000
首先GPU的U全称叫做Graphic Processing Unit

43
00:02:12,000 --> 00:02:16,000
所以我们这台一开始是用来处理一些图形图像

44
00:02:16,000 --> 00:02:19,000
视频编解码的一些相关的工作

45
00:02:19,000 --> 00:02:24,000
第二个问题就是GPU跟CPU最大的不同的点在哪

46
00:02:24,000 --> 00:02:26,000
我们在上一个内容里面

47
00:02:26,000 --> 00:02:28,000
特别是AI芯片技术里面

48
00:02:28,000 --> 00:02:30,000
其实已经给大家汇报过了

49
00:02:30,000 --> 00:02:34,000
GPU一颗设计目标是最大化它的Turntable Fullput

50
00:02:34,000 --> 00:02:36,000
针对性的是并行度

51
00:02:36,000 --> 00:02:39,000
就是同一时间我们可以执行多少任务

52
00:02:39,000 --> 00:02:41,000
而CPU更关心的是延迟

53
00:02:41,000 --> 00:02:43,000
还有并发两个内容

54
00:02:43,000 --> 00:02:46,000
它们所关心的内容或者聚焦的重点是不一样的

55
00:02:48,000 --> 00:02:50,000
我们来到第一个内容

56
00:02:50,000 --> 00:02:53,000
看一下AX加Y这一个Demo

57
00:02:53,000 --> 00:02:56,000
然后去了解一下GPU的工作原理

58
00:02:56,000 --> 00:02:57,000
其实在上一节里面

59
00:02:57,000 --> 00:02:59,000
我们以这个AX加Y作为例子

60
00:02:59,000 --> 00:03:00,000
给大家讲过了

61
00:03:00,000 --> 00:03:02,000
像这么简单的一个数据

62
00:03:02,000 --> 00:03:04,000
我们的Demo就是AX加Y

63
00:03:04,000 --> 00:03:06,000
然后AX加Y这一个公式

64
00:03:06,000 --> 00:03:09,000
这里面就有两个Fullput操作

65
00:03:09,000 --> 00:03:10,000
一个就是乘法

66
00:03:10,000 --> 00:03:12,000
一个就是加法

67
00:03:12,000 --> 00:03:16,000
操作我们需要在内存里面读取两个数据

68
00:03:16,000 --> 00:03:17,000
一个是XI

69
00:03:17,000 --> 00:03:18,000
一个是YI

70
00:03:18,000 --> 00:03:21,000
然后执行一个线性的操作

71
00:03:21,000 --> 00:03:23,000
存回YI这个里面

72
00:03:23,000 --> 00:03:26,000
在这里面我们就可以有一个融合的操作

73
00:03:26,000 --> 00:03:28,000
叫做FMA Fuse Multiply and Add

74
00:03:28,000 --> 00:03:31,000
就是把乘法和加法融合在一起

75
00:03:31,000 --> 00:03:33,000
然后不管我们的N有多少次

76
00:03:33,000 --> 00:03:34,000
我们就迭代多少次

77
00:03:34,000 --> 00:03:36,000
所以它是一个CPU里面的

78
00:03:36,000 --> 00:03:39,000
它是一个串行的按指令顺序去执行的一段程序

79
00:03:40,000 --> 00:03:43,000
英特尔8280这款芯片里面

80
00:03:43,000 --> 00:03:46,000
我们的内存带宽是113GB每秒

81
00:03:46,000 --> 00:03:49,000
而内存的延时是89纳秒

82
00:03:49,000 --> 00:03:53,000
这个时候我们就可以有11659个byte

83
00:03:53,000 --> 00:03:57,000
每一次也就是在89纳秒里面去执行

84
00:03:57,000 --> 00:04:01,000
1659个byte在89纳秒里面去执行

85
00:04:01,000 --> 00:04:03,000
它只是一个峰值的算力

86
00:04:03,000 --> 00:04:05,000
而针对我们的延迟时间内

87
00:04:05,000 --> 00:04:08,000
我们只搬运了16个byte的数据

88
00:04:08,000 --> 00:04:11,000
这个时候我们内存的利用率只有0.14

89
00:04:11,000 --> 00:04:13,000
非常非常的小

90
00:04:13,000 --> 00:04:16,000
就占了99.86的时间了

91
00:04:16,000 --> 00:04:20,000
现在我们看一下一个整个的内存的利用率

92
00:04:20,000 --> 00:04:22,000
不管是AMD Intel还是NVIDIA

93
00:04:22,000 --> 00:04:25,000
整体来说对于AS-JY这么刚才的一段程序

94
00:04:25,000 --> 00:04:29,000
我们的内存的利用率是非常的非常的低

95
00:04:29,000 --> 00:04:33,000
可以看到基本上都是不到0.1的范围内

96
00:04:33,000 --> 00:04:36,000
因为程序确实写得非常的不好

97
00:04:36,000 --> 00:04:38,000
没有充分利用并发和信心度

98
00:04:38,000 --> 00:04:40,000
下面我们看一下另外一个程序

99
00:04:40,000 --> 00:04:44,000
我们把刚才的那个程序进行了一次展开

100
00:04:44,000 --> 00:04:46,000
执行从0到7的数据

101
00:04:46,000 --> 00:04:48,000
然后迭代我们的8次

102
00:04:48,000 --> 00:04:51,000
这种就是把AS-JY的这种demo

103
00:04:51,000 --> 00:04:55,000
或者计算通过并发进行循环展开

104
00:04:55,000 --> 00:04:58,000
这个就处于一个忙碌的状态当中

105
00:04:58,000 --> 00:04:59,000
我们从计算来看一下

106
00:04:59,000 --> 00:05:04,000
就是11659次除以16就等于729

107
00:05:04,000 --> 00:05:08,000
展开之后我们在执行729次请求

108
00:05:08,000 --> 00:05:09,000
都是没有问题的

109
00:05:09,000 --> 00:05:11,000
这个时候我们叫做通过并发

110
00:05:11,000 --> 00:05:14,000
使得我们的总线处于忙碌的状态当中

111
00:05:14,000 --> 00:05:15,000
下面我们可以看到

112
00:05:15,000 --> 00:05:17,000
在真正的应用场景

113
00:05:17,000 --> 00:05:20,000
实际上我们的编译器很少会对整个循环

114
00:05:20,000 --> 00:05:23,000
进行超过100次以上的一个展开

115
00:05:23,000 --> 00:05:25,000
第2个问题就是一个线程

116
00:05:25,000 --> 00:05:28,000
每一次执行的指令数量是有限的

117
00:05:28,000 --> 00:05:31,000
它不可能执行非常非常多并发的数量

118
00:05:31,000 --> 00:05:33,000
第3个问题就是一个线程

119
00:05:33,000 --> 00:05:35,000
一个线程其实很难直接去处理

120
00:05:35,000 --> 00:05:38,000
700多个计算的负荷

121
00:05:38,000 --> 00:05:40,000
于是我们就引入了

122
00:05:40,000 --> 00:05:42,000
硬件架构的一个问题了

123
00:05:42,000 --> 00:05:43,000
它的操作

124
00:05:43,000 --> 00:05:46,000
让我们一次能够执行更多的指令

125
00:05:46,000 --> 00:05:47,000
流水的操作

126
00:05:47,000 --> 00:05:49,000
但是同样我们的架构

127
00:05:49,000 --> 00:05:52,000
也会受到所限制所约束

128
00:05:52,000 --> 00:05:55,000
我们看一下同样的例子

129
00:05:55,000 --> 00:05:56,000
z等于as加y

130
00:05:56,000 --> 00:06:00,000
这个demo通过并行的方式进行循环展开

131
00:06:00,000 --> 00:06:01,000
刚才是通过并发

132
00:06:01,000 --> 00:06:02,000
现在是通过并行

133
00:06:02,000 --> 00:06:03,000
我们的并行

134
00:06:03,000 --> 00:06:05,000
就是通过我们的并行处理器

135
00:06:05,000 --> 00:06:06,000
或者多个线程

136
00:06:06,000 --> 00:06:09,000
去执行as加y这个操作

137
00:06:09,000 --> 00:06:11,000
同样使得我们的总线

138
00:06:11,000 --> 00:06:12,000
处于忙碌的状态当中

139
00:06:12,000 --> 00:06:15,000
每一次可以执行729个迭代

140
00:06:15,000 --> 00:06:16,000
不过有点不一样的

141
00:06:16,000 --> 00:06:18,000
就是我们现在每个线程

142
00:06:18,000 --> 00:06:21,000
独立的去负责相关的运算

143
00:06:21,000 --> 00:06:22,000
也就是每个线程

144
00:06:22,000 --> 00:06:25,000
去计算一次as加y这么一个操作

145
00:06:25,000 --> 00:06:28,000
现在我们要执行729次计算

146
00:06:28,000 --> 00:06:30,000
一共我们就需要729个线程

147
00:06:30,000 --> 00:06:33,000
也就是我们票栏一共有729次

148
00:06:33,000 --> 00:06:36,000
这个时候我们遇到的瓶颈或者约束

149
00:06:36,000 --> 00:06:38,000
就会受到我们线程数量

150
00:06:38,000 --> 00:06:41,000
还有内存请求的一个约束了

151
00:06:41,000 --> 00:06:44,000
我们就引入了第二个内容

152
00:06:44,000 --> 00:06:45,000
或者第二个话题

153
00:06:45,000 --> 00:06:48,000
并行跟并发之间的关系

154
00:06:48,000 --> 00:06:52,000
主要是指我们能够同时处理多个任务

155
00:06:52,000 --> 00:06:54,000
是指我们能够处理多个任务的功能

156
00:06:54,000 --> 00:06:56,000
但不一定是同时

157
00:06:56,000 --> 00:06:57,000
所以我们叫做并发

158
00:06:57,000 --> 00:06:58,000
可以大量的并发

159
00:06:58,000 --> 00:07:02,000
但并行是同时去执行相同的任务

160
00:07:02,000 --> 00:07:04,000
记得硬件的工作过程当中

161
00:07:04,000 --> 00:07:06,000
我们更多的倾向于使用并行的

162
00:07:06,000 --> 00:07:09,000
对循环展开操作

163
00:07:09,000 --> 00:07:11,000
硬件的利用率

164
00:07:11,000 --> 00:07:13,000
GPU的最主要的原理

165
00:07:13,000 --> 00:07:16,000
看一下在硬件限制的情况下

166
00:07:16,000 --> 00:07:19,000
我们一般能够执行多少个线程

167
00:07:19,000 --> 00:07:21,000
我们以三款芯片作为例子

168
00:07:21,000 --> 00:07:22,000
一个是AMD

169
00:07:22,000 --> 00:07:23,000
一个是英特尔

170
00:07:23,000 --> 00:07:25,000
另外一个是英伟达的A100

171
00:07:26,000 --> 00:07:28,000
其实我们刚才只看了内存的利用率

172
00:07:28,000 --> 00:07:30,000
现在我们增加三列

173
00:07:30,000 --> 00:07:32,000
就是线程的可用数

174
00:07:32,000 --> 00:07:33,000
还有线程的比例

175
00:07:33,000 --> 00:07:36,000
去看一下我们到底需要多少线程

176
00:07:36,000 --> 00:07:40,000
才能够解决内存实验的问题

177
00:07:40,000 --> 00:07:41,000
从这个表

178
00:07:41,000 --> 00:07:43,000
我们可以看到几个关键的数据

179
00:07:43,000 --> 00:07:47,000
会比CPU要高出好几个倍数的等级

180
00:07:47,000 --> 00:07:48,000
但是有一个数据

181
00:07:48,000 --> 00:07:50,000
就是GPU的线程数

182
00:07:50,000 --> 00:07:53,000
是CPU的接近二三十倍

183
00:07:53,000 --> 00:07:55,000
也是非常夸张的一个数据

184
00:07:55,000 --> 00:07:57,000
现在我们看比较重要的数据

185
00:07:57,000 --> 00:07:59,000
就是线程的数量

186
00:07:59,000 --> 00:08:01,000
GPU的线程用线程数

187
00:08:01,000 --> 00:08:03,000
是CPU的一百多倍

188
00:08:03,000 --> 00:08:04,000
这个时候我们可以算出来

189
00:08:04,000 --> 00:08:05,000
线程的比例

190
00:08:05,000 --> 00:08:07,000
GPU是5.6倍

191
00:08:07,000 --> 00:08:08,000
而相对于CPU

192
00:08:08,000 --> 00:08:11,000
基本上是一点多倍

193
00:08:11,000 --> 00:08:13,000
GPU最重要的一个设计点

194
00:08:13,000 --> 00:08:15,000
它拥有非常多的线程

195
00:08:15,000 --> 00:08:19,000
为大量大规模任务并行而去设计的

196
00:08:19,000 --> 00:08:21,000
说GPU英伟达A100

197
00:08:21,000 --> 00:08:23,000
它是一个大型的吞吐器

198
00:08:23,000 --> 00:08:24,000
有一部分的线程

199
00:08:24,000 --> 00:08:25,000
它在等待的数据

200
00:08:25,000 --> 00:08:26,000
有一部分线程

201
00:08:26,000 --> 00:08:28,000
它在等待被激活去计算

202
00:08:28,000 --> 00:08:29,000
有一部分线程

203
00:08:29,000 --> 00:08:32,000
它已经正在计算的过程当中

204
00:08:32,000 --> 00:08:35,000
硬件设计师将所有的资源

205
00:08:35,000 --> 00:08:36,000
所有的硬件资源

206
00:08:36,000 --> 00:08:38,000
都投入到增加更多的线程当中

207
00:08:38,000 --> 00:08:39,000
而不是去减少

208
00:08:39,000 --> 00:08:42,000
办法去减少我们数据搬运的延迟

209
00:08:42,000 --> 00:08:45,000
我们只应执行延迟

210
00:08:45,000 --> 00:08:47,000
我们可以把CPU

211
00:08:47,000 --> 00:08:49,000
比喻成一台延迟机

212
00:08:49,000 --> 00:08:50,000
它的一个最大的工作

213
00:08:50,000 --> 00:08:52,000
是希望一个线程里面

214
00:08:52,000 --> 00:08:54,000
完成所有的工作

215
00:08:54,000 --> 00:08:55,000
这有一点多

216
00:08:55,000 --> 00:08:56,000
是因为我们希望

217
00:08:56,000 --> 00:08:58,000
能够用足够的线程

218
00:08:58,000 --> 00:09:00,000
去解决延迟的问题

219
00:09:00,000 --> 00:09:01,000
所以这个时候

220
00:09:01,000 --> 00:09:02,000
CPU的硬件设计子

221
00:09:02,000 --> 00:09:04,000
或者硬件设计加工师

222
00:09:04,000 --> 00:09:06,000
就会把所有的资源和重心

223
00:09:06,000 --> 00:09:09,000
都投入到减少我们的延迟上面

224
00:09:09,000 --> 00:09:11,000
这也是SIMD跟SIMT的

225
00:09:11,000 --> 00:09:13,000
一个架构最大的区别

226
00:09:13,000 --> 00:09:15,000
我们GPU不是通过增加线程

227
00:09:15,000 --> 00:09:16,000
来去解决问题

228
00:09:16,000 --> 00:09:18,000
而是使用相反的方式

229
00:09:18,000 --> 00:09:20,000
去优化我们线程的执行的

230
00:09:20,000 --> 00:09:21,000
速率和效率

231
00:09:21,000 --> 00:09:23,000
CPU跟GPU之间

232
00:09:23,000 --> 00:09:24,000
最大的区别

233
00:09:24,000 --> 00:09:26,000
也是它们的本质区别

234
00:09:26,000 --> 00:09:27,000
所以我们这里面留意

235
00:09:27,000 --> 00:09:28,000
我们在这里面

236
00:09:28,000 --> 00:09:31,000
引出一个非常重要的概念

237
00:09:31,000 --> 00:09:32,000
线程

238
00:09:32,000 --> 00:09:34,000
我们会进入到GPU的

239
00:09:36,000 --> 00:09:38,000
我们来到第三个内容

240
00:09:38,000 --> 00:09:39,000
GPU的cache

241
00:09:39,000 --> 00:09:41,000
我们的缓存机制

242
00:09:41,000 --> 00:09:42,000
GPU的一级流水

243
00:09:42,000 --> 00:09:43,000
二级流水

244
00:09:43,000 --> 00:09:44,000
三级流水

245
00:09:44,000 --> 00:09:45,000
现在我们看一下

246
00:09:45,000 --> 00:09:46,000
GPU的缓存机制

247
00:09:46,000 --> 00:09:47,000
首先

248
00:09:47,000 --> 00:09:48,000
我们希望去减少

249
00:09:48,000 --> 00:09:49,000
我们的内存的实验

250
00:09:49,000 --> 00:09:50,000
内存的搬运

251
00:09:50,000 --> 00:09:51,000
还有内存的贷款

252
00:09:51,000 --> 00:09:52,000
一些一系列

253
00:09:52,000 --> 00:09:53,000
关于内存的问题

254
00:09:53,000 --> 00:09:54,000
而这里面

255
00:09:54,000 --> 00:09:55,000
缓存对我们的内存来说

256
00:09:55,000 --> 00:09:57,000
就变得非常的重要

257
00:09:57,000 --> 00:09:58,000
HBM Memory

258
00:09:58,000 --> 00:09:59,000
有80G

259
00:09:59,000 --> 00:10:00,000
在我们的英伟达的

260
00:10:00,000 --> 00:10:01,000
A100的架构里面

261
00:10:01,000 --> 00:10:02,000
这80G

262
00:10:02,000 --> 00:10:04,000
就是我们经常谈到的

263
00:10:04,000 --> 00:10:05,000
显存

264
00:10:05,000 --> 00:10:06,000
在CPU里面

265
00:10:06,000 --> 00:10:08,000
独立的内存

266
00:10:08,000 --> 00:10:09,000
把一些寄存器文件

267
00:10:09,000 --> 00:10:10,000
we just file

268
00:10:10,000 --> 00:10:13,000
也当做我们的缓存

269
00:10:13,000 --> 00:10:14,000
我们的SM

270
00:10:14,000 --> 00:10:15,000
真正的自信单元

271
00:10:15,000 --> 00:10:16,000
是非常的近

272
00:10:16,000 --> 00:10:17,000
因为我们更SM

273
00:10:17,000 --> 00:10:19,000
我们实际的计算单元

274
00:10:19,000 --> 00:10:20,000
希望尽可能快速的

275
00:10:20,000 --> 00:10:21,000
去获取我们的数据

276
00:10:21,000 --> 00:10:22,000
于是

277
00:10:22,000 --> 00:10:23,000
就会从寄存器里面

278
00:10:23,000 --> 00:10:26,000
去读取里面的一些数据

279
00:10:26,000 --> 00:10:27,000
我们希望cache

280
00:10:27,000 --> 00:10:28,000
缓存

281
00:10:28,000 --> 00:10:29,000
离我们的内存

282
00:10:29,000 --> 00:10:30,000
或者显存

283
00:10:30,000 --> 00:10:31,000
更近

284
00:10:31,000 --> 00:10:32,000
因为我们可以

285
00:10:32,000 --> 00:10:33,000
把大量的数据

286
00:10:33,000 --> 00:10:34,000
直接搬运到我们的cache里面

287
00:10:34,000 --> 00:10:35,000
这个时候

288
00:10:35,000 --> 00:10:36,000
有两个矛盾

289
00:10:36,000 --> 00:10:37,000
于是

290
00:10:37,000 --> 00:10:38,000
GPU就设计了

291
00:10:38,000 --> 00:10:39,000
多级的缓存

292
00:10:39,000 --> 00:10:40,000
首先我们往右边

293
00:10:40,000 --> 00:10:41,000
去看一下

294
00:10:41,000 --> 00:10:42,000
对应的80G的显存

295
00:10:42,000 --> 00:10:44,000
是一个高带宽的内存

296
00:10:44,000 --> 00:10:45,000
或者我们叫做显存

297
00:10:45,000 --> 00:10:46,000
第2个

298
00:10:46,000 --> 00:10:47,000
就是我们的缓存

299
00:10:47,000 --> 00:10:49,000
真正到片内的缓存

300
00:10:49,000 --> 00:10:50,000
片内的缓存

301
00:10:50,000 --> 00:10:51,000
可能分为多级

302
00:10:51,000 --> 00:10:52,000
第1级

303
00:10:52,000 --> 00:10:53,000
就是我们是L2的cache

304
00:10:53,000 --> 00:10:55,000
离我们的显存最近的

305
00:10:55,000 --> 00:10:56,000
第2个

306
00:10:56,000 --> 00:10:57,000
就是L1的cache

307
00:10:57,000 --> 00:10:59,000
它里面的存储空间

308
00:10:59,000 --> 00:11:00,000
会更少

309
00:11:00,000 --> 00:11:02,000
只有192个KB

310
00:11:02,000 --> 00:11:04,000
都有自己独立的cache

311
00:11:04,000 --> 00:11:05,000
而真正的register

312
00:11:05,000 --> 00:11:07,000
我们也是作为cache里面的

313
00:11:07,000 --> 00:11:09,000
里面有256个KB

314
00:11:09,000 --> 00:11:11,000
每一个SM的寄存器

315
00:11:11,000 --> 00:11:13,000
在整个A100里面

316
00:11:13,000 --> 00:11:14,000
它有108个SM

317
00:11:14,000 --> 00:11:15,000
那SM是什么呢

318
00:11:15,000 --> 00:11:16,000
我们后面

319
00:11:16,000 --> 00:11:18,000
会从介绍GPU的一些

320
00:11:18,000 --> 00:11:19,000
基本的概念里面

321
00:11:19,000 --> 00:11:20,000
去给大家说

322
00:11:20,000 --> 00:11:22,000
现在大家去理解SM

323
00:11:22,000 --> 00:11:23,000
就是我们的自行单元

324
00:11:23,000 --> 00:11:24,000
就可以了

325
00:11:24,000 --> 00:11:26,000
简单理解为自行单元

326
00:11:26,000 --> 00:11:27,000
那现在我们来看看

327
00:11:27,000 --> 00:11:30,000
带宽和实验的一个问题

328
00:11:30,000 --> 00:11:32,000
主内存是具有一些

329
00:11:32,000 --> 00:11:35,000
高带宽的HBM的内存

330
00:11:35,000 --> 00:11:37,000
主内存作为内存带宽的

331
00:11:37,000 --> 00:11:38,000
基本单位

332
00:11:38,000 --> 00:11:39,000
来去看一下

333
00:11:39,000 --> 00:11:40,000
其他的传输数据

334
00:11:40,000 --> 00:11:41,000
有多少

335
00:11:41,000 --> 00:11:42,000
可以看到

336
00:11:42,000 --> 00:11:44,000
主内存带宽是它的三倍

337
00:11:44,000 --> 00:11:45,000
而LE款存的带宽

338
00:11:45,000 --> 00:11:47,000
是它的13倍

339
00:11:47,000 --> 00:11:48,000
上的时候

340
00:11:48,000 --> 00:11:49,000
我们希望款存

341
00:11:49,000 --> 00:11:51,000
能够尽快的去用完

342
00:11:51,000 --> 00:11:53,000
然后换下一批数据上来

343
00:11:53,000 --> 00:11:54,000
那这个时候

344
00:11:54,000 --> 00:11:56,000
我们就会遇到实验的问题

345
00:11:56,000 --> 00:11:58,000
我们从上往下去看一下

346
00:11:58,000 --> 00:11:59,000
首先

347
00:11:59,000 --> 00:12:01,000
离我们的SM最近的是

348
00:12:01,000 --> 00:12:02,000
LE的款存

349
00:12:02,000 --> 00:12:03,000
LE的cache

350
00:12:03,000 --> 00:12:04,000
所以基本单位

351
00:12:04,000 --> 00:12:05,000
我们的LE的cache的延迟

352
00:12:05,000 --> 00:12:06,000
就是5倍

353
00:12:06,000 --> 00:12:08,000
而对应的HBM的延迟

354
00:12:08,000 --> 00:12:09,000
就会更高

355
00:12:09,000 --> 00:12:11,000
我们就会很清楚

356
00:12:11,000 --> 00:12:12,000
理解到

357
00:12:12,000 --> 00:12:13,000
为什么我们的GPU里面

358
00:12:13,000 --> 00:12:15,000
有单独的显存

359
00:12:15,000 --> 00:12:16,000
因为我们的延迟

360
00:12:16,000 --> 00:12:17,000
实在是太高了

361
00:12:17,000 --> 00:12:20,000
假设我把CPU里面的DBM的数据

362
00:12:20,000 --> 00:12:21,000
传过来

363
00:12:21,000 --> 00:12:22,000
给我们的GPU进行计算

364
00:12:22,000 --> 00:12:23,000
那我们的实验

365
00:12:23,000 --> 00:12:24,000
会非常非常的高

366
00:12:24,000 --> 00:12:25,000
完全跟不上

367
00:12:25,000 --> 00:12:26,000
我们的计算的速度

368
00:12:26,000 --> 00:12:27,000
还有跟不上

369
00:12:27,000 --> 00:12:29,000
我们的带宽的传输的速度

370
00:12:29,000 --> 00:12:30,000
那这个时候

371
00:12:30,000 --> 00:12:31,000
我们就要求GPU里面

372
00:12:31,000 --> 00:12:33,000
有自己的一个

373
00:12:33,000 --> 00:12:35,000
高带宽的内存

374
00:12:35,000 --> 00:12:37,000
high bandwidth memory

375
00:12:37,000 --> 00:12:39,000
所以我们叫做HBM

376
00:12:40,000 --> 00:12:41,000
就通过PCIe

377
00:12:41,000 --> 00:12:44,000
来去对我们的数据进行传输

378
00:12:44,000 --> 00:12:46,000
把CPU里面的数据

379
00:12:46,000 --> 00:12:47,000
DBM里面的数据

380
00:12:47,000 --> 00:12:51,000
传输到GPU里面的HBM

381
00:12:51,000 --> 00:12:52,000
计算强度

382
00:12:52,000 --> 00:12:53,000
假设为100

383
00:12:53,000 --> 00:12:55,000
那LE的缓存的计算强度

384
00:12:55,000 --> 00:12:56,000
就会好很多

385
00:12:56,000 --> 00:12:57,000
只为39

386
00:12:57,000 --> 00:12:58,000
那39就意味着

387
00:12:58,000 --> 00:12:59,000
每个数据

388
00:12:59,000 --> 00:13:01,000
只需要执行39个操作

389
00:13:01,000 --> 00:13:03,000
而LE的缓存更少

390
00:13:03,000 --> 00:13:05,000
它的计算强度更少

391
00:13:05,000 --> 00:13:06,000
只需要8个操作

392
00:13:06,000 --> 00:13:07,000
8个操作

393
00:13:07,000 --> 00:13:08,000
这个时候

394
00:13:08,000 --> 00:13:09,000
对于我们的硬件来说

395
00:13:09,000 --> 00:13:11,000
是非常容易实现的

396
00:13:11,000 --> 00:13:12,000
这也是了

397
00:13:12,000 --> 00:13:13,000
为什么LE缓存

398
00:13:13,000 --> 00:13:15,000
LE缓存和寄存器

399
00:13:15,000 --> 00:13:16,000
对GPU来说

400
00:13:16,000 --> 00:13:17,000
这么的重要

401
00:13:17,000 --> 00:13:18,000
我们可以把数据

402
00:13:18,000 --> 00:13:19,000
放在LE缓存里面

403
00:13:19,000 --> 00:13:22,000
然后对数据进行8个操作

404
00:13:22,000 --> 00:13:23,000
使得我们的计算

405
00:13:23,000 --> 00:13:24,000
我们的GPU

406
00:13:24,000 --> 00:13:25,000
达到了一个饱和的状态

407
00:13:25,000 --> 00:13:27,000
让我们的GPU里面

408
00:13:27,000 --> 00:13:30,000
SM具体的算力利用率更高

409
00:13:30,000 --> 00:13:31,000
往下看看

410
00:13:31,000 --> 00:13:32,000
PCIe的带宽

411
00:13:32,000 --> 00:13:34,000
就变得非常的糟糕

412
00:13:34,000 --> 00:13:35,000
它的整体的时延

413
00:13:35,000 --> 00:13:37,000
延迟也会非常的高

414
00:13:37,000 --> 00:13:39,000
整体的算力的利用率

415
00:13:39,000 --> 00:13:40,000
也会很低

416
00:13:40,000 --> 00:13:41,000
算力的强度

417
00:13:41,000 --> 00:13:44,000
计算的强度太高了

418
00:13:44,000 --> 00:13:45,000
增加的同时

419
00:13:45,000 --> 00:13:47,000
我们的线程的数量

420
00:13:47,000 --> 00:13:48,000
或者线程的请求数

421
00:13:48,000 --> 00:13:49,000
也对

422
00:13:49,000 --> 00:13:50,000
增加这个时候

423
00:13:50,000 --> 00:13:51,000
才能够处理

424
00:13:51,000 --> 00:13:52,000
我们刚才所说到的

425
00:13:52,000 --> 00:13:54,000
我们的并行的操作

426
00:13:54,000 --> 00:13:55,000
每个线程

427
00:13:55,000 --> 00:13:57,000
执行一个对应的数据

428
00:13:57,000 --> 00:13:58,000
才能够把我们的算力利用率

429
00:13:58,000 --> 00:13:59,000
提升上去

430
00:13:59,000 --> 00:14:00,000
所以说

431
00:14:00,000 --> 00:14:01,000
数量足够多

432
00:14:01,000 --> 00:14:03,000
才能够让整个系统的内存

433
00:14:03,000 --> 00:14:05,000
处于忙碌的状态

434
00:14:05,000 --> 00:14:06,000
让我们的计算

435
00:14:06,000 --> 00:14:08,000
也处于忙碌的状态

436
00:14:08,000 --> 00:14:09,000
因此我们看到

437
00:14:09,000 --> 00:14:11,000
GPU里面的线程数

438
00:14:11,000 --> 00:14:14,000
是非常非常的多

439
00:14:14,000 --> 00:14:15,000
到线程了

440
00:14:15,000 --> 00:14:16,000
我们接下来看看

441
00:14:16,000 --> 00:14:17,000
GPU的线程

442
00:14:17,000 --> 00:14:18,000
它的整体原理

443
00:14:18,000 --> 00:14:19,000
下面这个就是

444
00:14:19,000 --> 00:14:22,000
GPU的一个线程的机制

445
00:14:22,000 --> 00:14:23,000
或者GPU的整体的架构

446
00:14:23,000 --> 00:14:25,000
左边就是GPU大的架构

447
00:14:25,000 --> 00:14:28,000
右边我们打开其中一个SM

448
00:14:28,000 --> 00:14:30,000
看一下里面有哪些内容

449
00:14:30,000 --> 00:14:31,000
刚才我们讲到了

450
00:14:31,000 --> 00:14:32,000
GPU其实里面

451
00:14:32,000 --> 00:14:34,000
有大量的SM

452
00:14:34,000 --> 00:14:35,000
SM我们可以看做

453
00:14:35,000 --> 00:14:37,000
一个基本的运算的单元

454
00:14:37,000 --> 00:14:38,000
GPU里面

455
00:14:38,000 --> 00:14:40,000
我们在一个时钟周期内

456
00:14:40,000 --> 00:14:42,000
可以执行多个web

457
00:14:42,000 --> 00:14:45,000
我们后面会单独的去展开

458
00:14:45,000 --> 00:14:47,000
里面有64个web

459
00:14:47,000 --> 00:14:50,000
它可以进行一个并发的执行

460
00:14:50,000 --> 00:14:52,000
尝试主要是增加线程

461
00:14:52,000 --> 00:14:54,000
增加web来解决

462
00:14:54,000 --> 00:14:56,000
或者掩盖我们的延迟的问题

463
00:14:56,000 --> 00:14:58,000
而不是去减少

464
00:14:58,000 --> 00:15:00,000
我们的延迟的时间

465
00:15:00,000 --> 00:15:02,000
可以有更多的线程

466
00:15:02,000 --> 00:15:03,000
去解决我们的问题了

467
00:15:03,000 --> 00:15:05,000
下面我们来看一下

468
00:15:05,000 --> 00:15:06,000
在每一个SM

469
00:15:06,000 --> 00:15:09,000
一共有20480个

470
00:15:09,000 --> 00:15:12,000
整个A100有20多万个线程

471
00:15:12,000 --> 00:15:14,000
可以提供非常多的线程给我们

472
00:15:14,000 --> 00:15:15,000
而这些大量的线程

473
00:15:15,000 --> 00:15:16,000
我们的程序

474
00:15:16,000 --> 00:15:18,000
是用不完我们的线程的

475
00:15:18,000 --> 00:15:19,000
这个是我们刚才讲到的

476
00:15:19,000 --> 00:15:20,000
有一些线程

477
00:15:20,000 --> 00:15:22,000
它处于计算的过程当中

478
00:15:22,000 --> 00:15:23,000
有一些线程

479
00:15:23,000 --> 00:15:24,000
它在搬运数据

480
00:15:24,000 --> 00:15:26,000
另外还有一些线程

481
00:15:26,000 --> 00:15:29,000
它在同步的等待计算

482
00:15:29,000 --> 00:15:30,000
很多时候会看到

483
00:15:30,000 --> 00:15:32,000
GPU它的算力利用率

484
00:15:32,000 --> 00:15:34,000
并不是非常的高

485
00:15:34,000 --> 00:15:36,000
但是我们完全不觉得它慢

486
00:15:36,000 --> 00:15:37,000
是因为我们的线程

487
00:15:37,000 --> 00:15:38,000
是超配的

488
00:15:38,000 --> 00:15:39,000
远远超过于

489
00:15:39,000 --> 00:15:43,000
大部分应用程序的使用范围

490
00:15:43,000 --> 00:15:44,000
可以在不同的web上面

491
00:15:44,000 --> 00:15:46,000
进行一个调度

492
00:15:46,000 --> 00:15:48,000
在GPU的工作本质里面

493
00:15:48,000 --> 00:15:50,000
我们来到了最后一个内容

494
00:15:50,000 --> 00:15:52,000
或者最后一个总结了

495
00:15:52,000 --> 00:15:54,000
CPU跟GPU的本质的区别

496
00:15:54,000 --> 00:15:56,000
主要是并行的问题

497
00:15:56,000 --> 00:15:57,000
而不是并发的问题

498
00:15:57,000 --> 00:16:00,000
大量的线程提供并行的能力

499
00:16:00,000 --> 00:16:02,000
而CPU就到这里为止

500
00:16:02,000 --> 00:16:03,000
我们简单的总结一下

501
00:16:03,000 --> 00:16:05,000
GPU的工作原理

502
00:16:05,000 --> 00:16:07,000
首先我们以AX加Y这个例子

503
00:16:07,000 --> 00:16:10,000
去了解一下并发和并行的区别

504
00:16:10,000 --> 00:16:11,000
还有串行的区别

505
00:16:11,000 --> 00:16:14,000
也就是CPU跟GPU最大的一个不一样

506
00:16:14,000 --> 00:16:17,000
GPU提供并行的机制

507
00:16:17,000 --> 00:16:19,000
下面我们就了解到了

508
00:16:19,000 --> 00:16:21,000
GPU的整体的缓存的机制

509
00:16:21,000 --> 00:16:23,000
通过多级的缓存

510
00:16:23,000 --> 00:16:24,000
多级的流水

511
00:16:24,000 --> 00:16:25,000
多级的cache

512
00:16:25,000 --> 00:16:27,000
我们把整个GPU的内存

513
00:16:27,000 --> 00:16:28,000
把GPU的数据

514
00:16:28,000 --> 00:16:30,000
充分的利用起来

515
00:16:30,000 --> 00:16:31,000
有了这一点

516
00:16:31,000 --> 00:16:32,000
我们怎么去执行

517
00:16:32,000 --> 00:16:33,000
那么大量的数据

518
00:16:33,000 --> 00:16:34,000
于是就引起了

519
00:16:34,000 --> 00:16:36,000
GPU的线程的原理

520
00:16:36,000 --> 00:16:38,000
GPU里面提供了大量的线程

521
00:16:38,000 --> 00:16:39,000
超配的线程

522
00:16:39,000 --> 00:16:40,000
去完成

523
00:16:40,000 --> 00:16:44,000
对不同层级的数据的搬运和计算

524
00:16:44,000 --> 00:16:46,000
今天的内容就到这里为止

525
00:16:46,000 --> 00:16:47,000
谢谢各位

526
00:16:47,000 --> 00:16:48,000
拜了个拜

527
00:16:48,000 --> 00:16:49,000
卷的不行了

528
00:16:49,000 --> 00:16:50,000
卷的不行了

529
00:16:50,000 --> 00:16:52,000
记得一键三连加关注

530
00:16:52,000 --> 00:16:53,000
所有的内容都会开源

531
00:16:53,000 --> 00:16:55,000
在下面这条链接里面

532
00:16:55,000 --> 00:16:56,000
拜拜

