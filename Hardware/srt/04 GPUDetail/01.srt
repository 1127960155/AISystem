1
00:00:00,000 --> 00:00:09,800
哈喽大家好我是上班不更新更新服务上班的周米

2
00:00:09,800 --> 00:00:13,600
这里面的所有的视频呢都是我用业余的时间来去搞的

3
00:00:13,600 --> 00:00:17,100
所以我的本质工作呢不是一位讲师

4
00:00:17,100 --> 00:00:19,200
我是一名研发的工程师

5
00:00:19,200 --> 00:00:22,100
来给大家去汇报AI芯片里面GPU详解

6
00:00:22,100 --> 00:00:27,400
里面最核心或者周米最关心的一部分就是Tensor Core的原理

7
00:00:27,600 --> 00:00:31,300
整个PPT里面的一共有50多页

8
00:00:31,300 --> 00:00:34,400
也是周米写过最长的一个PPT了

9
00:00:34,400 --> 00:00:36,300
关于这个系列里面

10
00:00:36,300 --> 00:00:38,700
现在我们看到整个英伟达的GPU架构

11
00:00:38,700 --> 00:00:40,800
已经来到了最后的两个内容

12
00:00:40,800 --> 00:00:42,000
第一个就是Tensor Core

13
00:00:42,000 --> 00:00:43,300
第二个就是NVLink

14
00:00:43,300 --> 00:00:46,200
而Tensor Core也是整个英伟达GPU架构里面

15
00:00:46,200 --> 00:00:47,700
最核心的一部分

16
00:00:47,700 --> 00:00:51,500
今天我们会给大家展开三个内容去汇报

17
00:00:51,500 --> 00:00:55,500
第一个内容就是卷迹跟Tensor Core之间的关系

18
00:00:55,600 --> 00:00:58,900
把软件的卷迹跟Tensor Core硬件结合起来

19
00:00:58,900 --> 00:01:01,700
接着我们去看看Tensor Core的基本原理

20
00:01:01,700 --> 00:01:04,200
它是怎么去组成怎么去运作的

21
00:01:04,200 --> 00:01:07,200
最后我们来看看Tensor Core的架构的演进

22
00:01:07,200 --> 00:01:09,900
从第一代的Tensor Core到Hope架构的Tensor Core

23
00:01:09,900 --> 00:01:11,500
它到底有什么不一样

24
00:01:11,500 --> 00:01:13,300
你以为到这里面就结束了

25
00:01:13,300 --> 00:01:13,900
不是

26
00:01:13,900 --> 00:01:14,700
我们在后面

27
00:01:26,500 --> 00:01:29,400
再回顾一下英伟达GPU的整个架构的发展

28
00:01:29,400 --> 00:01:32,500
从2010年的Fame到2022年的Hope

29
00:01:32,500 --> 00:01:33,500
经历了十二年

30
00:01:33,500 --> 00:01:36,100
十二年出现了九代的架构里面

31
00:01:36,100 --> 00:01:38,000
从2017年的福特架构开始

32
00:01:38,000 --> 00:01:40,500
就出现了第一代的Tensor Core

33
00:01:40,500 --> 00:01:43,100
从第一代的Tensor Core开始

34
00:01:43,100 --> 00:01:46,300
架构都会对Tensor Core进行一个

35
00:01:46,300 --> 00:01:48,900
我们来看看具体这些更新有什么不一样

36
00:01:48,900 --> 00:01:50,100
现在我们来到

37
00:01:50,100 --> 00:01:52,500
我们来到了第一个内容

38
00:01:53,300 --> 00:01:55,500
我们来到了第一个内容

39
00:01:55,500 --> 00:01:57,200
我们的卷积计算

40
00:01:57,200 --> 00:01:58,500
其实在之前

41
00:01:58,500 --> 00:02:00,100
周米的一系列的视频里面

42
00:02:00,100 --> 00:02:01,700
就讲过克诺的优化

43
00:02:01,700 --> 00:02:04,000
克诺的优化就去给大家去讲讲

44
00:02:04,000 --> 00:02:06,300
实际上与卷积神经网络的计算

45
00:02:06,300 --> 00:02:08,400
并不是简单的通过滑窗的方式

46
00:02:08,400 --> 00:02:10,500
进行滑动计算的

47
00:02:10,500 --> 00:02:12,000
也就面的这个图

48
00:02:12,000 --> 00:02:13,900
而真正的计算是通过

49
00:02:13,900 --> 00:02:17,600
卷通过GN或者Image2Clump的方式

50
00:02:17,600 --> 00:02:19,400
进行矩阵相乘计算的

51
00:02:19,400 --> 00:02:20,800
我们可以看回我们之前

52
00:02:20,900 --> 00:02:23,100
给大家分享的视频

53
00:02:23,100 --> 00:02:24,300
接着

54
00:02:24,300 --> 00:02:24,900
好了好了

55
00:02:24,900 --> 00:02:27,100
我们现在来到第二个内容

56
00:02:27,100 --> 00:02:29,700
也是正式进入到我们本节的内容

57
00:02:29,700 --> 00:02:31,600
Tensor Core的基本原理

58
00:02:31,600 --> 00:02:32,100
首先呢

59
00:02:32,100 --> 00:02:35,900
我要提一个问题就是什么是混合精度

60
00:02:35,900 --> 00:02:36,900
你好啊

61
00:02:36,900 --> 00:02:39,500
混合精度不是指网络模型里面

62
00:02:39,500 --> 00:02:42,800
就有FP16又有FP32吗

63
00:02:42,800 --> 00:02:44,800
这个这个答案是错的

64
00:02:44,800 --> 00:02:45,800
实际上啊

65
00:02:45,800 --> 00:02:48,300
我们这里面指的混合精度呢

66
00:02:48,400 --> 00:02:51,000
是指我们在底层硬件算子层面呢

67
00:02:51,000 --> 00:02:53,400
我们使用FP32作为输入输出

68
00:02:53,400 --> 00:02:54,500
就input output

69
00:02:54,500 --> 00:02:55,200
然后呢

70
00:02:55,200 --> 00:02:58,600
使用FP32作为中间结果进行存储

71
00:02:58,600 --> 00:02:59,100
从而呢

72
00:02:59,100 --> 00:03:01,000
不减少我们的训练过程的精度呢

73
00:03:01,000 --> 00:03:02,100
不降低

74
00:03:02,100 --> 00:03:03,700
这个硬件的实现呢

75
00:03:03,700 --> 00:03:05,200
主要就是Tensor Core

76
00:03:05,200 --> 00:03:07,700
可以看到FP16对比FP32

77
00:03:07,700 --> 00:03:08,500
整体的表示

78
00:03:08,500 --> 00:03:09,900
对小数位来看呢

79
00:03:09,900 --> 00:03:11,100
它所表示的范围呢

80
00:03:11,100 --> 00:03:12,200
要小很多

81
00:03:14,300 --> 00:03:17,000
来到了福特架构的第一代Tensor Core

82
00:03:17,100 --> 00:03:18,600
可以看到里面SL里面呢

83
00:03:18,600 --> 00:03:20,900
就有了很多个Tensor Core

84
00:03:20,900 --> 00:03:23,100
左边对应的就是CUDA Core

85
00:03:23,100 --> 00:03:25,400
而CUDA Core以前去计算FMA

86
00:03:25,400 --> 00:03:27,000
也就是Fused Map Mount N

87
00:03:28,000 --> 00:03:28,800
Fused Map Mount N

88
00:03:28,800 --> 00:03:30,600
我们需要来回的去把数据搬运

89
00:03:30,600 --> 00:03:31,900
进行一个层架的时候呢

90
00:03:31,900 --> 00:03:33,300
就要继续寄存器

91
00:03:33,300 --> 00:03:34,600
到ALU进行层

92
00:03:34,600 --> 00:03:35,500
到寄存器

93
00:03:35,500 --> 00:03:36,900
到ALU进行加

94
00:03:36,900 --> 00:03:38,200
然后到存回ALU

95
00:03:38,200 --> 00:03:38,900
整体来说

96
00:03:38,900 --> 00:03:40,700
我们要来回的去搬运数据

97
00:03:40,700 --> 00:03:41,200
但是呢

98
00:03:41,200 --> 00:03:43,300
引入了Tensor Core之后呢

99
00:03:44,100 --> 00:03:45,100
变成了矩阵乘法

100
00:03:45,100 --> 00:03:47,600
累加独立的单元

101
00:03:48,500 --> 00:03:49,800
进行加速

102
00:03:49,800 --> 00:03:52,000
那现在我们可以有8组Tensor Core

103
00:03:52,000 --> 00:03:53,100
而每一个Tensor Core呢

104
00:03:53,100 --> 00:03:54,400
在每个时钟周期内呢

105
00:03:54,400 --> 00:03:57,000
能执行4×4×4的GMM

106
00:03:57,000 --> 00:03:58,900
也就是64个FMA

107
00:03:58,900 --> 00:03:59,700
那整体来说呢

108
00:03:59,700 --> 00:04:03,700
就执行A×B加C等于D

109
00:04:03,700 --> 00:04:04,800
这么一个简单的运算

110
00:04:04,800 --> 00:04:05,300
其中呢

111
00:04:05,300 --> 00:04:07,100
ABC和D呢

112
00:04:07,100 --> 00:04:08,700
都是一个4×4的矩阵

113
00:04:08,700 --> 00:04:10,100
矩阵乘法里面的数呢

114
00:04:10,100 --> 00:04:12,300
A和B可以是Fp16

115
00:04:13,400 --> 00:04:15,300
那C和得到的结果D呢

116
00:04:15,300 --> 00:04:17,700
可以是Fp16或者Fp32

117
00:04:18,600 --> 00:04:19,700
底层硬件Tensor Core呢

118
00:04:19,700 --> 00:04:21,300
它是一个混合精度的计算

119
00:04:24,100 --> 00:04:24,700
A系层

120
00:04:24,700 --> 00:04:25,100
其实呢

121
00:04:25,100 --> 00:04:25,800
每个Tensor Core

122
00:04:25,800 --> 00:04:26,600
每个时钟周期

123
00:04:26,600 --> 00:04:27,100
刚才说了

124
00:04:27,100 --> 00:04:29,300
可以执行64个FMA的

125
00:04:29,300 --> 00:04:30,300
混合精度计算

126
00:04:30,300 --> 00:04:31,200
那SM里面呢

127
00:04:31,200 --> 00:04:32,800
一共有8个Tensor Core

128
00:04:32,800 --> 00:04:34,300
所以每个时钟周期内呢

129
00:04:34,300 --> 00:04:37,700
我们一共可以执行512个浮点运算的

130
00:04:37,700 --> 00:04:39,200
具体大家可以自己算一算

131
00:04:40,100 --> 00:04:41,400
福特的GPU呢

132
00:04:41,400 --> 00:04:42,800
吞吐量

133
00:04:42,800 --> 00:04:44,600
对于计算矩阵乘和累加呢

134
00:04:44,600 --> 00:04:46,300
会比PASCAL

135
00:04:46,300 --> 00:04:48,300
那每个SM的AI吞吐量呢

136
00:04:48,300 --> 00:04:49,300
增加了8倍

137
00:04:49,300 --> 00:04:50,700
总共增加12倍

138
00:04:50,700 --> 00:04:52,600
因为我们的SM会更多嘛

139
00:04:52,600 --> 00:04:53,700
因为我们SM的数呢

140
00:04:53,700 --> 00:04:55,800
是Fp16两个矩阵进行相乘

141
00:04:55,800 --> 00:04:56,400
然后呢

142
00:04:56,400 --> 00:04:58,200
进行Fp32的累加

143
00:04:58,200 --> 00:04:59,300
最后存储的时候呢

144
00:04:59,300 --> 00:05:01,600
是进行Fp32的存储方式

145
00:05:03,800 --> 00:05:04,800
根据网络里面的

146
00:05:04,800 --> 00:05:07,200
不仅仅只有一个矩阵乘这么简单

147
00:05:07,200 --> 00:05:09,000
我们在训练的过程当中呢

148
00:05:09,000 --> 00:05:11,900
就会遇到我们的卷积跟激活进行相乘

149
00:05:11,900 --> 00:05:13,700
得到另外一个新的feature map

150
00:05:13,700 --> 00:05:15,200
我们可以稍微一点看看

151
00:05:15,200 --> 00:05:17,100
我们的箭头是逆向过来的

152
00:05:18,400 --> 00:05:19,800
那就是反向传播的时候

153
00:05:19,800 --> 00:05:21,800
我们的权重还有激活层呢

154
00:05:21,800 --> 00:05:23,400
需要进行反向的计算

155
00:05:24,300 --> 00:05:25,800
方向呢是刚好相反的

156
00:05:25,800 --> 00:05:26,200
另外呢

157
00:05:26,200 --> 00:05:28,500
还有一种专门针对激活函数

158
00:05:28,500 --> 00:05:30,000
还有激活的梯度呢

159
00:05:30,000 --> 00:05:31,400
进行反向的计算

160
00:05:31,400 --> 00:05:32,000
这种呢

161
00:05:32,000 --> 00:05:33,000
不管在过程当中呢

162
00:05:33,000 --> 00:05:35,300
这里面有大量的绿色都是Fp16的

163
00:05:35,300 --> 00:05:36,600
真正存储的时候呢

164
00:05:36,600 --> 00:05:38,700
是使用Fp32进行存储的

165
00:05:38,700 --> 00:05:41,700
所以明确的混合精度训练

166
00:05:45,200 --> 00:05:47,000
现在我们来到了第三个内容

167
00:05:47,000 --> 00:05:48,500
也就是我插播的一个内容

168
00:05:48,500 --> 00:05:50,700
tensor core跟cuda programming

169
00:05:50,700 --> 00:05:52,100
之间的一个关系

170
00:05:52,100 --> 00:05:53,300
它里面呢

171
00:05:53,300 --> 00:05:55,700
其实我们并不是控制每一条弯弯的线呢

172
00:05:55,700 --> 00:05:56,900
进行线程控制的

173
00:05:56,900 --> 00:05:58,300
而是通过控制一个web

174
00:05:58,300 --> 00:06:01,300
一个web呢就包含很多个线程

175
00:06:01,300 --> 00:06:03,400
一个web并行并发的去执行

176
00:06:03,400 --> 00:06:04,800
那在真正执行的时候呢

177
00:06:04,800 --> 00:06:07,000
我们会做一个web同步的操作

178
00:06:07,000 --> 00:06:08,700
把所有的线程呢都进行同步

179
00:06:08,700 --> 00:06:10,100
然后获取同样的数据

180
00:06:10,100 --> 00:06:10,500
接着呢

181
00:06:10,500 --> 00:06:14,200
进行一个16x16的矩阵相乘和矩阵计算

182
00:06:14,200 --> 00:06:14,700
最后呢

183
00:06:14,700 --> 00:06:17,300
再把结果呢存储在不同的web上面

184
00:06:17,300 --> 00:06:17,700
web呢

185
00:06:17,700 --> 00:06:20,800
就是在软件上面做一个大的线程的概念

186
00:06:22,500 --> 00:06:23,900
在程序执行的过程当中呢

187
00:06:23,900 --> 00:06:26,100
我们可以通过线程的web来去调度

188
00:06:26,100 --> 00:06:27,000
我们的tensor core

189
00:06:27,000 --> 00:06:28,400
在一个web线程里面呢

190
00:06:28,400 --> 00:06:32,100
通过tensor core来提供一个16x16x16的

191
00:06:32,100 --> 00:06:33,300
矩阵运算

192
00:06:33,300 --> 00:06:33,800
哎

193
00:06:33,800 --> 00:06:36,000
刚才的tensor core不是4x4x4吗

194
00:06:36,000 --> 00:06:38,700
现在怎么变成16x16x16了

195
00:06:38,700 --> 00:06:39,300
不着急

196
00:06:39,300 --> 00:06:40,600
我们后面会展开的

197
00:06:42,200 --> 00:06:44,000
quota通过tensor core进行编程的

198
00:06:44,000 --> 00:06:45,600
我们通过web来提供quota

199
00:06:45,600 --> 00:06:49,900
C++ WMMA的API对外提供给我们的开发者

200
00:06:49,900 --> 00:06:51,200
这里面的WMMA呢

201
00:06:51,200 --> 00:06:53,600
主要是专门针对我们的tensor core

202
00:06:53,600 --> 00:06:55,300
进行矩阵的加载了

203
00:06:55,300 --> 00:06:55,900
存储了

204
00:06:55,900 --> 00:06:57,300
还有具体的计算

205
00:06:57,300 --> 00:06:58,400
那MMA sync呢

206
00:06:58,400 --> 00:06:59,700
这个就是具体的计算

207
00:06:59,700 --> 00:07:00,600
后面有个sync呢

208
00:07:00,600 --> 00:07:02,000
就是我们刚才提到的

209
00:07:02,000 --> 00:07:02,800
所有的web呢

210
00:07:02,800 --> 00:07:04,300
之间需要进行同步的

211
00:07:04,300 --> 00:07:05,100
现在我们可以看

212
00:07:05,200 --> 00:07:06,400
其实呢我们整体看一下

213
00:07:06,400 --> 00:07:08,500
其实刚才提到的tensor core是一个

214
00:07:08,500 --> 00:07:10,600
4x4的tensor core的核

215
00:07:10,600 --> 00:07:11,300
但实际上呢

216
00:07:11,300 --> 00:07:13,000
我们有多个tensor core

217
00:07:13,000 --> 00:07:14,800
我们不可能最犀利度的去控制

218
00:07:14,800 --> 00:07:15,700
每一个tensor core

219
00:07:15,700 --> 00:07:16,900
这样的效率会很低

220
00:07:16,900 --> 00:07:17,300
于是呢

221
00:07:17,300 --> 00:07:17,800
一个web呢

222
00:07:17,800 --> 00:07:20,000
就帮我们把好几个tensor core包装起来

223
00:07:21,200 --> 00:07:25,000
16x16x16的一个web level的卷积的指令

224
00:07:25,000 --> 00:07:25,700
那这个指令呢

225
00:07:25,700 --> 00:07:27,700
最后通过MMA sync呢

226
00:07:27,700 --> 00:07:29,200
这个API进行

227
00:07:30,300 --> 00:07:32,300
我们看看具体的quota代码

228
00:07:35,600 --> 00:07:37,000
再投了我们input的MMA

229
00:07:37,000 --> 00:07:38,500
然后类似MVquota

230
00:07:38,500 --> 00:07:39,500
在MVquota里面呢

231
00:07:39,500 --> 00:07:41,400
我们首先要声明有些fragment

232
00:07:41,400 --> 00:07:42,600
fragment就是我们的片段

233
00:07:42,600 --> 00:07:44,200
或者我们存储的数据

234
00:07:44,200 --> 00:07:44,800
这里面呢

235
00:07:44,800 --> 00:07:47,200
还会呈现的16x16的一个web level

236
00:07:47,200 --> 00:07:47,600
接着呢

237
00:07:47,600 --> 00:07:49,100
我们会指挥一个输出矩阵

238
00:07:49,100 --> 00:07:49,700
然后呢

239
00:07:49,700 --> 00:07:52,000
从内存里面加载A和B两个矩阵

240
00:07:52,000 --> 00:07:52,700
然后执行

241
00:07:52,700 --> 00:07:54,800
真正执行WMMA的计算

242
00:07:54,800 --> 00:07:55,600
计算完之后呢

243
00:07:55,600 --> 00:07:57,500
就把结果存储回来

244
00:07:57,500 --> 00:07:59,100
存储到我们的C里面

245
00:08:00,100 --> 00:08:00,900
底层计算呢

246
00:08:00,900 --> 00:08:01,900
就是这么简单

247
00:08:02,600 --> 00:08:05,700
钟鸣老师你好啊

248
00:08:05,700 --> 00:08:07,300
我现在有个问题啊

249
00:08:07,300 --> 00:08:09,700
就你讲了tensorflow跟quota之间的硬是

250
00:08:09,700 --> 00:08:12,300
我大概简单懂了一个16x16的

251
00:08:12,300 --> 00:08:14,700
跟4x4之间是怎么运营

252
00:08:14,700 --> 00:08:15,400
但是呢

253
00:08:16,600 --> 00:08:16,900
是呢

254
00:08:16,900 --> 00:08:19,000
提供4x4这么小的一个kernel

255
00:08:19,000 --> 00:08:21,400
或者16x16这么小的一个kernel

256
00:08:21,400 --> 00:08:24,100
我们怎么去处理像wasnet这种input image

257
00:08:24,100 --> 00:08:26,200
就是输入的图像要24x24

258
00:08:26,200 --> 00:08:28,600
kernel等于7x7的gmm呢

259
00:08:28,600 --> 00:08:31,700
或者在现在的大模型传输码结构里面呢

260
00:08:31,700 --> 00:08:34,700
一个input embedded就2x2048

261
00:08:34,700 --> 00:08:37,000
hidden size是1024x1024

262
00:08:37,000 --> 00:08:38,500
这么大的一个gmm

263
00:08:39,300 --> 00:08:41,000
因为到GPU这么小的一个

264
00:08:41,000 --> 00:08:42,600
Tensorflow里面去处理吗

265
00:08:43,600 --> 00:08:43,900
哎

266
00:08:43,900 --> 00:08:46,000
你前面提到的这个问题呢

267
00:08:46,000 --> 00:08:46,700
非常有意思

268
00:08:46,700 --> 00:08:47,900
我们现在看一看了

269
00:08:47,900 --> 00:08:48,500
实际上呢

270
00:08:48,500 --> 00:08:50,200
我们刚才提到了所有的卷积计算呢

271
00:08:50,200 --> 00:08:52,200
会变成gmm矩阵层的方式

272
00:08:52,200 --> 00:08:52,900
那矩阵层呢

273
00:08:52,900 --> 00:08:54,400
我们现在有一个蓝色的矩阵

274
00:08:54,400 --> 00:08:55,700
和一个黄色的矩阵

275
00:08:55,700 --> 00:08:57,500
两个相乘的到我们的绿色的矩阵

276
00:08:57,500 --> 00:08:58,500
但实际计算的时候呢

277
00:08:58,500 --> 00:09:00,000
我们会取片段的数据

278
00:09:00,000 --> 00:09:01,200
也就是我们的fragment

279
00:09:01,300 --> 00:09:02,800
取到了fragment这条长长的

280
00:09:02,800 --> 00:09:03,700
和横横的

281
00:09:03,700 --> 00:09:05,400
就变成我们fragment box

282
00:09:05,400 --> 00:09:06,800
就是我们的线程块

283
00:09:06,800 --> 00:09:08,300
在具体硬件变成线程块

284
00:09:08,300 --> 00:09:10,000
在真正线程块执行的时候呢

285
00:09:10,000 --> 00:09:11,300
我们就会把这里面的

286
00:09:11,300 --> 00:09:12,700
其中一部分数据

287
00:09:12,700 --> 00:09:13,600
再提取出来

288
00:09:13,600 --> 00:09:15,400
变成我们level的计算

289
00:09:15,400 --> 00:09:16,700
math level的计算呢

290
00:09:16,700 --> 00:09:17,900
其实还是很大了

291
00:09:17,900 --> 00:09:18,200
于是呢

292
00:09:18,200 --> 00:09:20,400
我们在真正fragment执行的时候呢

293
00:09:20,400 --> 00:09:21,600
又会把它变成

294
00:09:21,600 --> 00:09:22,500
提取出来

295
00:09:22,500 --> 00:09:23,700
矩阵数的计算了

296
00:09:23,700 --> 00:09:25,200
那从这里总结一句话

297
00:09:25,200 --> 00:09:26,300
就是从简单的

298
00:09:26,300 --> 00:09:27,300
我们看到矩阵层

299
00:09:27,300 --> 00:09:28,800
到实际上硬件执行的时候呢

300
00:09:28,800 --> 00:09:29,800
会把它变成

301
00:09:30,400 --> 00:09:31,500
会把数据的一部分呢

302
00:09:31,500 --> 00:09:32,800
根据硬件的多级缓存

303
00:09:32,800 --> 00:09:34,600
放在box wrap fret里面

304
00:09:34,600 --> 00:09:36,300
最终通过线程的box呢

305
00:09:36,300 --> 00:09:38,500
提供tensor core的核心的计算

306
00:09:41,000 --> 00:09:41,800
嗯

307
00:09:41,800 --> 00:09:42,600
时间来不及了

308
00:09:42,600 --> 00:09:43,400
时间来不及了

309
00:09:43,400 --> 00:09:44,200
我们现在

310
00:09:44,200 --> 00:09:45,000
或者我们后面的

311
00:09:45,000 --> 00:09:46,500
tensor core的历代的发展

312
00:09:46,500 --> 00:09:48,300
也就是从p100 v100

313
00:09:48,300 --> 00:09:51,600
h100里面的tensor core的变化

314
00:09:51,600 --> 00:09:53,000
我们在下个视频

315
00:09:53,000 --> 00:09:54,300
再给大家去分享

316
00:09:54,300 --> 00:09:55,400
今天的内容呢

317
00:09:55,400 --> 00:09:56,200
就到这里为止

318
00:09:56,200 --> 00:09:57,600
我们简单的回顾一下

319
00:09:58,600 --> 00:10:00,900
里面的所谓的混合进度计算呢

320
00:10:00,900 --> 00:10:02,500
就是指我们具体计算的时候呢

321
00:10:02,500 --> 00:10:03,700
是用fp16去计算

322
00:10:03,700 --> 00:10:04,700
但是存储的时候呢

323
00:10:04,700 --> 00:10:06,700
是用fp32进行存储

324
00:10:06,700 --> 00:10:08,900
tensor core的硬件之后呢

325
00:10:08,900 --> 00:10:10,400
进场的时候呢

326
00:10:10,400 --> 00:10:12,700
会通过web里面的线程呢

327
00:10:12,700 --> 00:10:13,800
聚集起来进行计算

328
00:10:13,800 --> 00:10:14,500
那最终呢

329
00:10:14,500 --> 00:10:17,500
对外提供一个16x16x16的矩阵

330
00:10:17,500 --> 00:10:18,700
API给到CUDA

331
00:10:20,700 --> 00:10:23,100
会看到的是一个16x16的web

332
00:10:23,100 --> 00:10:26,000
WMMA的API

333
00:10:26,800 --> 00:10:27,300
场景呢

334
00:10:27,300 --> 00:10:28,900
我们的GMM会非常大

335
00:10:28,900 --> 00:10:29,400
因此呢

336
00:10:29,400 --> 00:10:32,000
会通过多级的缓存

337
00:10:32,000 --> 00:10:33,600
优先的拆分成block

338
00:10:33,600 --> 00:10:34,200
web

339
00:10:34,200 --> 00:10:35,100
还有flat

340
00:10:35,100 --> 00:10:36,200
最终通过flat呢

341
00:10:36,200 --> 00:10:38,900
去提供实际的tensor core的运算

342
00:10:38,900 --> 00:10:39,800
那今天的内容呢

343
00:10:39,800 --> 00:10:41,000
都就到这里为止

344
00:10:41,000 --> 00:10:41,800
谢谢各位

345
00:10:41,800 --> 00:10:42,600
拜了个拜

