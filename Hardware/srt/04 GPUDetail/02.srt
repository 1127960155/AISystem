1
00:00:00,000 --> 00:00:03,525
Subtitle：PlusV98

2
00:00:04,750 --> 00:00:12,025
大家好,我是ZOMI,现在我们来到了Tensor Core的第二弹,去看看Tensor Core的架构演进

3
00:00:12,025 --> 00:00:18,212
其实呢,我们在整个英伟达的GPU架构里面呢,讲了非常多简单的概念,

4
00:00:18,212 --> 00:00:23,159
特别是从Turing到Hopper架构呢,整个Tensor Core呢是发展了非常多代

5
00:00:23,159 --> 00:00:26,043
那现在呢,我们来到了整个Tensor Core的架构的演进,

6
00:00:26,043 --> 00:00:34,000
去看看每一代不同的英伟达的GPU的架构,现在我们从Volta架构到Turing,Ampere,Hopper架构,

7
00:00:34,000 --> 00:00:37,732
去看看每一代的Tensor Core都有哪些不一样的点

8
00:00:39,400 --> 00:00:43,492
我们回顾一下,在第一代Volta架构里面的Tensor Core呢,

9
00:00:43,492 --> 00:00:49,699
主要是4x4的矩阵,输入是Fp16的A和B两个矩阵,

10
00:00:49,699 --> 00:00:52,534
然后加上Fp32和Fp16的矩阵之后呢,

11
00:00:52,534 --> 00:00:58,296
得到我们Fp16和Fp32的矩阵,通过这种方式呢进行混合精度的计算

12
00:00:58,296 --> 00:01:03,542
真正线程执行的时候呢,是把A矩阵的一行乘上B矩阵的一列,

13
00:01:03,542 --> 00:01:05,654
再加上C矩阵的一个元素,

14
00:01:05,654 --> 00:01:08,214
得到我们D矩阵里面的其中一个元素,

15
00:01:08,214 --> 00:01:12,758
这个呢就是Tensor Core里面的FMA指令执行的具体的内容

16
00:01:15,025 --> 00:01:20,765
现在我们来看看今天的主要的概念,Tensor Core的历代的发展,

17
00:01:20,765 --> 00:01:24,099
一共呢经历了四代,我们看一下每一代有哪些不一样

18
00:01:24,974 --> 00:01:27,545
首先呢Tensor Core是从Volta架构开始的,

19
00:01:27,545 --> 00:01:31,449
当初呢只是Fp32,直到Turing架构呢,它支持的更多,

20
00:01:31,449 --> 00:01:35,481
到现在的H100呢,它基本上的支持格式呢就更多了,

21
00:01:35,481 --> 00:01:37,721
每一代的支持格式也会越来越多,

22
00:01:37,721 --> 00:01:43,545
可以预见下一代英伟达的GPU的产品呢,也可能支持更多精度的指令

23
00:01:46,500 --> 00:01:51,936
现在呢我们来到了第一代英伟达的架构,Volta架构里面的SM,

24
00:01:51,936 --> 00:01:54,992
左边的这个就是SM的原来的架构图,

25
00:01:54,992 --> 00:01:58,614
SM里面呢有四个子核,我们叫做Subcore,

26
00:01:58,614 --> 00:02:04,314
那左边的这一块呢就是一个,右边的这一块呢又是一个,一共呢有四个子核,

27
00:02:04,314 --> 00:02:07,611
我们现在后面都会以右边的这个形式去呈现,

28
00:02:07,611 --> 00:02:12,913
首先呢一个SM里面我们有L1的I $,L1的指令缓存

29
00:02:12,913 --> 00:02:16,945
对应呢有四个子核Subcore,1234,

30
00:02:17,027 --> 00:02:20,552
而Subcore下面呢又有对应的L1的缓存,

31
00:02:20,552 --> 00:02:23,277
还有ShareMemory,就是我们的共享内存,

32
00:02:23,277 --> 00:02:26,752
值得注意的就是我们看看下面的箭头,

33
00:02:26,752 --> 00:02:30,692
首先呢L1 Cache里面呢会把具体的指令呢发到我们的Subcore,

34
00:02:30,692 --> 00:02:34,242
这里面的指令呢或者这里面的箭头呢是单向的,

35
00:02:34,242 --> 00:02:37,008
Subcore计算完之后呢,它的箭头呢是双向的,

36
00:02:37,008 --> 00:02:42,000
也就是我们可以获取计算数据或者对我们的数据呢进行重复的计算,

37
00:02:43,145 --> 00:02:47,291
现在呢我们重新的去解析一下第一代的TensorCore,

38
00:02:47,291 --> 00:02:48,336
也就是Volta架构的TensorCore,

39
00:02:48,336 --> 00:02:54,992
首先呢SM呢它是主要负责对寄存器里面的整体逻辑进行读和写和计算的,

40
00:02:55,108 --> 00:03:00,755
而这里面的每一个Subcore呢就包括了TensorCore,IP64,IP32,还有INT8,

41
00:03:00,755 --> 00:03:03,670
当然还有特殊的处理单元MFU,

42
00:03:03,670 --> 00:03:09,021
所以我们的Subcore呢不仅是指TensorCore,也不仅仅是指我们的CudaCore,

43
00:03:09,021 --> 00:03:14,114
Cuda Core啊 TensorCore啊,还有RTCore这些都包在Subcore里面,

44
00:03:14,414 --> 00:03:18,543
在每一个Subcore的上面呢就是我鼠标的所在的这一块位置呢,

45
00:03:18,600 --> 00:03:23,826
其实还有一个Warp Schedule,专门针对现成的Warp进行调度的,

46
00:03:23,826 --> 00:03:27,308
数据呢就存储在L1或者L0的Cache里面

47
00:03:28,233 --> 00:03:31,170
视频里谈到了针对第一代架构的TensorCore呢,

48
00:03:31,170 --> 00:03:35,429
每一个Subcore都有一个4x4x4的TensorCore,

49
00:03:35,429 --> 00:03:42,050
而WarpSchedule呢向我们的TensorCore发送具体的矩阵乘法,也就是GML,WAM的运算指令,

50
00:03:42,050 --> 00:03:50,261
计算完之后呢,就会从我们的寄存器,也就是中间的这个位置去读取或者去接收我们的ABC矩阵,

51
00:03:50,261 --> 00:03:57,388
A矩阵和B矩阵呢是IP16,C矩阵呢就是IP32或者IP16,执行多次的4x4的矩阵乘法。

52
00:03:57,388 --> 00:04:02,396
直到完成整个矩阵运算之后呢,将所得的的矩阵呢写回去我们的寄存器

53
00:04:02,396 --> 00:04:06,446
对,也就是Register File或者我们的ShareMemory里面。

54
00:04:07,350 --> 00:04:12,625
接下来我们对左边的这个SM图呢进行再打开看一下SM的V架构,

55
00:04:12,625 --> 00:04:18,000
首先呢也是从上往下看,上面就是一个共享的L1缓存。

56
00:04:18,000 --> 00:04:22,950
每个时钟周期呢可以执行4个WarpInstruction,下属4个Subcore是独立的,

57
00:04:22,950 --> 00:04:27,509
里面的数据呢是不进行缓存的,但Subcore里面的有两个TensorCore嘛,

58
00:04:27,509 --> 00:04:32,366
两个TensorCore的数据呢是可以共享的,再往下呢我们有一个共享内存,

59
00:04:32,366 --> 00:04:37,920
共享内存每个时钟周期呢可以执行或者可以传输128B的数据。

60
00:04:37,920 --> 00:04:40,391
当SMEM计算完这个全数矩阵之后呢

61
00:04:40,391 --> 00:04:46,041
,就会把数据呢回传到L2的Cache里面,最后呢就返回到我们的host CPU

62
00:04:47,239 --> 00:04:52,725
现在我们继续展开一下刚才的一个Subcore里面的V架构,Subcore里面的V架构呢

63
00:04:52,725 --> 00:04:57,458
就很多的内容了,刚才看到的L1Cache呢是在上面,L1Cache里面呢

64
00:04:57,458 --> 00:05:02,042
我们具体的执行的就会到我们的L0Cache或者叫我们的Register File,

65
00:05:02,042 --> 00:05:07,762
把一些数据呢传输到这,具体的指令的分发呢是通过WarpSchedule的,

66
00:05:07,762 --> 00:05:11,560
针对的计算呢我们通过MathDispatchUnit分发到具体的

67
00:05:11,560 --> 00:05:16,588
Fp64、Linux32、Fp32还有这几个具体的执行单元器计算,

68
00:05:16,588 --> 00:05:21,480
但是呢如果我们调用的是WMMA相关的API或者相关的指令呢,

69
00:05:21,480 --> 00:05:25,289
WarpSchedule呢就直接去触发或者去执行Tensor Core里面的计算,

70
00:05:25,312 --> 00:05:30,537
Tensor Core里面呢就有两个4x4x4的Tensor去每个时钟周期去执行,

71
00:05:30,537 --> 00:05:35,000
最后呢就把数据呢回存到我们的Register File,也就是我们的寄存器里面。

72
00:05:35,800 --> 00:05:40,800
寄存器再通过MIO的DataPipe呢跟我们的SharedMemory进行通讯。

73
00:05:42,425 --> 00:05:44,420
这里面有大量的数据传输,

74
00:05:44,420 --> 00:05:48,516
我们的数据呢存在哪里非常的关键,于是呢我们现在打开一下,

75
00:05:48,516 --> 00:05:52,516
看一下L1的缓存还有共享的内存之间的一个关系。

76
00:05:53,325 --> 00:05:57,312
在Volta架构里面呢对比起P100呢有一个很大的改进点,

77
00:05:57,312 --> 00:06:02,022
就是把L1的缓存呢跟我们的共享内存的合并成为同一块空间。

78
00:06:02,022 --> 00:06:08,486
共享内存的SMEM呢可以为整个SM呢提供高达96KB的存储空间。

79
00:06:08,486 --> 00:06:14,693
针对L2也就是对应的EverCast呢也进行了更新,已经有了一个5%到15%的提升。

80
00:06:14,693 --> 00:06:21,338
Volta架构里面的SubCore也就V架构里面呢单独提供了一个TensorCore的指令,

81
00:06:21,338 --> 00:06:27,321
提供给我们的WarpSchedule,WarpSchedule呢直接去执行,不需要通过MeshDispatchUnit去进行分发。

82
00:06:27,321 --> 00:06:33,321
除了Tensor Core是专门针对AI框的矩阵进行计算之外呢,Volta架构还减少了指令的延迟。

83
00:06:34,871 --> 00:06:39,296
现在我们来到了TensorCore的第二代,去看一下Turing架构里面的TensorCore。

84
00:06:39,296 --> 00:06:42,875
那首先呢SM我们就不管了,我们直接打开SubCore,

85
00:06:42,875 --> 00:06:45,232
就是我们的V核 在Turing架构里面呢,

86
00:06:45,232 --> 00:06:50,992
tensor core除了原先的IP16呢,其实还增加了INT8和INT4多种类型。

87
00:06:50,992 --> 00:06:56,400
另外的话还是IP16的FastPath每个时钟周期呢,可以执行32次,

88
00:06:56,400 --> 00:07:01,400
而原来的IP到8个时钟周期内呢,可以执行单个多线程GEM的计算,

89
00:07:01,400 --> 00:07:05,200
也就是我们的计算频率或者计算我们的计算吞吐就更高了。

90
00:07:06,000 --> 00:07:11,800
第二代的Turing架构提出了,其实距离上一代的Volta架构呢只是距离了一年,

91
00:07:11,800 --> 00:07:15,952
那现在我们看看第三代的TensorCore有哪些巨大的改变。

92
00:07:15,952 --> 00:07:22,373
首先呢,我们现在呢去澄清或者去给大家汇报的一个内容,就是多级的缓存或者多级数据的带宽。

93
00:07:22,373 --> 00:07:30,373
首先呢,我们看到的就是NVLink,NVLink呢是针对我们单节点多卡之间进行数据互联的。

94
00:07:30,373 --> 00:07:37,373
再往上,L2和DRAM呢,就是针对我们每一款每一块GPU卡里面的系统内存。

95
00:07:37,373 --> 00:07:41,550
再往上呢,就是每一个SM里面的内存,首先有共享内存呢,有IL了,

96
00:07:41,550 --> 00:07:44,100
针对具体的计算的就是我们的Math了,

97
00:07:44,100 --> 00:07:47,908
包括我们TensorCore或者CudaCore都是取决于我们的Math。

98
00:07:47,908 --> 00:07:53,725
而真正的A100呢,它最重要的改变就是Movement Efficiently,

99
00:07:53,725 --> 00:07:57,175
就是我们的数据搬运更加的快,有了一个三倍的提升,

100
00:07:57,175 --> 00:07:59,176
我们现在看看它具体是怎么做的。

101
00:08:00,301 --> 00:08:03,875
首先呢,我们看到NPA架构之前呢,包括我们的Turing架构,Volta架构,

102
00:08:03,875 --> 00:08:06,500
如果我们或者如果TensorCore呢,想要使用共享内存,

103
00:08:06,500 --> 00:08:12,592
就必须要把数据呢,从全局内存里面去加载到我们的寄存器,也就是Register File,

104
00:08:12,592 --> 00:08:17,621
然后再写进去共享内存,整体来说是我们的数据要搬来搬去,

105
00:08:17,621 --> 00:08:21,573
除了增加我们的数据的搬运呢,其实还影响了我们的时延

106
00:08:21,573 --> 00:08:26,149
于是呢,NPA架构呢,就提出了提供一个异步的内存拷贝机制,

107
00:08:26,149 --> 00:08:31,141
通过一个具体的新的指令,叫做LDGSTS,

108
00:08:31,141 --> 00:08:34,289
这个呢,指令叫做Load GoPro Storage Shared,

109
00:08:34,289 --> 00:08:42,850
实现了全局内存不需要经过具体的寄存器,直接加载到我们的共享内存里面,

110
00:08:42,850 --> 00:08:44,420
那这个呢,我们看看具体怎么做。

111
00:08:44,927 --> 00:08:49,875
首先,V100原来的操作方式,如果想要使用共享内存呢,

112
00:08:49,875 --> 00:08:53,248
就需要从LU把我们的共享内存的搬到寄存器堆里边

113
00:08:53,248 --> 00:08:54,433
也就是Register File里面呢,然后再给我们的内存呢,

114
00:08:54,433 --> 00:08:58,144
就会搬到我们的F里面,具体给我们的TensorCore去执行,

115
00:08:58,144 --> 00:09:00,547
那上面就是我们的TensorCore

116
00:09:00,547 --> 00:09:03,555
每一次数据搬运都会非常的占用我们的时延

117
00:09:03,555 --> 00:09:08,188
在A100里面呢,A100里面呢,就提出了一个软件的Sync Copy,

118
00:09:08,188 --> 00:09:13,350
异步的拷贝机制,通过新的指令,我们可以把L2 Cache里面的全局内存

119
00:09:13,350 --> 00:09:18,950
直接搬到我们的SMEM共享内存里面,然后给我们的RegisterFile呢,直接的去执行,

120
00:09:18,950 --> 00:09:21,850
每次数据搬运，都会增加我们的时延、功耗。

121
00:09:22,525 --> 00:09:26,964
大家有没有注意到,A100跟V100,除了中间的这个传输之外呢,

122
00:09:26,964 --> 00:09:33,108
其实有一点呢,我们漏掉了,就是在V100里面呢,我们需要4个读取的数据给到我们的warp

123
00:09:33,108 --> 00:09:34,647
warp只需要读取两次,

124
00:09:34,647 --> 00:09:38,790
这里面呢,又有另外一个技术,就是Ampere架构的TensorCore,

125
00:09:38,790 --> 00:09:42,211
就是一个Warp呢,就提供了32个线程,32个线程可以共享数据,

126
00:09:42,211 --> 00:09:45,795
而Volta架构里面呢,整个TensorCore只有8个线程,

127
00:09:45,795 --> 00:09:50,300
这样的好处 可以在线程之间呢,减少我们矩阵的数据搬运,

128
00:09:50,300 --> 00:09:51,329
因此呢,我们的数据搬运的次数呢,

129
00:09:51,329 --> 00:09:54,000
会比V100更少。

130
00:09:54,000 --> 00:10:02,315
看一下具体的FFMA,也就是Fuse Fold MathMath and Add,就是我们的矩阵乘加的操作,

131
00:10:02,315 --> 00:10:05,045
我们解读一下上面的这个图啊,

132
00:10:05,045 --> 00:10:09,150
绿色的这些小块呢,我们叫做Support,就是我们刚才提到的Support,

133
00:10:09,150 --> 00:10:15,413
包括这里面的图呢,就是我们的TensorCore,而连续的蓝色的框呢,就表示我们的寄存器Register。

134
00:10:15,413 --> 00:10:18,155
当寄存器纯粹使用CudaCore的时候呢

135
00:10:18,155 --> 00:10:24,278
,我们会把所有的数据呢,都放在Register里面,每个Register呢,针对一个CudaCore的数据呢,进行传输,

136
00:10:24,278 --> 00:10:26,774
所以使用CudaCore呢,我们是算得非常慢的,

137
00:10:26,774 --> 00:10:29,552
在V100里面呢,我们每个TensorCore呢,可以处理8个线程,

138
00:10:29,552 --> 00:10:31,052
每个线程都有自己的寄存器,

139
00:10:31,052 --> 00:10:35,235
所以我们在8个时钟周期内呢,可以执行1024个MAC的操作。

140
00:10:35,235 --> 00:10:40,304
那下面呢,就是A100的TC,就是TensorCore的指令啊,

141
00:10:40,304 --> 00:10:44,307
可以看到,每个TensorCore呢,可以32条线程,现在32条线程,

142
00:10:44,307 --> 00:10:51,000
因此呢,我们可以在8个时钟周期内呢,去寄存2048个MAC,每个时钟周期处理其中一块的数据。

143
00:10:51,675 --> 00:10:55,727
第三代的TensorCoreAmpere架构里面呢,除了制造工艺提升之外呢,

144
00:10:55,727 --> 00:11:01,809
我们还提供了更多的线程使得硬件执行更快,数据搬运的更少,每个时钟的吞吐呢,更大。

145
00:11:03,450 --> 00:11:08,337
大家看完这个图之后呢,有没有一点感觉,

146
00:11:08,337 --> 00:11:14,353
就是为什么出现了TensorCore之后呢,会比单纯的使用我们的CUDA Core执行的更快呢?

147
00:11:14,353 --> 00:11:16,319
针对我们的矩阵计算,

148
00:11:16,319 --> 00:11:18,401
具体呢,是因为我们的线程啊,每一次针对TensorCore,

149
00:11:18,401 --> 00:11:22,526
我们都可以处理的更快,处理的更多,我们的吞吐是不一样的。

150
00:11:24,900 --> 00:11:28,664
现在我们来到了2022年提出的Hopper架构,

151
00:11:28,664 --> 00:11:33,750
里面呢,就提出了第四代TensorCore,第四代TensorCore其实呢,有三个重要的改变的点。

152
00:11:33,750 --> 00:11:38,459
我们现在来回顾一下,其实前三代的TensorCore呢,都是基于Warp-Level进行编程的。

153
00:11:38,459 --> 00:11:45,874
那英伟达架构A100里面呢,做了软件的异步加载之后呢,其实它还是基于Web-Level进行编程。

154
00:11:45,874 --> 00:11:51,127
简单地来说呢,就是把数据呢,从HBM,就是我们的全局内存,加载到我们的寄存器,

155
00:11:51,127 --> 00:11:54,766
再通过Warp Scheduleh去调用,后来完成整个矩阵的运算,

156
00:11:54,766 --> 00:11:58,862
最后再把数据回传到我们的寄存器,再不断地去运算。

157
00:11:58,862 --> 00:12:01,256
这整一个过程呢,它其实有两个问题。

158
00:12:01,256 --> 00:12:06,862
那第一个问题呢,就是数据的搬运和计算是严重地去偶合的。

159
00:12:06,862 --> 00:12:11,048
线程加载矩阵数据的时候呢,其实会独立地去获取矩阵的地址,

160
00:12:11,048 --> 00:12:14,505
会非常消耗继承器的数量还有存储的带宽。

161
00:12:14,505 --> 00:12:19,435
第二个就是可扩展性受到约束,因为单个Warp里面的线程是有限的,

162
00:12:19,435 --> 00:12:24,235
而单个warp里面的线程有限呢,就会导致我们的矩阵的计算的规格受到约束。

163
00:12:24,235 --> 00:12:26,180
于是呢,在第四代TensorCore里面呢,

164
00:12:26,180 --> 00:12:34,180
就提出了一个TMA,我们叫做Tensor Memory Accelerator,就是增量内存加速的功能。

165
00:12:34,180 --> 00:12:41,700
左边这个图呢,就是A100,就是上一代的安培架构里面的一个整体的SM的架构图。

166
00:12:41,700 --> 00:12:44,356
右边呢,就是H100里面的SM的架构图,

167
00:12:44,356 --> 00:12:48,610
可以看到基本上没有太多的差别,除了因为工艺制程的原因呢,

168
00:12:48,610 --> 00:12:51,746
导致这里面的CUDA Core和Tensor Core的密度更高之外呢,

169
00:12:51,746 --> 00:12:55,522
最重要的就是多了一个Tensor Memory Accelerator。

170
00:12:55,522 --> 00:12:59,198
我们简称叫做TMA,也就是硬件的数据异步加载。

171
00:12:59,198 --> 00:13:06,580
在A100里面呢,其实已经提出了一个软件的数据异步加载,这里面呢,TMA直接把它硬件化了。

172
00:13:06,580 --> 00:13:11,188
非常方便把全局内存的数据呢,异步加载到我们的共享内存,

173
00:13:11,188 --> 00:13:14,580
直接给我们的Register File 寄存器去读写。

174
00:13:15,200 --> 00:13:20,185
H100之前的架构里面呢,我们大部分都是从左边的图所示啊,

175
00:13:20,185 --> 00:13:23,897
只有Grid和Block之分,线程我们是没有办法控制的。

176
00:13:23,897 --> 00:13:28,692
所以呢,因此呢,针对硬件呢,我们分别对应的是一个SM,SM对应的是我们的Block,

177
00:13:28,692 --> 00:13:33,044
而Grid呢,是对应我们整个Devices,也就是单块GPU。

178
00:13:33,044 --> 00:13:38,420
我们局部的数据呢,只能通过Shared Memory在SM内进行共享,

179
00:13:38,420 --> 00:13:41,172
跨SM之间呢,是不能够处理的。

180
00:13:41,172 --> 00:13:48,556
而Hopper架构呢,直接在硬件上面呢,引入了一个交叉互联网络,也就是直接把我们的数据拓展到四个SM。

181
00:13:48,556 --> 00:13:50,924
SM之间呢,可以互相通讯。

182
00:13:51,250 --> 00:13:58,250
于是在软件上或者CUDA上面呢,我们引用了一个新的概念,叫做TBC,也就是把4个SM聚合起来。

183
00:13:58,250 --> 00:14:06,250
SM跟SM之间呢,可以高效访问他们互相之间的内存,所以这种呢,我们叫做分布式共享内存。

184
00:14:06,250 --> 00:14:13,411
另外的话,你既然硬件有改变,所以我们的软件有改变,我们的软件呢,就提出了warp group这种编程的模式。

185
00:14:13,411 --> 00:14:17,411
对应的就引入了刚才说到的Flatbox Cluster这个概念。

186
00:14:18,325 --> 00:14:22,014
更直观的从软件层面去看一下,有什么区别啊?

187
00:14:22,014 --> 00:14:22,325
左边这个呢,就是没有进行分布式共享内存的。

188
00:14:22,325 --> 00:14:25,014
左边这个呢,就是没有进行分布式共享内存的。

189
00:14:25,014 --> 00:14:30,014
每个Thread Block呢,就是我们对应的SM,里面可以共享自己的内存。

190
00:14:30,014 --> 00:14:36,014
但是呢,SM跟SM之间呢,没有办法进行数据交互,于是呢,我们只能通过全局内存进行交互。

191
00:14:36,014 --> 00:14:41,553
但是呢,在H100里面呢,我们引入了一个SM的Cluster,或者我们的现成Block的Cluster。

192
00:14:41,553 --> 00:14:44,773
通过硬件来实现分布式的共享内存。

193
00:14:44,773 --> 00:14:47,773
SM跟SM之间的数据呢,可以互联。

194
00:14:47,773 --> 00:14:50,923
不需要再次把数据呢,放在HBM里面再进行交互。

195
00:14:50,923 --> 00:14:54,098
这样的话,可以减少我们寄存器的数量的利用。

196
00:14:54,098 --> 00:14:56,376
还可以减少数据传输的时延。

197
00:14:58,975 --> 00:15:03,544
现在来到最后一个内容了,就是TensorCore的应用。

198
00:15:03,544 --> 00:15:08,544
其实呢,我们在做TensorCore,或者在H100里面呢,主要是针对大模型。

199
00:15:08,544 --> 00:15:13,544
或者transformer的架构进行堆叠的这种像GPT,ChatGPT这种大模型啊。

200
00:15:13,544 --> 00:15:16,620
但是呢,这些大模型输入的时候呢,有非常多的词汇。

201
00:15:16,620 --> 00:15:20,012
我们会把词汇呢,embedded成具体的一些向量。

202
00:15:20,012 --> 00:15:23,012
然后呢,输出的时候呢,我们还是以一个向量为主。

203
00:15:23,012 --> 00:15:28,012
经过softmax我们就会输出一个比词表更大的一个向量。

204
00:15:28,012 --> 00:15:34,687
那这个时候呢,我们的词向量的表就会变得非常非常的大,或者我们的矩阵变得非常的大。

205
00:15:34,687 --> 00:15:38,508
在整个transformer计算的时候呢,也就变得非常大。

206
00:15:38,508 --> 00:15:41,644
我们刚才谈到,其实TensorCore它的数量是有限的。

207
00:15:41,644 --> 00:15:50,936
在V100里面,它是4x4x4,但是呢,在软件上面呢,我们拓展到16x16x16,不断地从局部进行搬运。

208
00:15:50,936 --> 00:15:58,488
那这个时候呢,其实我们不是说随随便便的就能够从软件上面去处理所有的embedded,

209
00:15:58,488 --> 00:16:00,315
或者处理所有的大矩阵的。

210
00:16:00,315 --> 00:16:02,900
更多呢,我们看一下刚才的那个例子。

211
00:16:02,900 --> 00:16:08,000
针对大模型呢,我们的inputSize等于1024,然后batchSize是5120

212
00:16:08,000 --> 00:16:11,330
在v100,我们使用IP16进行训练,

213
00:16:11,330 --> 00:16:16,194
然后呢,整个词汇表的大小是三万多。

214
00:16:16,194 --> 00:16:20,111
transformer里面的Attention架构呢,里面就会有很多Mac漫画矩阵层。

215
00:16:20,111 --> 00:16:26,111
如果我们pad到8的倍数,整体的性能呢,会比没有pad到8的倍数里面高很多。

216
00:16:26,111 --> 00:16:33,111
这个时候呢,就要求我们软件编程的时候,其实也需要注意到我们硬件怎么样才能实现的更加高效。

217
00:16:33,111 --> 00:16:38,374
那这个呢,我们叫做Padding Vocabulary Size,就对矩阵,需要进行Padding的操作。

218
00:16:40,000 --> 00:16:44,050
就到这里为止了,我们进行一个简单的总结。

219
00:16:44,050 --> 00:16:48,050
在历代的Tensor Core,我们主要有三个提升点。

220
00:16:48,050 --> 00:16:52,050
第一个呢,就是提升我们的内存,打破整体的内存墙。

221
00:16:52,050 --> 00:16:55,050
第二个呢,SM里面呢,提供更多的数据格式。

222
00:16:55,050 --> 00:17:00,050
从帕斯卡的标准的IP16到TF32、IP8、IP4。

223
00:17:00,050 --> 00:17:07,050
最后一个呢,就是对应的我们硬件改了,然后我们对应的软件编程呢,也会去修改。

224
00:17:07,050 --> 00:17:10,050
预设呢,又有了新的CUDA的编程的模式。

225
00:17:10,050 --> 00:17:12,050
那今天的内容呢,就到这里为止。

226
00:17:12,050 --> 00:17:14,050
谢谢各位,拜拜。

227
00:17:14,050 --> 00:17:16,050
卷的不行啦,卷的不行啦。

228
00:17:16,050 --> 00:17:18,050
记得一键三连加关注哦。

229
00:17:18,050 --> 00:17:21,050
所有的内容都会开源在下面这条链接里面。

230
00:17:21,050 --> 00:17:23,050
拜拜。

