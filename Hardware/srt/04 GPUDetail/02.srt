1
00:00:00,000 --> 00:00:11,000
大家好,我是周米,现在我们来到了Tensor Core的第二弹,去看看Tensor Core的架构演进

2
00:00:11,000 --> 00:00:21,000
其实呢,我们在整个英伟达的GPU架构里面呢,讲了非常多简单的概念,特别是从图灵到Hopper架构呢,整个Tensor Core呢是发展了非常多代

3
00:00:21,000 --> 00:00:36,000
那现在呢,我们来到了整个Tensor Core的架构的演进,去看看每一代不同的英伟达的GPU的架构,里面对每一个架构到图灵安倍,赫博架构,去看看每一代的Tensor Core都有哪些不一样的点

4
00:00:36,000 --> 00:00:56,000
我们回顾一下,在第一代福特架构里面的Tensor Core呢,主要是4x4的矩阵,是Fp16的A和B两个矩阵,然后加上Fp32和Fp16的矩阵之后呢,得到我们Fp16和Fp32的矩阵,通过这种方式呢进行计算

5
00:00:57,000 --> 00:01:12,000
而具体的计算,这种线程执行的时候呢,是把A矩阵的一行乘上B矩阵的一列,再加上C矩阵的一个元素,得到我们D矩阵里面的其中一个元素,这个呢就是Tensor Core里面的FMA指令执行的具体的内容

6
00:01:12,000 --> 00:01:41,000
现在我们来看看今天的主要的概念,Tensor Core的历代的发展,一共呢经历了四代,我们看一下每一代有哪些不一样,首先呢Tensor Core是从福特架构开始的,当初呢只是Fp32,直到图灵架构呢,它支持的更多,到现在的H100呢,它基本上的支持格式呢就更多了,每一代的支持格式也会越来越多,我们可以下一代英伟达的GPU的产品呢,也可能支持更多

7
00:01:42,000 --> 00:02:12,000
现在呢我们来到了第一代英伟达的架构,福特架构里面的SM,左边的这个就是SM的原来的架构图,SM里面呢有四个子核,我们叫做Subcore,那左边的这一块呢就是一个,右边的这一块呢又是一个,一共呢有四个子核,我们现在后面都会以右边的这个形式去呈现,首先呢一个SM里面我们有LE的Cache,TwinCast LE的指令缓存,然后呢呢我们会用这个Cache呢去呈现,然后呢我们会用这个Cache去呈现,然后呢我们会用这个Cache去呈现,然后呢我们会用这个Cache去呈现,然后呢我们会用这个Cache去呈现,然后呢我们会用这个Cache去呈现,然后呢我们

8
00:02:12,000 --> 00:02:42,000
然后呢登对面有四个子核Subcore,1234,而Subcore下面呢又有对应的LE的缓存,还有ShareMemory,就是我们的共享内存,值得注意的就是我们看看下面的箭头,首先呢LE Cache里面呢会把具体的指令呢发到我们的Subcore,这里面的指令呢或者这里面的箭头呢是单向的,直到我们的Subcore刷完之后呢,它的箭头呢是双向的,也就是我们可以通过共享内存数据或者对我们的数据呢进行重复的计算,接下来呢我们会用这个Subcore去呈现,然后呢我们会用这个Subcore去呈现,然后呢我们会用这个Subcore去呈现,然后呢我们会用这个Subcore去呈现,然后呢我们会用这个Subcore去

9
00:02:42,000 --> 00:02:42,320
现在呢我们重新的去解析一下第一代的TensorCore,也就是福特架构的TensorCore,首先呢SM呢它是主要负责对基层器里面的整体逻辑进行读和写和计算的,而这里面的每一个Subcore呢就包括了TensorCore,IP64,IP32,还有INT8,当然还有特殊的处理单元MFU,所以我们的Subcore呢不仅是指TensorCore,也不仅仅是指我们的CudaCore,它只要TensorCore啊,还有RTCore这些都包在Subcore里面,它只要TensorCore啊,还有RTCore这些都包在Subcore里面,它只要TensorCore啊,还有RTCore这些都包在Subcore里面,

10
00:03:13,000 --> 00:03:26,000
一个Subcore的上面呢就是我鼠标的所在的这一块位置呢,其实还有一个WebSchedule,专门针对现成的Web进行调度的,数据呢就存储在L1或者L0的Cache里面。

11
00:03:26,000 --> 00:03:55,000
视频里谈到了针对第一代架构的TensorCore呢,每一个Subcore都有一个4x4x4的TensorCore,而WebSchedule呢向我们的TensorCore发送具体的矩阵乘法,也就是GML,WAM的运算指令,计算完之后呢,就会从我们的寄存器,也就是中间的这个位置去读取或者去接收我们的ABC矩阵,A矩阵和B矩阵呢是IP16,C矩阵呢就是IP32或者IP16,执行多次的4x4的矩阵乘法。

12
00:03:56,000 --> 00:04:05,000
直到完成整个矩阵运算之后呢,把所有的数据上的矩阵呢写回去我们的寄存器对,也就是JustFile或者我们的ShareMemory里面。

13
00:04:05,000 --> 00:04:17,000
现在我们继续,接下来我们对左边的这个SM图呢进行再打开看一下SM的V架构,首先呢也是从上网下看,有上了我们就是一个共享的L1缓存。

14
00:04:18,000 --> 00:04:36,000
周期呢可以执行4个WebInstruction,而4个Subcore是独立的,里面的数据呢是不进行缓存的,但Subcore里面的有两个TensorCore嘛,两个TensorCore的数据呢是可以共享的,到最后呢我们有一个共享内存,共享内存每个时钟周期呢可以执行或者可以传输128B的数据。

15
00:04:37,000 --> 00:04:45,000
计算完这个全数矩阵之后呢,就会把数据呢回传到L2的Cache里面,最后呢就返回到我们的host CPU。

16
00:04:46,000 --> 00:05:05,000
现在我们继续展开一下刚才的一个Subcore里面的V架构,Subcore里面的V架构呢就很多的内容了,刚才看到的L1Cache呢是在上面,L1Cache里面呢我们具体的执行的就会到我们的L0Cache或者叫我们的WeJustFile,通过WeJustFile把一些数据呢传输到这,具体的指令的分发呢是通过WebSchedule的,

17
00:05:05,000 --> 00:05:20,000
然后呢的计算呢我们通过MathDispatchUnit呢,分发到具体的Fp64、Linux32、Fp32还有这几个具体的执行单元器计算,但是呢如果我们调用的是WMMA相关的API或者相关的指令呢,

18
00:05:20,000 --> 00:05:34,000
WebSchedule呢就直接去触发或者去执行Tensor Core里面的计算,Tensor Core里面呢就有两个4x4x4的Tensor去每个时钟周期去执行,最后呢就把数据呢回存到我们的WeJustFile,也就是我们的继承器里面。

19
00:05:35,000 --> 00:05:40,000
通过MIO的DataPipe呢跟我们的SharedMemory进行通讯。

20
00:05:40,000 --> 00:05:51,000
下面呢我们的数据传输,我们的数据呢存在哪里非常的关键,于是呢我们现在打开一下,看一下LE的款存还有共享的内存之间的一个关系。

21
00:05:51,000 --> 00:06:01,000
那刚才我们打开,在福特架构里面呢对比起P100呢有一个很大的改进点,就是把LE的款存呢跟我们的共享内存的合并成为同一块空间。

22
00:06:01,000 --> 00:06:08,000
共享内存的SMEM呢可以为整个SM呢提供高达96KB的存储空间。

23
00:06:08,000 --> 00:06:14,000
SMEM也就是对应的EverCast呢也提供了,已经有了一个5%到15%的提升。

24
00:06:14,000 --> 00:06:26,000
福特架构里面的SubCore也就V架构里面呢单独提供了一个TensorCore的指令,提供给我们的WebSchedule,WebSchedule呢直接去执行,不需要通过MeshDispatchUnit去进行分发。

25
00:06:26,000 --> 00:06:32,000
是专门针对AI框的矩阵进行计算之外呢,它减少了指令的延迟。

26
00:06:33,000 --> 00:06:38,000
那我们来到了TensorCore的第二台,去看一下图灵架构里面的TensorCore。

27
00:06:38,000 --> 00:06:49,000
那首先呢SM我们就不管了,我们直接打开SubCore,就是我们的V和TensorCore里面呢,里面的TensorCore呢除了原先的IP16呢,其实还增加了INT8和INT4多种类型。

28
00:06:50,000 --> 00:07:04,000
另外的话还是IP16的FastPath每个时钟周期呢,可以执行32次,而原来的IP到8个时钟周期内呢,可以执行单个多线程GEM的计算,也就是我们的计算频率或者计算我们的计算吞吐就更高了。

29
00:07:06,000 --> 00:07:15,000
第二代的图灵架构提出了,其实距离上一代的福特架构呢只是距离了一年,那现在我们看看第三代的TensorCore有哪些巨大的改变。

30
00:07:15,000 --> 00:07:21,000
首先呢,我们现在呢去澄清或者去给大家汇报的一个内容,就是多级的数据的带宽。

31
00:07:21,000 --> 00:07:29,000
首先呢,我们看到的就是NVLink,NVLink呢是针对我们单节点多卡之间进行一个互联的。

32
00:07:29,000 --> 00:07:36,000
然后呢,这包含的呢,就是针对我们每一款每一块GPU卡里面的系统内存。

33
00:07:36,000 --> 00:07:47,000
在网上呢,这每一个SM里面的内存呢,首先有共享内存呢,有IL了,针对具体的计算的就是我们的Math了,包括我们TensorCore或者CudaCore都是取决于我们的Math。

34
00:07:47,000 --> 00:07:58,000
而真正的A100呢,它最重要的改变就是Movement Efficiently,就是我们的数据搬运更加的快,有了一个三倍的提升,我们现在看看它具体是怎么做的。

35
00:07:58,000 --> 00:08:20,000
首先呢,我们看到NPA架构之前呢,包括我们的图灵架构,福特架构,如果我们或者如果TensorCore呢,想要使用共享内存,就必须要把数据呢,从全局内存里面去加载到我们的寄存器,也就是JustFile,然后再写进去共享内存,整体来说是我们的数据要搬来搬去,除了增加我们的数据的搬运呢,其实还影响了我们的实验。

36
00:08:20,000 --> 00:08:43,000
于是呢,NPA架构呢,就提出了提供一个异步的内存拷贝机制,通过一个具体的新的指令,叫做LDGSTS,这个呢,指令叫做Load GoPro Storage Shared,现在我们的全局内存不需要经过具体的寄存器,直接加载到我们的共享内存里面,那这个呢,我们看看具体怎么做。

37
00:08:43,000 --> 00:09:11,000
首先,V版原来的操作方式,V版原来的操作方式,共享内存呢,就需要从LU把我们的共享内存的包到堆里面,也就是JustFile里面呢,然后再给我们的内存呢,就会搬到我们的F里面,具体给我们的TensorCore去执行,那上面的这些的TensorCore每一次呢,每一次呢,都会非常的占用我们的实验,在A版里面呢,A版里面呢,就提出了一个软件的Sync Copy,异步的拷贝机制,通过异步的拷贝呢,我们可以把L2 Cache里面的,

38
00:09:12,000 --> 00:09:17,600
直接搬到我们的SMEM共享内存里面,然后给我们的JustFile呢,直接的去执行,

39
00:09:18,500 --> 00:09:21,000
参与呢,都会增加我们的实验功耗。

40
00:09:21,000 --> 00:09:50,000
到这里面呢,A版跟V版,除了中间的这个传输之外呢,其实有一点呢,我们漏掉了,就是在V版里面呢,我们需要4个读取的数据给到我们的Web,但是在A版读取两次,这里面呢,又有另外一个技术,就是安排架构的TensorCore,就是一个Web呢,就提供了32个线程,32个线程可以共享数据,而福特架构里面呢,整个TensorCore只有8个线程,我们的线程数可以在线程之间呢,减少我们矩阵的数据搬运,因此呢,我们的数据搬运的次数呢,

41
00:09:51,000 --> 00:09:53,000
会比V版更少。

42
00:09:54,000 --> 00:10:14,000
看一下具体的FFMA,也就是Fuse Fold MathMath and Add,就是我们的矩阵乘加的操作,那实际上呢,我们读一下上面的这个图啊,绿色的这些小块呢,我们叫做Support,就是我们刚才提到的Support,包括这里面的图呢,就是我们的TensorCore,而连续的蓝色的框呢,就表示我们的寄存器Register。

43
00:10:15,000 --> 00:10:34,000
那纯粹使用CudaCore的时候呢,我们会把所有的数据呢,都放在Register里面,每个Register呢,针对一个CudaCore的数据呢,进行传输,所以使用CudaCore呢,我们是算得非常慢的,在V版里面呢,我们每个TensorCore呢,可以处理8个线程,于是呢,我们把8个呢,都有自己的寄存器,所以我们在8个时钟周期内呢,可以执行1,2,4个MAC的操作。

44
00:10:34,000 --> 00:10:50,000
那下面呢,就是A100的TC,就是TensorCore的指令啊,可以看到,每个TensorCore呢,可以32条线程,现在32条线程,因此呢,我们可以在8个时钟周期内呢,去寄存2048个MAC,处理其中一块的数据。

45
00:10:51,000 --> 00:11:01,000
第三代的TensorCore安倍架构里面呢,除了我们的技术提升之外呢,我们还提供了更多的线程,更多的软件呢,进行了更快的数据搬运的更少,系列的吞吐呢,更大。

46
00:11:02,000 --> 00:11:13,000
大家看完这个图之后呢,有没有一点感觉,就是为什么出现了TensorCore之后呢,会比单纯的使用我们的CUDA Core执行的更快呢?

47
00:11:13,000 --> 00:11:21,000
针对我们的矩阵计算,具体呢,是因为我们的线程啊,每一次针对TensorCore,我们都可以处理的更快,处理的更多,我们的吞吐是不一样的。

48
00:11:22,000 --> 00:11:33,000
现在我们来到了2022年提出的Hopper架构,里面呢,就提出了第四代TensorCore,第四代TensorCore其实呢,有三个重要的改变的点。

49
00:11:33,000 --> 00:11:37,000
我们现在来回顾一下,其实前三代的TensorCore呢,都是基于Web-Level进行编程的。

50
00:11:37,000 --> 00:11:44,000
那英伟达架构A100里面呢,一部加载之后呢,其实它还是基于Web-Level进行编程。

51
00:11:44,000 --> 00:11:57,000
简单地来说呢,就是把数据呢,从HBM,就是我们的全局内存,加载到我们的继承器,再通过Web-Sketch去雕塑,后来完成整个矩阵的运算,最后再把数据回传到我们的继承器,再不断地去运算。

52
00:11:57,000 --> 00:12:00,000
这整一个过程呢,它其实有两个问题。

53
00:12:00,000 --> 00:12:05,000
那第一个问题呢,就是数据的搬运和计算是严重地去偶合的。

54
00:12:06,000 --> 00:12:13,000
而矩阵数据的时候呢,其实会独立地去过去矩阵的地址,像消耗继承器的数量还有存储的带宽。

55
00:12:13,000 --> 00:12:22,000
第二个就是可扩展性受到约束,因为我们整个Web里面的线程是有限的,整个Web里面的线程有限呢,就会导致我们的矩阵的计算的规格受到约束。

56
00:12:22,000 --> 00:12:33,000
于是呢,在第四代TensorCore里面呢,就提出了一个TMA,我们叫做Tensor Memory Accelerator,就是脏量内存加速的功能。

57
00:12:33,000 --> 00:12:40,000
左边这个图呢,就是A100,就是上一代的安排架构里面的一个整体的SM的架构图。

58
00:12:40,000 --> 00:12:54,000
右边呢,就是H100里面的SM的架构图,可以看到基本上没有太多的差别,除了因为工艺制程的原因呢,导致这里面的CUDA Core和Tensor Core的密度更高之外呢,最重要的就是多了一个Tensor Memory Accelerator。

59
00:12:54,000 --> 00:12:58,000
现在我们来看一层叫做TMA,也就是硬件的数据易部加载。

60
00:12:58,000 --> 00:13:06,000
在A100里面呢,其实已经提出了一个软件的数据易部加载,这里面呢,TMA直接把它硬件化了。

61
00:13:06,000 --> 00:13:14,000
为了把全球内存的数据呢,易部加载到我们的共享内存,直接给我们的Virtual File 寄存器去读写。

62
00:13:14,000 --> 00:13:22,000
A100之前的架构里面呢,我们大部分都是从左边的图所示啊,只有Grid和Block之分,现场的我们是没有办法控制的。

63
00:13:22,000 --> 00:13:32,000
所以呢,因此呢,针对硬件呢,我们分别对应的是一个SM,SM对应的是我们的Block,而Grid呢,是对应我们整个Devices,也就是单块GPU。

64
00:13:32,000 --> 00:13:39,000
局部的数据呢,我们局部的数据呢,只能通过Shared Memory在SM内进行共享,跨SM之间呢,是不能够处理的。

65
00:13:39,000 --> 00:13:47,000
而和本架构呢,直接在硬件上面呢,引入了一个交叉互联网络,也就是直接把我们的数据拓展到四个SM。

66
00:13:47,000 --> 00:13:50,000
SM之间呢,可以互相通讯。

67
00:13:50,000 --> 00:13:57,000
三块的CUDA上面呢,我们引用了一个新的概念,叫做TBC,也就是聚合起来。

68
00:13:57,000 --> 00:14:05,000
SM跟SM之间呢,可以高效访问他们互相之间的内存,所以这种呢,我们叫做分布式共享内存。

69
00:14:05,000 --> 00:14:12,000
另外的话,你既然硬件有改变,所以我们的软件有改变,我们的软件呢,就提出了WebGUI这种编程的模式。

70
00:14:13,000 --> 00:14:17,000
那就引入了刚才说到的Flatbox Cluster这个概念。

71
00:14:17,000 --> 00:14:21,000
更直观的从软件层面去看一下,有什么区别啊?

72
00:14:21,000 --> 00:14:24,000
左边这个呢,就是没有进行分布式共享内存的。

73
00:14:24,000 --> 00:14:29,000
每个Flatbox呢,就是我们对应的SM,里面可以共享自己的内存。

74
00:14:29,000 --> 00:14:35,000
但是呢,SM跟SM之间呢,没有办法进行数据交互,于是呢,我们只能通过全局内存进行交互。

75
00:14:35,000 --> 00:14:41,000
但是呢,在H1版里面呢,我们引入了一个SM的Cluster,或者我们的现成Box的Cluster。

76
00:14:41,000 --> 00:14:43,000
来实现分布式的共享内存。

77
00:14:43,000 --> 00:14:46,000
SM跟SM之间的数据呢,可以互联。

78
00:14:46,000 --> 00:14:50,000
不需要再次把数据呢,放在HBM里面再进行交互。

79
00:14:50,000 --> 00:14:53,000
这样的话,可以减少我们寄存器的数量的利用。

80
00:14:53,000 --> 00:14:55,000
数据传输的实验。

81
00:14:57,000 --> 00:15:02,000
现在来到最后一个内容了,就是TensorCore的应用。

82
00:15:02,000 --> 00:15:07,000
其实呢,我们在做TensorCore,或者在H1版里面呢,主要是针对大模型。

83
00:15:07,000 --> 00:15:12,000
或者TensorCore的架构进行堆叠的这种像GPT,ChartGPT这种大模型啊。

84
00:15:12,000 --> 00:15:16,000
但是呢,这些大模型输入的时候呢,有非常多的词汇。

85
00:15:16,000 --> 00:15:19,000
我们会把词汇呢,embedded成具体的一些向量。

86
00:15:19,000 --> 00:15:22,000
然后呢,输出的时候呢,我们还是以一个向量为主。

87
00:15:22,000 --> 00:15:27,000
不过呢,输出S城呢,我们就会输出一个比词表更大的一个向量。

88
00:15:27,000 --> 00:15:34,000
那这个时候呢,我们的词向量的表就会变得非常非常的大,或者我们的矩阵变得非常的大。

89
00:15:34,000 --> 00:15:37,000
所以整个全数码计算的时候呢,也就变得非常大。

90
00:15:37,000 --> 00:15:41,000
我们刚才谈到,其实TensorCore它的数量是有限的。

91
00:15:41,000 --> 00:15:50,000
在V版里面,它是4x4x4,但是呢,在软件上面呢,我们拓展到16x16x16,不断地从局部进行搬运。

92
00:15:50,000 --> 00:15:59,000
那这个时候呢,其实我们不是说随随便便的就能够从软件上面去处理所有的embedded,或者处理所有的大矩阵的。

93
00:15:59,000 --> 00:16:02,000
更多呢,我们看一下刚才的那个例子。

94
00:16:02,000 --> 00:16:08,000
针对大模型呢,我们的inputSize等于024,然后batchSize是在V版。

95
00:16:08,000 --> 00:16:15,000
在这边呢,我们使用IP16进行训练,然后呢,整个词汇表的大小是3370,三万多。

96
00:16:15,000 --> 00:16:18,000
里面的Attention架构呢,里面就会有很多Mac漫画矩阵层。

97
00:16:18,000 --> 00:16:24,000
那这个时候呢,8的倍数,整体的性能呢,会比没有派特到8的倍数里面高很多。

98
00:16:24,000 --> 00:16:31,000
这个时候呢,就要求我们软件编程的时候,其实也需要注意到我们硬件怎么样才能实现的更加高效。

99
00:16:31,000 --> 00:16:37,000
那这个呢,我们叫做Padding Vocabulary Size,就对我们呢,需要进行Padding的操作。

100
00:16:40,000 --> 00:16:42,000
就到这里为止了,我们进行一个简单的总结。

101
00:16:42,000 --> 00:16:46,000
在历代的Tensor Core,我们主要有三个提升点。

102
00:16:46,000 --> 00:16:50,000
第一个呢,就是提升我们的内存,打破整体的内存墙。

103
00:16:50,000 --> 00:16:53,000
第二个呢,SM里面呢,提供更多的数据格式。

104
00:16:53,000 --> 00:16:58,000
从IP10的标准的IP16到TF32、IP8、IP4。

105
00:16:58,000 --> 00:17:05,000
最后一个呢,就是对应的我们硬件改了,然后我们对应的软件编程呢,也会去修改。

106
00:17:05,000 --> 00:17:08,000
预设呢,又有了新的CUDA的编程的模式。

107
00:17:08,000 --> 00:17:10,000
那今天的内容呢,就到这里为止。

108
00:17:10,000 --> 00:17:12,000
谢谢各位,拜拜。

109
00:17:13,000 --> 00:17:15,000
卷的不行啦,卷的不行啦。

110
00:17:15,000 --> 00:17:17,000
记得一键三连加关注哦。

111
00:17:17,000 --> 00:17:20,000
所有的内容都会开源在下面这条链接里面。

112
00:17:20,000 --> 00:17:22,000
拜拜。

