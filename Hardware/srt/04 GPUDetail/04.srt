1
00:00:00,000 --> 00:00:07,880
大家好,我是三天打鱼,两天撒网的钟米

2
00:00:07,880 --> 00:00:09,920
这个视频确实拖更了很久

3
00:00:09,920 --> 00:00:14,520
现在我们回到了AI芯片的GPU详解这个系列里面

4
00:00:14,520 --> 00:00:19,320
在这个系列我们重点的去关注Tensor Core和NVLink

5
00:00:19,320 --> 00:00:20,960
两个主要的内容

6
00:00:20,960 --> 00:00:23,960
今天我想给大家去汇报一下

7
00:00:23,960 --> 00:00:27,880
分布式训练跟NVLink的一个简单的发展

8
00:00:27,920 --> 00:00:30,320
以前我们回顾一下整体的大纲

9
00:00:30,320 --> 00:00:32,560
其实之前的大纲我想分开GPU

10
00:00:32,560 --> 00:00:34,040
因为它的整体的架构

11
00:00:34,040 --> 00:00:37,920
Tensor Core和NVLink本来想用一小节去结束的

12
00:00:37,920 --> 00:00:42,600
后来就发现NVLink跟Tensor Core里面有非常多的内容

13
00:00:42,600 --> 00:00:45,520
所以就单独的GPU详解单独的提取出来

14
00:00:45,520 --> 00:00:48,000
Tensor Core就分享了三节

15
00:00:48,000 --> 00:00:51,920
而NVLink跟NVSwitch也会分开三节来看

16
00:00:51,920 --> 00:00:55,760
最后我们回顾一下GPU的图形处理这一块

17
00:00:55,760 --> 00:00:59,520
就不在AI系列里面我们就不详细的去展开

18
00:00:59,520 --> 00:01:00,760
那有机会的

19
00:01:00,760 --> 00:01:03,840
今天给大家汇报的内容主要分为两个

20
00:01:03,840 --> 00:01:06,080
第一个就是分布式训练

21
00:01:06,080 --> 00:01:08,440
看一下分布式训练到底是什么

22
00:01:08,440 --> 00:01:11,480
简单的回顾我们之前给大家汇报的内容

23
00:01:11,480 --> 00:01:14,120
接着我们看一下今天的主角NVLink

24
00:01:14,120 --> 00:01:16,080
NVSwitch的简单的发展

25
00:01:16,080 --> 00:01:19,960
后面我们分开两个小节给大家详细的汇报的

26
00:01:19,960 --> 00:01:22,440
第一个就是分布式训练

27
00:01:22,440 --> 00:01:24,840
其实周米在之前的课程里面

28
00:01:24,840 --> 00:01:27,520
给大家汇报过什么是AI集群

29
00:01:27,520 --> 00:01:29,400
当我们的模型越来越大的时候

30
00:01:29,400 --> 00:01:31,560
我们就需要分布式的去训练

31
00:01:31,560 --> 00:01:34,720
那分布式训练我们就涉及到PS的架构

32
00:01:34,720 --> 00:01:36,280
AI的软硬件通讯

33
00:01:36,280 --> 00:01:37,480
还有集合通讯

34
00:01:37,480 --> 00:01:40,760
当然我们的AI框架也需要具备分布式的能力

35
00:01:40,760 --> 00:01:43,800
接着去看一下我们分布式的大模型

36
00:01:43,800 --> 00:01:45,560
有哪些不一样的算法

37
00:01:45,560 --> 00:01:48,640
接着我要释子常用的数据并行

38
00:01:48,640 --> 00:01:49,320
张量并行

39
00:01:49,320 --> 00:01:50,200
流水线并行

40
00:01:50,200 --> 00:01:51,440
多维缓和并行

41
00:01:51,440 --> 00:01:53,680
各种各样的分布式并行的策略

42
00:01:53,680 --> 00:01:55,760
也欢迎大家去了解

43
00:01:55,760 --> 00:01:59,960
那现在我们回顾一下整个分布式训练

44
00:01:59,960 --> 00:02:01,000
有哪些不一样的

45
00:02:01,000 --> 00:02:02,400
整个人工智能的发展

46
00:02:02,480 --> 00:02:05,120
这个图给大家讲过很多遍了

47
00:02:05,120 --> 00:02:07,360
我们现在从深度学习迎来了

48
00:02:07,360 --> 00:02:08,280
Foundation Model

49
00:02:08,280 --> 00:02:09,840
也就是我们的大模型

50
00:02:09,840 --> 00:02:11,840
现在周米负责的事情

51
00:02:12,720 --> 00:02:15,160
看一下右边的图表

52
00:02:15,160 --> 00:02:16,840
可以看到红色的这条线

53
00:02:17,000 --> 00:02:19,520
就是真正的大模型来到的时代

54
00:02:19,560 --> 00:02:21,680
而所谓的大模型主要是指

55
00:02:21,680 --> 00:02:23,520
我们的模型的参数量

56
00:02:23,520 --> 00:02:25,360
进一步的急剧的膨胀

57
00:02:25,360 --> 00:02:28,120
那大模型给我们带来的好处有三个

58
00:02:28,120 --> 00:02:31,360
第一个就是引入了自监督的学习的方法

59
00:02:31,360 --> 00:02:32,360
我们的模型大了

60
00:02:32,360 --> 00:02:33,160
我们的数据大了

61
00:02:33,160 --> 00:02:35,400
没办法所有东西都要人工的去标注

62
00:02:35,400 --> 00:02:37,600
是我们的模型的参数量

63
00:02:37,600 --> 00:02:39,280
参数规模越来越大

64
00:02:39,280 --> 00:02:42,400
但是模型的精度却是进一步的去提升

65
00:02:42,400 --> 00:02:44,360
而且提升的非常的惊人

66
00:02:44,360 --> 00:02:47,560
第三个就是解决了模型碎片化的问题

67
00:02:47,560 --> 00:02:51,240
通过提出了预训链到我们后来的GPT-3

68
00:02:51,240 --> 00:02:54,440
GPT-4这种CircleSwap的能力越来越强

69
00:02:54,440 --> 00:02:57,680
所以大模型确实给我们的AIGC带来了

70
00:02:57,680 --> 00:02:59,520
非常多的想象力

71
00:02:59,520 --> 00:03:02,080
那现在我们回到我们的模型越来越大

72
00:03:02,080 --> 00:03:05,360
没办法把一个单独的模型放在一款芯片

73
00:03:05,360 --> 00:03:07,640
或者一款GPU NPU里面

74
00:03:07,640 --> 00:03:11,200
于是就出现了各种各样的并行的策略

75
00:03:11,200 --> 00:03:13,200
那我们可以在整体上来说

76
00:03:13,200 --> 00:03:14,760
不管是哪种并行

77
00:03:14,760 --> 00:03:17,200
它主要是对我们的模型进行切分

78
00:03:17,200 --> 00:03:19,160
那有横着切有竖着切

79
00:03:19,160 --> 00:03:21,440
那横着切就是把我们的模型的层数

80
00:03:21,440 --> 00:03:23,440
单独的切出来放在不同的机器

81
00:03:23,440 --> 00:03:27,240
竖着切就是指把我们的模型像三个并行的这种

82
00:03:27,240 --> 00:03:31,640
就是竖着切把我们的切出来放在不同的芯片上面

83
00:03:31,640 --> 00:03:33,040
或者不同的GPU上面

84
00:03:33,040 --> 00:03:35,000
有了这些基础的并行算法之后

85
00:03:35,000 --> 00:03:39,560
我们整个系统确实要支持这种计算节点的夸机

86
00:03:39,560 --> 00:03:41,400
原来的我们只是一个网络模型

87
00:03:41,400 --> 00:03:43,760
我们现在要把这一个网络模型

88
00:03:43,760 --> 00:03:45,280
分布在不同的机器

89
00:03:45,280 --> 00:03:49,080
这里面就涉及到我们整体的系统怎么去通讯

90
00:03:49,080 --> 00:03:52,600
那刚才看到我们刚才有两个节点是互相通讯的

91
00:03:52,600 --> 00:03:54,240
现在我们的节点越来越多

92
00:03:54,240 --> 00:03:55,760
分开不同的权重

93
00:03:55,760 --> 00:03:58,720
所以我们的参数量也放在不同的模型里面

94
00:03:58,720 --> 00:04:01,080
不同的去进行一个交互

95
00:04:01,080 --> 00:04:03,560
那这种方式就是计算图

96
00:04:03,560 --> 00:04:08,240
整个计算图跨节点同步数据进行交互

97
00:04:08,240 --> 00:04:10,160
这种就是分布式训练

98
00:04:10,160 --> 00:04:11,640
那既然谈到分布式训练

99
00:04:11,640 --> 00:04:12,600
我们刚才只是讲了

100
00:04:12,600 --> 00:04:14,960
大家汇报了软件层面

101
00:04:14,960 --> 00:04:17,960
做了一些相关的策略和算法

102
00:04:17,960 --> 00:04:19,800
接着我们看一下硬件层面

103
00:04:19,800 --> 00:04:22,880
在通讯硬件我们需要有哪些不一样的东西

104
00:04:23,720 --> 00:04:26,120
这是机器类的通讯

105
00:04:26,120 --> 00:04:27,160
简单的来说

106
00:04:27,320 --> 00:04:29,880
就是一款机器里面有很多张

107
00:04:29,880 --> 00:04:31,720
不过AI加速芯片这种

108
00:04:31,720 --> 00:04:33,440
就是机器类的通讯

109
00:04:33,440 --> 00:04:35,280
而机器间的通讯

110
00:04:35,480 --> 00:04:38,000
主要是指我们不同的机器之间

111
00:04:38,000 --> 00:04:40,440
不同的服务器主机之间

112
00:04:40,720 --> 00:04:42,120
如何进行通讯

113
00:04:42,320 --> 00:04:43,640
在机器间的通讯

114
00:04:43,800 --> 00:04:46,040
我们可以通过共享内存的方式

115
00:04:46,040 --> 00:04:47,360
也可以通过PCIE

116
00:04:47,360 --> 00:04:49,400
当然也有我们今天的主角

117
00:04:49,400 --> 00:04:51,200
NVLink直连的模式

118
00:04:51,200 --> 00:04:55,000
把GPU跟GPU之间直接互相连起来

119
00:04:55,000 --> 00:04:55,880
另外的话

120
00:04:55,880 --> 00:04:57,120
机器之间通讯

121
00:04:57,320 --> 00:04:58,320
就我们上面

122
00:04:58,320 --> 00:04:59,680
这是一排机柜

123
00:04:59,840 --> 00:05:02,520
机柜之间可能有一台华为的阿特拉斯

124
00:05:02,520 --> 00:05:04,840
下面也有一台华为的阿特拉斯

125
00:05:04,840 --> 00:05:07,440
一个机柜可以在8台服务器

126
00:05:07,720 --> 00:05:08,840
机器间的通信

127
00:05:08,960 --> 00:05:10,240
我们有TCPIP

128
00:05:10,240 --> 00:05:11,360
还有RDMA

129
00:05:11,560 --> 00:05:14,040
现在我们经常谈到的IB网络

130
00:05:14,040 --> 00:05:16,320
是RDMA的其中一种

131
00:05:16,320 --> 00:05:19,840
Rocky也是RDMA网络的其中一种

132
00:05:20,280 --> 00:05:21,760
了解到硬件之后

133
00:05:21,960 --> 00:05:23,480
硬件要实现通讯

134
00:05:23,680 --> 00:05:25,280
字里面就离不开

135
00:05:25,280 --> 00:05:28,160
提供集合通讯的一些库了

136
00:05:28,320 --> 00:05:29,360
集合通讯一些库

137
00:05:29,520 --> 00:05:31,560
最常用的是MPI

138
00:05:31,560 --> 00:05:33,720
在CPU上面用的非常多

139
00:05:33,720 --> 00:05:35,960
而在英伟达的GPU上面

140
00:05:36,080 --> 00:05:38,640
用的最多的就是NCCL

141
00:05:38,640 --> 00:05:40,160
N就是英伟达

142
00:05:40,160 --> 00:05:42,440
华为自己就推出了HCCL

143
00:05:42,640 --> 00:05:45,040
因为有个A-Connected-Layer

144
00:05:45,720 --> 00:05:46,440
这个库

145
00:05:46,600 --> 00:05:48,720
英伟达就使能了NVSwitch

146
00:05:48,720 --> 00:05:49,800
或者NVLink

147
00:05:49,800 --> 00:05:51,920
把我们不同的GPU跟GPU之间

148
00:05:51,920 --> 00:05:52,840
互联起来

149
00:05:52,840 --> 00:05:55,000
而是通过算法层面

150
00:05:55,000 --> 00:05:57,360
对外提供对应的API

151
00:05:57,360 --> 00:05:58,480
而这里面的API

152
00:05:58,640 --> 00:06:00,320
就涉及到我们的集合通讯

153
00:06:00,320 --> 00:06:02,000
有一对多的Gether

154
00:06:02,000 --> 00:06:04,360
有一对多的Sketch和Broadcast

155
00:06:04,360 --> 00:06:06,040
当然我们还多对多的

156
00:06:06,040 --> 00:06:06,960
All-Videosummer

157
00:06:06,960 --> 00:06:08,040
All-Gether等

158
00:06:08,040 --> 00:06:10,400
各种各样的通讯集合的API

159
00:06:10,400 --> 00:06:12,800
或者通讯集合的语言

160
00:06:15,240 --> 00:06:17,520
现在我们来到了第2个内容

161
00:06:17,520 --> 00:06:20,640
NVLink跟NVSwitch的发展

162
00:06:20,680 --> 00:06:22,760
在正式进入内容之前

163
00:06:22,920 --> 00:06:25,960
我想给大家去看一个简单的视频的

164
00:06:25,960 --> 00:06:27,280
首先在看视频之前

165
00:06:27,400 --> 00:06:28,480
我还想介绍一下

166
00:06:28,480 --> 00:06:29,600
什么是NVLink

167
00:06:29,600 --> 00:06:31,240
什么是NVSwitch

168
00:06:31,600 --> 00:06:33,080
首先我们可以了解一下

169
00:06:33,080 --> 00:06:34,240
NVLink其实是一种

170
00:06:34,240 --> 00:06:36,320
总线或者通讯的协议

171
00:06:36,320 --> 00:06:37,960
在CPU跟GPU之间互联

172
00:06:37,960 --> 00:06:41,040
也可以在GPU跟GPU之间互联

173
00:06:41,040 --> 00:06:42,880
现在用的最多的是

174
00:06:42,880 --> 00:06:44,480
GPU跟GPU之间互联

175
00:06:44,480 --> 00:06:46,360
而GPU跟CPU之间互联

176
00:06:46,560 --> 00:06:49,320
现在最新一代的是我们的H100

177
00:06:49,320 --> 00:06:52,280
还有之前的IBM的Power系列

178
00:06:52,280 --> 00:06:55,080
NVSwitch就是一种高速的互联技术

179
00:06:55,320 --> 00:06:58,560
主要是作为独立的NVLink的芯片

180
00:06:58,560 --> 00:07:01,480
提供了非常多路的NVLink的接口

181
00:07:02,280 --> 00:07:04,440
NVSwitch大家可以理解为

182
00:07:04,440 --> 00:07:06,360
它为具体模块的芯片

183
00:07:06,360 --> 00:07:07,840
而NVLink就是一种

184
00:07:07,840 --> 00:07:09,760
总线和通讯的协议

185
00:07:10,080 --> 00:07:12,280
现在我们看一个具体的视频

186
00:07:12,280 --> 00:07:13,280
来看看

187
00:07:13,280 --> 00:07:14,280
到底NVLink

188
00:07:14,280 --> 00:07:16,480
NVSwitch的具体的差别

189
00:07:34,440 --> 00:07:35,440
NVSwitch提供了

190
00:07:35,440 --> 00:07:37,560
7倍的PCIe Gen5的速度

191
00:07:37,560 --> 00:07:39,880
为了提供更快的总统性表现

192
00:07:40,320 --> 00:07:42,080
NVIDIA提供的这种表现

193
00:07:42,080 --> 00:07:45,880
是NVLink和NVIDIA NVSwitch

194
00:07:45,880 --> 00:07:47,440
的两个互联的能力

195
00:07:47,440 --> 00:07:49,720
来提供多个GPU的通讯

196
00:07:50,000 --> 00:07:51,720
NVSwitch支援

197
00:07:51,720 --> 00:07:53,880
16个GPU在一个伺服器上

198
00:07:53,880 --> 00:07:56,520
这里我们展示8个GPU在一个伺服器上

199
00:07:56,520 --> 00:07:58,160
任何4个GPU的双方

200
00:07:58,160 --> 00:08:00,000
可以同时通讯

201
00:08:00,000 --> 00:08:01,920
在900Gbps的情况下

202
00:08:01,960 --> 00:08:03,160
提供了一个不可思议的

203
00:08:03,160 --> 00:08:05,160
3.6Tbps的

204
00:08:05,160 --> 00:08:07,680
统一通讯在一个伺服器上

205
00:08:07,680 --> 00:08:10,080
NVSwitch还能迅速地

206
00:08:10,080 --> 00:08:11,560
通讯

207
00:08:11,560 --> 00:08:12,680
通过多层架构

208
00:08:12,680 --> 00:08:14,760
和鲜明的缩减在网络上

209
00:08:15,120 --> 00:08:16,560
NVLink延伸

210
00:08:16,560 --> 00:08:19,440
同样的高速GPU-GPU连接

211
00:08:19,440 --> 00:08:20,440
在伺服器上

212
00:08:20,440 --> 00:08:22,440
来连接伺服器的设备

213
00:08:22,840 --> 00:08:24,080
NVLink的连接器

214
00:08:24,080 --> 00:08:25,800
可以直接连接

215
00:08:25,800 --> 00:08:28,360
NVIDIA H100 Tensor Core GPU

216
00:08:28,360 --> 00:08:29,240
在每个伺服器上

217
00:08:29,240 --> 00:08:31,160
与高速NVLink连接

218
00:08:31,640 --> 00:08:35,080
直到32个NVIDIA DGX-H100伺服器

219
00:08:35,080 --> 00:08:37,680
每个都有8个H100的GPU

220
00:08:37,680 --> 00:08:39,400
都可以连接

221
00:08:39,400 --> 00:08:40,760
每个GPU

222
00:08:40,760 --> 00:08:42,600
都由NVLink连接

223
00:08:42,600 --> 00:08:44,440
到每个其他GPU

224
00:08:44,440 --> 00:08:47,080
每个18个NVLink的伺服器

225
00:08:47,080 --> 00:08:48,320
都连接到每个

226
00:08:48,320 --> 00:08:50,680
32个H100的伺服器

227
00:08:51,160 --> 00:08:52,240
这种架构

228
00:08:52,240 --> 00:08:55,400
组成了256个GPU

229
00:08:55,400 --> 00:08:57,520
连接伺服器的团体

230
00:08:57,520 --> 00:09:00,600
提供了57.6Tbps

231
00:09:00,640 --> 00:09:02,200
全线连接的快速连接

232
00:09:02,720 --> 00:09:05,240
NVLink的连接系统

233
00:09:05,240 --> 00:09:06,960
MagnumIO的设备

234
00:09:06,960 --> 00:09:09,040
与伺服器的专业程度

235
00:09:09,040 --> 00:09:12,240
提供了上至9次的AI训练连接

236
00:09:12,240 --> 00:09:14,360
与专业模式的混合

237
00:09:14,360 --> 00:09:15,920
这让研究员

238
00:09:15,920 --> 00:09:17,480
在业务的速度下

239
00:09:17,480 --> 00:09:19,360
连接大量的模式

240
00:09:24,240 --> 00:09:24,920
好嘞

241
00:09:24,920 --> 00:09:26,240
通过刚才的视频

242
00:09:26,240 --> 00:09:27,320
我们了解到了

243
00:09:27,320 --> 00:09:28,960
跟NVSwitch的关系

244
00:09:28,960 --> 00:09:30,200
还有NVLink、NVSwitch

245
00:09:30,200 --> 00:09:31,120
跟A100

246
00:09:31,120 --> 00:09:32,720
或者我们的DGX

247
00:09:32,720 --> 00:09:34,360
服务器主机

248
00:09:34,360 --> 00:09:35,400
跟服务器柜

249
00:09:35,400 --> 00:09:36,800
之间的一个关系了

250
00:09:36,800 --> 00:09:37,920
现在我们来看一下

251
00:09:37,920 --> 00:09:39,600
NVLink的整体的发展

252
00:09:39,600 --> 00:09:41,960
首先NVLink到现在为止

253
00:09:42,080 --> 00:09:43,720
也就是2023年年初

254
00:09:43,720 --> 00:09:45,400
已经经历了4代

255
00:09:45,400 --> 00:09:46,200
从第一代

256
00:09:46,200 --> 00:09:46,560
第二代

257
00:09:46,560 --> 00:09:47,040
第三代

258
00:09:47,040 --> 00:09:47,920
第四代

259
00:09:47,920 --> 00:09:50,440
每一代的NVLink的带宽

260
00:09:50,440 --> 00:09:52,760
也是不断的去提升的

261
00:09:52,760 --> 00:09:54,280
而NVLink之间

262
00:09:54,440 --> 00:09:56,040
能够酷年的GPU

263
00:09:56,040 --> 00:09:57,640
从只有4路

264
00:09:57,960 --> 00:10:00,120
到现在的18路

265
00:10:00,120 --> 00:10:00,960
整体的架构

266
00:10:00,960 --> 00:10:02,520
我们简单的对应一下

267
00:10:02,520 --> 00:10:04,240
第一代是我们的

268
00:10:04,240 --> 00:10:06,520
P型的PASCA架构

269
00:10:06,520 --> 00:10:08,120
第二代是Volted架构

270
00:10:08,120 --> 00:10:10,240
也就是V100用的非常的多

271
00:10:10,240 --> 00:10:11,440
到现在来说

272
00:10:11,440 --> 00:10:12,200
大量的供货

273
00:10:12,360 --> 00:10:14,520
应该是我们的第三代的

274
00:10:14,520 --> 00:10:16,600
NVLink的GPU

275
00:10:17,120 --> 00:10:19,080
可能到现在为止

276
00:10:19,080 --> 00:10:22,040
我们暂时买到的是Hopper架构

277
00:10:22,040 --> 00:10:23,920
那Hopper架构的NVLink

278
00:10:23,920 --> 00:10:26,840
因为带宽非常的夸张惊人

279
00:10:26,840 --> 00:10:29,560
所以这一代也是被国外所禁止

280
00:10:29,560 --> 00:10:31,000
进口到中国

281
00:10:31,000 --> 00:10:32,360
现在我们看一下

282
00:10:32,360 --> 00:10:34,000
NVLink每一代的发展

283
00:10:34,160 --> 00:10:35,720
其实从2015年

284
00:10:35,720 --> 00:10:37,680
刚才我们提到的PASCA架构

285
00:10:37,680 --> 00:10:40,880
到2022年的Hopper架构的推出

286
00:10:40,880 --> 00:10:42,640
整体的GPU的性能

287
00:10:42,880 --> 00:10:45,200
是不断的翻倍的增长

288
00:10:45,200 --> 00:10:47,120
到现在最夸张的是

289
00:10:47,120 --> 00:10:50,960
900GB每秒的数据的传输速率

290
00:10:51,240 --> 00:10:53,600
回顾一下整体的NVLink的架构

291
00:10:53,760 --> 00:10:55,440
刚才我们讲到的

292
00:10:55,480 --> 00:10:57,000
GPU跟GPU之间互联

293
00:10:57,200 --> 00:10:59,560
在一开始2016年的时候

294
00:10:59,560 --> 00:11:02,920
PASCA架构只有4路数据进行互联

295
00:11:02,920 --> 00:11:04,600
到现在的Hopper架构

296
00:11:04,760 --> 00:11:06,320
它有18路的数据

297
00:11:06,320 --> 00:11:09,040
进行一个GPU跟GPU之间的互联

298
00:11:09,040 --> 00:11:10,000
整体来说

299
00:11:10,240 --> 00:11:11,880
每一条NVLink的链路

300
00:11:12,040 --> 00:11:15,400
在第一代其实只有40GB每秒

301
00:11:15,400 --> 00:11:16,360
但是总体来说

302
00:11:16,640 --> 00:11:19,680
NVLink里面每路的数据

303
00:11:19,680 --> 00:11:22,360
基本上到后面都是50GB

304
00:11:22,360 --> 00:11:24,480
每秒每一条链路

305
00:11:24,480 --> 00:11:27,320
但是随着链路数整体的增加了

306
00:11:27,320 --> 00:11:30,520
GPU跟带宽是不断的增长的

307
00:11:30,520 --> 00:11:33,920
现在我们来看一看NVSwitch

308
00:11:33,920 --> 00:11:36,200
NVSwitch其实只有三代

309
00:11:36,480 --> 00:11:38,120
从第一代的First Generation

310
00:11:38,280 --> 00:11:40,440
其实是在Voltage架构

311
00:11:40,440 --> 00:11:42,240
也就Voltage架构开始

312
00:11:42,240 --> 00:11:43,440
到AMP架构

313
00:11:43,440 --> 00:11:44,320
Hopper架构

314
00:11:44,320 --> 00:11:45,680
每一款架构

315
00:11:45,800 --> 00:11:47,440
演进新的一代

316
00:11:47,440 --> 00:11:49,520
所以PASCA架构

317
00:11:49,840 --> 00:11:52,720
它其实是没有出现NVSwitch的

318
00:11:52,720 --> 00:11:53,840
它只有NVLink

319
00:11:53,840 --> 00:11:55,120
NVLink之间

320
00:11:55,120 --> 00:11:57,440
进行一个CubeMesh的互联

321
00:11:57,440 --> 00:11:58,920
现在我们来看一下

322
00:11:58,920 --> 00:12:01,320
这几代NVSwitch有什么不一样

323
00:12:01,640 --> 00:12:03,800
首先GPU之间互联的节点

324
00:12:03,960 --> 00:12:05,640
也就是我们第一行

325
00:12:05,640 --> 00:12:08,200
基本上每一代最大的是8

326
00:12:08,200 --> 00:12:09,520
但是NVSwitch

327
00:12:09,520 --> 00:12:10,920
GPU跟GPU之间的互联

328
00:12:10,920 --> 00:12:11,920
我们刚才讲到了

329
00:12:11,920 --> 00:12:14,320
其实它跟我们的NVLink是相关的

330
00:12:14,320 --> 00:12:15,520
因为NVSwitch

331
00:12:15,680 --> 00:12:16,760
就是我们NVLink

332
00:12:16,760 --> 00:12:19,080
具体的承载的芯片模组

333
00:12:19,080 --> 00:12:20,760
里面从300GB每秒

334
00:12:20,760 --> 00:12:22,360
到600GB每秒

335
00:12:22,400 --> 00:12:24,120
到现在的900GB每秒

336
00:12:24,120 --> 00:12:27,040
当然了国内进口限制在400GB每秒

337
00:12:27,040 --> 00:12:28,400
所以我们大部分时候

338
00:12:29,080 --> 00:12:31,760
我们只能买到H800

339
00:12:31,760 --> 00:12:33,360
或者A800

340
00:12:33,360 --> 00:12:34,280
买不到A100

341
00:12:34,280 --> 00:12:35,120
我们看一下

342
00:12:35,120 --> 00:12:37,760
整个NVSwitch的一个关系

343
00:12:37,960 --> 00:12:39,960
一开始从PASCA架构

344
00:12:40,120 --> 00:12:41,800
其实在DGS1里面

345
00:12:42,040 --> 00:12:43,480
只有一个CubeMesh

346
00:12:43,480 --> 00:12:46,320
也就是这里面是没有NVSwitch的

347
00:12:46,320 --> 00:12:47,200
只有NVLink

348
00:12:47,360 --> 00:12:50,280
NVLink每一款GPU里面有4路

349
00:12:50,280 --> 00:12:52,920
所以这里面有1234

350
00:12:52,920 --> 00:12:56,000
只能跟4条链路进行互联

351
00:12:56,000 --> 00:12:58,280
同样的这里面这一台P100

352
00:12:58,600 --> 00:13:01,760
距1234上下左右

353
00:13:01,760 --> 00:13:03,760
这几个只有连接4条

354
00:13:03,760 --> 00:13:06,600
里面就各自组成一个CubeMesh

355
00:13:06,600 --> 00:13:08,760
但是在我们的Volt架构

356
00:13:08,760 --> 00:13:10,360
DGS2里面

357
00:13:10,560 --> 00:13:12,280
应该是2018年推出的

358
00:13:12,280 --> 00:13:13,440
每一款V100

359
00:13:13,600 --> 00:13:15,560
就可以跟NVSwitch里面

360
00:13:15,560 --> 00:13:17,160
另外一款V100

361
00:13:17,320 --> 00:13:19,040
进行一个互联

362
00:13:19,080 --> 00:13:20,800
到了2022年

363
00:13:20,800 --> 00:13:22,440
也就是我们的A100

364
00:13:22,440 --> 00:13:24,120
Amber架构的出现的时候

365
00:13:24,360 --> 00:13:25,920
我们NVSwitch的模组

366
00:13:26,080 --> 00:13:27,640
经过了一个更新

367
00:13:27,640 --> 00:13:28,520
每一款A100

368
00:13:28,880 --> 00:13:30,680
可以跟任何一款A牌

369
00:13:30,680 --> 00:13:31,920
进行一个互联的

370
00:13:31,920 --> 00:13:33,480
这个时候对于硬件来说

371
00:13:33,640 --> 00:13:35,560
是节省了非常多的链路

372
00:13:35,560 --> 00:13:37,720
而技术也是进一步的提升的

373
00:13:37,720 --> 00:13:40,000
到最新的A100里面

374
00:13:40,160 --> 00:13:42,920
又有了一个新的技术的突破

375
00:13:43,760 --> 00:13:44,360
下面

376
00:13:45,640 --> 00:13:46,760
简单的总结一下

377
00:13:46,760 --> 00:13:48,520
刚才讲过的几个概念

378
00:13:48,520 --> 00:13:52,120
首先NCCL是一个集合的通讯库

379
00:13:52,120 --> 00:13:53,920
能够实现集合的通讯

380
00:13:53,920 --> 00:13:55,040
和点对点的通讯

381
00:13:55,040 --> 00:13:55,840
另外的话

382
00:13:55,840 --> 00:13:58,360
NVLink它是一个总线的

383
00:13:58,360 --> 00:13:59,600
或者通讯的协议

384
00:13:59,600 --> 00:14:02,520
NVSwitch是独立的NVLink的芯片

385
00:14:02,520 --> 00:14:04,680
而HGS或者DGS

386
00:14:04,880 --> 00:14:07,960
是一个AI的超级计算的平台

387
00:14:07,960 --> 00:14:09,600
也就是简单的一台主机

388
00:14:09,600 --> 00:14:12,320
一台主机里面挂了很多GPU

389
00:14:12,320 --> 00:14:14,440
和多个NVSwitch

390
00:14:14,680 --> 00:14:16,560
今天的内容卷的不行了

391
00:14:16,560 --> 00:14:17,400
卷的不行了

392
00:14:17,520 --> 00:14:19,320
记得一键三连加关注哦

393
00:14:19,320 --> 00:14:20,960
所有的内容都会开源在

394
00:14:20,960 --> 00:14:22,480
下面这条链接里面

395
00:14:22,920 --> 00:14:23,760
摆了个掰

