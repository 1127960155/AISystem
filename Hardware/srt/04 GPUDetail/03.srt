1
00:00:00,000 --> 00:00:10,000
哈喽大家好,我是在大大的公司里面哇呀哇呀哇的中米

2
00:00:20,000 --> 00:00:26,000
今天我们来到了AI芯片GPU详解里面的Tensor Core第三个内容,深度剖析

3
00:00:26,000 --> 00:00:30,000
那这个所谓的深度剖析呢更多的是我中米个人的理解

4
00:00:30,000 --> 00:00:33,000
今天呢我们主要给大家去分享三个内容

5
00:00:33,000 --> 00:00:38,000
第一个呢就是Tensor Core回顾一下整个Tensor Core的具体执行的流程

6
00:00:38,000 --> 00:00:42,000
第二个呢就我们去看一下指令的流水

7
00:00:42,000 --> 00:00:45,000
特别是指我们的Tensor Core的Instruction Pipeline

8
00:00:45,000 --> 00:00:48,000
第三个呢我们来一起去看看CUDA Flat

9
00:00:48,000 --> 00:00:54,000
就是CUDA的现成执行跟我们的硬件具体是怎么去结合的

10
00:00:54,000 --> 00:00:58,000
这里面的深度就会给大家带来一些非常之个人主观的

11
00:00:58,000 --> 00:01:03,000
如果大家觉得不对的也可以去补槽和指正和指点

12
00:01:06,000 --> 00:01:08,000
现在呢我们来到了第一个内容

13
00:01:08,000 --> 00:01:10,000
去看看Tensor Core的执行

14
00:01:10,000 --> 00:01:12,000
一个4x4的矩阵A

15
00:01:12,000 --> 00:01:13,000
4x4的矩阵B

16
00:01:13,000 --> 00:01:16,000
再加上一个4x4的矩阵C

17
00:01:16,000 --> 00:01:21,000
那两个A和经度呢就是在计算的过程当中呢使用FP16去计算

18
00:01:21,000 --> 00:01:26,000
但是存储的时候呢使用FP32或者FP16进行存储

19
00:01:26,000 --> 00:01:29,000
那在整一个在数学我们实际计算的时候呢

20
00:01:29,000 --> 00:01:33,000
是把矩一行乘以矩阵的一列

21
00:01:33,000 --> 00:01:38,000
然后再加上单独一个元素得到我们D矩阵的第一个元素

22
00:01:38,000 --> 00:01:39,000
所以由下面

23
00:01:39,000 --> 00:01:41,000
接下来呢我们把第一行跟第一列进行相乘

24
00:01:41,000 --> 00:01:46,000
实际上我们在整个计算的时候呢会把第二行跟第二列进行相乘

25
00:01:46,000 --> 00:01:48,000
再加上对角线的C1E

26
00:01:48,000 --> 00:01:51,000
再接下来我们还有更多的操作

27
00:01:51,000 --> 00:01:55,000
就是每一行跟每一列都需要相乘

28
00:01:55,000 --> 00:01:57,000
才能够得到我们所有的元素

29
00:01:57,000 --> 00:01:59,000
也就是D行跟第一列相乘

30
00:01:59,000 --> 00:02:01,000
D行跟第二列相乘

31
00:02:01,000 --> 00:02:02,000
第三列第四列

32
00:02:02,000 --> 00:02:03,000
每一行都是

33
00:02:03,000 --> 00:02:06,000
V100其实并不是一行一行的去计算

34
00:02:06,000 --> 00:02:09,000
而是整一个矩阵整一个矩阵的去计算的

35
00:02:09,000 --> 00:02:12,000
我们来看看下面官方给出来的模拟图

36
00:02:12,000 --> 00:02:14,000
PASCA呢就是上一代的架构

37
00:02:14,000 --> 00:02:15,000
没有Tensor Core之前的

38
00:02:15,000 --> 00:02:17,000
是一个元素跟一行进行相乘

39
00:02:17,000 --> 00:02:20,000
每个时钟周期呢执行四次相乘

40
00:02:20,000 --> 00:02:21,000
得到一列数据

41
00:02:21,000 --> 00:02:23,000
而在V100里面呢

42
00:02:23,000 --> 00:02:27,000
也就是我们右矩阵A跟整个矩阵B进行相乘

43
00:02:27,000 --> 00:02:30,000
然后呢得到我们整一个矩阵的输出

44
00:02:30,000 --> 00:02:31,000
整体来说

45
00:02:31,000 --> 00:02:33,000
右边的这个Tensor Core呢

46
00:02:33,000 --> 00:02:34,000
在单个时钟周期内呢

47
00:02:34,000 --> 00:02:36,000
就能够执行四乘四乘四

48
00:02:36,000 --> 00:02:38,000
等于64次的FMA

49
00:02:38,000 --> 00:02:41,000
也就是乘加的计算操作

50
00:02:41,000 --> 00:02:42,000
它的吞吐呢

51
00:02:42,000 --> 00:02:45,000
会比左边的PASCA架构呢快了12倍

52
00:02:45,000 --> 00:02:47,000
那PASCA架构这种计算呢

53
00:02:47,000 --> 00:02:49,000
使用的是CUDA Core

54
00:02:49,000 --> 00:02:51,000
而V100呢就出现了Tensor Core

55
00:02:51,000 --> 00:02:53,000
专门去计算我们的进行加速

56
00:02:53,000 --> 00:02:56,000
我们现在呢来看一下V100的一个V架构

57
00:02:56,000 --> 00:02:57,000
或者回顾一下

58
00:02:57,000 --> 00:02:58,000
一个时钟周期呢

59
00:02:58,000 --> 00:03:01,000
其实只能执行16个FMA

60
00:03:01,000 --> 00:03:03,000
但是呢这100的Tensor Core里面呢

61
00:03:03,000 --> 00:03:05,000
我们一个时钟周期内呢

62
00:03:05,000 --> 00:03:08,000
可以执行两个四乘四乘四的FMA的操作

63
00:03:08,000 --> 00:03:09,000
整体来说

64
00:03:09,000 --> 00:03:11,000
所以Tensor Core的计算吞吐呢

65
00:03:11,000 --> 00:03:14,000
比右边的这一坨要高12倍

66
00:03:14,000 --> 00:03:16,000
那下面呢我们再看一下

67
00:03:16,000 --> 00:03:18,000
因为呢SM里面呢有四个Support

68
00:03:18,000 --> 00:03:21,000
每个Support里面呢有两个Tensor Core

69
00:03:21,000 --> 00:03:23,000
一个时钟周期内呢

70
00:03:23,000 --> 00:03:25,000
能执行64个FMA

71
00:03:25,000 --> 00:03:27,000
里面的一个SM呢

72
00:03:27,000 --> 00:03:31,000
就可以执行1024个FMA了

73
00:03:33,000 --> 00:03:35,000
现在我们来到了第二个内容

74
00:03:35,000 --> 00:03:37,000
Tensor Core的指令流水

75
00:03:37,000 --> 00:03:39,000
有两个不同的符号

76
00:03:39,000 --> 00:03:40,000
一个是加号

77
00:03:40,000 --> 00:03:41,000
一个是乘号

78
00:03:41,000 --> 00:03:43,000
我们要在Tensor Core里面呢

79
00:03:43,000 --> 00:03:46,000
去实现刚才的一条简单的矩阵相乘

80
00:03:46,000 --> 00:03:49,000
把A的一行呢乘以B矩阵的一列

81
00:03:49,000 --> 00:03:51,000
也就是下面对应的这条公式

82
00:03:51,000 --> 00:03:53,000
于是呢我们的电路呢

83
00:03:53,000 --> 00:03:55,000
假设这里面只是我的臆想

84
00:03:55,000 --> 00:03:57,000
不一定Tensor Core里面就是这么去实现的

85
00:03:57,000 --> 00:04:01,000
我们可能会把A跟B进行相乘再相加

86
00:04:01,000 --> 00:04:04,000
通过这种虚拟硬件电路的方式呢

87
00:04:04,000 --> 00:04:07,000
去实现我们整个相关的硬件电路

88
00:04:07,000 --> 00:04:09,000
下面呢我们再看一下

89
00:04:09,000 --> 00:04:11,000
实际上呢我们在中间的过程当中

90
00:04:11,000 --> 00:04:13,000
或者计算的过程当中呢

91
00:04:13,000 --> 00:04:14,000
类似的是我们的寄存器

92
00:04:14,000 --> 00:04:17,000
我们离不开相关的寄存器

93
00:04:17,000 --> 00:04:19,000
数的呢是一个32位的

94
00:04:19,000 --> 00:04:21,000
输出呢也是32位的

95
00:04:21,000 --> 00:04:24,000
但是呢我中间去计算A和B矩阵呢

96
00:04:24,000 --> 00:04:26,000
它是可以是16位的

97
00:04:26,000 --> 00:04:28,000
而在乘加完之后呢

98
00:04:28,000 --> 00:04:31,000
它确实中间需要有一个简单的寄存器

99
00:04:31,000 --> 00:04:33,000
去存储中间的数据

100
00:04:33,000 --> 00:04:34,000
可以看到寄存器

101
00:04:34,000 --> 00:04:37,000
离我们的实际的计算单元呢

102
00:04:37,000 --> 00:04:39,000
是非常的接近

103
00:04:39,000 --> 00:04:40,000
通过这种方式呢

104
00:04:40,000 --> 00:04:41,000
我们可以简单的实现

105
00:04:41,000 --> 00:04:44,000
一行一行乘以B矩阵的一列

106
00:04:44,000 --> 00:04:47,000
诶现在提到的只是一行跟一列

107
00:04:47,000 --> 00:04:49,000
那原来在CUDA Core里面呢

108
00:04:49,000 --> 00:04:52,000
是做一个点跟一行进行相乘

109
00:04:52,000 --> 00:04:53,000
现在在V100里面呢

110
00:04:53,000 --> 00:04:56,000
它是一个矩阵跟另外一个矩阵

111
00:04:56,000 --> 00:04:59,000
直接相乘得到一个新的矩阵

112
00:04:59,000 --> 00:05:00,000
那这种方式才提到的

113
00:05:00,000 --> 00:05:04,000
只是其中一行跟其中一列进行相乘

114
00:05:04,000 --> 00:05:06,000
得到中间一个元素

115
00:05:06,000 --> 00:05:08,000
那更多的怎么办呢

116
00:05:09,000 --> 00:05:11,000
展示的A的一行跟B的一列相乘呢

117
00:05:11,000 --> 00:05:13,000
得到一个元素

118
00:05:13,000 --> 00:05:14,000
那下面呢我们看一下

119
00:05:14,000 --> 00:05:15,000
把这么一个简单的元素呢

120
00:05:15,000 --> 00:05:16,000
进行一个组成

121
00:05:16,000 --> 00:05:20,000
我把A0I A1I A2I A3I

122
00:05:20,000 --> 00:05:25,000
A的每一行跟B的每一列进行相乘

123
00:05:25,000 --> 00:05:26,000
这个时候呢

124
00:05:26,000 --> 00:05:28,000
我们就可以得到整个矩阵的

125
00:05:28,000 --> 00:05:30,000
每一个元素

126
00:05:30,000 --> 00:05:31,000
那这个时候

127
00:05:31,000 --> 00:05:33,000
对应的A矩阵的寄存器呢

128
00:05:33,000 --> 00:05:34,000
就应该是一堆

129
00:05:34,000 --> 00:05:36,000
对应B矩阵的寄存器呢

130
00:05:36,000 --> 00:05:37,000
也应该是一堆

131
00:05:37,000 --> 00:05:40,000
而不像刚才的只有单一个了

132
00:05:40,000 --> 00:05:42,000
下面呢不是非常了解

133
00:05:42,000 --> 00:05:44,000
我尝试从我个人理解的角度

134
00:05:44,000 --> 00:05:46,000
去给大家做一个简单的分享

135
00:05:46,000 --> 00:05:48,000
如果大家觉得有不对呢

136
00:05:48,000 --> 00:05:50,000
随时欢迎大家随时指正

137
00:05:50,000 --> 00:05:51,000
现在呢我们有元素

138
00:05:51,000 --> 00:05:53,000
也就是Scalar的乘价操作的指令呢

139
00:05:53,000 --> 00:05:55,000
就像下面所示

140
00:05:55,000 --> 00:05:56,000
但实际上呢

141
00:05:56,000 --> 00:05:57,000
Tensor Core里面的Mod呢

142
00:05:57,000 --> 00:05:59,000
只有Fp16存储的时候呢

143
00:05:59,000 --> 00:06:01,000
说呢是用到Fp32的

144
00:06:01,000 --> 00:06:03,000
于是呢我们把刚才的一个Mod呢

145
00:06:03,000 --> 00:06:04,000
把它节省掉

146
00:06:04,000 --> 00:06:06,000
现在呢我们实现两个元素相乘呢

147
00:06:06,000 --> 00:06:09,000
我们就要把两条流水并行起来

148
00:06:09,000 --> 00:06:10,000
这个就指令的流水

149
00:06:10,000 --> 00:06:12,000
当然了我们用A的一行乘以B的一列

150
00:06:12,000 --> 00:06:15,000
于是呢我们就有四条Pipeline的流水

151
00:06:15,000 --> 00:06:18,000
我们现在只是现在简单计算一个元素

152
00:06:18,000 --> 00:06:20,000
就需要四条流水

153
00:06:20,000 --> 00:06:22,000
接下来我们开绿色的指令流水呢

154
00:06:22,000 --> 00:06:23,000
我们计算出了D00

155
00:06:23,000 --> 00:06:24,000
通过黄色的流水呢

156
00:06:24,000 --> 00:06:26,000
我们计算出了D01

157
00:06:26,000 --> 00:06:27,000
接下来我们想要把

158
00:06:27,000 --> 00:06:29,000
所有的元素计算出来了

159
00:06:29,000 --> 00:06:32,000
我们就有大量的指令的流水去拼接

160
00:06:32,000 --> 00:06:34,000
从而呢去把四条流水拼接起来

161
00:06:34,000 --> 00:06:37,000
就简单的实现了一个矩阵的

162
00:06:37,000 --> 00:06:38,000
D03的一个结果

163
00:06:38,000 --> 00:06:42,000
那我们现在其实还要把所有的都拼起来

164
00:06:42,000 --> 00:06:43,000
那整个指令的流水呢

165
00:06:43,000 --> 00:06:45,000
我们在一个屏幕呢已经放不下了

166
00:06:45,000 --> 00:06:47,000
从这里面的颜色呢

167
00:06:47,000 --> 00:06:49,000
其实在某一个时间段

168
00:06:49,000 --> 00:06:52,000
我们对数据的读写是有规律的

169
00:06:52,000 --> 00:06:55,000
Mod呢就是我们需要对数据读取出来

170
00:06:55,000 --> 00:06:56,000
读取出来之后呢

171
00:06:56,000 --> 00:06:58,000
下面这个算完Round呢

172
00:06:58,000 --> 00:06:59,000
就是我们数据的写入

173
00:06:59,000 --> 00:07:01,000
所以在某一个时刻呢

174
00:07:01,000 --> 00:07:04,000
我们争取了是有四个数据

175
00:07:04,000 --> 00:07:06,000
从计存器里面读到我们的计算单元

176
00:07:06,000 --> 00:07:07,000
然后有一个数据呢

177
00:07:07,000 --> 00:07:09,000
存到我们的计存器里面

178
00:07:09,000 --> 00:07:11,000
同样指令流水呢

179
00:07:11,000 --> 00:07:14,000
我们实现了整个Tensor Core的计算

180
00:07:16,000 --> 00:07:18,000
这次呢中米的语速呢放得非常的慢

181
00:07:18,000 --> 00:07:20,000
然后也没有那么多废话了

182
00:07:20,000 --> 00:07:22,000
现在呢我们来看一下第三个内容

183
00:07:22,000 --> 00:07:24,000
Tensor Core的现成的执行

184
00:07:24,000 --> 00:07:25,000
在软件设计方面呢

185
00:07:25,000 --> 00:07:28,000
其实我们是希望能够去匹配

186
00:07:28,000 --> 00:07:30,000
计算和存储分层的整个结构

187
00:07:30,000 --> 00:07:33,000
那整体呢我们英伟达对于Tensor Core的定义呢

188
00:07:33,000 --> 00:07:35,000
其实主要是通过CUDA来提供一个

189
00:07:35,000 --> 00:07:36,000
范型的编程

190
00:07:36,000 --> 00:07:37,000
那这个所谓的范型编程

191
00:07:37,000 --> 00:07:39,000
我们可以抛开一边先不谈

192
00:07:39,000 --> 00:07:40,000
他们的一个术语呢

193
00:07:40,000 --> 00:07:42,000
是General Programming

194
00:07:42,000 --> 00:07:43,000
后面的我的例子呢

195
00:07:43,000 --> 00:07:45,000
我们会有一个简单的

196
00:07:45,000 --> 00:07:47,000
A乘以B等于C

197
00:07:47,000 --> 00:07:48,000
作为Demo

198
00:07:48,000 --> 00:07:49,000
就没有了刚才

199
00:07:49,000 --> 00:07:51,000
D等于A乘以B加C

200
00:07:51,000 --> 00:07:52,000
没有了这个概念

201
00:07:52,000 --> 00:07:54,000
我们就简单的一个矩阵层

202
00:07:54,000 --> 00:07:55,000
可以看到呢

203
00:07:55,000 --> 00:07:57,000
其实矩阵A呢跟矩阵B呢

204
00:07:57,000 --> 00:07:59,000
跟到我们的矩阵C

205
00:07:59,000 --> 00:08:00,000
但实际上呢

206
00:08:00,000 --> 00:08:01,000
我们不可能把这么大的一个矩阵

207
00:08:01,000 --> 00:08:03,000
栽到具体的Tensor Core里面

208
00:08:03,000 --> 00:08:04,000
因为Tensor Core

209
00:08:04,000 --> 00:08:06,000
只能从容纳四乘四的

210
00:08:06,000 --> 00:08:07,000
一个简单的计算

211
00:08:07,000 --> 00:08:08,000
那这个时候呢

212
00:08:08,000 --> 00:08:10,000
我们会对矩阵进行切片

213
00:08:10,000 --> 00:08:11,000
放到我们的Flat Board

214
00:08:11,000 --> 00:08:12,000
就是我们的线程块里面

215
00:08:12,000 --> 00:08:13,000
接着呢

216
00:08:13,000 --> 00:08:15,000
再放在我们的软件上面

217
00:08:15,000 --> 00:08:16,000
定义一个的Web

218
00:08:16,000 --> 00:08:17,000
Web就是我们的线呢

219
00:08:17,000 --> 00:08:18,000
我们在之前已经给大家讲过了

220
00:08:18,000 --> 00:08:20,000
最后整个线程去执行的

221
00:08:20,000 --> 00:08:22,000
就是我们真正的Tensor Core

222
00:08:22,000 --> 00:08:25,000
下面呢我们逐层去打开具体的内容

223
00:08:27,000 --> 00:08:28,000
英伟达的所有概念呢

224
00:08:28,000 --> 00:08:29,000
我们看一个矩阵层

225
00:08:29,000 --> 00:08:31,000
也就是所谓的GMM

226
00:08:31,000 --> 00:08:32,000
其实呢一次呢

227
00:08:32,000 --> 00:08:34,000
是计算一个小的矩阵块

228
00:08:34,000 --> 00:08:35,000
也就是把我们的矩阵A呢

229
00:08:35,000 --> 00:08:36,000
拿出一个小块

230
00:08:36,000 --> 00:08:37,000
把矩阵B呢

231
00:08:37,000 --> 00:08:38,000
拿出一个小块

232
00:08:38,000 --> 00:08:40,000
算出来一个矩阵C

233
00:08:40,000 --> 00:08:41,000
那这个时候呢

234
00:08:41,000 --> 00:08:42,000
我们在整体的软件

235
00:08:42,000 --> 00:08:44,000
去编程的时候呢

236
00:08:44,000 --> 00:08:46,000
就会沿着我们每一个维度

237
00:08:46,000 --> 00:08:47,000
也就是沿着我们

238
00:08:47,000 --> 00:08:48,000
每个m啊k啊

239
00:08:48,000 --> 00:08:49,000
还有n啊里面

240
00:08:49,000 --> 00:08:51,000
具体呢就划分成为

241
00:08:51,000 --> 00:08:52,000
mTile跟mTile的

242
00:08:52,000 --> 00:08:54,000
一个独立的矩阵层法

243
00:08:54,000 --> 00:08:55,000
通过这种累积

244
00:08:55,000 --> 00:08:57,000
n维跟n维还有k维的Tile呢

245
00:08:57,000 --> 00:08:59,000
把整个矩阵层累积起来

246
00:08:59,000 --> 00:09:00,000
计算出整个大的

247
00:09:00,000 --> 00:09:01,000
矩阵的结果

248
00:09:01,000 --> 00:09:02,000
在整个编程里面呢

249
00:09:02,000 --> 00:09:03,000
我们每一个维度呢

250
00:09:03,000 --> 00:09:04,000
就进行分开

251
00:09:04,000 --> 00:09:06,000
nB nB kB

252
00:09:06,000 --> 00:09:07,000
然后下面就算

253
00:09:07,000 --> 00:09:08,000
每一个小的

254
00:09:08,000 --> 00:09:11,000
nTile by kTile

255
00:09:11,000 --> 00:09:12,000
非常的要口啊

256
00:09:12,000 --> 00:09:13,000
通过这种方式呢

257
00:09:13,000 --> 00:09:16,000
去逐层的拆分出来

258
00:09:16,000 --> 00:09:17,000
最终去的时候呢

259
00:09:17,000 --> 00:09:19,000
C等于A乘以B

260
00:09:19,000 --> 00:09:20,000
简单的最核心的

261
00:09:20,000 --> 00:09:21,000
就是这段话

262
00:09:21,000 --> 00:09:22,000
我们现在呢

263
00:09:22,000 --> 00:09:23,000
逐段的去拆出来

264
00:09:23,000 --> 00:09:24,000
在扩大里面的GM呢

265
00:09:24,000 --> 00:09:26,000
我们主要是在线程块

266
00:09:26,000 --> 00:09:28,000
也就是flat box上面

267
00:09:28,000 --> 00:09:29,000
去进行并行

268
00:09:29,000 --> 00:09:30,000
那我们可以看到

269
00:09:30,000 --> 00:09:32,000
刚才的一个大的循环

270
00:09:32,000 --> 00:09:33,000
也就是外面的循环

271
00:09:33,000 --> 00:09:34,000
我们会把kB里面

272
00:09:34,000 --> 00:09:36,000
这个里面的所有的内容呢

273
00:09:36,000 --> 00:09:38,000
通过CUDA的线程块

274
00:09:38,000 --> 00:09:39,000
去承接

275
00:09:39,000 --> 00:09:41,000
进行一个并行的操作

276
00:09:41,000 --> 00:09:43,000
flat tile线程块之后呢

277
00:09:43,000 --> 00:09:44,000
在线程块里面

278
00:09:44,000 --> 00:09:46,000
实际上具体的执行

279
00:09:46,000 --> 00:09:48,000
是分配到web level上面

280
00:09:48,000 --> 00:09:51,000
去执行具体的计算

281
00:09:51,000 --> 00:09:52,000
那这个时候呢

282
00:09:52,000 --> 00:09:54,000
我们会把A矩阵跟B矩阵

283
00:09:54,000 --> 00:09:57,000
加载到这里面的SMEM

284
00:09:57,000 --> 00:09:59,000
也就是共享内存里面

285
00:09:59,000 --> 00:10:00,000
把A矩阵

286
00:10:00,000 --> 00:10:02,000
就会分布到不同的web上面

287
00:10:02,000 --> 00:10:03,000
每个web呢

288
00:10:03,000 --> 00:10:04,000
复制独立的一个计算

289
00:10:04,000 --> 00:10:05,000
那我们现在看看

290
00:10:05,000 --> 00:10:06,000
往左边看看

291
00:10:06,000 --> 00:10:08,000
具体的计算的公式

292
00:10:08,000 --> 00:10:09,000
刚才这一个大的循环呢

293
00:10:09,000 --> 00:10:10,000
我们已经分配到

294
00:10:10,000 --> 00:10:12,000
不同的线程块上面去执行

295
00:10:12,000 --> 00:10:14,000
那针对每一个CUDA web呢

296
00:10:14,000 --> 00:10:16,000
我们去真正执行的

297
00:10:16,000 --> 00:10:17,000
是下面最核心的

298
00:10:17,000 --> 00:10:19,000
kTile这一块循环

299
00:10:19,000 --> 00:10:21,000
接下来我们再打开一层

300
00:10:21,000 --> 00:10:23,000
对于web level这一层

301
00:10:23,000 --> 00:10:25,000
来看看具体是怎么执行的

302
00:10:25,000 --> 00:10:26,000
首先呢

303
00:10:26,000 --> 00:10:27,000
我们会把ATile跟BTile

304
00:10:27,000 --> 00:10:29,000
就是共享内存里面的数据呢

305
00:10:29,000 --> 00:10:30,000
加载到RF

306
00:10:30,000 --> 00:10:33,000
也就是具体的寄存器

307
00:10:33,000 --> 00:10:34,000
那AFragment跟BFragment

308
00:10:34,000 --> 00:10:37,000
就是具体的空间或者数据

309
00:10:37,000 --> 00:10:39,000
值得注意的就是

310
00:10:39,000 --> 00:10:41,000
数据加载要快于计算

311
00:10:41,000 --> 00:10:42,000
否则的话

312
00:10:42,000 --> 00:10:43,000
就会严重的影响

313
00:10:43,000 --> 00:10:45,000
我们整个计算的吞吐

314
00:10:45,000 --> 00:10:46,000
结果矩阵CD呢

315
00:10:46,000 --> 00:10:47,000
会比较大

316
00:10:47,000 --> 00:10:48,000
所以我们会存储在

317
00:10:48,000 --> 00:10:49,000
现场参加执行的

318
00:10:49,000 --> 00:10:50,000
寄存器上面

319
00:10:50,000 --> 00:10:51,000
那说白了

320
00:10:51,000 --> 00:10:53,000
就是都存在寄存器上面

321
00:10:53,000 --> 00:10:55,000
里面的整体的格式

322
00:10:55,000 --> 00:10:56,000
所谓的Layout呢

323
00:10:56,000 --> 00:10:58,000
我们是以K为进行存储

324
00:10:58,000 --> 00:10:59,000
所以这样的话

325
00:10:59,000 --> 00:11:00,000
会使得我们的

326
00:11:00,000 --> 00:11:02,000
现场执行的更加高尚

327
00:11:02,000 --> 00:11:04,000
所以在整体的循环里面

328
00:11:04,000 --> 00:11:06,000
我们再打开一层

329
00:11:06,000 --> 00:11:07,000
具体的现场

330
00:11:07,000 --> 00:11:09,000
M跟N之间的相乘呢

331
00:11:09,000 --> 00:11:11,000
就是在我们的CUDA file里面

332
00:11:11,000 --> 00:11:12,000
去执行的

333
00:11:12,000 --> 00:11:13,000
就我们的具体的现场

334
00:11:13,000 --> 00:11:14,000
去执行了

335
00:11:14,000 --> 00:11:15,000
那现场我们控制不了

336
00:11:15,000 --> 00:11:16,000
大部分的时候呢

337
00:11:16,000 --> 00:11:17,000
我们还是控制web

338
00:11:17,000 --> 00:11:18,000
现在呢

339
00:11:18,000 --> 00:11:20,000
我们再打开看看Tensor Core

340
00:11:20,000 --> 00:11:22,000
上面的并行执行

341
00:11:22,000 --> 00:11:23,000
实际上呢

342
00:11:23,000 --> 00:11:24,000
Tensor Core并行执行

343
00:11:24,000 --> 00:11:25,000
就是执行我们刚才

344
00:11:25,000 --> 00:11:27,000
A乘以B等于C

345
00:11:27,000 --> 00:11:28,000
这么一个简单

346
00:11:28,000 --> 00:11:31,000
最核心最里面的一个操作

347
00:11:31,000 --> 00:11:32,000
会拿对应的硬件

348
00:11:32,000 --> 00:11:33,000
但是呢

349
00:11:33,000 --> 00:11:34,000
具体的软件的API呢

350
00:11:34,000 --> 00:11:35,000
是WMMA

351
00:11:35,000 --> 00:11:36,000
通过WMMA呢

352
00:11:36,000 --> 00:11:37,000
对外提供

353
00:11:37,000 --> 00:11:39,000
具体的计算的能力

354
00:11:39,000 --> 00:11:40,000
也就是MMA的

355
00:11:40,000 --> 00:11:41,000
Load Storage

356
00:11:41,000 --> 00:11:42,000
MMA Sync

357
00:11:42,000 --> 00:11:44,000
这几个API呢

358
00:11:44,000 --> 00:11:46,000
完成整个Tensor Core的计算

359
00:11:46,000 --> 00:11:47,000
现在呢

360
00:11:47,000 --> 00:11:48,000
我们看到

361
00:11:48,000 --> 00:11:50,000
GMM矩阵的软硬件分层里面呢

362
00:11:50,000 --> 00:11:51,000
每一层都需要

363
00:11:51,000 --> 00:11:52,000
对数据进行复用

364
00:11:52,000 --> 00:11:54,000
我们在整一个块里面呢

365
00:11:54,000 --> 00:11:55,000
大矩阵呢

366
00:11:55,000 --> 00:11:56,000
我们会把大矩阵

367
00:11:56,000 --> 00:11:57,000
放在不同的

368
00:11:57,000 --> 00:11:58,000
全局内存里面

369
00:11:58,000 --> 00:11:59,000
在对于局部内存呢

370
00:11:59,000 --> 00:12:01,000
我们分块来进行存

371
00:12:01,000 --> 00:12:02,000
那分块里面呢

372
00:12:02,000 --> 00:12:04,000
我们会把一些具体的数据呢

373
00:12:04,000 --> 00:12:06,000
存在我们的寄存器里面

374
00:12:06,000 --> 00:12:07,000
所以它整体呢

375
00:12:07,000 --> 00:12:08,000
每一层的数据

376
00:12:08,000 --> 00:12:10,000
都进行大量的去复用

377
00:12:10,000 --> 00:12:11,000
因为我们的矩阵

378
00:12:11,000 --> 00:12:12,000
确实太大了

379
00:12:12,000 --> 00:12:13,000
有非常多的数据呢

380
00:12:13,000 --> 00:12:14,000
进行每一块的利用

381
00:12:14,000 --> 00:12:15,000
那现在呢

382
00:12:15,000 --> 00:12:16,000
我们GMM

383
00:12:16,000 --> 00:12:17,000
具体的计算

384
00:12:17,000 --> 00:12:18,000
已经算完了

385
00:12:18,000 --> 00:12:19,000
但是我们算完

386
00:12:19,000 --> 00:12:20,000
每一次的结果呢

387
00:12:20,000 --> 00:12:21,000
都是很小的一块结果

388
00:12:21,000 --> 00:12:23,000
我们怎么把整个矩阵

389
00:12:23,000 --> 00:12:24,000
或者累积矩阵

390
00:12:24,000 --> 00:12:25,000
的结果呢

391
00:12:25,000 --> 00:12:26,000
写回去

392
00:12:26,000 --> 00:12:29,000
外面的这个C里面呢

393
00:12:29,000 --> 00:12:30,000
这个时候呢

394
00:12:30,000 --> 00:12:31,000
我们其实刚才

395
00:12:31,000 --> 00:12:34,000
漏了一个收尾的工作

396
00:12:35,000 --> 00:12:36,000
现在我们看看

397
00:12:38,000 --> 00:12:40,000
WMMA的结果回存呢

398
00:12:40,000 --> 00:12:41,000
刚才结果呢

399
00:12:41,000 --> 00:12:42,000
我们会存在

400
00:12:42,000 --> 00:12:43,000
regest file

401
00:12:43,000 --> 00:12:44,000
刚才我们提到了

402
00:12:44,000 --> 00:12:45,000
这个WMMA的API

403
00:12:45,000 --> 00:12:46,000
这里面有个store

404
00:12:46,000 --> 00:12:48,000
有个store matrix sync

405
00:12:48,000 --> 00:12:49,000
这个工作呢

406
00:12:49,000 --> 00:12:50,000
去把所有的数据呢

407
00:12:50,000 --> 00:12:51,000
都搬到我们的

408
00:12:51,000 --> 00:12:52,000
共享内存

409
00:12:52,000 --> 00:12:53,000
也就是SMEM里面

410
00:12:53,000 --> 00:12:54,000
对于SMEM呢

411
00:12:54,000 --> 00:12:55,000
我们会做大量的

412
00:12:55,000 --> 00:12:56,000
累积的操作

413
00:12:56,000 --> 00:12:57,000
那累积的操作呢

414
00:12:57,000 --> 00:12:59,000
既然是一条计算

415
00:12:59,000 --> 00:13:00,000
我们会搬到

416
00:13:00,000 --> 00:13:01,000
具体的集成器里面

417
00:13:01,000 --> 00:13:02,000
去进行计算

418
00:13:02,000 --> 00:13:03,000
最后呢

419
00:13:03,000 --> 00:13:04,000
把所有的数据

420
00:13:04,000 --> 00:13:05,000
这些数据

421
00:13:05,000 --> 00:13:06,000
累积起来

422
00:13:06,000 --> 00:13:07,000
然后放在我们的

423
00:13:07,000 --> 00:13:08,000
全局内存里面

424
00:13:08,000 --> 00:13:09,000
通过全局内存呢

425
00:13:09,000 --> 00:13:10,000
把一个块一个块的

426
00:13:10,000 --> 00:13:11,000
拼接起来

427
00:13:11,000 --> 00:13:12,000
把数据呢

428
00:13:12,000 --> 00:13:13,000
恢复回来

429
00:13:13,000 --> 00:13:14,000
那下面呢

430
00:13:14,000 --> 00:13:15,000
最后呢

431
00:13:15,000 --> 00:13:16,000
我们总结一下

432
00:13:16,000 --> 00:13:17,000
整个完整的

433
00:13:17,000 --> 00:13:18,000
GM的计算

434
00:13:18,000 --> 00:13:19,000
或者存储的

435
00:13:19,000 --> 00:13:20,000
数据流呢

436
00:13:20,000 --> 00:13:21,000
就是首先

437
00:13:21,000 --> 00:13:22,000
我们会对矩阵

438
00:13:22,000 --> 00:13:23,000
进行分块

439
00:13:23,000 --> 00:13:24,000
放在我们的

440
00:13:24,000 --> 00:13:25,000
global memory里面

441
00:13:25,000 --> 00:13:26,000
然后呢

442
00:13:26,000 --> 00:13:27,000
我们对network

443
00:13:27,000 --> 00:13:28,000
进行划分

444
00:13:28,000 --> 00:13:29,000
把具体的

445
00:13:29,000 --> 00:13:30,000
局部的数据

446
00:13:30,000 --> 00:13:31,000
或者fragment呢

447
00:13:31,000 --> 00:13:32,000
放在shared memory

448
00:13:32,000 --> 00:13:33,000
共享内存里面

449
00:13:33,000 --> 00:13:34,000
针对

450
00:13:34,000 --> 00:13:35,000
那时候呢

451
00:13:35,000 --> 00:13:36,000
是放在计存器里面

452
00:13:36,000 --> 00:13:37,000
以webtype

453
00:13:37,000 --> 00:13:38,000
进行执行

454
00:13:38,000 --> 00:13:39,000
在我们的tensor core里面

455
00:13:39,000 --> 00:13:40,000
执行完之后呢

456
00:13:40,000 --> 00:13:41,000
我们会回传到

457
00:13:41,000 --> 00:13:42,000
SMEM里面

458
00:13:42,000 --> 00:13:43,000
不是存储到

459
00:13:43,000 --> 00:13:44,000
我们会把tensor core的

460
00:13:44,000 --> 00:13:45,000
we just file

461
00:13:45,000 --> 00:13:46,000
写出来

462
00:13:46,000 --> 00:13:47,000
然后呢

463
00:13:47,000 --> 00:13:48,000
进行一个

464
00:13:48,000 --> 00:13:49,000
累加或者

465
00:13:49,000 --> 00:13:50,000
融合算子的操作

466
00:13:50,000 --> 00:13:51,000
这里面可能会

467
00:13:51,000 --> 00:13:52,000
融合其他算子

468
00:13:52,000 --> 00:13:53,000
例如

469
00:13:53,000 --> 00:13:54,000
卷积加为路啊

470
00:13:54,000 --> 00:13:55,000
matmul加

471
00:13:55,000 --> 00:13:56,000
这种激活啊

472
00:13:56,000 --> 00:13:57,000
再往后呢

473
00:13:57,000 --> 00:13:58,000
就把所有的数据呢

474
00:13:58,000 --> 00:13:59,000
写回我们的

475
00:13:59,000 --> 00:14:00,000
全局内存

476
00:14:00,000 --> 00:14:01,000
那今天的内容呢

477
00:14:01,000 --> 00:14:02,000
就到这里为止

478
00:14:02,000 --> 00:14:03,000
大家分享了

479
00:14:03,000 --> 00:14:04,000
tensor core的

480
00:14:04,000 --> 00:14:05,000
具体的执行

481
00:14:05,000 --> 00:14:06,000
我们把每一个计算呢

482
00:14:06,000 --> 00:14:07,000
打开看一看

483
00:14:07,000 --> 00:14:08,000
然后呢

484
00:14:08,000 --> 00:14:09,000
见指令流水

485
00:14:09,000 --> 00:14:10,000
是怎么去执行的

486
00:14:10,000 --> 00:14:11,000
有了指令流水之后

487
00:14:11,000 --> 00:14:12,000
我们的cuda的

488
00:14:12,000 --> 00:14:13,000
线程

489
00:14:13,000 --> 00:14:14,000
是怎么完成

490
00:14:14,000 --> 00:14:15,000
或者怎么跟我们的

491
00:14:15,000 --> 00:14:16,000
指令流水配合的

492
00:14:16,000 --> 00:14:17,000
今天的内容

493
00:14:17,000 --> 00:14:18,000
到这里为止

494
00:14:18,000 --> 00:14:19,000
谢谢各位

495
00:14:19,000 --> 00:14:20,000
拜拜

