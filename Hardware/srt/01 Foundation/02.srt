1
00:00:00,000 --> 00:00:14,000
大家好,我是ZOMI。今天我们来到AI编译器里面的AI的计算体系的第一个内容,就去讲讲深度学习的计算模式。

2
00:00:14,000 --> 00:00:22,000
那所谓的深度学习的计算模式呢,就在我们的第一节内容,它主要是指我们现在要讲的几个内容。

3
00:00:22,000 --> 00:00:26,000
第一个呢,就是AI的,还有它的一个基本的方式。

4
00:00:26,000 --> 00:00:32,000
然后呢,我们去看看深度学习,或者AI里面的几个经典的网络模型结构。

5
00:00:32,000 --> 00:00:36,000
那这里面呢,其实很简单,就是WestNet,EfficientNet,MobileNet这种经典的网络。

6
00:00:36,000 --> 00:00:42,000
去看看模型的量化和简直就整体的网络加速的模块。

7
00:00:42,000 --> 00:00:48,000
然后我们会去看看轻量化的网络模型,MobileNet,那些能够在手机端测部署的模型。

8
00:00:48,000 --> 00:00:51,000
最后我们看看大模型,还有分布式并行。

9
00:00:51,000 --> 00:00:57,000
对我们整个内容呢,稍微有点多,所以我会分开两个视频给大家去汇报的。

10
00:00:57,000 --> 00:01:00,000
那第一个呢,我们会去讲讲前面三个内容。

11
00:01:00,000 --> 00:01:03,000
第二个视频呢,我们讲讲后面两个内容。

12
00:01:03,000 --> 00:01:06,000
注意,我们这里面非常关心的是计算模式。

13
00:01:06,000 --> 00:01:12,000
通过了解我们AI的发展,了解我们神级网络的发展,了解我们人工智能的发展。

14
00:01:12,000 --> 00:01:18,000
去看看对于我们硬件的一个计算模式的改变,或者我们的牵引,或者一些suggestion也好。

15
00:01:18,000 --> 00:01:21,000
那下面呢,我们进入正式的内容里面。

16
00:01:21,000 --> 00:01:26,000
首先第一个呢,就是AI的发展和AI的范式。

17
00:01:26,000 --> 00:01:32,000
AI总体的发展路线,从1950年代时候开始呢,已经提出了人工智能。

18
00:01:32,000 --> 00:01:36,000
接着呢,在八九十年代的时候呢,机器学识是非常非常的火。

19
00:01:36,000 --> 00:01:40,000
而深度学习呢,真正迎来爆发是在2010年之后。

20
00:01:40,000 --> 00:01:45,000
而在2010年之后,更重要或者更加outstanding的一种技术,叫做Foundation Model。

21
00:01:45,000 --> 00:01:50,000
那为啥我最近更新的特别慢呢,是因为确实在公司呢,非常注重大模型。

22
00:01:50,000 --> 00:01:54,000
而ChurchPT大模型这个技术呢,就是我在负责的一个主要的项目。

23
00:01:54,000 --> 00:01:58,000
所以呢,就可以经常脱更,因为确实工作压力太大了。

24
00:01:58,000 --> 00:02:05,000
下面呢,我们回到正题,我们看看三大的AI的三大范式,也就是机器学习的三大范式。

25
00:02:05,000 --> 00:02:09,000
之二有强化学习,监督学习,还有无监督学习。

26
00:02:09,000 --> 00:02:15,000
那不管AI的三种方式呢,其实我们现在呢,都可以用深度学习计算模式去使用的。

27
00:02:15,000 --> 00:02:22,000
那我们现在看看,深度学习里面跟这三种模式结合呢,有哪些不一样,或者有哪些区别,或者有哪些结合点。

28
00:02:22,000 --> 00:02:27,000
那下面我们可以看到呢,像这种呢,假设我们绿色的框框呢,就是一个深度学习的模型。

29
00:02:27,000 --> 00:02:33,000
我们在有绿色学习里面呢,我们会输一个已经label完,就我们已经做好标签的数据。

30
00:02:33,000 --> 00:02:38,000
然后呢,输给我们的输出,中间呢,通过一个损失函数去反馈我们的一个模式。

31
00:02:38,000 --> 00:02:42,000
这种方式呢,就用深度学习去解决我们有监督的问题。

32
00:02:42,000 --> 00:02:48,000
那无监督的问题呢,就是无监督学习这一个内容呢,也是通过一个神经网络呢,去处理的。

33
00:02:48,000 --> 00:02:53,000
像强化学习呢,现在也跟深度学习结合的非常紧密,叫做深度强化学习。

34
00:02:53,000 --> 00:02:56,000
那我之前出过一本书,也是讲相关的内容。

35
00:02:56,000 --> 00:03:02,000
这一面呢,假设这是一个agent,或者是一个神经网络呢,就会输出一些policy,或者value,给最后的输出,给出一个action。

36
00:03:02,000 --> 00:03:08,000
然后在环境当中呢,给出一个新的状态,要不断地去进行一个学习反馈的过程。

37
00:03:08,000 --> 00:03:13,000
这个就是深度学习跟整个AI的三大主流的方式的一种结合的形态。

38
00:03:13,000 --> 00:03:23,000
那既然我们了解完整个AI的过程,我们现在来看看深度学习的网络模型的一些结构,还有设计,还有它的演进的方式。

39
00:03:23,000 --> 00:03:30,000
那现在呢,很神奇的就是一层一层套下来,它实际上呢,是一个卷积神经网络。

40
00:03:30,000 --> 00:03:40,000
那通过反卷积和内向可视化的工程当中呢,我们可以发现,其实每一层神经网络呢,它都有不同的感知,或者不同的一个特征的提取层。

41
00:03:40,000 --> 00:03:49,000
不同的特征呢,或者不同的层数呢,会提取不同的特征出来,直到最后呢,我们可以看到我们想得到的哪些具体的信号。

42
00:03:49,000 --> 00:03:54,000
那这种类,接下来呢,我们主要是看一看神经网络的一个主要的计算的方式。

43
00:03:54,000 --> 00:04:05,000
那我们简单的里面呢,它的主要的计算模式,大部分都是乘加,乘加这种模式呢,或者乘加这种计算呢,已经占了整个神经网络超过90%的计算量。

44
00:04:05,000 --> 00:04:14,000
所以说,乘加了我们可以化为一个权重求和的过程,像下面左下角的这个图呢,就是一个经典的神经网络。

45
00:04:14,000 --> 00:04:30,000
左边的这个,左边的这个呢,就是我们的求和,右边的这个就是我们的激活,激活层跟求和层就是一个简单的神经元的组合,每个圈圈代表一个简单的神经元,它会做一个乘加的操作,或者举正相乘的操作。

46
00:04:31,000 --> 00:04:44,000
这个呢,就是我们的激活函数,也是里面的圆圈里面右边的一个F,这里面呢,我们主要说明的就是神经网络的主要计算,就是权重的求和,乘加的操作。

47
00:04:44,000 --> 00:04:53,000
那下面呢,有了这个知识之年以后呢,我们看看主流的神经网络或者网络模型的架构,总共有四种,四种。

48
00:04:54,000 --> 00:05:11,000
就是全连接网络,Fully Kinetic Layer,那第二种就是卷积层啊,Convolution Layer,这两层呢,我相信大家也是非常之熟悉的,像全连接网络里面最出名的就是MLP,那卷积层最出名的就是CNN这种网络模型架构。

49
00:05:12,000 --> 00:05:29,000
下面呢,我们看看另外两种也是非常非常的火的,像循环神经网络呢,其实在早些年呢,主要是用在信号的处理和自然语言的处理,不过后来呢,出现了注意力机制,就是我们的Attention或者Transmit的结构,引爆了整个或者改变了整个CV或者虚拟处理。

50
00:05:29,000 --> 00:05:44,000
那这三到的这四种呢,就是现阶段的主要的网络模型结构,大家了解一下,知道一下这些名词就好了,如果你没有了解过太多的深度学习,大家可以根据相关的词评或者相关的知识呢,去反向的检索。

51
00:05:44,000 --> 00:06:13,840
下面我们看,我们以卷积的计算或者卷积神经网络这个CNN层呢,作为一个主要的例子,我们看看里面有哪些不一样的东西,那首先呢,还是刚才的那一段话,我们的key operation,就是主要的计算模式呢,集中在我们的层加的操作,占了大量的一个计算,所以说神经网络模型啊,大部分都是通过矩阵的相乘,矩阵的操作进行一个处理的,而矩阵的更看看卷积计算里面呢,有个很重要的特点,

52
00:06:13,840 --> 00:06:43,800
就是会有大量的channel,还有大量的output channel,这个channel是非常的多的,每一个群众也好,feature map也好,包括我们的输出也好,都有大量的channel,channel非常非常的深,非常非常的,除了channel之外呢,我们还有非常多的batch size,就是我们的feature map有非常多轮,所以它会有NCHW,里面的N就变得非常的大了,另外呢,也在一些特殊的情况下,例如摇杆的神经网络模型啊,

53
00:06:43,840 --> 00:07:12,720
它的一个图片的输入是非常非常的庞大的,可能万乘以万的级别,就证明我们整个feature map会变得和非常的,看一个dynamic shape,就是我们的shape啊,在不同的层里面会不断的去变化,或者不断的去,层里面呢,它的shape呢是不一样的,有些层呢,它的长宽高,跟下一层的长宽高也是不相同的,可能每一层的体,我们叫做various across layer这种方式,

54
00:07:14,120 --> 00:07:43,640
这些神经网络讲完例子之后呢,我们现在来看看一些非常经典的神经网络的模型,那我们这里面呢,从1998年开始到2019年呢,也就是确实还挺近的,跨度接近20年的时间,我们的网络模型从IronFive到我们后来的谷歌出的EfficientNet B4的层数,网络模型的计算量,网络模型的权重也是越来越大,越来越夸张,

55
00:07:43,720 --> 00:08:09,400
再用我们的内存也是越来越多的,这里面呢,就需要,就是计算的一个flow,就是计算力,就是top5的一个accuracy,在EfficientNet这个数据机里面,那可以看到网络模型越大,那下面这几个我们先忽略,网络模型越大,它需要的算力也就越高,但是同时它的网络模型的精度也就是越好,

56
00:08:09,560 --> 00:08:36,960
所以这里面呢,我们可以看到网络模型大步进,消耗大量的算力,它也确实对我们的整个精度是有提升的,那基于上面这几点呢,我们对整个AI的计算模式提出了几个思考,这也是我所总结的一些思考,首先呢,我们芯片过程,我们的AI计算的模式里面呢,我们需要支持神经网络模型的一个计算的逻辑,计算逻辑非常的重要,利用我们刚才讲到的权重的数据的共享呢,

57
00:08:36,960 --> 00:09:06,920
以便于我们对神经网络神经元呢,进行一个矩阵的求和,或者一个卷积全连接这种呢,我们同时也是需要支持像激活softmax等非常多的vector的计算,虽然很多时候我们不仅需要quip等计算,我们还需要大量的vector scalar的计算,那第二种呢,就是需要支持高维的张量的存储,可以看到其实我们刚才讲到了在神经网络里面有NCHW这种数据结构,就张量的数据结构,对于张量的数据结构呢,我们的内存地址确实

58
00:09:06,960 --> 00:09:36,920
如果能够支持随机或者自动的,所以呢,能够大大的去增加我们计算的效率,或者内存读取的效率,另外呢,好朋友,神经网络里面呢,会有大的圈头或者大的feature map,那这个时候呢,如何对我们的张量进行高效的加载也是非常重要的,就是支持常用的神经网络的网络模型结构,我们刚才讲到了像卷积metamark transform,lstm等高效的一些常用的或者经典的算法,确实你必须要支持它,

59
00:09:37,160 --> 00:09:58,520
越多越好,当然了,这些是需要快速的应对新的AI算法和结构,而像现在GPU为什么大家用的越来越多在AI领域,是因为它确实能够应对新的快速的AI算法,而像TPU或者NPU确实也不能提,大家懂的都懂,接下来四个勾,你都知道,

60
00:09:58,520 --> 00:09:59,040
在接下来第二大内容里面呢,我们看看模型量化和网络简直对整个计算模式的一些改变或者冲击,还有思考,那下面呢有两张图,上面一张图呢,就是模型的简直或者网络的简直,可以看到网络的简直呢,在一个权重里面,我们会减掉或者去掉很多没有用的一些数据,但是呢,我们每一个数据的元素呢,都是保持在32bit,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32

61
00:10:28,520 --> 00:10:58,520
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32

62
00:10:58,520 --> 00:11:28,520
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32

63
00:11:28,520 --> 00:11:32,240
把原来的我们的量化都是线性的变成一个非线性的

64
00:11:32,240 --> 00:11:35,920
最后还有一些减少权重的或者激活的计算的

65
00:11:35,920 --> 00:11:38,640
例如加勒亚研究室提出的一种家法网络了

66
00:11:38,640 --> 00:11:42,600
还有一些有一些很好玩的亿货飞的网络模型

67
00:11:42,600 --> 00:11:46,120
这些都是网络模型量化相关的热门研究

68
00:11:47,520 --> 00:11:49,440
看一下简直简直是一道

69
00:11:49,440 --> 00:11:53,280
左边这个图就是一个简单的多层的神经网络

70
00:11:53,280 --> 00:11:56,520
然后像这种就是对我们的全连接进行剪辑

71
00:11:56,520 --> 00:11:58,720
把我们的连接的线剪掉

72
00:11:58,720 --> 00:12:01,560
第二种就是对我们的神经元进行剪辑

73
00:12:01,560 --> 00:12:05,000
就把我们的神经元绿色的这个圈圈把它剪掉

74
00:12:05,000 --> 00:12:06,440
所以有两种的方式

75
00:12:07,440 --> 00:12:09,320
一种就是非结构化的剪辑

76
00:12:09,320 --> 00:12:11,560
往右走就是结构化的剪辑

77
00:12:11,560 --> 00:12:15,720
所谓的非结构化的剪辑就是我随机的随机对我们的权重

78
00:12:15,720 --> 00:12:18,920
或者我们的神经元像刚才上一页去介绍的

79
00:12:18,920 --> 00:12:20,680
进行随机的剪辑

80
00:12:20,680 --> 00:12:25,480
结构化结构化的剪辑就是我有组织有效率的去剪辑

81
00:12:25,480 --> 00:12:30,800
例如对我们的fitter对我们的channel对我们的layer进行分布的剪辑

82
00:12:30,800 --> 00:12:32,320
这种就是叫做结构化

83
00:12:32,320 --> 00:12:35,240
非结构化就更多的随机性引入

84
00:12:35,240 --> 00:12:38,080
那对于模型压缩我们来到最后一个话题

85
00:12:38,080 --> 00:12:40,560
就是AI的一个计算模式的思考

86
00:12:40,560 --> 00:12:43,680
因为我们讲了很多网络模型压缩里面对我们的第一个思考

87
00:12:43,680 --> 00:12:46,760
就是需要提供不同的低比特的位数

88
00:12:46,760 --> 00:12:50,160
那至于低比特的位数有可能会提供int8或者低int4

89
00:12:50,160 --> 00:12:54,280
跟低比特的一些可能我们在nbit和ebit里面

90
00:12:54,320 --> 00:12:56,240
我们需要进行一个全行的

91
00:12:56,240 --> 00:12:59,040
例如有TF32还有BF16

92
00:12:59,040 --> 00:13:02,600
这种新的浮点精度去加速我们整体的运算

93
00:13:02,600 --> 00:13:06,880
那第二点就是希望能够充分的利用硬件去提升西数的计算

94
00:13:06,880 --> 00:13:08,160
实际上我们的神经网络

95
00:13:08,160 --> 00:13:12,800
我们的大模型里面或者我们的一些小模型里面有非常多的零值

96
00:13:12,800 --> 00:13:15,240
那这个时候我重复计算是没有意义的

97
00:13:15,240 --> 00:13:18,840
我们能不能利用硬件去提供西数化的计算

98
00:13:18,840 --> 00:13:22,960
那这个对我们的GNA网络里面确实非常的奏效

99
00:13:22,960 --> 00:13:25,560
第二种就是希望能够对我们的网络模型

100
00:13:25,560 --> 00:13:26,720
我们剪辑完之后

101
00:13:26,720 --> 00:13:29,720
确实能够减少很多内存的需求

102
00:13:29,720 --> 00:13:32,000
减少我们很多内存IO的通讯

103
00:13:32,000 --> 00:13:35,040
那这个时候西数化的网络就变得更加重要了

104
00:13:35,040 --> 00:13:38,440
我们的硬件怎么更好的去支持我们的西数化的网络模型

105
00:13:38,440 --> 00:13:42,160
需要也是我们的整个AI模式进行思考了

106
00:13:42,160 --> 00:13:43,240
好了今天的

107
00:13:43,840 --> 00:13:45,600
好了今天的内容就这么多

108
00:13:45,600 --> 00:13:48,640
我们在对于深度学习过了AI计算模式的思考

109
00:13:48,640 --> 00:13:51,560
主要去讲了经典的网络模型结构

110
00:13:51,560 --> 00:13:53,160
CAN LSTM Transformer

111
00:13:53,160 --> 00:13:54,880
这种经典的网络模型结构

112
00:13:54,880 --> 00:13:56,800
对整个计算模式的一些思考

113
00:13:56,800 --> 00:14:00,160
对我们硬件应该往哪些方面去设计和牵引

114
00:14:00,160 --> 00:14:01,080
做了一个思考

115
00:14:01,080 --> 00:14:04,080
接着我们又去看了一下网络模型量化和剪辑

116
00:14:04,080 --> 00:14:05,720
就是模型的压缩

117
00:14:05,720 --> 00:14:07,280
对我们计算模式

118
00:14:07,280 --> 00:14:11,200
对整个AI芯片带来哪些新的思考的点

119
00:14:11,200 --> 00:14:13,160
那今天的内容卷的不行了

120
00:14:13,160 --> 00:14:14,040
卷的不行了

121
00:14:14,040 --> 00:14:15,880
记得一键三连加关注哦

122
00:14:15,880 --> 00:14:19,440
所有的内容都会开源在下面这条链接里面

123
00:14:19,440 --> 00:14:20,240
摆了个掰

