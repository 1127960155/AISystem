1
00:00:00,000 --> 00:00:07,240
大家好,我是经常脱更的周米

2
00:00:07,240 --> 00:00:09,280
最近更新的频率确实低了好多

3
00:00:09,280 --> 00:00:13,760
现在我们来到了AI芯片里面的AI计算体系

4
00:00:13,760 --> 00:00:16,640
我们还是在深度学习计算模式

5
00:00:16,640 --> 00:00:20,960
去了解深度学习的计算模式有哪些不一样的点

6
00:00:20,960 --> 00:00:23,120
在这里面上一节课还记得吗

7
00:00:23,120 --> 00:00:25,320
我们讲了三个主要的内容

8
00:00:25,320 --> 00:00:29,200
第一个就是AI的发展和AI的范式

9
00:00:29,200 --> 00:00:32,120
第二个就是一些经典的网络模型

10
00:00:32,120 --> 00:00:33,080
website那些

11
00:00:33,080 --> 00:00:35,640
第三个就是网络模型和剪子

12
00:00:35,640 --> 00:00:39,400
这三个技术对深度学习计算的模式

13
00:00:39,400 --> 00:00:40,520
带来哪些冲击

14
00:00:40,520 --> 00:00:42,280
带来哪些不一样的点

15
00:00:42,280 --> 00:00:44,200
接下来我们在这一节里面

16
00:00:44,200 --> 00:00:46,040
将会给大家去汇报和分享

17
00:00:46,040 --> 00:00:48,120
轻量化的网络模型

18
00:00:48,120 --> 00:00:49,640
例如我们之前谈到的

19
00:00:49,640 --> 00:00:52,280
MobileNet, EfficientNet这种轻量化的

20
00:00:52,280 --> 00:00:53,680
看看大模型

21
00:00:53,680 --> 00:00:55,680
特别是分布式并行里面

22
00:00:55,680 --> 00:00:58,200
对我们整个深度学习的计算体系

23
00:00:58,240 --> 00:01:02,120
又有哪些带来新的冲击和新的idea

24
00:01:03,240 --> 00:01:05,880
第一个内容要给大家汇报和分享的

25
00:01:05,880 --> 00:01:08,920
就是轻量化的网络模型的设计

26
00:01:08,920 --> 00:01:11,280
其实我们在之前

27
00:01:12,040 --> 00:01:15,360
系列里面的模型小型化和轻量模型里面

28
00:01:15,440 --> 00:01:18,160
就讲了两个主要的系列

29
00:01:18,160 --> 00:01:21,480
和跟大家一起去分享了很多相关的内容

30
00:01:21,480 --> 00:01:24,640
第一个就是左边的CNN的系列

31
00:01:24,640 --> 00:01:27,120
第二个就是Transform的系列

32
00:01:27,160 --> 00:01:29,160
我们从现在我们可以看得出来

33
00:01:29,160 --> 00:01:30,560
从一开始的SqueezeNet

34
00:01:30,560 --> 00:01:32,720
到后来的GhostNet, EfficientNet

35
00:01:32,720 --> 00:01:35,480
基本上是从2016年到2019年

36
00:01:35,480 --> 00:01:37,520
那这个时间段随着非常多的

37
00:01:37,520 --> 00:01:40,040
CNN的轻量化网络模型的提出

38
00:01:40,040 --> 00:01:42,920
CNN也越来越多的运用在我们的端侧

39
00:01:42,920 --> 00:01:45,840
而Transform确实在后面才起来的

40
00:01:45,840 --> 00:01:48,440
能够做好非常多的attention的机制

41
00:01:48,440 --> 00:01:50,560
然后让我们的网络模型越来越大

42
00:01:50,560 --> 00:01:52,520
模型的精度也越来越好

43
00:01:52,520 --> 00:01:54,160
但是怎么轻量化的部署

44
00:01:54,440 --> 00:01:56,280
于是从2021年开始

45
00:01:56,560 --> 00:01:58,840
到去年年底到今年年初

46
00:01:59,120 --> 00:02:01,640
现在有非常多的去把MobileNet

47
00:02:01,640 --> 00:02:05,120
还有VisionTransform这个结构相结合

48
00:02:05,120 --> 00:02:07,000
然后又衍生了一系列的

49
00:02:07,000 --> 00:02:09,520
Transform的轻量级的模型

50
00:02:10,200 --> 00:02:12,760
在这里面我给大家回顾一下

51
00:02:12,760 --> 00:02:15,240
其实我们在之前做了很多的分享

52
00:02:15,240 --> 00:02:18,160
就是在推理系统里面的模型小型化

53
00:02:18,160 --> 00:02:21,120
然后群聚的朋友们也可以去了解一下

54
00:02:21,120 --> 00:02:22,800
之前分享的课程

55
00:02:22,800 --> 00:02:25,320
里面确实讲了很多相关的论文

56
00:02:25,320 --> 00:02:28,880
而这里面我们简单再串一串相关的知识

57
00:02:28,880 --> 00:02:31,920
最重要的是引起我们对于深度学习

58
00:02:32,120 --> 00:02:34,080
计算模式的思考

59
00:02:35,160 --> 00:02:37,360
轻量化网络模型的主要的设计方式

60
00:02:37,480 --> 00:02:38,960
其实有两个

61
00:02:39,120 --> 00:02:41,880
第一个就是改变网络模型的不同的层数

62
00:02:41,880 --> 00:02:43,360
包括我们的layer的shape

63
00:02:43,520 --> 00:02:45,000
包括我们的channel的计算方式

64
00:02:45,240 --> 00:02:46,920
还有卷积的计算方式

65
00:02:46,920 --> 00:02:50,000
从而衍生出一种新的网络模型结构

66
00:02:50,160 --> 00:02:51,200
这是第一种

67
00:02:51,360 --> 00:02:54,160
第二种就是通过NAS来搜索

68
00:02:54,160 --> 00:02:56,360
一些更轻量化的网络模型

69
00:02:56,360 --> 00:02:57,680
而通过NAS来搜索

70
00:02:57,840 --> 00:03:01,200
很多时候会根据硬件来做定制化的

71
00:03:01,200 --> 00:03:04,240
下面两个图就是后面会展开来讲

72
00:03:04,480 --> 00:03:06,720
第一个就是把传统的一些卷积

73
00:03:07,040 --> 00:03:09,760
变成两个不同的卷积去做

74
00:03:09,960 --> 00:03:12,920
这种就是MobileNet里面的DepthMask卷积

75
00:03:13,160 --> 00:03:15,960
右边这种就是去减少卷积里面的

76
00:03:16,520 --> 00:03:17,600
channel的层数

77
00:03:17,600 --> 00:03:19,760
然后让我们的网络模型更浅

78
00:03:20,040 --> 00:03:21,800
下面看几个具体的例子

79
00:03:22,000 --> 00:03:23,520
第一个例子比较明显的

80
00:03:23,520 --> 00:03:25,920
就减少整个空间的大小

81
00:03:25,920 --> 00:03:27,320
SpatialSized

82
00:03:27,640 --> 00:03:29,360
里面就像GoogleNet

83
00:03:29,360 --> 00:03:31,080
还有ExceptionNet V3

84
00:03:31,400 --> 00:03:33,880
就会把5×5的一种卷积

85
00:03:34,200 --> 00:03:36,840
decompose成为一种5×1的卷积

86
00:03:36,840 --> 00:03:38,640
加一种1×5的卷积

87
00:03:38,880 --> 00:03:40,600
最后从最右边的图

88
00:03:40,680 --> 00:03:41,560
我们可以看到

89
00:03:41,560 --> 00:03:44,560
基本上我们可以减少大量的计算的空间

90
00:03:44,840 --> 00:03:47,280
像VGG其实一开始VGG之前

91
00:03:47,640 --> 00:03:49,360
有很多5×5的卷积

92
00:03:49,480 --> 00:03:51,720
VGG把它给decompose成为

93
00:03:51,720 --> 00:03:52,920
3×3的卷积

94
00:03:52,920 --> 00:03:54,280
加3×3的卷积

95
00:03:54,280 --> 00:03:56,280
后面再加一个残插层

96
00:03:56,520 --> 00:03:57,800
从这里面可以看出

97
00:03:57,800 --> 00:04:00,640
也是减少了大量的内存空间

98
00:04:01,280 --> 00:04:02,160
我们再往下看

99
00:04:02,360 --> 00:04:04,520
第二种可能我们也可以做

100
00:04:04,520 --> 00:04:06,960
减少一些channels的计算

101
00:04:07,160 --> 00:04:09,080
这里面channels非常多

102
00:04:09,080 --> 00:04:11,560
像MobileNet就改成了

103
00:04:11,560 --> 00:04:12,840
Pointwise的卷积

104
00:04:12,840 --> 00:04:13,560
一层一

105
00:04:13,560 --> 00:04:16,360
还有一些直接使用一层一的卷积

106
00:04:16,360 --> 00:04:18,600
去代替掉原来的卷积层

107
00:04:18,600 --> 00:04:19,560
这是很有用的

108
00:04:19,560 --> 00:04:22,560
到了后面还有减少feature的数量

109
00:04:22,560 --> 00:04:23,520
像DanceNet这种

110
00:04:23,720 --> 00:04:26,320
就是重复的去使用feature map

111
00:04:26,320 --> 00:04:28,720
而GhostNet华为诺亚去发明的

112
00:04:28,720 --> 00:04:31,760
就是直接基于我们得到的feature map

113
00:04:31,760 --> 00:04:34,640
再去提取新的feature map

114
00:04:34,640 --> 00:04:35,320
很有意思

115
00:04:35,320 --> 00:04:36,800
一开始就是直接减少到

116
00:04:36,800 --> 00:04:38,520
我们的feature的数量

117
00:04:38,880 --> 00:04:41,120
我们现在我们回过头来思考

118
00:04:41,120 --> 00:04:42,160
青年化网络模型

119
00:04:42,280 --> 00:04:44,240
对我们AI的计算模式的

120
00:04:44,240 --> 00:04:46,640
一种不一样的启示

121
00:04:46,880 --> 00:04:48,640
第一个我分开三个方面

122
00:04:48,800 --> 00:04:49,400
这三个方面

123
00:04:49,400 --> 00:04:51,440
其实我之前在推丁引擎里面

124
00:04:51,440 --> 00:04:52,280
已经总结过了

125
00:04:52,280 --> 00:04:54,120
我们简单的把它算起来看一看

126
00:04:54,120 --> 00:04:56,160
首先是卷积核方面

127
00:04:56,400 --> 00:04:57,920
会使用大的卷积核

128
00:04:58,120 --> 00:05:01,000
把变成很多小的卷积核来代替

129
00:05:01,000 --> 00:05:02,680
然后单一尺寸的卷积核

130
00:05:02,800 --> 00:05:05,800
会用很多不同的卷积核来代替

131
00:05:05,800 --> 00:05:07,560
就是一个变成多个

132
00:05:07,560 --> 00:05:08,720
还有一些固定的

133
00:05:08,720 --> 00:05:11,120
就使用大量的使用一层一的卷积

134
00:05:11,120 --> 00:05:12,800
作为我们的buttonlet

135
00:05:13,240 --> 00:05:16,600
第二种就是卷积层通道方面

136
00:05:16,600 --> 00:05:18,600
根据我们的channel的数

137
00:05:18,640 --> 00:05:19,760
去修改

138
00:05:19,760 --> 00:05:21,000
例如我们刚才讲到的

139
00:05:21,000 --> 00:05:22,040
把一些标准的卷积

140
00:05:22,160 --> 00:05:23,880
变成一个device的卷积

141
00:05:23,880 --> 00:05:25,800
然后做一个group的卷积

142
00:05:25,800 --> 00:05:27,040
就分组的卷积

143
00:05:27,200 --> 00:05:28,240
分组卷积之前

144
00:05:28,360 --> 00:05:30,680
可能还会做一个suffer的操作

145
00:05:30,680 --> 00:05:32,080
另外通过像这种

146
00:05:32,240 --> 00:05:33,840
确实就是suffernet的

147
00:05:33,840 --> 00:05:35,800
一些新结构的提出

148
00:05:35,800 --> 00:05:36,240
另外

149
00:05:37,960 --> 00:05:40,600
我们看一下大模型分布式并行

150
00:05:40,600 --> 00:05:41,440
第二个内容

151
00:05:41,440 --> 00:05:43,760
对AI计算模式的一个重要的改变

152
00:05:43,760 --> 00:05:45,680
大模型现在非常的火了

153
00:05:45,680 --> 00:05:47,680
现在像GPT已经火的不行了

154
00:05:47,920 --> 00:05:48,800
其实我们

155
00:05:49,080 --> 00:05:50,880
同样中米在很早之前

156
00:05:51,120 --> 00:05:53,600
讲AI框架的核心技术的时候

157
00:05:53,800 --> 00:05:55,720
就分开了三个主要的内容

158
00:05:55,720 --> 00:05:57,400
因为AI框架的最核心的技术

159
00:05:57,400 --> 00:05:58,880
我觉得分布式

160
00:05:58,920 --> 00:06:00,640
绝对占了其中的一个大头

161
00:06:00,640 --> 00:06:01,880
所以我分开了

162
00:06:02,120 --> 00:06:03,080
三个主要大的内容

163
00:06:03,080 --> 00:06:04,640
给大家去汇报过的

164
00:06:04,640 --> 00:06:07,240
第一个就是分布式的系统

165
00:06:07,240 --> 00:06:09,000
是讲系统的AI框架

166
00:06:09,000 --> 00:06:10,440
通讯集群的管理

167
00:06:10,440 --> 00:06:12,040
这个是跟系统相关的

168
00:06:12,080 --> 00:06:14,520
接着我又给大家去汇报了一个

169
00:06:14,520 --> 00:06:16,160
分布式的算法

170
00:06:16,160 --> 00:06:18,000
就是从10亿规模的模型

171
00:06:18,000 --> 00:06:19,240
到万亿规模

172
00:06:19,240 --> 00:06:20,400
从我们的transformer

173
00:06:20,400 --> 00:06:21,240
bird出现

174
00:06:21,240 --> 00:06:23,480
最后到我们的moet5

175
00:06:23,480 --> 00:06:25,240
这种大规模的网络模型

176
00:06:25,240 --> 00:06:26,680
就讲分布式的算法

177
00:06:26,720 --> 00:06:29,200
接着有了分布式的系统和算法之外

178
00:06:29,480 --> 00:06:30,920
确实我们基于系统之上

179
00:06:31,000 --> 00:06:32,040
要做很多工作

180
00:06:32,040 --> 00:06:33,000
例如模型并行

181
00:06:33,000 --> 00:06:33,760
数据并行

182
00:06:33,760 --> 00:06:34,920
张亮并行

183
00:06:35,440 --> 00:06:37,200
很多并行的操作

184
00:06:37,200 --> 00:06:38,440
不同的并行的操作了

185
00:06:38,440 --> 00:06:39,640
决定我们这些模型

186
00:06:39,640 --> 00:06:40,480
这些算法

187
00:06:40,520 --> 00:06:44,440
怎么跑在我们的AI系统里面

188
00:06:44,680 --> 00:06:46,880
这个时候也可以回头去看一看

189
00:06:46,920 --> 00:06:49,920
接下来我们重新的回到这一个内容里面

190
00:06:49,920 --> 00:06:50,560
可以看到

191
00:06:50,560 --> 00:06:52,200
在最新的2020年里面

192
00:06:52,360 --> 00:06:53,200
foundation model

193
00:06:53,200 --> 00:06:54,240
就我们的大模型

194
00:06:54,480 --> 00:06:56,280
已经成为一种新的范式

195
00:06:56,280 --> 00:06:58,160
新的一种AI的吸引的方向

196
00:06:58,160 --> 00:06:59,920
特别是church GPT

197
00:07:00,160 --> 00:07:01,880
这个时候看到我们并不是

198
00:07:01,880 --> 00:07:03,320
望向高龙从头见的

199
00:07:03,320 --> 00:07:05,120
而是它有一个时间的序列

200
00:07:05,120 --> 00:07:06,640
或者有一些演进的

201
00:07:06,640 --> 00:07:08,800
从一开始的GPT123

202
00:07:08,800 --> 00:07:10,000
到现在church GPT

203
00:07:10,000 --> 00:07:11,520
也是有一个改变的

204
00:07:11,520 --> 00:07:13,040
这里面最明显的一个趋势

205
00:07:13,280 --> 00:07:15,800
就是我们的训练的flop数

206
00:07:15,800 --> 00:07:17,160
我们的计算量

207
00:07:17,200 --> 00:07:19,640
极大的提升和膨胀

208
00:07:19,640 --> 00:07:20,880
包括现在的church GPT

209
00:07:21,040 --> 00:07:22,760
用到的网络模型的规模

210
00:07:22,760 --> 00:07:24,880
已经超过千亿了

211
00:07:25,040 --> 00:07:27,240
下面我们一起简单的回顾一下

212
00:07:27,240 --> 00:07:29,080
之前给大家分享过的内容

213
00:07:29,280 --> 00:07:30,880
第一个就是数据并行

214
00:07:30,880 --> 00:07:33,400
数据并行里面其实分为DP

215
00:07:33,640 --> 00:07:35,680
这个时候最简单的数据并行

216
00:07:35,680 --> 00:07:37,040
还有分布式数据并行

217
00:07:37,040 --> 00:07:37,760
DDP

218
00:07:37,760 --> 00:07:40,000
另外还有FSDP

219
00:07:40,160 --> 00:07:42,280
这三种都可以在PyTorch里面

220
00:07:42,280 --> 00:07:43,400
去看得到的

221
00:07:43,400 --> 00:07:45,480
而这里面DP是最简单的

222
00:07:45,480 --> 00:07:46,880
就是把我们的数据

223
00:07:47,240 --> 00:07:48,840
分布在不同的机器

224
00:07:49,040 --> 00:07:51,800
这也是最简单的其中一种

225
00:07:51,800 --> 00:07:54,280
而我们下面我们看看这个图

226
00:07:54,280 --> 00:07:55,560
我们的训练的数据

227
00:07:55,560 --> 00:07:56,920
分布在不同的机器

228
00:07:57,080 --> 00:07:58,440
上面是一台机器

229
00:07:58,440 --> 00:08:00,840
下面也是另外一台机器

230
00:08:00,840 --> 00:08:02,720
分在不同的机器上面去执行

231
00:08:02,720 --> 00:08:04,560
不过值得注意的就是DP

232
00:08:04,560 --> 00:08:06,560
所谓的data parallelism

233
00:08:06,560 --> 00:08:08,520
不仅仅是指我们的训练的数据

234
00:08:08,520 --> 00:08:10,040
还有我们的权重的数据

235
00:08:10,040 --> 00:08:11,480
我们同步的数据

236
00:08:11,480 --> 00:08:13,520
我们训练的去做并行的

237
00:08:13,720 --> 00:08:14,800
这个时候我们可以看到

238
00:08:14,800 --> 00:08:16,520
这里面就有一条直线

239
00:08:16,520 --> 00:08:18,480
我们可以做一个dust的操作

240
00:08:18,480 --> 00:08:20,280
把数据进行同步

241
00:08:20,440 --> 00:08:23,400
另外的话我们来到了FSDP

242
00:08:23,400 --> 00:08:25,120
FSDP的并行的模式

243
00:08:25,280 --> 00:08:26,640
就更加复杂了

244
00:08:26,640 --> 00:08:29,440
它做了一个全切分的操作

245
00:08:29,440 --> 00:08:31,240
就把我们很温的网络模型

246
00:08:31,240 --> 00:08:32,640
大到一定程度的时候

247
00:08:32,800 --> 00:08:35,600
我们就没办法去把一个网络模型

248
00:08:35,600 --> 00:08:37,080
直接塞到一张卡里面

249
00:08:37,320 --> 00:08:39,320
这个时候我们可能把N层layer

250
00:08:39,360 --> 00:08:40,640
放在一台机器

251
00:08:40,640 --> 00:08:41,560
放在一张卡

252
00:08:41,560 --> 00:08:43,720
再把N层layer放在一张卡

253
00:08:43,920 --> 00:08:45,960
这个时候我们会去做很多

254
00:08:45,960 --> 00:08:47,800
大量的并行的操作

255
00:08:47,800 --> 00:08:48,840
并行的模式

256
00:08:48,840 --> 00:08:49,880
于是一听

257
00:08:50,040 --> 00:08:51,280
我们就有很多数据

258
00:08:51,280 --> 00:08:52,840
进行交互和同步

259
00:08:52,840 --> 00:08:53,920
我们有很多数据

260
00:08:54,080 --> 00:08:55,360
要做并行的操作

261
00:08:55,360 --> 00:08:57,720
所以这里面就会对权重的数据

262
00:08:57,720 --> 00:08:58,840
进行一个同步

263
00:08:58,840 --> 00:09:00,400
对权重的数据进行同步

264
00:09:00,400 --> 00:09:02,480
对优化器的数据进行同步

265
00:09:02,480 --> 00:09:04,520
所以说有了FSDP

266
00:09:04,720 --> 00:09:05,960
FSDP具体的原理

267
00:09:06,200 --> 00:09:08,960
我们在前面其实也是跟大家分享过的

268
00:09:09,160 --> 00:09:11,880
这个就是FSDP的更加复杂的操作

269
00:09:11,880 --> 00:09:14,480
我们就不在这里面去详细的打开

270
00:09:14,480 --> 00:09:15,480
有兴趣的同学

271
00:09:15,600 --> 00:09:17,960
可以回看我们之前的一些分享

272
00:09:18,880 --> 00:09:23,200
下面以MetronLM大模型作为例子

273
00:09:23,200 --> 00:09:25,200
去看看整体的解决方案

274
00:09:25,200 --> 00:09:27,120
因为我们其实有很多并行的模式

275
00:09:27,320 --> 00:09:28,960
这里面就来就融合了

276
00:09:28,960 --> 00:09:29,920
Pipeline的并行

277
00:09:29,920 --> 00:09:32,520
还有张亮的并行两种方式

278
00:09:32,520 --> 00:09:33,880
通过不同的并行方式

279
00:09:33,880 --> 00:09:34,320
当然了

280
00:09:34,320 --> 00:09:36,920
DP就我们的数据并行是默认使用的

281
00:09:36,960 --> 00:09:39,240
这里面只把前两个融合起来

282
00:09:39,240 --> 00:09:40,920
就变成我们下面的图

283
00:09:40,920 --> 00:09:43,240
具体的就是我们的Pipeline的持续图

284
00:09:43,240 --> 00:09:45,560
中间的空的就是我们的一些bubble

285
00:09:45,560 --> 00:09:47,920
这里面可以看到MetronLM

286
00:09:48,080 --> 00:09:50,280
就提出了更多新的算法

287
00:09:50,280 --> 00:09:53,240
把很多的batch变成一个microbatch

288
00:09:53,240 --> 00:09:56,080
然后同步的方式也进行了一个改进

289
00:09:56,280 --> 00:09:57,520
这里面简单的来说

290
00:09:57,640 --> 00:09:58,760
就是在devices

291
00:09:58,760 --> 00:10:01,840
就我们的硬件卡的数量不变之下

292
00:10:02,200 --> 00:10:04,800
分出更多的Pipeline的stage

293
00:10:04,800 --> 00:10:07,960
就假设我们现在所有的一些卡

294
00:10:08,160 --> 00:10:09,360
是放在GPU一的

295
00:10:09,360 --> 00:10:12,760
于是我们可以分开更多不同的stage

296
00:10:12,760 --> 00:10:14,920
然后以更多的通讯量

297
00:10:14,920 --> 00:10:17,320
换取更少的空泡率

298
00:10:17,520 --> 00:10:19,320
这个就是具体的MetronLM

299
00:10:19,320 --> 00:10:20,600
所做的一些工作

300
00:10:20,600 --> 00:10:23,440
所以可以看到也是非常的好

301
00:10:23,720 --> 00:10:25,600
最后我们还是一样的

302
00:10:25,600 --> 00:10:27,440
回到我们的AI的计算模式

303
00:10:27,440 --> 00:10:28,240
对我们大模型

304
00:10:28,240 --> 00:10:29,480
对我们分布是并行

305
00:10:29,480 --> 00:10:30,880
提出的一些思考

306
00:10:30,880 --> 00:10:33,080
这里面我总结了两点

307
00:10:33,080 --> 00:10:33,880
不一定对

308
00:10:33,880 --> 00:10:35,000
大家可以听一听

309
00:10:35,000 --> 00:10:36,240
也可以吐槽吐槽

310
00:10:36,240 --> 00:10:38,200
那下面就是芯片间

311
00:10:38,200 --> 00:10:40,160
必须要互联技术的支持

312
00:10:40,160 --> 00:10:42,440
提供xxgb的带宽

313
00:10:42,440 --> 00:10:44,120
也就是我们芯片间

314
00:10:44,120 --> 00:10:45,880
NPU跟NPU的卡之间

315
00:10:45,880 --> 00:10:47,920
需要进行一个互联互通

316
00:10:47,920 --> 00:10:49,600
而且需要有一个大的带宽

317
00:10:49,600 --> 00:10:52,760
就是因为能够适配到我们的大模型

318
00:10:52,760 --> 00:10:53,720
大模型刚才讲了

319
00:10:53,720 --> 00:10:55,640
我们有很多的并行的操作

320
00:10:55,640 --> 00:10:56,680
一旦涉及到并行

321
00:10:56,680 --> 00:10:57,520
我们的数据

322
00:10:57,520 --> 00:10:59,000
就需要不断的去传输

323
00:10:59,000 --> 00:10:59,800
我们的模型

324
00:10:59,800 --> 00:11:01,280
就要进行一个切分

325
00:11:01,280 --> 00:11:03,680
这个就是同构的芯片类

326
00:11:03,680 --> 00:11:06,840
希望能够有更好的互联的技术

327
00:11:06,840 --> 00:11:08,240
那英伟达就推出了

328
00:11:08,240 --> 00:11:10,400
NVLink和NVSwitch

329
00:11:10,400 --> 00:11:12,760
而另外一种就是需要支持

330
00:11:12,760 --> 00:11:15,280
CPU加GPU的双架构

331
00:11:15,280 --> 00:11:16,840
为整个大规模的AI

332
00:11:16,840 --> 00:11:17,920
就我们大模型

333
00:11:18,120 --> 00:11:19,480
还有HPC的依构平台

334
00:11:19,840 --> 00:11:21,480
提供高带宽

335
00:11:21,480 --> 00:11:24,640
因为我们从英伟达的H100

336
00:11:24,640 --> 00:11:26,400
虽然现在没有太多的供货

337
00:11:26,400 --> 00:11:29,160
可以看到PU加GPU的这种双架构

338
00:11:29,160 --> 00:11:32,840
确实能给性能带来极大的提升

339
00:11:32,960 --> 00:11:35,600
那第二种就是整体的高速的

340
00:11:35,600 --> 00:11:36,720
Transformer引擎

341
00:11:36,720 --> 00:11:38,200
因为大模型主要是以

342
00:11:38,200 --> 00:11:39,360
Transformer为结构

343
00:11:39,360 --> 00:11:40,520
进行一个堆叠的

344
00:11:40,520 --> 00:11:42,600
所以这个时候提供高速的

345
00:11:42,600 --> 00:11:44,000
Transformer引擎来说

346
00:11:44,000 --> 00:11:46,400
对整个大模型的速度提升

347
00:11:46,400 --> 00:11:48,200
是非常有帮助的

348
00:11:48,440 --> 00:11:50,880
第二个就是大模型的时候

349
00:11:51,040 --> 00:11:52,920
其实并不是完全都要使用

350
00:11:52,920 --> 00:11:54,360
FP32去训练的

351
00:11:54,360 --> 00:11:56,040
我们还可以用FP16

352
00:11:56,040 --> 00:11:56,640
另外的话

353
00:11:56,640 --> 00:11:58,160
可能大模型还可以支持

354
00:11:58,160 --> 00:11:59,800
M1的去构建

355
00:11:59,800 --> 00:12:01,480
万亿规模的大模型的结构

356
00:12:01,640 --> 00:12:04,080
这个就是对我们AI计算模式的思考

357
00:12:04,520 --> 00:12:06,760
讲完所有之后

358
00:12:07,200 --> 00:12:10,000
我们总体的去回顾一下

359
00:12:10,000 --> 00:12:11,560
做一个summary

360
00:12:11,560 --> 00:12:13,440
整个AI的计算模式

361
00:12:13,440 --> 00:12:14,960
其实我们把之前的工作

362
00:12:15,080 --> 00:12:16,480
或者之前的汇报

363
00:12:16,520 --> 00:12:17,840
简单的串一串

364
00:12:17,840 --> 00:12:19,560
这里面没有新的知识

365
00:12:19,920 --> 00:12:21,360
我们可以回顾一下

366
00:12:21,360 --> 00:12:22,680
第一个就是我们讲了

367
00:12:22,680 --> 00:12:24,360
一些经典的网络模型的

368
00:12:24,360 --> 00:12:25,280
结构的知识

369
00:12:25,480 --> 00:12:26,400
经典的网络模型

370
00:12:26,520 --> 00:12:27,840
我们需要支持高危的

371
00:12:27,840 --> 00:12:28,720
脏量的存储

372
00:12:28,720 --> 00:12:30,480
支持一些非常复杂的

373
00:12:30,480 --> 00:12:31,840
神经网络的计算

374
00:12:31,840 --> 00:12:34,320
因为很多sota的网络模型

375
00:12:34,320 --> 00:12:35,880
很多sota的算法

376
00:12:36,080 --> 00:12:37,280
AI的芯片

377
00:12:37,280 --> 00:12:38,520
AI的计算模式

378
00:12:38,520 --> 00:12:39,600
必须要考虑到的

379
00:12:39,600 --> 00:12:42,480
第二点就是模型的压缩和量化

380
00:12:42,720 --> 00:12:43,800
提到压缩量化

381
00:12:44,040 --> 00:12:45,240
基本上很明显

382
00:12:45,240 --> 00:12:48,000
量化就需要提供低比特的位数

383
00:12:48,000 --> 00:12:51,360
而量就需要硬件提供稀疏的计算

384
00:12:51,360 --> 00:12:53,760
另外来到我们今天给大家去分享的

385
00:12:53,760 --> 00:12:53,960
内容

386
00:12:53,960 --> 00:12:54,840
就是轻量化

387
00:12:54,840 --> 00:12:55,560
轻量化的时候

388
00:12:55,760 --> 00:12:57,120
希望能够硬件

389
00:12:57,120 --> 00:12:58,080
或者我们的计算模式

390
00:12:58,320 --> 00:13:01,000
会需要有支持很多复杂的计算

391
00:13:01,000 --> 00:13:02,400
例如小卷集合的计算

392
00:13:02,520 --> 00:13:03,320
一乘一的卷集

393
00:13:03,680 --> 00:13:05,520
另外我们还会附用卷集合的

394
00:13:05,520 --> 00:13:07,360
很多内存的信息

395
00:13:07,360 --> 00:13:08,960
就是convolution的weight

396
00:13:08,960 --> 00:13:11,440
或者fit进行一个reused

397
00:13:11,640 --> 00:13:13,760
最后一个就是大模型分布式

398
00:13:13,760 --> 00:13:15,200
并行这个内容

399
00:13:15,200 --> 00:13:17,960
我们是希望能够有大的

400
00:13:17,960 --> 00:13:20,080
内存容量和高速带宽

401
00:13:20,080 --> 00:13:22,160
另外我们还希望有一个大模型的

402
00:13:22,160 --> 00:13:23,240
DSA模块

403
00:13:23,240 --> 00:13:25,640
特别是针对transform的结构

404
00:13:25,680 --> 00:13:28,120
另外还可以提供一些低匹特的

405
00:13:28,120 --> 00:13:28,840
bit wave

406
00:13:28,840 --> 00:13:30,920
然后进行一个高速的运算

407
00:13:30,920 --> 00:13:33,000
这个就是对整个

408
00:13:33,000 --> 00:13:34,640
AI计算模式的总结和思考

409
00:13:34,640 --> 00:13:35,080
好了

410
00:13:35,080 --> 00:13:36,760
今天的内容就到这里为止

411
00:13:36,760 --> 00:13:37,640
谢谢各位

412
00:13:37,640 --> 00:13:39,640
卷的不行了

413
00:13:39,640 --> 00:13:41,040
记得一键三连加关注

414
00:13:41,440 --> 00:13:42,800
所有的内容都会开源

415
00:13:42,800 --> 00:13:44,560
在下面这条链接里面

416
00:13:45,000 --> 00:13:45,880
摆了个拜

