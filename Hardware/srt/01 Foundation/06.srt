1
00:00:00,000 --> 00:00:11,000
哈喽大家好 我是那个正义都能迟到为什么我上班不能迟到的ZOMI

2
00:00:11,000 --> 00:00:16,000
今天呢我要给大家分享的一个内容呢还是在我们的AI计算体系里面

3
00:00:16,000 --> 00:00:19,000
今天讲的比较特别是bitwise

4
00:00:19,000 --> 00:00:22,000
我们的比特位或者叫做比特位宽

5
00:00:22,000 --> 00:00:27,000
其实呢我们现在呢还是在AI计算体系这个内容里面

6
00:00:27,000 --> 00:00:32,000
现在呢主要是在计算体系和矩阵运算这个大内容

7
00:00:32,000 --> 00:00:34,000
那这里面呢我们分开了四个内容

8
00:00:34,000 --> 00:00:37,000
之前我们讲了AI芯片的关键指标

9
00:00:37,000 --> 00:00:42,000
给大家分享了AI计算体系里面具体的计算的最简单的单元

10
00:00:42,000 --> 00:00:43,000
矩阵的运算

11
00:00:43,000 --> 00:00:46,000
现在呢我们来看看比特的位宽

12
00:00:46,000 --> 00:00:52,000
其实呢我这里面的有点意思就是四年前的我是这样的

13
00:00:52,000 --> 00:00:56,000
首先呢我不理解硬件的为什么不提供int8的指令

14
00:00:56,000 --> 00:00:58,000
那第二个呢我又提出另外一个问题

15
00:00:58,000 --> 00:01:02,000
为什么硬件呢不支持int8的比特的位数

16
00:01:02,000 --> 00:01:06,000
接着呢这个时候呢我就去提出了一个更大的疑问的

17
00:01:06,000 --> 00:01:08,000
我现在搞的量化算法怎么落地啊

18
00:01:08,000 --> 00:01:09,000
怎么去加速呀

19
00:01:09,000 --> 00:01:11,000
你硬件都不支持我的算法

20
00:01:11,000 --> 00:01:13,000
那你硬件是不是得改了

21
00:01:13,000 --> 00:01:15,000
于是呢我就开始找硬件的同事呢

22
00:01:15,000 --> 00:01:19,000
催他们你们赶紧把int4 int8都支持起来

23
00:01:19,000 --> 00:01:20,000
我要混合比特的加速

24
00:01:20,000 --> 00:01:23,000
你的芯片才能够让我更好的利用起来

25
00:01:23,000 --> 00:01:25,000
那四年前的我呢是这样的

26
00:01:25,000 --> 00:01:26,000
现在我们来

27
00:01:26,000 --> 00:01:27,000
好了

28
00:01:27,000 --> 00:01:28,000
讲完那么多

29
00:01:28,000 --> 00:01:29,000
我们来到第二个内容

30
00:01:29,000 --> 00:01:31,000
就是比特的位宽

31
00:01:31,000 --> 00:01:32,000
比特的wife

32
00:01:33,000 --> 00:01:35,000
硬件遇到什么问题

33
00:01:35,000 --> 00:01:38,000
到底硬件它是怎么去支持我们的比特位宽的

34
00:01:40,000 --> 00:01:42,000
深度学习的计算模式里面呢

35
00:01:42,000 --> 00:01:44,000
给大家讲过的就是量化的原理

36
00:01:44,000 --> 00:01:48,000
我们希望把一些很宽的一些数据或者一些浮点的数据

37
00:01:48,000 --> 00:01:50,000
约束到我们-127到127

38
00:01:51,000 --> 00:01:55,000
就可以把很多不同样的数据呢都约束到这个范围

39
00:01:55,000 --> 00:01:58,000
同样我们可以把超出的一些数据呢

40
00:01:58,000 --> 00:01:59,000
直接把它丢掉

41
00:01:59,000 --> 00:02:01,000
这也是一种量化的方式

42
00:02:01,000 --> 00:02:02,000
通过这种方式

43
00:02:02,000 --> 00:02:04,000
大家对量化的技术原理呢

44
00:02:04,000 --> 00:02:05,000
非常感兴趣

45
00:02:05,000 --> 00:02:06,000
也可以去看一下

46
00:02:06,000 --> 00:02:08,000
我之前给大家的一个分享

47
00:02:08,000 --> 00:02:11,000
就是在推理系统里面的模型压缩

48
00:02:11,000 --> 00:02:14,000
特意的去讲到了低比特量化的一些原理了

49
00:02:14,000 --> 00:02:16,000
还有在真正训练的时候

50
00:02:16,000 --> 00:02:18,000
我们怎么去做杆子量化训练的

51
00:02:18,000 --> 00:02:19,000
那训练完之后呢

52
00:02:19,000 --> 00:02:22,000
我们去看一下怎么去把我们量化后的模型

53
00:02:22,000 --> 00:02:25,000
量化后的数据进行一个真正的部署起来

54
00:02:25,000 --> 00:02:28,000
欢迎大家去回看一下之前的内容

55
00:02:28,000 --> 00:02:29,000
如果都知道了

56
00:02:29,000 --> 00:02:30,000
那没关系

57
00:02:30,000 --> 00:02:31,000
我们下面呢

58
00:02:31,000 --> 00:02:32,000
就其实量化里面呢

59
00:02:32,000 --> 00:02:33,000
有很多种方法

60
00:02:33,000 --> 00:02:34,000
总结起来呢

61
00:02:34,000 --> 00:02:35,000
主要有三种

62
00:02:35,000 --> 00:02:36,000
第一种呢

63
00:02:36,000 --> 00:02:37,000
是量化训练

64
00:02:37,000 --> 00:02:38,000
就训练的时候呢

65
00:02:38,000 --> 00:02:39,000
进行量化

66
00:02:39,000 --> 00:02:40,000
第二种呢

67
00:02:40,000 --> 00:02:42,000
就是静态离线量化

68
00:02:42,000 --> 00:02:44,000
我们叫做ptq static

69
00:02:44,000 --> 00:02:45,000
是我们通过数据呢

70
00:02:45,000 --> 00:02:46,000
在推理之前

71
00:02:46,000 --> 00:02:47,000
离线转换的时候呢

72
00:02:47,000 --> 00:02:49,000
进行一个简单的量化

73
00:02:49,000 --> 00:02:53,000
找到我们需要进行量化的scale和offset

74
00:02:53,000 --> 00:02:54,000
最后一种呢

75
00:02:54,000 --> 00:02:55,000
就是动态离线量化

76
00:02:55,000 --> 00:02:57,000
我们叫做dynamic ptq

77
00:02:57,000 --> 00:02:59,000
或者ptq dynamic都行

78
00:02:59,000 --> 00:03:00,000
这种方式呢

79
00:03:00,000 --> 00:03:01,000
用的还是比较少的

80
00:03:01,000 --> 00:03:03,000
大家用的更多的是第一种

81
00:03:03,000 --> 00:03:05,000
还有第二种

82
00:03:05,000 --> 00:03:06,000
下面呢

83
00:03:06,000 --> 00:03:07,000
量化的简单的原理

84
00:03:07,000 --> 00:03:08,000
不过呢

85
00:03:08,000 --> 00:03:09,000
我们现在想去看看

86
00:03:09,000 --> 00:03:12,000
什么决定我们的比特位宽

87
00:03:12,000 --> 00:03:14,000
到底是什么决定的

88
00:03:14,000 --> 00:03:15,000
所以我们需要去看看

89
00:03:15,000 --> 00:03:17,000
整个在训练的流程

90
00:03:17,000 --> 00:03:18,000
在推理的流程

91
00:03:18,000 --> 00:03:20,000
有哪些数据

92
00:03:20,000 --> 00:03:21,000
这些数据

93
00:03:21,000 --> 00:03:23,000
哪些能够转换成为低比特

94
00:03:23,000 --> 00:03:26,000
哪些不能够转换成为低比特

95
00:03:26,000 --> 00:03:28,000
这个时候对我们的系统的挑战

96
00:03:28,000 --> 00:03:29,000
对我们对AI

97
00:03:29,000 --> 00:03:30,000
对我们对算法的了解

98
00:03:30,000 --> 00:03:32,000
就有要求了

99
00:03:32,000 --> 00:03:33,000
我们接下来

100
00:03:33,000 --> 00:03:34,000
我们先要了解一下

101
00:03:34,000 --> 00:03:38,000
比特位宽bitwise是怎么定义的

102
00:03:38,000 --> 00:03:39,000
下面有一个图啊

103
00:03:39,000 --> 00:03:40,000
我们可以看到

104
00:03:40,000 --> 00:03:41,000
这个图花了很久啊

105
00:03:41,000 --> 00:03:46,000
确实32P16到最近比较特殊的TF32

106
00:03:46,000 --> 00:03:48,000
还有比较特殊的BF16

107
00:03:48,000 --> 00:03:49,000
到传统的int32

108
00:03:49,000 --> 00:03:50,000
int16

109
00:03:50,000 --> 00:03:51,000
int8

110
00:03:51,000 --> 00:03:54,000
基本上我们可以看到总比特数呢

111
00:03:54,000 --> 00:03:56,000
是用X加1

112
00:03:56,000 --> 00:03:58,000
再加M

113
00:03:58,000 --> 00:03:59,000
而X呢

114
00:03:59,000 --> 00:04:01,000
具体的指的就是我们的符号位

115
00:04:01,000 --> 00:04:04,000
到底是正的还是负的

116
00:04:04,000 --> 00:04:09,000
代表是我们子数的一个动态的范围

117
00:04:09,000 --> 00:04:12,000
也就是二的一次方

118
00:04:12,000 --> 00:04:13,000
这种方式

119
00:04:13,000 --> 00:04:14,000
而M呢

120
00:04:14,000 --> 00:04:15,000
就是小数位了

121
00:04:15,000 --> 00:04:18,000
能够代表我们具体的浮点的精度

122
00:04:18,000 --> 00:04:20,000
通过这种方式

123
00:04:20,000 --> 00:04:21,000
S加1加M

124
00:04:21,000 --> 00:04:25,000
然后得到我们的比特位的位宽

125
00:04:25,000 --> 00:04:26,000
所以说FB30

126
00:04:26,000 --> 00:04:27,000
有一点很重要的

127
00:04:27,000 --> 00:04:29,000
就是我们刚才谈到了

128
00:04:29,000 --> 00:04:32,000
我们还是希望能够降低我们的比特的位宽

129
00:04:32,000 --> 00:04:33,000
但是呢

130
00:04:33,000 --> 00:04:36,000
不影响我们的网络模型的训练

131
00:04:36,000 --> 00:04:38,000
不影响我们网络模型的精度

132
00:04:38,000 --> 00:04:39,000
那这个时候呢

133
00:04:39,000 --> 00:04:42,000
我们对硬件就有要求了

134
00:04:42,000 --> 00:04:44,000
我们希望对于MAC

135
00:04:44,000 --> 00:04:47,000
就是我们的累积层加的操作呢

136
00:04:47,000 --> 00:04:48,000
它的速输出

137
00:04:48,000 --> 00:04:51,000
能够有效地减少数据的搬运和存储

138
00:04:51,000 --> 00:04:54,000
那这个时候使用降低比特位宽

139
00:04:54,000 --> 00:04:55,000
另外的话

140
00:04:55,000 --> 00:04:56,000
第二点就是我们希望

141
00:04:56,000 --> 00:04:59,000
能够减少MAC的计算和开销的代价

142
00:04:59,000 --> 00:05:00,000
另外int8xint8呢

143
00:05:00,000 --> 00:05:02,000
其实我们只要存一个int16

144
00:05:02,000 --> 00:05:03,000
就可以完全解决问题了

145
00:05:03,000 --> 00:05:05,000
但是FB16xFB16

146
00:05:05,000 --> 00:05:07,000
我们使用FB30来存

147
00:05:07,000 --> 00:05:09,000
这个时候我们整个位宽呢

148
00:05:09,000 --> 00:05:10,000
是大了很多

149
00:05:10,000 --> 00:05:12,000
整个计算的形态也会大很多

150
00:05:12,000 --> 00:05:14,000
整个需要的额外的电路呢

151
00:05:14,000 --> 00:05:15,000
也会大很多

152
00:05:15,000 --> 00:05:19,000
那下面我们看一个非常有意思的表格

153
00:05:19,000 --> 00:05:20,000
降低位宽

154
00:05:20,000 --> 00:05:22,000
有什么好处

155
00:05:22,000 --> 00:05:24,000
那其实好处非常的多

156
00:05:24,000 --> 00:05:25,000
第一个呢

157
00:05:25,000 --> 00:05:27,000
就是我们下面左边的这个图

158
00:05:27,000 --> 00:05:28,000
我们看一下这里面呢

159
00:05:28,000 --> 00:05:31,000
说了是energy的一个cost

160
00:05:31,000 --> 00:05:33,000
我们的功耗的节省

161
00:05:33,000 --> 00:05:34,000
可以看到这里面

162
00:05:34,000 --> 00:05:37,000
大部分都是一些小单位的计算

163
00:05:37,000 --> 00:05:39,000
小单位计算可以看到

164
00:05:39,000 --> 00:05:40,000
最耗时的是什么

165
00:05:40,000 --> 00:05:43,000
最耗时的是IO的读写

166
00:05:43,000 --> 00:05:45,000
对DVM和SVM的一个读写

167
00:05:45,000 --> 00:05:47,000
占的时间特别的多

168
00:05:47,000 --> 00:05:49,000
但是我8bit 16bit

169
00:05:49,000 --> 00:05:51,000
这些低比特的

170
00:05:51,000 --> 00:05:54,000
基本上energy的消耗是非常少的

171
00:05:54,000 --> 00:05:56,000
当我们的比特位数更多的时候

172
00:05:56,000 --> 00:05:59,000
我们的一些功耗会越大

173
00:05:59,000 --> 00:06:00,000
消耗的电

174
00:06:00,000 --> 00:06:02,000
这是第一点

175
00:06:02,000 --> 00:06:03,000
第二点呢

176
00:06:03,000 --> 00:06:05,000
就是芯片的面积

177
00:06:05,000 --> 00:06:07,000
或者叫做晶体管的面积

178
00:06:07,000 --> 00:06:10,000
基本上比特位数越少

179
00:06:10,000 --> 00:06:12,000
晶体管的占用面积也就越少

180
00:06:12,000 --> 00:06:14,000
到32V的时候呢

181
00:06:14,000 --> 00:06:17,000
晶片的晶体管的面积就会急剧的上升

182
00:06:17,000 --> 00:06:19,000
所以可能单一块芯片里面呢

183
00:06:19,000 --> 00:06:21,000
我用同样的一些制程

184
00:06:21,000 --> 00:06:24,000
我大部分都是塞8bit的时候

185
00:06:24,000 --> 00:06:26,000
可能我就可以塞非常多的计算单元

186
00:06:26,000 --> 00:06:27,000
在某个时钟周期内呢

187
00:06:27,000 --> 00:06:29,000
我可以计算更多的数据

188
00:06:29,000 --> 00:06:32,000
而且还更能够节省我们的电源

189
00:06:32,000 --> 00:06:33,000
更能够省电

190
00:06:33,000 --> 00:06:35,000
基于这个前提之下呢

191
00:06:35,000 --> 00:06:36,000
现在市面上呢

192
00:06:36,000 --> 00:06:38,000
已经推出了很多

193
00:06:38,000 --> 00:06:39,000
假设我在推理的时候呢

194
00:06:39,000 --> 00:06:40,000
使用8bit

195
00:06:40,000 --> 00:06:41,000
但是呢我在训练的时候呢

196
00:06:41,000 --> 00:06:44,000
使用16bit的一些产品

197
00:06:44,000 --> 00:06:45,000
那这个产品呢

198
00:06:45,000 --> 00:06:47,000
就有华为升腾910

199
00:06:47,000 --> 00:06:48,000
确实在推理的时候呢

200
00:06:48,000 --> 00:06:49,000
和训练的时候呢

201
00:06:49,000 --> 00:06:53,000
我们都可以采用很低比特的去训练

202
00:06:53,000 --> 00:06:54,000
让我们的训练呢跑得更快

203
00:06:54,000 --> 00:06:55,000
另外的话

204
00:06:55,000 --> 00:06:56,000
英伟达的A100呢

205
00:06:56,000 --> 00:06:58,000
确实也采用了这种方式

206
00:06:58,000 --> 00:07:00,000
而它就推出了一个TensorGo

207
00:07:00,000 --> 00:07:03,000
里面有了一个TF32

208
00:07:03,000 --> 00:07:05,000
那实际上TF32呢

209
00:07:05,000 --> 00:07:07,000
它只有19个位宽

210
00:07:07,000 --> 00:07:09,000
并不是32个位宽呢

211
00:07:09,000 --> 00:07:10,000
大家值得注意

212
00:07:12,000 --> 00:07:14,000
在最后的一个过程当中

213
00:07:14,000 --> 00:07:16,000
我们在对AI芯片的设计呢

214
00:07:16,000 --> 00:07:17,000
需要进行一个思考的

215
00:07:17,000 --> 00:07:18,000
第一个

216
00:07:18,000 --> 00:07:19,000
我们降低位宽的时候

217
00:07:19,000 --> 00:07:20,000
一般降低什么呢

218
00:07:20,000 --> 00:07:22,000
我们会降低我们的精度

219
00:07:22,000 --> 00:07:23,000
precision

220
00:07:23,000 --> 00:07:24,000
在网络模型

221
00:07:24,000 --> 00:07:25,000
或者在神经网络里面呢

222
00:07:25,000 --> 00:07:27,000
我们其实在经过训练的过程当中

223
00:07:27,000 --> 00:07:28,000
就发现

224
00:07:28,000 --> 00:07:29,000
最后面的精度

225
00:07:29,000 --> 00:07:31,000
其实我们可以降下来的

226
00:07:31,000 --> 00:07:32,000
因为神经网络

227
00:07:32,000 --> 00:07:34,000
它是有非常大的容错率的

228
00:07:34,000 --> 00:07:35,000
第二个呢

229
00:07:35,000 --> 00:07:38,000
就是降低我们的一个动态范围1

230
00:07:38,000 --> 00:07:40,000
而动态范围降低了

231
00:07:40,000 --> 00:07:42,000
其实我们数据的表达能力

232
00:07:42,000 --> 00:07:43,000
会是降低的

233
00:07:43,000 --> 00:07:44,000
所以这个时候

234
00:07:44,000 --> 00:07:45,000
怎么去设计一款

235
00:07:45,000 --> 00:07:47,000
有用的数据的格式

236
00:07:47,000 --> 00:07:51,000
那像就设计出了自己的TF32

237
00:07:51,000 --> 00:07:53,000
就是对我们的Mbit和Ebit

238
00:07:53,000 --> 00:07:55,000
做了一个很好的权衡

239
00:07:55,000 --> 00:07:56,000
刚才我们说了

240
00:07:56,000 --> 00:07:57,000
这个加起来呢

241
00:07:57,000 --> 00:07:58,000
位宽只有19

242
00:07:58,000 --> 00:08:00,000
1加8加10

243
00:08:00,000 --> 00:08:01,000
很有意思

244
00:08:01,000 --> 00:08:02,000
为什么这么设计呢

245
00:08:02,000 --> 00:08:03,000
我们在这里面呢

246
00:08:03,000 --> 00:08:06,000
就简单的给大家去讲一讲

247
00:08:06,000 --> 00:08:08,000
我的动态范围表示呢

248
00:08:08,000 --> 00:08:10,000
是跟FP32相同的

249
00:08:10,000 --> 00:08:14,000
但是我能够表达的精度范围呢

250
00:08:14,000 --> 00:08:19,000
反倒是跟FP16保持相同的

251
00:08:19,000 --> 00:08:20,000
那这个时候呢

252
00:08:20,000 --> 00:08:22,000
它就能够保持比较好的

253
00:08:22,000 --> 00:08:24,000
动态的范围的位宽

254
00:08:24,000 --> 00:08:26,000
然后也保证了比较好的精度

255
00:08:26,000 --> 00:08:28,000
所以说英伟达呢

256
00:08:28,000 --> 00:08:30,000
就针对A100设计出了

257
00:08:30,000 --> 00:08:34,000
赌出FP32的这种位宽

258
00:08:34,000 --> 00:08:35,000
在最后呢

259
00:08:35,000 --> 00:08:37,000
就是对AI芯片设计的

260
00:08:37,000 --> 00:08:39,000
一些个人的思考

261
00:08:39,000 --> 00:08:40,000
首先呢

262
00:08:40,000 --> 00:08:41,000
我们需要考虑到

263
00:08:41,000 --> 00:08:42,000
低的位宽

264
00:08:42,000 --> 00:08:44,000
会不会对我们的网络模型

265
00:08:44,000 --> 00:08:45,000
进行影响

266
00:08:45,000 --> 00:08:46,000
因为呢

267
00:08:46,000 --> 00:08:47,000
不同的网络

268
00:08:47,000 --> 00:08:48,000
不同的数据

269
00:08:48,000 --> 00:08:49,000
及不同的任务

270
00:08:49,000 --> 00:08:50,000
不同的位宽

271
00:08:50,000 --> 00:08:52,000
可能还是有差异的

272
00:08:52,000 --> 00:08:53,000
我举个具体的例子

273
00:08:53,000 --> 00:08:55,000
就是在分类的时候

274
00:08:55,000 --> 00:08:56,000
其实

275
00:08:56,000 --> 00:08:58,000
位宽int8的训练也好

276
00:08:58,000 --> 00:09:00,000
FP10也好

277
00:09:00,000 --> 00:09:02,000
它没有像检测网络模型

278
00:09:02,000 --> 00:09:03,000
这么敏感

279
00:09:03,000 --> 00:09:04,000
这个时候精度的差异

280
00:09:04,000 --> 00:09:05,000
其实是不大的

281
00:09:05,000 --> 00:09:07,000
所以说对精度的影响

282
00:09:07,000 --> 00:09:09,000
可能还跟不同的任务相关

283
00:09:09,000 --> 00:09:10,000
那第二点呢

284
00:09:10,000 --> 00:09:12,000
就是训练和推理的位宽

285
00:09:12,000 --> 00:09:13,000
是完全不一样的

286
00:09:13,000 --> 00:09:15,000
其实32bit呢

287
00:09:15,000 --> 00:09:17,000
我们只是做一个弱基线

288
00:09:17,000 --> 00:09:18,000
很多时候啊

289
00:09:18,000 --> 00:09:20,000
并不会使用32bit去训练

290
00:09:20,000 --> 00:09:22,000
因为实在是太慢了

291
00:09:22,000 --> 00:09:23,000
而且很耗电

292
00:09:23,000 --> 00:09:24,000
还耗时

293
00:09:24,000 --> 00:09:25,000
那另外呢

294
00:09:25,000 --> 00:09:26,000
我们对于训练呢

295
00:09:26,000 --> 00:09:28,000
可以使用IP16啦

296
00:09:28,000 --> 00:09:29,000
BF16啦

297
00:09:29,000 --> 00:09:30,000
还有TF16

298
00:09:30,000 --> 00:09:31,000
不同的格式

299
00:09:31,000 --> 00:09:33,000
而在推理的时候

300
00:09:33,000 --> 00:09:34,000
我们CV呢

301
00:09:34,000 --> 00:09:36,000
一般是以int8作为推理的

302
00:09:36,000 --> 00:09:37,000
而NLP里面呢

303
00:09:37,000 --> 00:09:38,000
很少以int8

304
00:09:38,000 --> 00:09:40,000
作为主要的推理的位宽

305
00:09:40,000 --> 00:09:41,000
而是以FP16

306
00:09:41,000 --> 00:09:43,000
或者以int8到FP16

307
00:09:43,000 --> 00:09:45,000
这种混合的模式

308
00:09:45,000 --> 00:09:48,000
我们现在在做大模型过程当中呢

309
00:09:48,000 --> 00:09:50,000
就发现int8跟FP16的混合

310
00:09:50,000 --> 00:09:52,000
其实对我们的大模型的推理

311
00:09:52,000 --> 00:09:54,000
并不是说非常的敏感

312
00:09:54,000 --> 00:09:56,000
而且精度基本上没怎么降

313
00:09:56,000 --> 00:09:57,000
所以说我们点了

314
00:09:57,000 --> 00:09:58,000
也就是第三点

315
00:09:58,000 --> 00:09:59,000
我们还是要权衡

316
00:09:59,000 --> 00:10:01,000
硬件的成本的开销

317
00:10:01,000 --> 00:10:04,000
因为支持额外的数据的位宽呢

318
00:10:04,000 --> 00:10:07,000
我们需要引入更多的电路

319
00:10:07,000 --> 00:10:08,000
这是真的

320
00:10:08,000 --> 00:10:09,000
我们等一下有个图啊

321
00:10:09,000 --> 00:10:11,000
看一下英伟达的图就知道了

322
00:10:11,000 --> 00:10:12,000
那另外一个问题呢

323
00:10:12,000 --> 00:10:15,000
就是新增多少个额外的数据的位宽

324
00:10:15,000 --> 00:10:16,000
比较合适呢

325
00:10:16,000 --> 00:10:19,000
硬盘常在一些硬件的指标看到呢

326
00:10:19,000 --> 00:10:21,000
FP16支持多少TFLUST

327
00:10:21,000 --> 00:10:23,000
int8支持多少TFLUST

328
00:10:23,000 --> 00:10:24,000
FP32呢

329
00:10:24,000 --> 00:10:25,000
支持多少TFLUST

330
00:10:25,000 --> 00:10:28,000
就是因为我们需要去增加额外的数据位宽

331
00:10:28,000 --> 00:10:30,000
就会引入更多的电路

332
00:10:30,000 --> 00:10:33,000
我们现在来看看英伟达A100的

333
00:10:33,000 --> 00:10:35,000
这个整体的架构图

334
00:10:35,000 --> 00:10:36,000
然后呢

335
00:10:36,000 --> 00:10:39,000
拿出里面其中一个XM出来

336
00:10:39,000 --> 00:10:41,000
进行一个剖析

337
00:10:41,000 --> 00:10:43,000
可以看到这里面的一个SM啊

338
00:10:43,000 --> 00:10:46,000
就是我们刚才看到的一扎SM里面

339
00:10:46,000 --> 00:10:47,000
我们这里面呢

340
00:10:47,000 --> 00:10:48,000
就有很多int32

341
00:10:48,000 --> 00:10:49,000
FP32

342
00:10:49,000 --> 00:10:50,000
FP64

343
00:10:50,000 --> 00:10:54,000
还有Tensor Core里面的TF32的

344
00:10:54,000 --> 00:10:56,000
不同的计算的格式

345
00:10:56,000 --> 00:10:57,000
里面都是支持的

346
00:10:57,000 --> 00:10:58,000
但呢

347
00:10:58,000 --> 00:11:00,000
它里面还可能还是有一些int8

348
00:11:00,000 --> 00:11:01,000
或者其他数据格式

349
00:11:01,000 --> 00:11:02,000
而这里面的int32呢

350
00:11:02,000 --> 00:11:05,000
可能还会拆分成为两个int16

351
00:11:05,000 --> 00:11:07,000
或者四个int8的这种形式

352
00:11:07,000 --> 00:11:08,000
所以说呢

353
00:11:08,000 --> 00:11:09,000
我们简单的总结一下

354
00:11:09,000 --> 00:11:12,000
就是我们在芯片设计的之前

355
00:11:12,000 --> 00:11:15,000
我会给大家去讲讲一些很无聊的工作

356
00:11:15,000 --> 00:11:17,000
就是AI的计算体系

357
00:11:17,000 --> 00:11:18,000
希望让大家去了解

358
00:11:18,000 --> 00:11:21,000
去知道整个深度学习的计算模式

359
00:11:21,000 --> 00:11:22,000
是怎么样的

360
00:11:22,000 --> 00:11:24,000
我们的计算体系

361
00:11:24,000 --> 00:11:25,000
又应该长什么样子

362
00:11:25,000 --> 00:11:26,000
于是呢

363
00:11:26,000 --> 00:11:29,000
就引起AI的最重要的计算的方式

364
00:11:29,000 --> 00:11:30,000
卷机呢

365
00:11:30,000 --> 00:11:34,000
其实可以化成我们的矩阵运算MM

366
00:11:34,000 --> 00:11:35,000
而MM的计算呢

367
00:11:35,000 --> 00:11:38,000
我们可以引入更低比特的位宽

368
00:11:38,000 --> 00:11:39,000
去进行计算

369
00:11:39,000 --> 00:11:42,000
从而会太影响我们训练和推理的精度

370
00:11:42,000 --> 00:11:44,000
那在下一节内容里面呢

371
00:11:44,000 --> 00:11:46,000
我们会去讲讲专用的硬件

372
00:11:46,000 --> 00:11:47,000
那所谓专用硬件呢

373
00:11:47,000 --> 00:11:51,000
就是指我们DOMAIN SPECIFIC ARCHITECTURE

374
00:11:51,000 --> 00:11:53,000
我们的AI芯片的内容呢

375
00:11:53,000 --> 00:11:55,000
慢慢的深入起来了

376
00:11:55,000 --> 00:11:56,000
好了 今天的内容呢

377
00:11:56,000 --> 00:11:57,000
就到这里为止

378
00:11:57,000 --> 00:11:58,000
谢谢各位

379
00:12:02,000 --> 00:12:03,000
回过头来

380
00:12:03,000 --> 00:12:04,000
四年前的我呢

381
00:12:04,000 --> 00:12:07,000
我们刚才提出了很多一些

382
00:12:07,000 --> 00:12:08,000
argue的情况

383
00:12:08,000 --> 00:12:10,000
或者不理解硬件为什么设计的情况

384
00:12:10,000 --> 00:12:11,000
现在的我呢

385
00:12:11,000 --> 00:12:12,000
是这么想的

386
00:12:12,000 --> 00:12:13,000
其实呢

387
00:12:13,000 --> 00:12:14,000
一切呢

388
00:12:14,000 --> 00:12:16,000
都没有我们想象中的这么简单

389
00:12:16,000 --> 00:12:19,000
在比特位宽的缩减和增加呢

390
00:12:19,000 --> 00:12:20,000
它是一个系统性的工程

391
00:12:20,000 --> 00:12:21,000
我们需要考虑到

392
00:12:21,000 --> 00:12:23,000
AI的整个计算体系

393
00:12:23,000 --> 00:12:24,000
硬件的架构

394
00:12:24,000 --> 00:12:25,000
硬件的成本

395
00:12:25,000 --> 00:12:26,000
还有系统的综合成本

396
00:12:26,000 --> 00:12:28,000
等非常多的问题

397
00:12:28,000 --> 00:12:30,000
所以说一切没有那么简单

398
00:12:30,000 --> 00:12:31,000
现在呢

399
00:12:31,000 --> 00:12:32,000
今天的内容就到这里为止了

400
00:12:32,000 --> 00:12:33,000
谢谢各位

401
00:12:33,000 --> 00:12:34,000
拜拜

