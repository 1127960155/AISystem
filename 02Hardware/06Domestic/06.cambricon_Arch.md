# 理论基础

本节我们将会深入硬件细节为大家介绍寒武纪MLUv3的一些架构细节。在此之前我们先简单介绍一下寒武纪架构的一些学术基础。

首先，在2014年左右，寒武纪公司的前身中科院计算所计算机体系结构国家重点实验室的智能处理器团队（现在的智能处理器研究中心），与国际合作者Olivier Temam一起发表了一篇名称为DianNao（[1]） 的论文，这篇论文指出当机器学习算法尤其是神经网络算法越来越普遍应用的情况下，使用专门的机器学习加速器或许可以既能提供提供优秀的性能又能支撑广泛的应用场景。要知道，在2012年AlexNet大火，刷新了CV领域的一些记录，在2014年左右，最为典型最为重要的DNN模型，也就是CNN了（卷积神经网络），因此这个工作也对CNN进行了一定的设计。在当时，虽然也有一些关于设计专用硬件来实现机器学习算法的论文，但这篇工作尤其强调加速器的存储设计、性能和功耗。此工作设计了一个性能为452GOP/s，只有3.02平方毫米，485mW功耗的高吞吐加速器，与一个128比特2GHz的SIMD处理器相比，它快117.87倍，可以把总功耗降低21.08倍，使用65nm工艺。在此工作中，重点关注神经网络的推理阶段而不是训练阶段，文章分析了一个简单的神经网络的访存模式，通过对输入输出做tiling（切片）完成计算。分析了如果把整个神经网络的权值全部硬化在下来需要很高的成本，因此无法大规模应用，因此设计了自己的体系结构，一个小规模的加速器，它由以下几个部分组成：专门的输入神经元Buffer（NBin），输出神经元Buffer（NBout），存放权重的buffer（SB），还有专门的计算单元NFU（Neural Function Unit）以及控制单元CP。此设计中的NBin、NBout等是scratchpad memory，不同于一些国内外GPGPU方案中所使用的寄存器文件（register file），这个思想也一直影响着寒武纪的设计哲学。

之后上述团队还进一步提出了DaDianNao([2])，

# 寒武纪MLU架构细节

## MLUv3 IPU （MLU core）

IPU在官方文档中也叫作MLU core，

# 参考文献

[1] Chen T , Du Z , Sun N ,et al.DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning[C]//International Conference on Architectural Support for Programming Languages & Operating Systems.ACM, 2014.DOI:10.1145/2541940.2541967.

[2] Chen Y , Luo T , Liu S ,et al.DaDianNao: A Machine-Learning Supercomputer[C]//2014 47th Annual IEEE/ACM International Symposium on Microarchitecture.0[2024-04-14].DOI:10.1109/MICRO.2014.58.


## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=403940478&bvid=BV1TV411j7Yx&cid=1210634113&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
