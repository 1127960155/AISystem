# 理论基础

本节我们将会深入硬件细节为大家介绍寒武纪MLUv3的一些架构细节。在此之前我们先简单介绍一下寒武纪架构的一些学术基础。

首先，在2014年左右，寒武纪公司的前身中科院计算所计算机体系结构国家重点实验室的智能处理器团队（现在的智能处理器研究中心），与国际合作者Olivier Temam一起发表了一篇名称为DianNao（[1]） 的论文，这篇论文指出当机器学习算法尤其是神经网络算法越来越普遍应用的情况下，使用专门的机器学习加速器或许可以既能提供提供优秀的性能又能支撑广泛的应用场景。要知道，在2012年AlexNet大火，刷新了CV领域的一些记录，在2014年左右，最为典型最为重要的DNN模型，也就是CNN了（卷积神经网络），因此这个工作也对CNN进行了一定的设计。在当时，虽然也有一些关于设计专用硬件来实现机器学习算法的论文，但这篇工作尤其强调加速器的存储设计、性能和功耗。此工作设计了一个性能为452GOP/s，只有3.02平方毫米，485mW功耗的高吞吐加速器，与一个128比特2GHz的SIMD处理器相比，它快117.87倍，可以把总功耗降低21.08倍，使用65nm工艺。在此工作中，重点关注神经网络的推理阶段而不是训练阶段，文章分析了一个简单的神经网络的访存模式，通过对输入输出做tiling（切片）完成计算。分析了如果把整个神经网络的权值全部硬化在下来需要很高的成本，因此无法大规模应用，因此设计了自己的体系结构，一个小规模的加速器，它由以下几个部分组成：专门的输入神经元Buffer（NBin），输出神经元Buffer（NBout），存放权重的buffer（SB），还有专门的计算单元NFU（Neural Function Unit）以及控制单元CP。此设计中的NBin、NBout等是scratchpad memory，不同于一些国内外GPGPU方案中所使用的寄存器文件（register file），这个思想也一直影响着寒武纪的设计哲学。

之后计算所智能处理器团队还进一步发表了DaDianNao([2])、ShiDianNao([3])、PuDianNao([4])等工作，它们有的在之前的基础上将芯片扩展到multi chip系统，支持训练与推理，有的专注于计算机视觉任务，有的专注于传统机器学习算法。Cambricon指令集（[5]）这篇论文为了解决日新月异的算法难以全部通过硬化适配的问题，借鉴RISC的指令集的设计原则，通过将描述神经网络的复杂指令分解成更短，更简单的指令，来扩大加速器的应用范围。使用简单和短的指令来减少设计和验证的风险，以及译码逻辑的功耗和面积。这一思想也被后续应用到了寒武纪公司的产品中。

# 寒武纪MLU架构细节

## MLUv3 IPU （MLU core）

IPU在官方文档中也叫作MLU core，下面我们简单根据zomi的资料介绍一下MLU core的工作原理，再对照寒武纪官方文档来了解一下寒武纪产品的编程模型。

![MLU core核心模块](images/cambricon09.png)

从zomi找到的资料图中，我们可以看到，Control Unit比较重要，负责指令的读取、译码和发射。自研指令可以通过 Control Unit 被负责计算和访存的调度器Dispatch到ALU、VFU、TFU、IO-DMA、Move-DMA 四个队列。IO-DMA用来实现片外DRAM与W/N-RAM数据传输，也可以用于实现Register与片内RAM之间的Load/Store操作以及原子操作。Move-DMA用于IPU中W/N-RAM与MPU S-RAM间数据传输和类型转换。I-Cache顾名思义就是指令缓存，有64KB，如512bit指令可以缓存1024条。VA-Cache(Vector Addressing)是离散数据访问指令的专用缓存。用于加速离散向量访问效率，减少对总线和存储单元读写次数。Neural-RAM（nram）是768KB，需16byte对齐，存储Input和Feature Map数据。Weight-RAM（wram）是1024KB，需8byte对齐，存储权重和优化器数据。

ALU就是标量 Scale 数据的算术逻辑运算。GPR是指一组位宽48bit的寄存器，IPU为Load/Store架构，除立即数以外所有操作加载到GPR才能参与算术逻辑运算。SREG是指位宽32bit的特殊寄存器，用于存储硬件特定属性，如Perf计数、任务索引等。

VFU/TFU是计算单元，实现张量 Tensor 和向量 Vector 运算；输入输出：Vector 运算输入输出在Neuron-RAM；Tensor 运算输入自Neuron-RAM和Weight-RAM，输出根据调度情况到Neuron-RAM和Weight-RAM。

## 存储层次

说到Neural-RAM（nram）和Weight-RAM（wram），细心的读者可能已经意识到了，它们就相当于前面介绍过的DianNao那篇论文里面的NBin+NBout与SB。它们属于scratchpad memory，一种当做存储空间来用的SRAM。那么我们就来了解一下寒武纪体系结构下的存储层次。

抽象硬件模型提供了丰富的存储层次，包括GPR（General Purpose Register，通用寄存器）、NRAM、WRAM、SRAM、L2 Cache、LDRAM（Local DRAM，局部 DRAM 存储单元）、GDRAM（Global DRAM，全局 DRAM 存储空间）等。GPR、WRAM 和 NRAM 是一个 MLU Core 的私有存储，Memory Core 没有私有的 WRAM 和 NRAM 存储资源。L2 Cache 是芯片的全局共享存储资源，目前主要用于缓存指令、Kernel 参数以及只读数据。LDRAM 是每个 MLU Core 和 Memory Core 的私有存储空间，其容量比 WRAM 和 NRAM 更大，主要用于解决片上存储空间不足的问题。GDRAM 是全局共享的存储资源，可以用于实现主机端与设备端的数据共享，以及计算任务之间的数据共享。

### GPR

GPR 是每个 MLU Core 和 Memory Core 私有的存储资源。MLU Core 和 Memory Core 的标量计算系统都采用精简指令集架构，所有的标量数据，无论是整型数据还是浮点数据，在参与运算之前必须先加载到 GPR。GPR 的最大位宽为 48位，一个GPR 可以存储一个 8bit、16bit、32bit或者48bit的数据。GPR 中的数据不仅可以用来实现标量运算和控制流功能，还用于存储向量运算所需要的地址、长度和标量参数等。

GPR 不区分数据类型和位宽，GPR 中存储的数据的类型和位宽由操作对应数据的指令决定。当实际写入 GPR 中的数据的位宽小于48bit时，高位自动清零。

Cambricon BANG 异构并行编程模型中的隐式数据迁移都是借助 GPR 实现的。例如，将 GDRAM 上的标量数据赋值给一个位于 LDRAM 的变量时，编译器会自动插入访存指令将 GDRAM 中的数据先加载到 GPR 中，再插入一条访存指令将 GPR 中的数据写入 LDRAM中。

### NRAM(Neural-RAM)

NRAM 是每个 MLU Core 私有的片上存储空间，主要用来存放向量运算和张量运算的输入和输出数据，也可以用于存储一些运算过程中的临时标量数据。相比 GDRAM 和 LDRAM 等片外存储空间，NRAM 有较低的访问延迟和更高的访问带宽。NRAM 的访存效率比较高但空间大小有限，而且不同硬件的 NRAM 容量不同。用户需要合理利用有限的 NRAM 存储空间，以提高程序的性能。对于频繁访问的数据，应该尽量放在 NRAM 上，仅仅当 NRAM 容量不足时，才将数据临时存储在片上的 SRAM 或者片外的 LDRAM 或者 GDRAM 上。

### WRAM(Weight-RAM)

WRAM 是每个 MLU Core 私有的片上存储空间，主要用来存放卷积运算的卷积核数据。为了高效地实现卷积运算，WRAM 上的数据具有特殊的数据布局。

### SRAM(Shared-RAM，此SRAM非彼SRAM，因为S代表Shared而不是Static)

SRAM 是一个 Cluster 内所有 MLU Core 和 Memory Core 都可以访问的共享存储空间。SRAM 可以用于缓存 MLU Core 的中间计算结果，实现 Cluster 内不同 MLU Core 或 Memory Core 之间的数据共享及不同 Cluster 之间的数据交互。

SRAM 有较高的访存带宽，但是容量有限。用户需要合理利用有限的 SRAM 存储空间，以提高程序的性能。

SRAM 仅支持 MLUv02 及后续硬件架构。

### L2 Cache

L2 Cache 是位于片上的全局存储空间，由硬件保证一致性，目前主要用于缓存指令、Kernel 参数以及只读数据。

### LDRAM

LDRAM 是每个 MLU Core 和 Memory Core 私有的存储空间，可以用于存储无法在片上存放的私有数据。LDRAM 属于片外存储，不同 MLU Core 和 Memory Core 之间的LDRAM空间互相隔离，软件可以配置其容量。与GDRAM相比，LDRAM的访存性能更好，因为LDRAM的访存冲突比较少。

### GDRAM

与 LDRAM 类似，GDRAM 也是片外存储。位于 GDRAM 中的数据被所有的 MLU Core 和 Memory Core 共享。GDRAM 空间的作用之一是用来在主机侧与设备侧传递数据，如 Kernel 的输入、输出数据等。Cambricon BANG 异构编程模型提供了专门用于在主机侧和设备侧之间进行数据拷贝的接口。

## 指令流水





# 参考文献

[1] Chen T , Du Z , Sun N ,et al.DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning[C]//International Conference on Architectural Support for Programming Languages & Operating Systems.ACM, 2014.DOI:10.1145/2541940.2541967.

[2] Chen Y , Luo T , Liu S ,et al.DaDianNao: A Machine-Learning Supercomputer[C]//2014 47th Annual IEEE/ACM International Symposium on Microarchitecture.0[2024-04-14].DOI:10.1109/MICRO.2014.58.

[3] Du, Z., Fasthuber, R., Chen, T., Ienne, P., Li, L., Luo, T., Feng, X., Chen, Y., Temam, O., 2015. ShiDianNao: shifting vision processing closer to the sensor, in: Proceedings of the 42nd Annual International Symposium on Computer Architecture. Presented at the ISCA ’15: The 42nd Annual International Symposium on Computer Architecture, ACM, Portland Oregon, pp. 92–104. https://doi.org/10.1145/2749469

[4] Liu, D., Chen, T., Liu, S., Zhou, J., Zhou, S., Teman, O., Feng, X., Zhou, X., Chen, Y., 2015. PuDianNao: A Polyvalent Machine Learning Accelerator, in: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems. Presented at the ASPLOS ’15: Architectural Support for Programming Languages and Operating Systems, ACM, Istanbul Turkey, pp. 369–381. https://doi.org/10.1145/2694344

[5] Liu S, Du Z, Tao J, et al. Cambricon: An Instruction Set Architecture for Neural Networks[C]// Acm/ieee International Symposium on Computer Architecture. 2016.

[6] [寒武纪CAMBRICON BANG C/C++ 编程指南](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/programming_guide_1.7.0/hardware_implementation/index.html#)


## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=403940478&bvid=BV1TV411j7Yx&cid=1210634113&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
