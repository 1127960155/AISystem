1
00:00:06,900 --> 00:00:08,500
人生得意须尽欢

2
00:00:17,900 --> 00:00:19,100
特别是AI系列里面呢

3
00:00:19,100 --> 00:00:20,500
我们现在已经来到了

4
00:00:22,300 --> 00:00:23,600
谷歌TPU系列

5
00:00:23,700 --> 00:00:25,266
在谷歌TPU系列里面呢

6
00:00:30,000 --> 00:00:32,100
就离不开谷歌最重要的一个

7
00:00:32,100 --> 00:00:34,000
不管是TPU1234啊

8
00:00:34,266 --> 00:00:36,866
它里面的最核心的计算就是脉动阵列

9
00:00:36,900 --> 00:00:39,366
所以我们呢在谷歌TPU第一代里面呢

10
00:00:39,366 --> 00:00:43,666
详细的去展开脉动阵列的具体的细节

11
00:01:08,200 --> 00:01:09,900
从Alpha Go的第一代

12
00:01:10,000 --> 00:01:14,100
跟韩国的李世石和中国的柯洁去对战

13
00:01:18,366 --> 00:01:20,566
非常非常重要的一个应用

14
00:01:21,900 --> 00:01:23,200
在这些应用背后呢

15
00:01:23,200 --> 00:01:26,800
都离不开谷歌TPU第一代芯片的架构

16
00:01:33,500 --> 00:01:36,866
来到了TPU第一代的芯片的架构呢

17
00:01:36,866 --> 00:01:37,866
有点花花绿绿

18
00:01:48,966 --> 00:01:49,466
这里面的

19
00:02:09,466 --> 00:02:12,066
虽然你叫它mmumxu也行啊

20
00:02:12,166 --> 00:02:15,366
在谷歌后面官网的宣称呢都会叫它MXU

21
00:02:22,166 --> 00:02:25,333
每个时钟周期呢可以计算256个乘法

22
00:02:25,466 --> 00:02:29,133
计算的结果呢是一个半精度FP16

23
00:02:36,866 --> 00:02:39,466
还有一个双缓存的单元

24
00:02:46,466 --> 00:02:47,933
回调的一个作用

25
00:02:49,000 --> 00:02:49,466
接下来呢

26
00:02:49,466 --> 00:02:52,866
我们看下一个内容就是accumulators

27
00:02:54,700 --> 00:02:58,566
主要是用来收集刚才MXU计算之后

28
00:03:04,266 --> 00:03:05,666
实际呢这里面的计算呢

29
00:03:05,666 --> 00:03:06,866
还是很有意思的

30
00:03:12,100 --> 00:03:13,500
而整个MXU里面去执行

31
00:03:21,466 --> 00:03:24,333
最后呢再执行32比特的累加

32
00:03:26,100 --> 00:03:28,300
activation进行累加的

33
00:03:28,366 --> 00:03:31,166
整体的运算呢就变成4096*256*32b

34
00:03:42,700 --> 00:03:44,566
都会有控制的模块

35
00:03:46,900 --> 00:03:48,000
在我们的区里面呢

36
00:03:48,066 --> 00:03:49,766
分开四级的流水

37
00:03:50,166 --> 00:03:51,066
控制单元呢

38
00:03:52,666 --> 00:03:53,566
那这些指令呢

39
00:03:53,600 --> 00:03:55,500
是通过PCIe的总线

40
00:03:59,666 --> 00:04:01,766
传到我们的芯片里面

41
00:04:13,766 --> 00:04:16,333
也就是我们的复杂指令集

42
00:04:18,600 --> 00:04:20,700
哎小新就有疑问了

43
00:04:23,900 --> 00:04:25,900
但是却用CSIC指令集呢

44
00:04:41,500 --> 00:04:42,566
所以这条指令呢

45
00:04:42,566 --> 00:04:44,166
就变得非常的复杂

46
00:04:44,166 --> 00:04:46,366
于是呢就使用了CISC指令

47
00:04:48,266 --> 00:04:50,866
而为了去控制里面的各种各样的单元

48
00:04:50,900 --> 00:04:52,066
特别是MXU啊

49
00:04:52,066 --> 00:04:54,333
UB啊还有AU等模块呢

50
00:04:54,366 --> 00:04:55,866
里面呢就定义了十几个

51
00:04:55,866 --> 00:04:57,166
专门为神经网络推理

52
00:04:58,700 --> 00:04:59,566
都比较长

53
00:05:14,366 --> 00:05:15,966
主要是读数据

54
00:05:25,800 --> 00:05:27,500
所以ZOMI觉得这5条指令呢

55
00:05:35,366 --> 00:05:38,066
可以去这条链接里面去了解

56
00:05:38,100 --> 00:05:40,700
谷歌TPU里面的instruction set Architecture

57
00:05:45,100 --> 00:05:47,700
我们看一下它的整体的芯片的布局啊

58
00:05:53,400 --> 00:05:55,500
就是Local unified buffer

59
00:06:05,000 --> 00:06:08,166
其实只有2%的芯片电路面积

60
00:06:10,766 --> 00:06:12,466
留下了更多的空间

61
00:06:12,500 --> 00:06:15,500
那这个呢就是TPU的芯片的布局了

62
00:06:19,500 --> 00:06:21,200
现在已经到凌晨1点钟了

63
00:06:24,500 --> 00:06:26,800
最重要的内容了

64
00:06:26,800 --> 00:06:30,000
那脉动阵列的英文叫做Systolic array

65
00:06:38,166 --> 00:06:40,466
Img2col的这种方式

66
00:06:40,466 --> 00:06:41,066
实际上呢

67
00:06:41,066 --> 00:06:42,066
我们知道啊

68
00:06:54,500 --> 00:06:56,966
或者对我们的feature map进行卷积

69
00:06:56,966 --> 00:06:58,366
而是通过Img2col

70
00:06:58,400 --> 00:07:00,466
的方式把我们的卷积呢

71
00:07:02,766 --> 00:07:03,266
那ZOMI呢

72
00:07:08,766 --> 00:07:10,166
最终把原始的卷积

73
00:07:30,866 --> 00:07:32,766
那矩阵乘的就是一行乘以一列

74
00:07:34,066 --> 00:07:36,133
那大家看到这个内容没有

75
00:07:38,566 --> 00:07:40,733
我们把矩阵里面的每一行

76
00:07:40,800 --> 00:07:43,900
跟矩阵里面的每一列的方式拿出来

77
00:07:45,000 --> 00:07:46,866
每一次相乘相加

78
00:07:57,300 --> 00:07:59,000
那面向这种计算的结构呢

79
00:08:01,200 --> 00:08:03,100
可不可以用1987年发明的

80
00:08:03,100 --> 00:08:04,566
脉冲阵列的方式呢

81
00:08:04,666 --> 00:08:07,566
去提供具体的矩阵的运算呢

82
00:08:07,600 --> 00:08:08,200
那于是呢

83
00:08:08,200 --> 00:08:10,500
这里面呢就引入了脉动阵列

84
00:08:11,700 --> 00:08:13,566
对我们的数据呢进行计算

85
00:08:14,400 --> 00:08:15,066
控制模块呢

86
00:08:15,066 --> 00:08:15,766
把我们的数据呢

87
00:08:15,800 --> 00:08:17,700
一波一波根据FIFO队列

88
00:08:25,300 --> 00:08:26,166
进行累加

89
00:08:28,266 --> 00:08:30,566
就像心脏的脉动血液一样啊

90
00:08:30,566 --> 00:08:32,333
每次时钟周期执行一次

91
00:08:46,100 --> 00:08:47,100
每次运算的结果呢

92
00:08:47,100 --> 00:08:47,900
都像脉动一样

93
00:08:47,900 --> 00:08:50,266
把上一次计算的结果呢缓存起来

94
00:08:54,666 --> 00:08:57,066
也是谷歌引入的概念了

95
00:09:01,266 --> 00:09:02,666
虽然PPT呢有点旧

96
00:09:05,200 --> 00:09:06,300
首先我们看到啊

97
00:09:06,300 --> 00:09:07,666
左边的这个图呢

98
00:09:08,966 --> 00:09:10,766
m就是我们的计算器memory

99
00:09:18,700 --> 00:09:19,100
算一次

100
00:09:19,100 --> 00:09:21,400
从我们的计算机里面储一次数据

101
00:09:25,566 --> 00:09:27,333
那脉动阵列的发明的时候呢

102
00:09:27,866 --> 00:09:30,066
我们数据可不可以不断不断地累加

103
00:09:34,400 --> 00:09:35,466
给下次PE

104
00:09:35,500 --> 00:09:36,900
把下次PE的计算结果呢

105
00:09:38,300 --> 00:09:39,500
最终计算完之后呢

106
00:09:41,900 --> 00:09:42,900
通过这种方式呢

107
00:09:44,866 --> 00:09:45,533
那当时呢

108
00:09:47,066 --> 00:09:48,366
而整个脉动的阵列

109
00:09:51,300 --> 00:09:52,366
矩阵的计算

110
00:09:53,100 --> 00:09:55,500
一个3乘以3的矩阵

111
00:09:55,500 --> 00:09:57,500
去讲讲我们的脉动的阵列

112
00:10:01,700 --> 00:10:04,500
我们虽然是一个3*3的数据

113
00:10:07,500 --> 00:10:08,300
这是a矩阵

114
00:10:10,100 --> 00:10:11,300
数据的排布呢

115
00:10:11,300 --> 00:10:13,400
并不是完全相同的

116
00:10:13,466 --> 00:10:15,366
我们输进去脉动阵列的时候呢

117
00:10:15,366 --> 00:10:16,966
数据进行要错开

118
00:10:26,100 --> 00:10:27,266
第一次计算的时候呢

119
00:10:27,266 --> 00:10:30,166
是把我们队列里面的A00呢

120
00:10:32,600 --> 00:10:33,300
相乘之后呢

121
00:10:36,700 --> 00:10:40,700
我们下次呢会把A01跟B10 进行相乘

122
00:10:40,700 --> 00:10:43,000
累加到我们第一个寄存器里面

123
00:10:49,766 --> 00:10:52,466
然后呢就得到我们现在的结果

124
00:10:52,500 --> 00:10:55,400
接着呢我们还会不断的循环累加

125
00:10:55,466 --> 00:10:57,766
直到呢把所有的数据呢

126
00:10:57,766 --> 00:11:01,133
都加载到我们的整个脉动阵列里面

127
00:11:01,700 --> 00:11:05,000
就完成我们刚才的三个矩阵的运算

128
00:11:13,266 --> 00:11:14,466
最终经过八步之后呢

129
00:11:24,000 --> 00:11:26,166
不过呢大家有没有留意到呢

130
00:11:29,900 --> 00:11:31,666
一次一次的累加起来

131
00:11:31,666 --> 00:11:32,866
最后再获取的

132
00:11:33,600 --> 00:11:34,566
计算的延迟呢

133
00:11:34,566 --> 00:11:36,866
对我们的程序或者对我们的性能来说

134
00:11:37,966 --> 00:11:39,933
就变得非常非常的重要

135
00:11:40,200 --> 00:11:41,700
在实际的程序过程当

136
00:11:41,700 --> 00:11:45,200
中呢其实并不感知MXU的脉动的实现

137
00:11:47,300 --> 00:11:49,700
就需要考虑到对应的延迟

138
00:11:50,500 --> 00:11:53,400
因为刚才讲到数据一级一级的去传导

139
00:11:53,466 --> 00:11:54,766
这意味着我们的延迟呢

140
00:12:00,566 --> 00:12:01,366
那这种情况呢

141
00:12:04,100 --> 00:12:07,000
TPU的CISC卡指令呢就使用了4级的流水

142
00:12:15,866 --> 00:12:17,166
准备数据的时候呢

143
00:12:36,900 --> 00:12:37,666
那其实呢

144
00:12:54,166 --> 00:12:57,466
以对角波的形式呢通过阵列进行输出

145
00:13:01,900 --> 00:13:04,300
输到我们的脉动阵列里面

146
00:13:04,300 --> 00:13:07,266
不断不断的去移动累加进行计算

147
00:13:16,666 --> 00:13:18,066
肯定离不开GPU

148
00:13:42,266 --> 00:13:44,666
更符合神经网络的计算

149
00:14:09,766 --> 00:14:11,066
那通过这个目的不一样了

150
00:14:16,100 --> 00:14:18,300
具体呢能达到92TFLOPS

151
00:14:18,400 --> 00:14:20,500
比英伟达的K80的推理芯片呢

152
00:14:20,500 --> 00:14:22,300
要高出非常多倍

153
00:14:59,700 --> 00:15:00,666
谷歌的TPU呢

154
00:15:17,166 --> 00:15:19,266
谷歌TPU还把片上缓存呢

155
00:15:34,500 --> 00:15:36,500
是节省我们PCIe传输慢的

156
00:15:36,500 --> 00:15:37,900
一个效率的问题

157
00:15:51,100 --> 00:15:53,566
使用8比特进行一个推理

158
00:16:27,500 --> 00:16:29,700
当时候TPU的架构呢

159
00:16:33,100 --> 00:16:35,600
讲到这其实我们接近尾声了

160
00:16:55,000 --> 00:16:56,466
而当时候黄仁勋呢

161
00:16:58,700 --> 00:17:00,866
打算搞手机里面的GPU

162
00:17:37,966 --> 00:17:38,966
或者回顾一下

163
00:17:41,166 --> 00:17:41,466
第六个呢

164
00:17:44,666 --> 00:17:45,266
这些

165
00:17:49,366 --> 00:17:50,466
那今天的内容呢

166
00:17:50,466 --> 00:17:51,933
就到这里为止了

167
00:17:52,966 --> 00:17:53,933
拜了个拜

168
00:00:05,700 --> 00:00:06,866
哈喽大家好

169
00:00:13,400 --> 00:00:15,500
特别是第一代的详细解读

170
00:00:15,700 --> 00:00:16,200
我们知道呢

171
00:00:16,266 --> 00:00:17,933
在整个课程里面

172
00:00:20,566 --> 00:00:22,333
国外AI芯片里面的

173
00:00:28,566 --> 00:00:29,966
而说到芯片架构呢

174
00:00:47,200 --> 00:00:48,100
我们一开始呢

175
00:00:48,100 --> 00:00:51,866
看看谷歌TPU第一代的整体的电路板

176
00:00:51,966 --> 00:00:54,466
那这个呢就是它的整体的产品形态

177
00:00:54,566 --> 00:00:57,366
中间最核心的这一块灰色的呢

178
00:00:57,400 --> 00:00:59,466
就是它的TPU的地带

179
00:00:59,500 --> 00:01:01,866
用来做推理应用的

180
00:01:02,266 --> 00:01:03,866
ZOMI觉得谷歌TPU里面呢

181
00:01:03,900 --> 00:01:05,500
最著名的一个应用呢

182
00:01:05,500 --> 00:01:08,166
就是2015年到2017年

183
00:01:14,200 --> 00:01:17,100
把两个世界围棋的高手呢都打败了

184
00:01:17,100 --> 00:01:18,300
这是谷歌TPU里面

185
00:01:28,866 --> 00:01:32,166
特别是里面特别重要遇到的脉动阵列

186
00:01:32,200 --> 00:01:33,200
那我们现在呢

187
00:01:38,566 --> 00:01:41,166
会详细的对这个芯片的架构图呢

188
00:01:43,500 --> 00:01:46,566
首先呢第一个内容就是Weight FIFO

189
00:01:46,566 --> 00:01:48,766
就这里面的一个内容

190
00:01:53,066 --> 00:01:54,466
DDR是我们的内存

191
00:01:54,500 --> 00:01:56,200
这里面呢把内存的内容呢

192
00:01:56,300 --> 00:01:57,400
从DRAM上面呢

193
00:01:57,466 --> 00:01:59,566
读取到我们的芯片上面

194
00:01:59,800 --> 00:02:01,100
进行一个计算的

195
00:02:01,100 --> 00:02:02,000
那具体计算呢

196
00:02:02,066 --> 00:02:04,366
就会给这个MXU的正念

197
00:02:04,500 --> 00:02:06,300
进行一个核心的计算

198
00:02:06,366 --> 00:02:08,666
那提到了刚才的MXU

199
00:02:08,666 --> 00:02:09,466
这个阵列呢

200
00:02:18,100 --> 00:02:22,066
提供256*256*8的乘加的计算

201
00:02:29,100 --> 00:02:31,900
那这个点呢大家需要注意的是FP16

202
00:02:32,466 --> 00:02:34,366
整个MXU矩阵单元里面呢

203
00:02:34,366 --> 00:02:36,866
还包含64KB的Weight tile

204
00:02:42,366 --> 00:02:43,966
我们后面会展开的

205
00:02:43,966 --> 00:02:46,466
它更多的是用来做一个缓存

206
00:03:01,800 --> 00:03:03,700
把结果存下来

207
00:03:08,566 --> 00:03:12,066
整个MXU呢会取256个元素

208
00:03:19,100 --> 00:03:21,066
存放在我们的accumulators

209
00:03:24,300 --> 00:03:26,100
也就是给我们下面的这个模块

210
00:03:33,700 --> 00:03:37,566
所以accumulators的大小呢刚好是4MB

211
00:03:38,700 --> 00:03:40,100
对于控制指令来说

212
00:03:40,100 --> 00:03:41,466
我们的Control啊

213
00:03:41,500 --> 00:03:42,700
因为每块芯片呢

214
00:03:45,866 --> 00:03:46,933
整个控制单元呢

215
00:03:57,866 --> 00:03:59,566
去通过4级的流水

216
00:04:04,400 --> 00:04:05,900
而指令的来源呢

217
00:04:05,900 --> 00:04:08,566
是CPU向TPU去发射指令

218
00:04:08,566 --> 00:04:09,666
让它去执行的

219
00:04:09,966 --> 00:04:12,266
整个的芯片的指令的架构呢

220
00:04:12,266 --> 00:04:13,766
采用的是CSIC卡指令

221
00:04:16,466 --> 00:04:18,366
里面呢一共有12条

222
00:04:21,366 --> 00:04:23,866
为什么TPU提供十几条指令

223
00:04:27,400 --> 00:04:29,500
RISC不是更简单吗

224
00:04:30,500 --> 00:04:33,166
小新提出这个疑问非常有意思

225
00:04:33,400 --> 00:04:35,866
因为谷歌定义的每一条指令呢

226
00:04:35,866 --> 00:04:37,466
平均的时钟周期呢

227
00:04:37,500 --> 00:04:39,366
需要10到20个时钟周期

228
00:04:39,366 --> 00:04:41,466
才能够执行完一条指令

229
00:04:57,166 --> 00:04:58,666
而设计的高级指令

230
00:05:01,700 --> 00:05:02,300
那这里面呢

231
00:05:05,266 --> 00:05:06,733
一个是Read_Host_Memory

232
00:05:07,466 --> 00:05:08,766
还有matmul的计算

233
00:05:08,766 --> 00:05:12,066
卷积计算activate和Write_Host_Memory

234
00:05:12,066 --> 00:05:14,366
那从这几个顺序可以看出来

235
00:05:16,366 --> 00:05:19,166
写数据计算算激活

236
00:05:19,300 --> 00:05:21,266
然后写回数据

237
00:05:21,366 --> 00:05:22,766
完成我们神经网络

238
00:05:22,766 --> 00:05:24,966
对每一层的具体的计算

239
00:05:27,500 --> 00:05:28,300
是最核心的

240
00:05:28,366 --> 00:05:29,866
也单独的拿出来

241
00:05:29,900 --> 00:05:31,466
当然他还有其他指令啊

242
00:05:31,600 --> 00:05:32,700
如果开发者和你们呢

243
00:05:32,700 --> 00:05:35,366
非常关心谷歌TPU里面的指令呢

244
00:05:40,766 --> 00:05:42,766
具体是怎么定义的

245
00:05:43,666 --> 00:05:45,133
了解完谷歌TPU的架构图呢

246
00:05:47,800 --> 00:05:49,166
那我们从这个图里面呢

247
00:05:50,500 --> 00:05:52,566
属于一个专用的电路里面

248
00:05:52,566 --> 00:05:53,366
最大的面积呢

249
00:05:58,966 --> 00:06:02,133
一个是我们的具体的计算的阵列

250
00:06:02,500 --> 00:06:03,766
对于刚才讲到的指令

251
00:06:03,800 --> 00:06:05,000
我们的control呢

252
00:06:21,266 --> 00:06:22,333
我还在录课

253
00:06:22,466 --> 00:06:24,466
现在我们才来到了脉动阵列

254
00:06:30,000 --> 00:06:30,900
整体的脉动呢

255
00:06:30,900 --> 00:06:33,566
就像我们的心跳一样啊

256
00:06:33,800 --> 00:06:35,566
在正式进入到脉动阵列之前呢

257
00:06:35,566 --> 00:06:38,133
ZOMI想跟大家一起去回顾一下

258
00:06:42,100 --> 00:06:43,400
关于推理的场景

259
00:06:43,466 --> 00:06:45,733
或者在2015年2017年的时候呢

260
00:06:46,300 --> 00:06:49,500
用到的最多的是卷积神经网络

261
00:06:49,566 --> 00:06:51,266
那整个卷积神经网络里面呢

262
00:06:51,300 --> 00:06:51,700
实际上

263
00:06:51,700 --> 00:06:54,500
我们不会去真正的对我们的图片呢

264
00:07:04,766 --> 00:07:06,566
给大家讲过上面的这个呢

265
00:07:06,566 --> 00:07:08,566
就是真正的原始的卷积

266
00:07:10,166 --> 00:07:12,266
转换成为img2col的方式呢

267
00:07:12,300 --> 00:07:14,966
变成一个具体的矩阵乘的操作

268
00:07:15,166 --> 00:07:16,566
去得到我们跟卷积

269
00:07:16,566 --> 00:07:19,166
数学上原理一致的矩阵乘法

270
00:07:19,200 --> 00:07:21,900
最后再把结果col2img反复回来

271
00:07:21,900 --> 00:07:23,066
变成我们的feature map

272
00:07:23,066 --> 00:07:24,566
整体流程呢是这样的

273
00:07:25,100 --> 00:07:25,700
而在这里面呢

274
00:07:25,700 --> 00:07:27,700
我们刚才提到一个很核心的功能

275
00:07:27,700 --> 00:07:30,800
就是变成了一个矩阵乘的计算的

276
00:07:32,766 --> 00:07:34,066
得到一个元素

277
00:07:36,100 --> 00:07:38,300
这个图我觉得是非常非常核心的

278
00:07:43,900 --> 00:07:44,900
进行计算

279
00:07:46,866 --> 00:07:47,733
相乘相加

280
00:07:48,766 --> 00:07:50,866
最后得到我们橙色的

281
00:07:50,866 --> 00:07:52,466
这一个模块的计算

282
00:07:52,500 --> 00:07:55,300
有没有有点类似于我们的脉动阵列

283
00:07:55,300 --> 00:07:56,900
或者我们的脉冲呢

284
00:08:17,700 --> 00:08:19,766
也就是左边蓝色这个模块

285
00:08:19,900 --> 00:08:22,400
流到MXU里面进行计算

286
00:08:22,466 --> 00:08:22,966
最终呢

287
00:08:22,966 --> 00:08:25,333
流出到下面的这个计算器里面

288
00:08:26,200 --> 00:08:27,100
然后输出

289
00:08:27,200 --> 00:08:28,266
整体的计算方式呢

290
00:08:32,300 --> 00:08:32,900
非常庞

291
00:08:35,400 --> 00:08:37,300
又一拨一拨的流出

292
00:08:40,600 --> 00:08:42,100
所以在一个时钟周期内呢

293
00:08:42,100 --> 00:08:45,500
可以处理665536次矩阵的运算

294
00:08:50,300 --> 00:08:52,066
再进行下一次的累积

295
00:08:59,000 --> 00:09:01,266
我们正式的来到了脉动阵列的原理

296
00:09:02,666 --> 00:09:05,166
不过呢并不影响我们的理解

297
00:09:07,666 --> 00:09:08,966
就是m跟pe

298
00:09:10,766 --> 00:09:12,533
pe呢就是我们的process Unit

299
00:09:14,166 --> 00:09:16,066
里面的具体的计算的执行单元

300
00:09:16,166 --> 00:09:17,466
传统的冯洛伊曼呢

301
00:09:17,466 --> 00:09:18,733
就是我算一次乘一次

302
00:09:21,466 --> 00:09:23,566
在计算不断的往回迭代

303
00:09:23,600 --> 00:09:25,566
这种呢就是我们传统的计算方式

304
00:09:30,066 --> 00:09:30,966
然后进行计算

305
00:09:31,300 --> 00:09:33,100
于是计算呢就变成一个串形的方式

306
00:09:36,900 --> 00:09:38,100
再给下次PE

307
00:09:42,900 --> 00:09:44,866
提供一个脉动的阵列

308
00:09:45,500 --> 00:09:47,000
只是一个串形的方式

309
00:09:52,366 --> 00:09:53,066
那现在呢

310
00:09:57,666 --> 00:09:58,666
那脉动的阵列呢

311
00:09:58,700 --> 00:09:59,766
这里面的数据的

312
00:09:59,766 --> 00:10:01,733
排布比较有意思

313
00:10:04,566 --> 00:10:07,533
也就是意味着我们假设a矩阵哦

314
00:10:22,566 --> 00:10:25,766
是错开一位进去输进去去计算的

315
00:10:30,166 --> 00:10:32,466
跟B00进行相乘

316
00:10:33,300 --> 00:10:35,866
就变成我们现在所是的样子

317
00:10:35,866 --> 00:10:36,666
算完这个之后呢

318
00:10:44,600 --> 00:10:47,766
我们会把A02跟B20进行相乘

319
00:10:47,766 --> 00:10:49,366
累加到这里面

320
00:11:01,100 --> 00:11:01,700
最后呢

321
00:11:05,266 --> 00:11:07,066
完成三个矩阵的运算之后呢

322
00:11:07,066 --> 00:11:10,133
我们就会把整个矩阵的运算的结果呢

323
00:11:14,466 --> 00:11:16,466
就把整个3*3的a矩阵呢

324
00:11:16,500 --> 00:11:18,500
乘以3*3的b矩阵呢

325
00:11:18,666 --> 00:11:21,133
完全的计算出了那个结果

326
00:11:26,166 --> 00:11:28,166
刚才的计算其实是把数据呢

327
00:11:28,166 --> 00:11:29,766
一波一波的数据去

328
00:11:32,900 --> 00:11:33,600
那这个时候呢

329
00:11:56,866 --> 00:11:57,866
直到计算完之后

330
00:11:57,866 --> 00:12:00,466
我们才能够把所有的数据取出来

331
00:12:01,366 --> 00:12:03,366
就会导致我们有计算的延迟

332
00:12:03,800 --> 00:12:04,100
于是呢

333
00:12:07,066 --> 00:12:08,566
用其他指令的执行呢

334
00:12:08,566 --> 00:12:11,166
与刚才MXU的指令重新堆叠

335
00:12:11,166 --> 00:12:13,466
从而隐藏计算的延迟

336
00:12:13,500 --> 00:12:15,866
也就是我们在下一次取指的时候

337
00:12:17,200 --> 00:12:18,666
它已经在计算了

338
00:12:18,900 --> 00:12:20,500
通过指令流水的重叠呢

339
00:12:20,500 --> 00:12:22,566
从而隐藏我们的时延

340
00:12:22,600 --> 00:12:26,500
这也是SIMD里面一个非常重要的功能

341
00:12:27,900 --> 00:12:30,100
从数学和数值上面来看呢

342
00:12:30,166 --> 00:12:31,466
TPU整个脉动阵列呢

343
00:12:31,500 --> 00:12:33,700
就提供了256个乘积的计算

344
00:12:33,700 --> 00:12:36,866
以对角波的形式呢通过阵列

345
00:12:37,666 --> 00:12:38,366
刚才讲到的

346
00:12:38,400 --> 00:12:41,300
是一个比较简单的脉动阵列的原理

347
00:12:41,300 --> 00:12:41,966
但实际上呢

348
00:12:41,966 --> 00:12:44,133
我们在卷积神经网络里面呢

349
00:12:44,300 --> 00:12:45,400
权重的数据呢

350
00:12:45,466 --> 00:12:47,966
会预先的放在我们的脉动阵列里面

351
00:12:48,000 --> 00:12:49,266
就先放进去

352
00:12:49,300 --> 00:12:49,800
接着呢

353
00:12:49,800 --> 00:12:53,666
会把数据或者中间产生的计算结果呢

354
00:12:57,500 --> 00:13:00,100
所以看到了我们会先置一个0

355
00:13:00,200 --> 00:13:01,900
然后呢以对角波的方式

356
00:13:10,266 --> 00:13:10,666
哎

357
00:13:10,666 --> 00:13:14,066
现在我们来到了接近最后一个小内容

358
00:13:14,100 --> 00:13:15,500
里面就是竞品的对比

359
00:13:15,500 --> 00:13:16,666
那讲到竞品呢

360
00:13:18,266 --> 00:13:18,966
那这里面呢

361
00:13:18,966 --> 00:13:22,366
谷歌地带的TPU呢就用CPU GPU跟TPU进

362
00:13:22,400 --> 00:13:24,900
行对比在硬件并行形态里面呢

363
00:13:24,900 --> 00:13:27,966
谷歌TPU1呢采用的是SIMD的模式

364
00:13:27,966 --> 00:13:30,966
而GPU呢采用的是SIMT的模式

365
00:13:31,066 --> 00:13:33,066
虽然TPU采用的是SIMD

366
00:13:33,100 --> 00:13:35,200
但是经过我们刚才讲到的

367
00:13:35,466 --> 00:13:38,166
通过多级流水呢去掩盖我们的时延

368
00:13:38,266 --> 00:13:40,466
使得TPU确定性执行的方式呢

369
00:13:40,466 --> 00:13:42,166
会比CPU或者GPU呢

370
00:13:45,066 --> 00:13:46,566
有助于提高TPU的吞吐

371
00:13:46,566 --> 00:13:48,166
而不是降低它的延迟

372
00:13:48,366 --> 00:13:49,466
那这里面这个概念呢

373
00:13:49,466 --> 00:13:51,366
宗敏觉得非常有意思

374
00:13:51,500 --> 00:13:52,500
TPU的目的呢

375
00:13:52,566 --> 00:13:54,066
是提高我们神经网络

376
00:13:54,100 --> 00:13:55,800
我们AI的计算的吞吐

377
00:13:55,900 --> 00:13:59,066
而我们在之前的GPU的核心内容

378
00:13:59,066 --> 00:14:01,166
或者GPU的技术分享里面呢

379
00:14:01,166 --> 00:14:02,466
去讲到GPU呢

380
00:14:02,466 --> 00:14:03,733
是通过多级的缓存

381
00:14:05,166 --> 00:14:07,466
去降低数据和计算的延迟

382
00:14:07,500 --> 00:14:09,766
所以他们两个的目的是不一样的

383
00:14:11,066 --> 00:14:12,866
我们可以看到谷歌的TPU呢

384
00:14:12,900 --> 00:14:16,100
提供一个非常庞大非常澎湃的算力

385
00:14:23,566 --> 00:14:25,066
GPU呢主要是解决延迟

386
00:14:25,100 --> 00:14:26,700
那就离不开我们看到的GPU

387
00:14:26,700 --> 00:14:28,766
主要是线程分层的去执行

388
00:14:28,866 --> 00:14:31,866
通过网格到分开的网格的线程块

389
00:14:31,900 --> 00:14:34,100
到最终的线程去执行

390
00:14:34,100 --> 00:14:34,700
而这里面呢

391
00:14:34,700 --> 00:14:37,266
英伟达就提出了多级的缓存

392
00:14:37,500 --> 00:14:38,500
从HBM的缓存呢

393
00:14:38,566 --> 00:14:39,766
到L2Cache的缓存呢

394
00:14:39,766 --> 00:14:40,966
到L1Cache的缓存

395
00:14:41,100 --> 00:14:44,500
再到里面SM的register file寄存器

396
00:14:44,566 --> 00:14:45,866
通过多级的缓存呢

397
00:14:45,866 --> 00:14:47,866
去提升整体数据的吞吐

398
00:14:47,900 --> 00:14:50,100
减少数据搬运的延迟

399
00:14:50,166 --> 00:14:52,666
这个是GPU主要的核心的机制

400
00:14:52,966 --> 00:14:54,866
跟谷歌TPU推出出来目的

401
00:14:54,900 --> 00:14:56,666
主要是提升计算的核心

402
00:14:56,666 --> 00:14:58,933
计算的吞吐不是一个概念

403
00:15:00,666 --> 00:15:02,933
它的计算性能就非常的夸张

404
00:15:06,100 --> 00:15:08,000
也就是92T PROS

405
00:15:08,500 --> 00:15:10,600
那具体的计算方式呢就在这里面

406
00:15:10,600 --> 00:15:12,200
大家可以慢慢的去看

407
00:15:12,200 --> 00:15:15,100
ZOMI就不一一的去给大家练起来了

408
00:15:16,366 --> 00:15:17,166
另外一方面呢

409
00:15:19,300 --> 00:15:20,900
做的非常非常的大

410
00:15:20,900 --> 00:15:23,666
从而去节省片外访存的消耗

411
00:15:23,700 --> 00:15:26,200
因为在2015年刚发出来的时候

412
00:15:26,200 --> 00:15:27,900
或者谷歌在做项目预研的时候

413
00:15:27,900 --> 00:15:29,200
其实在2013年

414
00:15:29,300 --> 00:15:31,100
当时还没有用到HBM

415
00:15:31,100 --> 00:15:32,066
而是使用DDR

416
00:15:32,166 --> 00:15:34,266
所以他把片上的缓存做大

417
00:15:39,000 --> 00:15:39,466
另外呢

418
00:15:39,466 --> 00:15:42,066
不得不提谷歌里面的技术前瞻性呢

419
00:15:42,066 --> 00:15:44,466
是非常非常的outstanding的

420
00:15:44,666 --> 00:15:46,766
里面就提出了一个量化的概念

421
00:15:46,800 --> 00:15:47,600
谷歌TPU呢

422
00:15:47,600 --> 00:15:51,100
主要是针对推理场景的首个芯片

423
00:15:53,600 --> 00:15:55,400
谷歌非常具有前瞻性

424
00:15:55,400 --> 00:15:57,000
与之对应的就是英伟达

425
00:15:57,000 --> 00:15:59,300
当时候的推理卡K80呢

426
00:15:59,300 --> 00:16:01,366
还是使用FP32的精度

427
00:16:01,400 --> 00:16:05,000
所以说谷歌呢属于引领的阶段

428
00:16:05,200 --> 00:16:07,366
那通过下面这个图我们可以看到啊

429
00:16:07,366 --> 00:16:08,266
batch size呢

430
00:16:08,266 --> 00:16:11,266
谷歌TPU呢是能够放的非常非常的大

431
00:16:11,466 --> 00:16:13,666
所以可以放到非常大的Batch size

432
00:16:13,666 --> 00:16:14,466
那这个时候呢

433
00:16:14,500 --> 00:16:17,300
它的吞吐也就是对应的当时候图片呢

434
00:16:17,300 --> 00:16:19,600
它的单位呢是以IPS来说

435
00:16:19,666 --> 00:16:21,166
IPS是非常的高

436
00:16:21,266 --> 00:16:23,866
比GPU最好的一款推理卡呢

437
00:16:23,900 --> 00:16:25,400
高了将近10倍

438
00:16:25,400 --> 00:16:27,500
所以说当时候的TPU的提出

439
00:16:29,800 --> 00:16:31,400
是非常有意义的

440
00:16:35,600 --> 00:16:37,000
最后引起一些思考

441
00:16:37,000 --> 00:16:37,666
那谷歌呢

442
00:16:37,666 --> 00:16:38,866
在2015年的时候呢

443
00:16:38,866 --> 00:16:40,666
就已经部署了TPU

444
00:16:40,700 --> 00:16:43,166
第一代ASIC的张量处理器

445
00:16:43,200 --> 00:16:45,966
那这意味着从芯片到逆向往前推呀

446
00:16:46,100 --> 00:16:49,566
其实谷歌在2013年的时候已经去立项了

447
00:16:49,566 --> 00:16:49,766
所以

448
00:16:49,766 --> 00:16:52,566
谷歌AI系统的思想还是非常超前的

449
00:16:52,666 --> 00:16:54,966
当时英伟达呢还没有出现Tensor Core

450
00:16:56,466 --> 00:16:58,666
也是来到中国跟小米去合作

451
00:17:19,966 --> 00:17:21,066
当时候的英伟达呀

452
00:17:21,100 --> 00:17:22,100
想都没想过

453
00:17:22,166 --> 00:17:25,133
居然能够在AI领域发光发热

454
00:17:25,966 --> 00:17:26,566
因此ZOMI呢

455
00:17:26,566 --> 00:17:29,266
就提出了两个简单的小问题

456
00:17:29,266 --> 00:17:30,766
谷歌TPU做对了什么

457
00:17:30,766 --> 00:17:32,866
有哪些超越时代的事迹

458
00:17:32,900 --> 00:17:33,800
那刚才这个呢

459
00:17:33,800 --> 00:17:35,500
ZOMI已经给大家去汇报过了

460
00:17:35,500 --> 00:17:37,900
大家可以简单的去思考一下

461
00:17:38,966 --> 00:17:40,966
今天所给大家汇报的内容

462
00:17:41,500 --> 00:17:43,500
就是针对目前的AI的发展

463
00:17:43,500 --> 00:17:44,600
特别是大模型啊

464
00:17:45,266 --> 00:17:48,966
谷歌有哪些在2015年的时候没有做到的

465
00:17:51,966 --> 00:17:52,866
谢谢各位

466
00:00:01,900 --> 00:00:04,600
字幕生成：mkwei  字幕校准：mkwei

467
00:00:08,500 --> 00:00:11,200
不知啥时候能下班的ZOMI

468
00:05:06,700 --> 00:05:07,400
Read_Weights

469
00:05:55,500 --> 00:05:58,966
还有MXU那一个是我们的缓存

470
00:06:45,700 --> 00:06:46,300
神经网络

471
00:07:47,700 --> 00:07:48,666
相乘相加

472
00:09:12,500 --> 00:09:13,900
或者process execution

473
00:11:10,100 --> 00:11:12,600
同时间的去取出来

474
00:11:21,100 --> 00:11:22,766
然后输出出来

475
00:14:03,700 --> 00:14:05,100
多级的计算核心呢

476
00:14:58,900 --> 00:14:59,666
那这里面呢

477
00:00:11,266 --> 00:00:13,366
今天呢我们来到了谷歌TPU系列

478
00:00:25,266 --> 00:00:28,566
最核心的是谷歌的芯片的架构

479
00:00:45,766 --> 00:00:47,166
跟其他视频一样啊

480
00:01:26,800 --> 00:01:28,866
提供澎湃的散力

481
00:01:37,900 --> 00:01:38,566
我们后面呢

482
00:01:41,200 --> 00:01:42,700
进行打开

483
00:01:49,500 --> 00:01:53,066
Weight FO呢主要是负责把8GB的off chip的DDR

484
00:02:15,800 --> 00:02:18,100
里面呢就以脉动阵列的方式呢

485
00:02:39,500 --> 00:02:42,300
那至于Weight tile呢和双缓存的单元呢

486
00:02:53,366 --> 00:02:54,733
这个accumulators呢

487
00:02:58,666 --> 00:03:01,766
乘法产生的16比特的结果

488
00:03:06,866 --> 00:03:08,466
就是每个时钟周期呢

489
00:03:13,500 --> 00:03:16,500
256*256乘以8比特的乘加

490
00:03:16,500 --> 00:03:19,066
计算产生16比特的结果

491
00:03:31,166 --> 00:03:33,533
b就等于4MB

492
00:03:44,666 --> 00:03:45,733
控制的单元

493
00:03:51,100 --> 00:03:52,666
需要获取具体的指令

494
00:03:55,500 --> 00:03:56,566
通过host主机

495
00:03:56,600 --> 00:03:57,766
就是CPU呢

496
00:04:01,766 --> 00:04:04,366
特别是TPU里面去进行控制的

497
00:04:25,966 --> 00:04:27,266
而不用RISC

498
00:04:46,700 --> 00:04:48,266
而不是用RISC的指令

499
00:04:59,600 --> 00:05:01,700
所以呢它只能用RISC了

500
00:05:02,300 --> 00:05:05,266
有5个比较核心的指令

501
00:05:49,166 --> 00:05:50,466
可以看到整个TPU呢

502
00:06:08,166 --> 00:06:10,766
给整个片上的存储和计算的单元呢

503
00:06:17,466 --> 00:06:19,533
哎讲到口水都干了

504
00:07:00,466 --> 00:07:02,533
换成矩阵乘的方式

505
00:07:03,300 --> 00:07:04,766
在之前推理系统里面呢

506
00:07:59,000 --> 00:08:01,200
谷歌的天才工程师就想到了哎

507
00:08:10,500 --> 00:08:11,666
去对我们的矩阵

508
00:08:32,966 --> 00:08:35,366
大的计算数据一拨一拨的流入

509
00:08:37,866 --> 00:08:40,566
因为整个脉动阵列呢是256*256

510
00:08:52,066 --> 00:08:54,666
这个呢就是脉动阵列最原始的来源

511
00:08:58,666 --> 00:08:58,966
接下来

512
00:09:27,300 --> 00:09:27,800
就觉得哎

513
00:09:33,100 --> 00:09:34,366
把我们的PE计算结果呢

514
00:09:39,500 --> 00:09:41,900
再存到我们的memory里面

515
00:09:48,400 --> 00:09:51,000
其实非常符合我们刚才讲到的

516
00:10:08,300 --> 00:10:10,000
右边的这个是b矩阵

517
00:10:17,000 --> 00:10:22,566
例如这里面的A00 A01 A02 到A01 A11 A12呢

518
00:10:43,100 --> 00:10:44,600
接着在第二个寄存器里面

519
00:11:36,866 --> 00:11:37,966
对我们的吞吐来说呢

520
00:11:45,300 --> 00:11:47,300
所以呢我们性能去计算的时候

521
00:11:54,800 --> 00:11:56,866
会一步一步的去累加

522
00:15:02,966 --> 00:15:06,066
每秒能达到92万亿次计算

