1
00:00:00,258 --> 00:00:04,699
【字幕生成: 奔崩 字幕校对: 奔崩】

2
00:00:04,800 --> 00:00:08,800
Hello，大家好，我是ZOMI

3
00:00:08,800 --> 00:00:10,800
今天来到一个新的内容

4
00:00:10,800 --> 00:00:13,400
就是分布式集群系列

5
00:00:13,400 --> 00:00:17,400
我也叫它作为分布式训练一个系列

6
00:00:17,400 --> 00:00:19,400
今天来到一个基本的介绍

7
00:00:19,400 --> 00:00:20,938
了解一下分布式训练

8
00:00:20,938 --> 00:00:23,400
到底讲些啥,学些啥

9
00:00:23,400 --> 00:00:26,400
大家不要觉得分布式很难或者很遥远

10
00:00:26,400 --> 00:00:29,400
现在不管是训练模型也好

11
00:00:29,400 --> 00:00:31,400
基本上都会用到AI集群

12
00:00:31,400 --> 00:00:34,800
现在最近特别火的就是大模型

13
00:00:34,800 --> 00:00:38,200
有了大模型之后就引入了一个分布式训练

14
00:00:38,200 --> 00:00:43,200
让模型真正的能够在集群里面跑起来

15
00:00:43,200 --> 00:00:46,400
后面主要分开三大内容去介绍的

16
00:00:46,400 --> 00:00:48,400
第一个就是分布式集群

17
00:00:48,400 --> 00:00:49,398
先来看看

18
00:00:49,398 --> 00:00:51,800
硬件,集群,通信

19
00:00:51,800 --> 00:00:55,400
接着去看看分布式的大模型算法

20
00:00:55,400 --> 00:00:58,600
最后看一下一些并行的策略

21
00:00:58,600 --> 00:01:00,800
分为三大部分去介绍

22
00:01:00,800 --> 00:01:04,200
接着打开一下最近的

23
00:01:04,200 --> 00:01:05,800
将要介绍的一个系列

24
00:01:05,800 --> 00:01:07,400
就是分布式AI集群

25
00:01:07,400 --> 00:01:10,800
首先会去看看AI集群的服务器的架构

26
00:01:10,800 --> 00:01:12,000
有这么多的服务器

27
00:01:12,000 --> 00:01:14,400
就要有一个架构统一的去做管理

28
00:01:14,400 --> 00:01:17,400
另外要去处理同步和异步的并行

29
00:01:17,400 --> 00:01:20,400
另外讲一个环同步的算法

30
00:01:20,400 --> 00:01:22,175
让网络模型

31
00:01:22,175 --> 00:01:25,200
在AI集群里面真正的跑起来

32
00:01:25,200 --> 00:01:27,171
接着就会去讲讲

33
00:01:27,171 --> 00:01:29,800
AI集群里面的软硬件通信

34
00:01:29,800 --> 00:01:32,605
首先肯定是软件和硬件

35
00:01:32,605 --> 00:01:34,200
怎么去互相通信的

36
00:01:34,200 --> 00:01:37,200
接着实现了软硬件的通信

37
00:01:37,200 --> 00:01:38,998
真正通信的时候

38
00:01:38,998 --> 00:01:41,400
会讲到通信的原语

39
00:01:41,400 --> 00:01:44,000
具体是用什么方式去通信的

40
00:01:44,000 --> 00:01:46,000
讲完一个集群的内容之后

41
00:01:46,000 --> 00:01:49,200
就看看一个比较跟贴近的

42
00:01:49,200 --> 00:01:51,800
就是AI框架的分布式的能力

43
00:01:51,800 --> 00:01:53,800
也就是这个AI框架

44
00:01:53,800 --> 00:01:56,400
不管是MindSpore、PyTorch、TensorFlow也好

45
00:01:56,400 --> 00:01:59,200
它也必须要具备分布式训练的能力

46
00:01:59,200 --> 00:02:02,200
才能够把集群调度起来

47
00:02:02,200 --> 00:02:03,446
在后面的内容

48
00:02:03,446 --> 00:02:05,323
就会介绍大模型算法

49
00:02:05,323 --> 00:02:07,200
和分布式并行的一些策略

50
00:02:07,200 --> 00:02:10,600
这些留在后面再给大家详细的展开

51
00:02:11,400 --> 00:02:13,711
现在回到分布式训练

52
00:02:13,711 --> 00:02:15,800
或者分布式AI集群里面可以看到

53
00:02:15,800 --> 00:02:17,400
从1950年开始

54
00:02:17,400 --> 00:02:19,128
人工智能第一次提出

55
00:02:19,128 --> 00:02:22,200
到了1980年或者90年代的时候

56
00:02:22,200 --> 00:02:24,600
机器学习是非常火的

57
00:02:24,600 --> 00:02:27,000
直到2010年的时候

58
00:02:27,000 --> 00:02:30,000
深度学习真正的可以落地了

59
00:02:30,000 --> 00:02:31,200
然后也越来越火

60
00:02:31,200 --> 00:02:35,000
现在发文章你不用深度学习都感觉效果不如人意

61
00:02:35,000 --> 00:02:37,600
但是自从2020年之后

62
00:02:37,600 --> 00:02:40,400
迎来了一个叫做Foundation Model

63
00:02:40,400 --> 00:02:42,800
也就是大模型

64
00:02:42,800 --> 00:02:46,356
现在谷歌和Facebook还有微软比拼的

65
00:02:46,356 --> 00:02:47,600
都是一些大模型

66
00:02:47,600 --> 00:02:50,200
因为大模型的性能确实好很多

67
00:02:50,400 --> 00:02:53,000
那再从另外一个角度来看看

68
00:02:53,000 --> 00:02:55,300
现在网络模型的训练

69
00:02:55,300 --> 00:02:57,600
也是模型的参数量越来越大

70
00:02:57,600 --> 00:02:59,600
随着模型参数量越来越大

71
00:02:59,600 --> 00:03:03,600
accuracy就是精度也是越来越高的

72
00:03:03,600 --> 00:03:07,200
所以说现在有这么多的模型有这么多的精度

73
00:03:07,200 --> 00:03:10,400
训练的就需要从串行到并行

74
00:03:10,400 --> 00:03:13,800
然后从并行再到大规模的集群

75
00:03:13,800 --> 00:03:17,200
不过这里面的模型的参数量其实还不够大

76
00:03:17,200 --> 00:03:19,200
往另外一个图看看

77
00:03:19,200 --> 00:03:21,000
刚才提到的网络模型

78
00:03:21,000 --> 00:03:24,000
它的参数规模大概到这个位置为止

79
00:03:24,000 --> 00:03:27,400
但是从16年AlphaGo出来之后到20年

80
00:03:27,400 --> 00:03:28,600
特别是20年

81
00:03:28,600 --> 00:03:32,000
这里面大模型就集堆式的爆发了

82
00:03:32,000 --> 00:03:33,800
大模型可以看到

83
00:03:33,800 --> 00:03:36,874
就一个语言大模型GPT-3

84
00:03:36,874 --> 00:03:38,600
使用8张V100的话

85
00:03:38,600 --> 00:03:41,000
训练时长要到36年

86
00:03:41,000 --> 00:03:43,400
我猴年马月才能训练起来

87
00:03:43,400 --> 00:03:46,379
（什么富哥）
OpenAI就用了512张V100

88
00:03:46,379 --> 00:03:48,200
训练了接近7个月

89
00:03:48,200 --> 00:03:51,800
然后就对外宣布GPT-3终于诞生了

90
00:03:51,800 --> 00:03:53,161
这个时候可以看到

91
00:03:53,161 --> 00:03:55,800
GPT-3的效果还是非常好的

92
00:03:55,800 --> 00:03:58,000
它解决了自监督的方法

93
00:03:58,000 --> 00:04:01,000
而且模型的参数量突破了千亿的规模

94
00:04:01,000 --> 00:04:04,000
还解决了模型碎片化的难题

95
00:04:04,000 --> 00:04:07,600
但是一方面有很多人就开始提出质疑了

96
00:04:07,600 --> 00:04:10,400
一味的让模型参数量变大

97
00:04:10,400 --> 00:04:12,400
参数量爆炸式的增长

98
00:04:12,400 --> 00:04:14,600
就真正的是智能的吗?

99
00:04:14,600 --> 00:04:17,200
这是通向智能的一条真正道路吗?

100
00:04:17,200 --> 00:04:19,000
虽然很多人去质疑这个问题

101
00:04:19,000 --> 00:04:20,800
但是现在来看了

102
00:04:20,800 --> 00:04:23,400
确实模型参数量变大

103
00:04:23,400 --> 00:04:27,200
确实是通向通用智能的其中一种方式

104
00:04:27,200 --> 00:04:29,800
但它不一定代表真正的智能

105
00:04:29,800 --> 00:04:31,600
很多这些时候的判断

106
00:04:31,600 --> 00:04:34,000
要留给后面再做判断

107
00:04:34,000 --> 00:04:35,800
先不要判断的太早

108
00:04:35,800 --> 00:04:38,600
而为了要大模型真正训练起来

109
00:04:38,600 --> 00:04:40,000
其实很重要的

110
00:04:40,000 --> 00:04:42,600
就是解决训练耗时的问题

111
00:04:42,600 --> 00:04:44,000
而训练耗时

112
00:04:44,000 --> 00:04:47,000
主要是由三个参数量来决定的

113
00:04:47,000 --> 00:04:49,200
第一个就是训练的数据规模

114
00:04:49,200 --> 00:04:51,200
数据规模可能是越大越好的

115
00:04:51,200 --> 00:04:53,000
另外一个是单步的计算量

116
00:04:53,000 --> 00:04:56,400
单步计算量也是跟网络模型相关的

117
00:04:56,400 --> 00:04:58,400
因为现在的模型量越大

118
00:04:58,400 --> 00:05:01,400
它单步的计算的时间肯定是越长的

119
00:05:01,400 --> 00:05:04,200
另外一个就是计算的速率

120
00:05:04,200 --> 00:05:06,000
计算速率越快

121
00:05:06,000 --> 00:05:09,000
总体的耗时肯定也是越快的

122
00:05:09,000 --> 00:05:12,400
计算速率是一个可变的因素

123
00:05:12,400 --> 00:05:14,600
于是现在最重要的目的

124
00:05:14,600 --> 00:05:17,200
就是解决计算速率的问题

125
00:05:17,200 --> 00:05:20,400
希望计算速率越大越好

126
00:05:20,400 --> 00:05:24,000
计算速率又分开三个变量

127
00:05:24,000 --> 00:05:24,800
不要着急

128
00:05:24,800 --> 00:05:26,400
还是逐个的来看一下

129
00:05:26,400 --> 00:05:29,400
首先第一个就是单设备的计算速率

130
00:05:29,400 --> 00:05:33,200
就是每一台设备或者每一张NPU或GPU的卡

131
00:05:33,200 --> 00:05:34,200
制程越高

132
00:05:34,200 --> 00:05:36,000
肯定的 速率就越快

133
00:05:36,000 --> 00:05:38,000
第二个就是设备数

134
00:05:38,000 --> 00:05:39,000
设备数越多

135
00:05:39,000 --> 00:05:40,000
集群数越多

136
00:05:40,000 --> 00:05:41,400
可能训练的越快

137
00:05:41,400 --> 00:05:43,800
第三个就是设备的并行效率

138
00:05:43,800 --> 00:05:46,000
称的加速比越高

139
00:05:46,000 --> 00:05:47,200
越快越大

140
00:05:47,200 --> 00:05:50,200
那整体的计算速率也会提升

141
00:05:50,200 --> 00:05:51,600
而在这个系列里面

142
00:05:51,600 --> 00:05:55,200
围绕的更是设备数的增加的时候

143
00:05:55,200 --> 00:05:56,600
设备数增加了

144
00:05:56,600 --> 00:05:58,000
慢慢的多起来了

145
00:05:58,000 --> 00:05:59,600
就变成一个集群了

146
00:05:59,600 --> 00:06:00,400
有了集群

147
00:06:00,400 --> 00:06:03,000
就要去解决服务器的架构问题

148
00:06:03,000 --> 00:06:05,200
要去解决通信拓扑的问题

149
00:06:05,200 --> 00:06:08,000
要解决软硬件通信的问题

150
00:06:08,000 --> 00:06:09,800
还要解决AI框架

151
00:06:09,800 --> 00:06:12,800
能在这个集群里面去跑的问题

152
00:06:12,800 --> 00:06:13,800
在这个时候

153
00:06:13,800 --> 00:06:15,200
华为就推出了

154
00:06:15,200 --> 00:06:18,200
Atlas 900这个AI集群

155
00:06:18,200 --> 00:06:19,200
那这个AI集群

156
00:06:19,200 --> 00:06:22,600
其实现在在全国有22个地方去布局了

157
00:06:22,600 --> 00:06:24,800
包括西安的AI集群中心

158
00:06:24,800 --> 00:06:26,400
南昌的人工智能中心

159
00:06:26,400 --> 00:06:28,000
南京的人工智能中心

160
00:06:28,000 --> 00:06:30,000
杭州的人工智能中心

161
00:06:30,000 --> 00:06:32,600
都布局了Atlas服务器

162
00:06:32,600 --> 00:06:34,200
后面将会去介绍

163
00:06:34,200 --> 00:06:36,400
怎么去提高加速比

164
00:06:36,400 --> 00:06:38,000
提高加速比很重要的

165
00:06:38,000 --> 00:06:39,200
就是要了解

166
00:06:39,200 --> 00:06:42,200
机器跟机器之间是怎么通信的

167
00:06:42,200 --> 00:06:44,600
机器内部加速卡跟加速卡之间

168
00:06:44,600 --> 00:06:45,800
是怎么通信的

169
00:06:45,800 --> 00:06:47,200
有了硬件之后

170
00:06:47,200 --> 00:06:48,800
就要真正的去看看

171
00:06:48,800 --> 00:06:51,400
软件是怎么进行一个集合通信的

172
00:06:51,400 --> 00:06:52,600
可以通信之后

173
00:06:52,600 --> 00:06:54,600
现在就要看一个问题

174
00:06:54,600 --> 00:06:56,000
什么东西

175
00:06:56,000 --> 00:06:58,200
用什么东西可以控制我这些通信

176
00:06:58,600 --> 00:07:00,200
肯定是AI框架

177
00:07:00,400 --> 00:07:02,000
所以在最后一小节里面

178
00:07:02,000 --> 00:07:04,800
将会去介绍分布式训练系统

179
00:07:04,800 --> 00:07:06,200
就是AI框架

180
00:07:06,200 --> 00:07:08,200
怎么去控制这些集群

181
00:07:08,200 --> 00:07:10,000
怎么控制通信原语

182
00:07:10,000 --> 00:07:12,000
软硬件进行一个通信的

183
00:07:12,000 --> 00:07:12,800
那这个时候

184
00:07:12,800 --> 00:07:15,000
就会引入了一个分布式的

185
00:07:15,000 --> 00:07:16,200
并行架构

186
00:07:16,200 --> 00:07:17,600
而这个整个架构

187
00:07:17,600 --> 00:07:19,400
其实在AI框架里面

188
00:07:19,400 --> 00:07:20,800
已经做好了

189
00:07:20,800 --> 00:07:22,200
只需要去调用

190
00:07:22,200 --> 00:07:23,800
只要去用就行了

191
00:07:23,800 --> 00:07:24,600
好吧

192
00:07:24,600 --> 00:07:25,600
时不宜迟

193
00:07:25,600 --> 00:07:28,400
让开展下一个内容

194
00:07:28,400 --> 00:07:29,400
谢谢各位

195
00:07:29,400 --> 00:07:31,800
卷的不行了

196
00:07:31,800 --> 00:07:33,600
记得一键三连加关注哦

197
00:07:33,600 --> 00:07:35,200
所有的内容都会开源在

198
00:07:35,200 --> 00:07:37,200
下面这条链接里面

199
00:07:37,200 --> 00:07:38,400
拜了个拜