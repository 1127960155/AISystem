# 分布式优化器

**分布式优化器**在深度学习领域具有重要地位，不仅能提高并行效率，还能减少显式内存消耗，这两方面的优势为模型的大规模训练带来了显著的好处。值得注意的是，并行效率和内存消耗之间存在着密切关联关系，降低内存占用可以使我们使用更大的并行度，进而提升整体的并行加速效果。在本章中，我们将从**混合精度训练**、**梯度检查点**、**零冗余优化器**等常用技术入手，深入剖析如何降低内存开销并提高训练效率。与此同时，我们还将拓展到最新的分布式优化器，如：低内存优化器（LOMO）等，以呈现分布式优化器发展的前沿动态。

## 混精度训练

在当今大规模模型训练的背景下，混合精度训练已然成为一种备受推崇的普遍做法 [<sup>[1]</sup>](#ref1)。通过采用混合精度训练，我们能够将训练速度显著提升数倍，而又不会对模型的整体性能产生重大影响。在数据科学领域，精度一直是评判的重要考量因素——在传统的科学计算领域，人们通常追求较高的精度，如 FP128 或 FP64 等。然而，在深度学习中，我们所面临的实际上是一个高维函数拟合（或近似）的优化问题，因此并不需要过于精确的数值表示，且使用低精度将会带来显著的计算速度提升：在 NVIDIA A00 SXM 与 NVIDIA H00 SXM 中， FP16 浮点运算能力的理论峰值是 FP32 的近 30 倍。

<center>

| GPU 型号         | NVIDIA H100 SXM 80GB  | NVIDIA A100 SXM 80GB |
| ---------------- | -------------------- | -------------------- |
| FP32             | 67 TFLOPS            | 19.5 TFLOPS          |
| TF32 Tensor Core | 989 TFLOPS           | 312 TFLOPS           |
| FP16 Tensor Core | 1,979 TFLOPS         | 624 TFLOPS           |

</center>

### 常用精度

在深度学习中，常用的精度 [<sup>[2]</sup>](#ref2)包括 **FP32**、**FP16**、**BF16** 和 **TF32**。

![常用精度 FP32 FP16 BF16](images/05.optimizer01.png)
:width:`650px`

**FP32**：这种格式在很长一段时间内都是深度学习的主力，它是 IEEE 754 标准下的单精度浮点数。长期以来，它一直是神经网络计算的标准类型。长期以来，神经网络中的权重、激活和其他值都默认用 FP32 表示。

**FP16**：同样是 IEEE 754 标准下的半精度浮点格式。随着深度学习的发展，FP16 逐渐取代了 FP32 的地位。因为相较于FP32，更低的精度并不会对神经网络的性能产生重大影响。额外的精度不会带来任何好处，反而会更慢、占用更多内存并降低通信速度。FP16 通常用于混合精度训练（TensorFlow/PyTorch）。也用于训练后量化，以加快推理速度（TensorFlow Lite）。其他用于量化的格式还有整数 INT8（8 位整数）、INT4（4 位）甚至 INT1（二进制值）。

**BF16**：谷歌最初开发的另一种 16 位格式被称为 "Brain Floating Point Format"，简称 "bfloat16"。这个名字源于 "谷歌大脑"（Google Brain），谷歌大脑是谷歌的一个人工智能研究小组，这种格式就是在这个小组构思出来的。最开始是被使用在 Google 芯片 TPU 中，后被广泛使用在 GPU 中。由于具有更多的指数位，常被用于处理 FP16 的溢出问题。

![常用精度 TF32](images/05.optimizer02.png)
:width:`650px`

**TF32**：NVIDIA 在 Ampere GPU 后引入的新数学模式，这是一种十分特殊的格式（无需显示设置，而是自动执行），它将 FP32 数据截断为 TF32 进行计算，然后再转换回 FP32。这一创新的最大优势在于编译器只需在最底层（即 CUDA 编译器内部）提供支持。其他代码部分则可以继续使用动态范围相同但精度较高的 FP32，无需进行修改。TF32 的快速插入性使得利用 Tensor Core 的速度成为可能，而无需过多修改现有代码。

```python
# The flag below controls whether to allow TF32 on matmul. This flag defaults to False
# in PyTorch 1.12 and later.
torch.backends.cuda.matmul.allow_tf32 = True

# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
torch.backends.cudnn.allow_tf32 = True
```

TF32 采用与半精度（FP16）相同的 10 位尾数，这满足了人工智能工作负载的精度要求，并且使用了与 FP32 相同的 8 位指数，因此具有相同的数值范围。从技术上讲，它可以视为一种 19 位格式，也可以视为扩展精度的 BF16。TF32 的优势在于其格式与 FP32 相同。当使用 TF32 进行计算时，输入 FP32 操作数的尾数从 23 位舍入到 10 位，然后进行精确乘法运算，最后以正常的 FP32 格式进行累加。TF32 Tensor Core 在 FP32 输入上运行并生成 FP32 结果，而不需要修改代码，而非矩阵操作则继续使用 FP32。相比之下，FP16 和 BF16 等格式需要更多工作，因为它们涉及不同的位布局。尽管如此，也值得使用这些格式，因为它们可以减少内存带宽，从而提升执行速度。

### 混精度训练

在深度学习中，使用半精度（FP16）训练有时会出现下溢出的问题：FP16 的有效的动态范围约为 ${5.96e}^{-8} \sim 65504$，在训练后期，例如激活函数的梯度会非常小，甚至在梯度乘以学习率后，值会更加小。由于 FP16 的精度范围有限，过小的梯度可能导致更新无效——这个时候就需要我们使用混精度训练。混精度训练可以分为两个部分：**半精度** 和 **权重备份**，这里我们拿 FP16 和 FP32 来举例。在训练开始时，我们准备两套模型状态，其中一套为 FP32 类型（优化器状态和模型参数），另一套为 FP16 类型（模型参数），在前向传播、反向传播时，我们都使用 FP16 类型的模型参数进行计算；而在参数更新时，我们将梯度成与学习率 $\eta$ 相乘，更新到 FP32 类型的模型状态上，在新一轮的训练中，我们再次将 FP32 类型的模型拷贝为 FP16 类型的模型。这个过程就是**混精度训练**。由于在计算密集的前向传播、反向传播中，我们使用了半精度进行计算，与单精度相比，训练的速度会大幅度提升。另外，由于激活值在训练过程中占用内存的很大部分，使用 FP16 储存激活值在大批量训练时也会节省内存。同时，在分布式环境下使用 FP16 梯度通信量也会降低。

![混精度训练](images/05.optimizer03.png)
:width:`650px`

为了获得最佳性能，在混精度中我们需要额外选择合适的批量大小。通常建议使用 2 的幂次方作为批量大小，并与输入/输出神经元的数量相匹配，通常为 8 的倍数，但也可能更高，具体取决于所使用的硬件和模型的数据类型。NVIDIA 为全连接层提供了关于输入/输出神经元计数和批量大小选择的建议。根据数据类型和硬件的不同，tensor core 的要求也不尽相同。以 FP16 数据类型为例，通常建议使用 8 的倍数作为批量大小，除非是在 A100 GPU 上，在这种情况下应使用 64 的倍数。这是因为 GPU 对 FP16 数据的并行计算方式决定的。GPU 通常会以 128 位（16个字节）为一组同时处理多个 FP16 数据。为了保证计算效率，张量在内存中的排布需要与 GPU 的计算方式相匹配，即维度需要是 8 的整数倍

### 损失缩放 （Loss Scale）

解决半精度（FP16）下溢问题的另一个方法是损失缩放（Loss Scale）。刚才提到，训练到了后期，梯度（特别是激活函数平滑段的梯度）会特别小，半精度（FP16）表示容易产生下溢现象。为了解决梯度过小的问题，我们需要对损失进行缩放，由于链式法则的存在，损失的缩放也会作用在梯度上。缩放过后的梯度，就会平移到半精度（FP16）有效的展示范围内。不过缩放并非对于所有网络而言都是必须的，而缩放的取值为也会特别大，一般在 8 - 32k 之间。在 Pytorch 中，可以通过这样的方式实现自动损失缩放：

```python
from torch.cuda.amp import GradScaler, autocast
scaler = GradScaler()

with autocast():
    output = model(input)
    loss = loss_fn(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## 梯度检查点（Gradient Checkpointing）

在深度神经网络的训练过程中，为了计算权重的梯度，需要利用前向传播时保留的中间激活值。但是随着网络层数和输入尺寸的增加，存储所有中间激活值会消耗大量内存资源。梯度检查点 [<sup>[3]</sup>](#ref3)通过重新计算部分激活值来减少存储激活值所需的内存开销，从而使大型神经网络模型能够在有限的内存下进行训练。Turing-NLG 17.2B 和 GPT-3 175B 等大型模型都是通过激活检查点训练出来的。

我们来详细分析一下激活值的重要性。以全连接层为例，设该层的输入为 $a$，权重为 $w$，偏置为 $b$，激活函数为 $f$，则前向传播计算为：

$$
y = f(z), z = w * a + b
$$

其中 $y$ 是该层的输出，也是向后传播的前向激活值。假设该层之后的损失函数为 $L$，根据链式法则，我们有:

$$
\frac{\partial L}{\partial w} 
= \frac{\partial L}{\partial y} * \frac{\partial y}{\partial z} * \frac{\partial z}{\partial w}
$$

1) $\frac{\partial L}{\partial y}$ 可以通过后续层传递回来
2) $\frac{\partial y}{\partial z}$ 则是该层激活函数 $f$ 对于输出 $y$ 的导数，可由输出 $y$ 和函数 $f$ 确定，如 $ReLU$ 函数 $\frac{\partial y}{\partial z} = 1(z>0)$  
3) $\frac{\partial z}{\partial w} = a$， 即输入激活值 $a$

可以看到，计算权重梯度的关键之一就是利用了前向激活值 $a$

梯度检查点在前向传播时，不存储所有层的激活值，而是周期性地只存储部分关键层的输入输出。在反向传播计算梯度时，如果需要重新获取被丢弃的激活值，就利用存储的关键层输出，再次执行正向计算来重新计算出所需的激活值。这种方法的优点是能够有效节省内存，使得大型模型可以在相对较小的设备上训练。缺点是它增加了一些额外的计算开销，因为需要重新计算部分激活值。梯度检查点通常与其他内存优化技术结合使用，例如梯度剪裁（Gradient Clipping）、混精度训练等，以充分利用有限的资源来训练大型模型。

在 PyTorch 中实现梯度检查点时,它在前向传播时使用 `torch.no_grad()` 来告诉 PyTorch 不需要计算梯度,因为这些激活值会在反向传播时重新计算。

## 内存消耗估算

在深度学习模型的训练中，合理估算和管理内存消耗是非常重要的。我们的内存存储主要分为两大块：**模型状态（Model States）** 和**剩余状态（Residual States）**。

**模型状态**指和模型本身相关的，必须存储的内容，具体包括：

- 优化器状态（Optimizer States）：Adam 优化算法中的 Momentum 和 Variance

- 梯度（Gradients）：模型梯度 G

- 参数（Parameters）：模型参数 W

**剩余状态**是并非模型必须的，但在训练过程中会额外产生的内容，具体包括：

- 激活值（Activation）：在反向传播过程中使用链式法则计算梯度时会用到。有了它算梯度会更快，但它不是必须存储的，因为可以通过重新前向传播来计算。

- 临时存储（Temporary Buffers）: 例如把梯度发送到某个设备上进行 All-Reduce 时产生的存储。

- 碎片化的存储空间（Unusable Fragment Memory）：虽然总存储空间是够的，但是如果取不到连续的存储空间相关的请求也会失败。对这类空间浪费可以通过内存整理来解决。

拿 FP32 与 FP16 的混合精度训练举例，假设模型的参数量是 $\Phi$，那么模型状态所消耗的空间为：

<center>

| Model States                 | Size (Byte) |
|----------------------------- |------------ |
| FP32 Parameters              | 4 $\Phi$    |
| FP32 Adam Optimizer Momentum | 4 $\Phi$    |
| FP32 Adam Optimizer Variance | 4 $\Phi$    |
| FP16 Gradients               | 2 $\Phi$    |
| FP16 Parameters              | 2 $\Phi$    |
| Total                        | 16 $\Phi$   |

</center>

而由于剩余状态和具体模型架构有关，因此需要具体分析。

接下来我们基于 Transformer 的架构进行具体分析，因为所有参数超过 10 亿的 SOTA 模型都遵循这一架构。我们的分析假设使用 Adam 优化器进行混合精度训练，因为此配方是训练基于 Transformer 的模型的事实标准。

**模型状态**：模型状态由优化器状态、梯度和参数组成。基于 Transformer 的模型中的参数总数主要取决于隐藏维度 （$ℎd$） 和 Transformer 层数（$nl$）。 Transformer 块中的几乎所有参数都来自每个块内的四个线性层，其大小分别为：（$ℎd$, $3ℎd$）、（$ℎd$, $ℎd$）、（$ℎd$, $4ℎd$） 和（$4ℎd$, $ℎd$）。因此，基于 Transformer 的模型中的总参数可以近似为：

$$
12 × nl × hd^2
$$

**剩余状态**：剩余状态主要是指激活内存，它取决于模型结构、批量大小（$𝑏𝑠𝑧$）和序列长度（$𝑠𝑒𝑞$），而且可能相当大。不过激活所需的内存可以通过激活检查点（activation checkpointing）大大减少，我们假设 $𝑐𝑖$ 是两个激活检查点之间的 Transformer 块数，$𝑏𝑠𝑧 × 𝑠𝑒𝑞 × ℎ𝑑$ 是每个 Transformer 块的输入大小，激活检查点所需的内存估计为：

$$
2 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × ℎ𝑑 × 𝑛𝑙 / 𝑐𝑖
$$

**激活工作内存（AWM）**：激活工作内存是反向传播过程中所需的内存，用于在执行实际反向传播之前重新计算激活。是两个连续激活检查点之间的激活量。例如，如果我们为每个 Transformer 块创建一个激活检查点，那么内存就是每个 Transformer 块的总激活量。其字节数约为：

$$
𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑐𝑖 × (16 × ℎ𝑑 + 2 × 𝑎𝑡𝑡𝑛\_ℎ𝑒𝑎𝑑𝑠 × 𝑠𝑒𝑞)
$$

**模型状态工作内存（MSWM）**：模型状态工作内存是指在将所有模型状态卸载到 CPU 或 NVMe 之后，对模型中最大的单个算子执行前向或后向传播所需的 GPU 内存最小量。这大约是由模型中该算子的参数和梯度的大小决定的，因为必须至少有足够的内存来保存向后传播的参数及其梯度。Transformer 的最大的算子是将隐藏状态从 $ℎ$ 转换为 $4ℎ$ 的线性层。该线性层的参数和梯度的大小为：

$$
4 × ℎ𝑑 × 4ℎ𝑑
$$

## 零冗余优化器（Zero Redundancy Optimizer，ZeRO）

在数据并行（DP）中，每个设备都需要保存一份完整的参数（模型状态和剩余状态），而不是所有的参数在训练的整个过程中都会被使用到，而是在特定的阶段中（某个层的前向或反向传播），因此我们可以在不需要使用的时候将它转移到其他地方节省内存空间。ZeRO [<sup>[4]</sup>](#ref4)有两套优化方案：ZeRO-DP，旨在减少模型状态的内存占用。ZeRO-R，旨在减少剩余状态内存消耗。我们将详细阐述这些优化及其背后的启示，这些优化使 ZeRO 能够在保持高效的同时减少内存占用。

### ZeRO-DP

ZeRO-DP 对模型状态进行切分，具体来说，每个设备都只会会存储 $\frac{1}{N_d}$ 的模型状态（其中 $N_d$ 为并行度），在需要时通过集合通讯 All-gather 获取参数。ZeRO-DP 保留了数据并行训练（DP）的高效率，同时实现了模型并行（MP）的内存效率优势。由于数据并行的模型状态在所有数据并行进程中冗余存储，因此内存效率低下，但数据并行具有更高的计算粒度和更低的通信量，从而具有更高的训练效率。模型并行的通信开销很大，因此可扩展性比数据并行低，但 MP 对模型状态进行分区，获得了较高的内存效率。ZeRO-DP 对模型状态进行分区而不是复制它们，并使用动态通信调度最小化通信量。通过这样做，ZeRO-DP 随着数据并行程度的增加线性减少模型在每块设备的内存占用，同时保持通信量接近默认数据并行的通信量，从而保持效率。

![混精度训练](images/05.optimizer04.jpg)
:width:`650px`

ZeRO-DP 有三个主要优化阶段，分别对应于优化器状态、梯度和参数的划分，在累积启用时：

1) **优化状态分区**（Partition optimizer states，$P_{os}$）：又称为 ZeRO-1，将优化器状态按并行度均匀分区，每个进程只需存储 $\frac{1}{N_d}$ 的优化器状态（其中 $N_d$ 为并行度）。这可将内存消耗减少到 1 / 4，且无额外通信开销。

2) **添加梯度分区**（Partition gradients，$P_{os+g}$）：又称为 ZeRO-2，在优化器状态分区的基础上，对梯度也进行分区。每个进程只需存储用于更新自身参数分区所需的梯度。这可减少 8 倍的内存消耗，且无额外通信开销。

3) **添加参数分区**（Partition parameters，$P_{os+g+p}$）：又称为 ZeRO-3，在优化器状态和梯度分区的基础上，对参数也进行分区。每个进程只存储自身的参数分区，在前向反向传播时需要从其他进程收集所需的参数分区。这会使通信量增加约 50%，但可以实现与并行度 $N_d$ 成正比的内存减少。

通过这三个阶段的优化，ZeRO-DP 最终能够在保持数据并行高效的同时，将每个设备的内存消耗降低至 $\frac{1}{N_d}$ 的水平，使得利用少量硬件资源训练万亿参数等超大模型成为可能。

### ZeRO-R

除了优化模型状态（优化器状态、梯度和参数）的内存利用率，ZeRO 还专门针对剩余状态（如激活数据、临时缓冲区和内存碎片等）进行了优化，以进一步减少内存开销。ZeRO-R 对剩余状态进行了切分和优化，主要包括以下几个策略:

1) **分区激活检查点**（Partitioned Activation Checkpointing，$P_{a}$）：解决了模型并行时激活内存冗余的问题。在模型并行中，每个设备需要保存完整的输入激活数据才能计算自己分到的模型部分。ZeRO-R 将激活检查点按模型并行度 $N_m$ 进行分区，每个设备只需存储 $\frac{1}{N_m}$ 的激活检查点。在需要时通过 All-gather 操作重构出完整激活数据，从而按 $N_m$ 的比例减少激活内存。 在极端情况下，当模型规模很大时，ZeRO-R 甚至可以将分区后的激活检查点卸载到 CPU 内存（$P_{a+cpu}$），再次降低设备内存占用，代价是额外的主机-设备通信开销。该策略在大规模模型训练时会自动开启，以保证足够的设备内存用于计算。

2) **恒定大小的缓冲区**（Constant Size Buffer，$C_{b}$）：一些操作如 All-reduce 需要将张量拼成连续的临时缓冲区，使用恒定大小的缓冲区来避免临时缓冲区随着模型大小的增加而爆炸，同时使它们足够大以保持效率。

3) **内存碎片化整理**（Memory Defragmentation，$M_{d}$）：在训练过程中，由于激活检查点、梯度等张量生命周期的差异，会产生大量内存碎片。ZeRO-R 通过预分配和动态管理这些张量的内存，减少了内存碎片和内存分配器的开销，提高了内存利用率。

通过以上优化策略，ZeRO-R 很好地补充和完善了 ZeRO-DP 优化模型状态内存的功能。两者相结合，ZeRO 优化器能最大限度减少大规模模型训练的内存占用，为未来万亿参数级别的深度学习模型铺平了道路。

### ZeRO-Infinity

ZeRO-Infinity [<sup>[5]</sup>](#ref5)是 ZeRO 的扩展功能，可以将深度学习训练扩展到前所未有的规模。具体来说它突破了 GPU 内存壁垒的限制，并使得能够训练具有数万亿个参数的模型成为可能，这是迄今为止最先进系统所无法企及的量级。此外，它为训练具有一千万亿个参数的模型铺平了道路——充分利用系统的全部内存容量，利用 GPU、CPU 和 Non-Volatile Memory Express（NVMe）等所有异构内存组件的能力。

![ZeRO-Infinity](images/05.optimizer05.png)
:width:`650px`

在 ZeRO-Infinity 中，参数从较慢的内存源（如 CPU 和 NVMe）无缝迁移到 GPU，其中它们被合并为完整的层。梯度计算完成后，这些参数被聚合、重新分区，然后重新卸载回较慢的内存组件。其中内存资源的编排确保了最佳利用和最小的开销。这种创新的方法不仅克服了 GPU 内存的常规限制，而且提升了分布式框架的可扩展性。

## ZeRO 通讯分析

无论是零冗余优化，还是卸载到 CPU 和 NVMe 内存，一个关键问题是，它们有限的带宽是否会影响训练效率。我们很自然地会问是否在用通信量来换取内存效率。换句话说，与标准 DP 方法相比，ZeRO 驱动的 DP 方法的通讯量是多少？

### ZeRO-DP

最先进的 All-reduce 实现采用两步法，第一步是 Reduce-scatte 操作，一个是 All-gather 操作，每个流程的总数据移动量为 $\Psi$ 个元素（对于 $\Psi$ 个元素的数据）。因此，标准 DP 在每个训练步骤中会产生 2 $\Psi$ 次数据移动。

通过梯度分区（$P_{os+g}$），每个进程只存储更新相应参数分区所需的梯度部分。因此，ZeRO 只需要对梯度先进行 Reduce-scatte 操作，产生的通信量为 $\Psi$。在每个进程更新完自己负责的参数分区后，会执行一次 All-gather，从所有数据并行进程中收集所有更新的参数。这也会产生 $\Psi$ 的通信量。因此，每个训练步骤的总通信量为 $\Psi$ + $\Psi$ = 2 $\Psi$，与标准 DP 相同。

在参数分区（$P_{os+g+p}$）后，每个数据并行进程只存储其更新的参数。因此，在前向传播过程中，它需要接收所有其他分区的参数。不过，这可以通过流水线操作来避免内存开销——在对模型中与特定分区对应的部分进行前向传播计算之前，负责该分区的数据并行进程可以向所有数据并行进程广播权重。一旦该分区的前向传播计算完成，参数就可以被丢弃。因此，总通信量为 $\Psi × N_d / N_d = \Psi$。我们通过在整个前向传播中通过 All-gather 传播参数已重新获取参数，并在使用完参数后将其丢弃。而在后向传播时，需要以相反的顺序再次进行参数获取。参数的通信为 2 $\Psi$，在参数更新时只需要执行一个 Reduce-scatte 操作，通信量为 $\Psi$，因此总通信量是 3 $\Psi$，是标准 DP 的 1.5 倍。

### ZeRO-R

ZeRO-R 的通信开销取决于模型大小、检查点策略和模型并行（MP）策略。与标准模型并行相比（其中没有对激活进行分区），ZeRO-R $P_{a}$ 的通信开销通常不到标准模型并行的十分之一。

在使用激活检查点的 Megatron-LM 中，每个 Transformer 块在前向传播中执行两次大小为 $batch × seq × length × hidden\_dim$ 的 All-reduce 操作，然后在反向传播中再执行两次。在使用激活检查点的 ZeRO-R 中，每个前向重计算激活之前需要执行一个额外的 All-gather 操作。通常情况下，对于每个 Transformer 块的输入激活进行检查点，因此每个 Transformer 块需要一个 All-gather 操作。因此，ZeRO-R $P_{a}$ 的通信开销为 $seq\_length × hidden\_dim$，仅增加不到 10%。

当 MP 与 DP 一起使用时， ZeRO-R $P_{a}$ 可以将数据并行通信量减少一个数量级，而模型并行通信量只增加 10%，并且当数据并行通信是性能瓶颈时，可以显着提高效率。通过模型并行可以减少数据并行的内存消耗，从而可以成比例地增加批处理大小。对于大型模型，MP 可以增加到 16（DGX-2 节点上的 GPU 数量），从而可以将批处理大小增加多达 16 倍。数据并行训练的通信量与批处理大小成反比，由于 $P_{a}$ 导致批处理大小增加一个数量级，可能会导致数据并行通信量减少一个数量级。

如果应用 $P_{a+cpu}$，则分区激活检查点会被卸载到 CPU，将激活内存需求减少到接近零，但与 $P_{a}$ 相比，往返 CPU 内存的数据移动增加了 2 倍。如果 DP 通信量是主要瓶颈，由于批处理大小较小，$P_{a+cpu}$ 也可以通过增加批处理大小来提高效率，只要 CPU 数据传输开销小于 DP 通信量开销。

### ZeRO-Infinity

我们可以使用**峰值计算吞吐量（$peak_{tp}$）**、**数据移动带宽（$𝑏𝑤$）** 及其**算术强度（$𝑎𝑖𝑡$）** 来估算训练效率。

工作负载的**算术强度（AIT）** 是总计算量与计算所需数据量之间的比率。它描述了每次数据移动所需的计算量。AIT 越高，意味着对数据移动带宽的要求越低，因为每加载一个数据，加速器就能完成更多计算。

$$
𝑎𝑖𝑡 = \frac{𝑡𝑜𝑡𝑎𝑙\_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛}{𝑡𝑜𝑡𝑎𝑙\_𝑑𝑎𝑡𝑎\_𝑚𝑜𝑣𝑒𝑚𝑒𝑛𝑡}
$$

因此我们的效率可以大致估算为：

$$
\begin{aligned}

compute\_time 
&= \frac{𝑡𝑜𝑡𝑎𝑙\_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛}{𝑝𝑒𝑎𝑘_{tp}} \\

𝑐𝑜𝑚𝑚𝑢𝑛𝑖𝑐𝑎𝑡𝑖𝑜𝑛\_𝑡𝑖𝑚𝑒 
&= \frac{𝑡𝑜𝑡𝑎𝑙\_𝑑𝑎𝑡𝑎\_𝑚𝑜𝑣𝑒𝑚𝑒𝑛𝑡}{bw} \\
&= \frac{𝑡𝑜𝑡𝑎𝑙\_𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛}{ait × bw} \\

𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑐𝑦 
&= \frac{𝑐𝑜𝑚𝑝𝑢𝑡𝑒\_𝑡𝑖𝑚𝑒}{𝑐𝑜𝑚𝑝𝑢𝑡𝑒\_𝑡𝑖𝑚𝑒+𝑐𝑜𝑚𝑚𝑢𝑛𝑖𝑐𝑎𝑡𝑖𝑜𝑛\_𝑡𝑖𝑚𝑒} \\
&= \frac{𝑎𝑖𝑡 × 𝑏𝑤}{𝑎𝑖𝑡 × 𝑏𝑤 + 𝑝𝑒𝑎𝑘_{tp}}

\end{aligned}
$$

我们同样以 Transformer 为例：每次迭代的总计算量可以由参数数量、序列长度和批量大小估算，即对于前向传播为 $2 × bsz × seq × 𝑝𝑎𝑟𝑎𝑚𝑠$，反向传播的成本大约是正向传播的两倍。因此我们可以估算计算量：

$$
𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛\_𝑝𝑒𝑟\_𝑖𝑡𝑒𝑟 = 2 × 4 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠 = 2 × 4 × 12 × 𝑏𝑠𝑧 × 𝑠𝑒𝑞 × 𝑛𝑙 × ℎ𝑑^2
$$

在前向和反向传播期间，模型参数必须从源位置加载到 GPU 寄存器至少两次（前向传播期间和实际后向传播期间），导致 2 次的数据移动。在存在激活检查点的情况下，可以在向后传递过程中额外加载一次参数以进行重新计算。此外，梯度必须至少从 GPU 寄存器存储到其最终位置一次。因此，假设参数和梯度存储在相同的最终位置，则前向和后向传递期间的总数据移动将为 $4 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠$，即 $2 × 4 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠$（以字节为单位）。因此参数和梯度的 ait 为：

$$
𝑠𝑒𝑞 × 𝑏𝑠𝑧
$$

在优化器迭代期间，必须至少读取一次优化器状态，​​并且必须至少写入一次优化器状态。因此，总数据移动量为 $2 × 𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟_𝑠𝑡𝑎𝑡𝑒𝑠$，大约为 $2 × 16 × 𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠$ 字节。因此，在完整的训练迭代期间，优化器状态的 ait 为：

$$
𝑠𝑒𝑞 × 𝑏𝑠𝑧/4
$$

在前向传播期间，激活检查点必须保存到其最终位置，并且必须在后向传播期间获取。因此，激活检查点的总数据移动量（以字节为单位）为 $2 × 𝑡𝑜𝑡𝑎𝑙\_𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛\_𝑐ℎ𝑒𝑐𝑘𝑝𝑜𝑖𝑛𝑡𝑠\_𝑖𝑛\_𝑏𝑦𝑡𝑒𝑠$，带入我们之间计算的激活检查点大小，可以得到总数据移动量 $4 × 𝑛𝑙/𝑐𝑖 × ℎ𝑑 × 𝑠𝑒𝑞 × 𝑏𝑠𝑧$。所以激活检查点的 ait 为：

$$
24 × ℎ𝑑 × 𝑐𝑖
$$

模型状态和激活检查点对带宽的要求大不相同。前者只取决于**批量大小和序列长度**，而后者只取决于**激活检查点的频率和模型的隐藏维度大小**。在实际中，参数和梯度的带宽超过 70 GB/s，即使是最小的批处理量，也能实现超过 50% 的效率。在这种带宽下，数据移动理论上可以与计算完全重叠，从而实现 100% 的效率。与参数和梯度相比，优化器状态需要高出近 4 倍的带宽才能达到 50% 的效率。此外，优化器状态在前向和后向传播结束时更新，不能与计算重叠。因此，它们需要更大的带宽来保持整个 DL 工作负载的效率。例如，在每个 GPU 的批处理量为 2 的情况下，要达到 90% 的效率，需要近 1.5 TB/s 的有效带宽，甚至超过了 GPU 内存带宽。启用激活检查点后，即使隐藏大小为 2K，2 GB/s 的微薄带宽也能维持 50% 以上的效率。 当隐藏大小超过 8K 时，带宽需求降至 1 GB/s 以下。

## 低内存优化器（Low-Memory Optimization，LOMO）

LOMO（Low-Memory Optimization）[<sup>[6]</sup>](#ref6)是一种新型优化器，专为资源有限的大型语言模型（LLMs）全参数微调而设计。它通过在一步中融合梯度计算和参数更新，将梯度张量的内存使用降低到 O（1），显著减少了内存使用，使得在普通 GPU 上微调巨大模型成为可能。LOMO 结合了梯度归一化、损失缩放等技术，保证了训练过程的稳定性和效率。此外，LOMO 还支持与其他内存节省技术如激活检查点和混合精度训练相结合，进一步降低内存需求。在 SGD 优化器下，LOMO 训练的内存消耗与推理保持一致。

![LOMO](images/05.optimizer06.png)
:width:`650px`

低内存优化器在 PyTorch 的反向传播中注入定制的钩子函数。定制的钩子函数会扫描所有参数，如果参数的 `.grad` 属性不为空，则更新参数，然后清除并释放 `.grad` 属性，保证了梯度的无储存。LOMO 优化器将梯度计算和参数更新合并为一步，减少了梯度张量的内存使用。这一创新方法使得只需存储一个参数的梯度，大幅降低内存使用。

## 参考文献

<div id="ref1"></div>
[1] Boris Ginsburg, Sergei Nikolaev, Paulius Micikevicius, (2017). TRAINING WITH MIXED PRECISION. Retrieved from https://on-demand.gputechconf.com/gtc/2017/presentation/s7218-training-with-mixed-precision-boris-ginsburg.pdf.

<div id="ref2"></div>
[2] Wickipedia. Half-precision floating-point format. Retrieved from https://en.wikipedia.org/wiki/Half-precision_floating-point_format.

<div id="ref3"></div>
[3] The huggingface Authors. (2024). Methods and tools for efficient training on a single GPU. Retrieved from https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one

<div id="ref4"></div>
[4] Rajbhandari S, Rasley J, Ruwase O, et al. Zero: Memory optimizations toward training trillion parameter models[C]//SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020: 1-16.

<div id="ref5"></div>
[5] Rajbhandari S, Ruwase O, Rasley J, et al. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning[C]//Proceedings of the international conference for high performance computing, networking, storage and analysis. 2021: 1-14.

<div id="ref6"></div>
[6] Lv K, Yang Y, Liu T, et al. Full parameter fine-tuning for large language models with limited resources[J]. arXiv preprint arXiv:2306.09782, 2023.

## 本节视频

<html>
<iframe src="https:&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
