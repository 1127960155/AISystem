> 待更新中，卷的不行了卷得不行了！ZOMI 一个人晚上下班后才能更新视频和文章，如果您有兴趣也非常希望您能够参与进来（Github Issues 区留言或者B站私信ZOMI哦），一起写AI系统，一起分享AI系统的知识。

# 本章内容
到底什么是推理系统？它与推理引擎有什么区别？具体推理的工作流程是怎样的？在实际使用时应该如何去做？这些都是我们需要去学习的，本章将会围绕推理系统与推理引擎这两个内容进行介绍，介绍什么是推理系统，然后聚焦于推理引擎上，了解如何将模型小型化，如何离线优化压缩，最后如何进行部署与运行优化。

## 推理系统的介绍
在这一节中，我们需要了解什么是推理系统与推理引擎，这两个概念有什么区别。然后进一步去了解推理系统与推理引擎的工作流程，了解推理系统具体有哪些工作，了解推理引擎的整体架构；

## 模型小型化
在这一节中，我们将会进一步了解一些小型化的主干网络（backbone）或小型化的SOTA（state of the art）网络模型，随后我们会介绍在CNN结构下的小型化工作，最后我们还会介绍在Transformer结构中的一些小型化工作。这些模型的小型化工作更多是与算法相关，而不是跟系统相关。

## 离线的优化压缩
离线的优化压缩在端侧上的使用可以说是非常热门的或者说基本上是离不开的，在节中将会围绕离线的优化压缩展开，介绍什么是低比特量化，什么是二值化网络，模型的剪枝与蒸馏是什么，这些方法具体是怎么去使用的。

## 推理引擎的在线部署和优化
在本章的最后，我们会围绕推理引擎的在线部署和优化这个话题，去了解图转换优化，了解在线部署中的并发执行与内存分配，了解推理引擎具体是怎么实现的、底层的runtime的优化。

## 典型深度学习推理的应用
**人脸landmark的应用**（人脸landmark、人脸的识别、眼睛的识别、人脸的朝向）
![人脸landmark的应用](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example01.gif?raw=true)
**利用华为HMS core去实现**（检测人脸、检测手势)

![利用华为HMS core实现人脸检测](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example02.gif?raw=true)![利用华为HMS core实现手势检测](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example03.gif?raw=true)

**对话机器人**（可以减少大量的人力咨询问题、ChatGPT）

**推荐系统**（淘宝中的商品的推送，使用的AI推理的算法）

**AIGC**（利用AI去生成内容，改变图片的风格）

上述介绍的应用是深度学习的一些推理应用不涉及模型的训练；

而在推理系统需要考虑以下的问题：

- 如何生成被用户所调用的API的接口；
- 这些数据应该怎么去生成；
- 在网络的影响下，如何低延时的反馈用户的结果；
- 在手机上如何利用多样性的加速器或手机SoC加速的资源；
- 在用户的吞吐量大的情况下，如何保证访问的服务能顺利进行；
- 怎么去做一些冗灾或者扩容；
- 如何在之后上线新的网络模型、怎么去做AB test；

总结：
- 单纯复用原有的 Web 服务器或者移动端应用软件，只能解决其中一部分问题，而在深度学习模型推理的场景下，产生了新的系统设计需求与挑战；
- 从深度学习训练过程和推理过程对比两者的相同点和不同点，以及在生命周期所处的环节，进而便于理解深度学习推理系统所侧重的目标；

## 参考文献

1. [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications](https://arxiv.org/abs/1811.09886)
2. [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
3. [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)
4. [TensorFlow-Serving: Flexible, High-Performance ML Serving](https://arxiv.org/abs/1712.06139)
5. [Optimal Aggregation Policy for Reducing Tail Latency of Web Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
6. [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/pdf/1710.09282.pdf)
7. [CSE 599W: System for ML - Model Serving](http://dlsys.cs.washington.edu/pdf/lecture12.pdf)
8. [https://developer.nvidia.com/deep-learning-performance-training-inference](https://developer.nvidia.com/deep-learning-performance-training-inference)
9. [DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/pdf/1510.00149.pdf)
10. [Learning both Weights and Connections for Efficient Neural Networks](https://pdfs.semanticscholar.org/1ff9/a37d766e3a4f39757f5e1b235a42dacf18ff.pdf)
11. [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](http://on-demand.gputechconf.com/gtcdc/2017/presentation/dc7172-shashank-prasanna-deep-learning-deployment-with-nvidia-tensorrt.pdf)
12. [Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
13. [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://www.usenix.org/system/files/osdi18-chen.pdf)
14. [8-bit Inference with TensorRT](http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
15. [https://github.com/microsoft/AI-System](https://github.com/microsoft/AI-System)




## 本节视频
<html>

<iframe src="https://player.bilibili.com/player.html?bvid=BV1J8411K7pj&as_wide=1&high_quality=1&danmaku=0&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

</html>
