# 引言

在了解完 AI 编译原理之后，我们再展开一个新的内容，这个内容与我们日常生活息息相关，我们的平时使用的购物软件与它有关，看在线视频的软件与它有关，我们玩的游戏与它有关，生活处处都可以看到它的身影，这个内容就是推理系统与推理引擎。

那么到底什么是推理系统、什么是推理引擎？这两个概念有什么区别？具体推理的工作流程是怎样的？在实际使用时应该如何去做？这些都是我们需要去学习的，本章也将会围绕推理系统与推理引擎这两个内容进行展开，介绍什么是推理系统，什么是推理引擎，然后聚焦于推理引擎上，了解如何将模型小型化，如何去做离线优化压缩，最后再看如何去对推理引擎进行部署与运行优化。

## 推理系统的介绍

在介绍推理系统与推理引擎之前，我们需要明白推理是什么，推理其实就是我们在使用大批量数据训练好模型结构和参数之后，再使用小批量的数据进行一次前向传播得到模型输出的过程，在这个过程中不涉及模型梯度和损失优化。推理的最终目标是将训练好的模型部署到生产环境中，让 AI 能够运行起来。

其中，推理系统是用于部署人工智能模型，执行推理预测任务的人工智能系统，类似传统 Web 服务或移动端应用系统的作用。推理系统首先会加载模型到内存，同时会对模型进行一定的版本管理，支持新版本上线和旧版本回滚，对输入数据进行批尺寸（Batch Size）动态优化，并提供服务接口（例如，HTTP，gRPC 等），供客户端调用。用户不断向推理服务系统发起请求并接受响应。除了被用户直接访问，推理系统也可以作为一个微服务，被数据中心中其他微服务所调用，完成整个请求处理中一个环节的功能与职责。

而推理引擎在推理系统当中，它主要负责 AI 模型的加载与执行，可分为调度与执行两层，它聚焦于 Runtime 执行部分和 Kernel 算子内核层，为不同的硬件提供更加高效、快捷的执行引擎。它可以看作是一个基础软件，提供了一组 API 用于在特定加速器平台（如 CPU、GPU 和 TPU）上进行推理任务。常见的推理引擎有：字节跳动的开源序列推理引擎 LightSeq、Meta AI 的 AITemplate、英伟达自家推理引擎 TensorRT、还有华为的 MindSpore Lite，腾讯的 NCNN 等等。

在这一节中，我们需要了解什么是推理系统与推理引擎，这两个概念有什么区别。然后进一步去了解推理系统与推理引擎的工作流程，了解推理系统具体有哪些工作，了解推理引擎的整体架构；

### 模型小型化

在端侧推理引擎中，主要是执行轻量的模型结构，而这就涉及到本节的重点——模型小型化、轻量化。模型小型化的主要思想是针对神经网络模型设计出更高效的网络计算方式，从而使神经网络模型的参数量减少的同时，不损失网络精度，并进一步提高模型的执行效率。

在这一节中，我们将会集中介绍模型小型化中需要注意的参数和指标，然后进一步了解一些小型化的主干网络（backbone）或小型化的 SOTA（state of the art）网络模型，深入了解在 CNN 结构下的小型化工作，最后我们还会介绍在 Transformer 结构中的一些小型化工作。

### 离线优化压缩

推理系统类似传统的 Web 服务，需要响应用户请求，并保证一定的服务等级协议（例如，响应时间低于 100ms），因此也需要通过一定的策略和优化。 离线的优化压缩在端侧上的使用可以说是非常热门的或者说基本上是离不开的，它与轻量化网络模型不同，压缩主要是对轻量化或者非轻量化模型执行剪枝、蒸馏、量化等压缩算法和手段，使得模型更加小、更加轻便、更加利于执行。

因此在本节中将会围绕离线的优化压缩展开，介绍什么是低比特量化，什么是二值化网络，模型的剪枝与蒸馏是什么，以及这些方法具体是怎么去使用的。

### 推理引擎在线部署和优化

推理系统进行模型部署时，需要应对多样的框架，多样的部署硬件，以及持续集成和持续部署的模型上线发布等诸多的软件工程问题。

而在本章的最后，我们会围绕推理引擎的在线部署和优化这个话题，去了解图转换优化，了解在线部署中的并发执行与内存分配，了解推理引擎具体是怎么实现的、底层的 runtime 的优化。

## 推理应用

**人脸 landmark 的应用**（人脸 landmark、人脸的识别、眼睛的识别、人脸的朝向）
![人脸 landmark 的应用](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example01.gif?raw=true)

**利用华为 HMS core 去实现检测人脸与检测手势**

![利用华为 HMS core 实现人脸检测](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example02.gif?raw=true)![利用华为 HMS core 实现手势检测](https://github.com/chenzomi12/DeepLearningSystem/blob/main/04Inference/01Inference/images/example03.gif?raw=true)

上面介绍的应用是关于深度学习的一些推理应用不涉及模型的训练；

而在维护推理系统中需要考虑以下的问题：
- 如何生成被用户所调用的 API 的接口；
- 这些数据应该怎么去生成；
- 在网络的影响下，如何低延时的反馈用户的结果；
- 在手机上如何利用多样性的加速器或手机 SoC 加速的资源；
- 在用户的吞吐量大的情况下，如何保证访问的服务能顺利进行；
- 怎么去做一些冗灾或者扩容；
- 如何在之后上线新的网络模型、怎么去做 AB test；

## 小结

## 本节视频
<html>
<iframe src="https://player.bilibili.com/player.html?bvid=BV1J8411K7pj&as_wide=1&high_quality=1&danmaku=0&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

## 参考文献

1. [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications](https://arxiv.org/abs/1811.09886)
2. [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
3. [TFX: A TensorFlow-Based Production-Scale Machine Learning Platform](https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)
4. [TensorFlow-Serving: Flexible, High-Performance ML Serving](https://arxiv.org/abs/1712.06139)
5. [Optimal Aggregation Policy for Reducing Tail Latency of Web Search](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
6. [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/pdf/1710.09282.pdf)
7. [CSE 599W: System for ML - Model Serving](http://dlsys.cs.washington.edu/pdf/lecture12.pdf)
8. [https://developer.nvidia.com/deep-learning-performance-training-inference](https://developer.nvidia.com/deep-learning-performance-training-inference)
9. [DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING](https://arxiv.org/pdf/1510.00149.pdf)
10. [Learning both Weights and Connections for Efficient Neural Networks](https://pdfs.semanticscholar.org/1ff9/a37d766e3a4f39757f5e1b235a42dacf18ff.pdf)
11. [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](http://on-demand.gputechconf.com/gtcdc/2017/presentation/dc7172-shashank-prasanna-deep-learning-deployment-with-nvidia-tensorrt.pdf)
12. [Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
13. [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://www.usenix.org/system/files/osdi18-chen.pdf)
14. [8-bit Inference with TensorRT](http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
15. [https://github.com/microsoft/AI-System](https://github.com/microsoft/AI-System)
16. [推理系统&引擎](https://chenzomi12.github.io/040Inference/README.html) 
17. [NCNN、OpenVino、 TensorRT、MediaPipe、ONNX，各种推理部署架构，到底哪家强？](https://zhuanlan.zhihu.com/p/423551635)
18. [【AI System】第 8 章：深度学习推理系统](https://zhuanlan.zhihu.com/p/665146747)
19. [【AI】推理系统和推理引擎的整体架构](https://blog.csdn.net/weixin_45651194/article/details/132872588)