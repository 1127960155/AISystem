1
00:00:00,000 --> 00:00:05,750
字幕生成：mkwei 字幕校对：qiaokai

2
00:00:05,750 --> 00:00:12,000
Hello 大家好,我是ZOMI,我们来到了ChartGPT狂飙原理铺锡的第三个内容

3
00:00:12,000 --> 00:00:18,000
那在第三个内容里面呢我们会深入的去看看InstructGPT它到底有什么不一样

4
00:00:18,000 --> 00:00:24,000
它到底是怎么的把之前的PPO还有强化学习还有我们之前的GPT融合在一起

5
00:00:24,000 --> 00:00:27,000
那这一节呢才是真正的内容也是非常深入的内容

6
00:00:27,000 --> 00:00:33,000
如果对前面强化学习和GPT不太了解的同学呢也可以翻看之前的相关的视频

7
00:00:33,000 --> 00:00:38,000
那今天呢我们首先简单的去看一看论文的一个简单的原理

8
00:00:38,000 --> 00:00:41,000
因为其实论文它不是讲的非常非常的深入

9
00:00:41,000 --> 00:00:44,000
所以我们会更加深入的去看看它具体的原理

10
00:00:44,000 --> 00:00:47,000
因为ChartGPT呢它并没有开放论文

11
00:00:47,000 --> 00:00:50,000
但是呢它说明了它的前身呢是去InstructGPT

12
00:00:50,000 --> 00:00:56,000
而InstructGPT呢是基于Reinforcement Learning Human Feedback微调的GPT-3

13
00:00:56,000 --> 00:00:59,000
这里面就很有意思了分开两个技术点

14
00:00:59,000 --> 00:01:03,000
那InstructGPT它整体的训练流程呢就是下面这个图

15
00:01:03,000 --> 00:01:07,000
它整篇文章呢基本上只有两个公式一个图

16
00:01:07,000 --> 00:01:09,000
然后呢有68页这么多

17
00:01:09,000 --> 00:01:12,000
感兴趣的同学呢也可以看一下这篇论文

18
00:01:12,000 --> 00:01:15,000
那这里面呢就结合了监督学习和强化学习

19
00:01:15,000 --> 00:01:20,000
首先呢是用监督学习去给GPT-3进行一个微调

20
00:01:20,000 --> 00:01:24,000
对应的就是下面这个图左边最左边这个位置

21
00:01:24,000 --> 00:01:29,000
然后用强化学习PPO算法呢来更新微调过后的GPT-3的参数

22
00:01:29,000 --> 00:01:31,000
那这个模型呢叫做SFT

23
00:01:32,000 --> 00:01:37,000
但是呢有一个很大的问题就是这三个模型到底是怎么配合的

24
00:01:37,000 --> 00:01:40,000
里面又有个RM模型有个SFT模型有个PPO模型

25
00:01:40,000 --> 00:01:43,000
这里面也有个RM模型这两个是同一个模型嘛

26
00:01:43,000 --> 00:01:46,000
这两个模型之间又有什么关系呢

27
00:01:46,000 --> 00:01:48,000
我们往下继续打开

28
00:01:48,000 --> 00:01:51,000
首先呢我们分开三个阶段

29
00:01:52,000 --> 00:01:56,000
第一个阶段呢就是利用人类标注的一个数据

30
00:01:56,000 --> 00:01:59,000
对GPT进行监督的训练

31
00:01:59,000 --> 00:02:04,000
那我们现在看一看右边的这个图里面呢就分开三个步骤

32
00:02:04,000 --> 00:02:07,000
第一步呢就是先设计好一个Prompt的DataSet

33
00:02:07,000 --> 00:02:10,000
里面呢就有大量提示的样本

34
00:02:10,000 --> 00:02:13,000
给出各种各样任务的描述

35
00:02:13,000 --> 00:02:18,000
例如对一个六岁的小孩解析月亮登陆或者登陆月球

36
00:02:18,000 --> 00:02:23,000
第二步呢就是标注的团队会这个Prompt的DataSet进行一个标注

37
00:02:23,000 --> 00:02:25,000
也就是人工的回答

38
00:02:25,000 --> 00:02:28,000
那第三个呢就是用标注好的数据

39
00:02:28,000 --> 00:02:30,000
上面标注好的Prompt的数据

40
00:02:30,000 --> 00:02:34,000
微调我们的GPT3这个网络模型

41
00:02:34,000 --> 00:02:37,000
微调后的网络模型呢我们叫做SFT

42
00:02:37,000 --> 00:02:40,000
SFT呢其实就是supervision fine-tuning

43
00:02:40,000 --> 00:02:42,000
有监督的微调

44
00:02:42,000 --> 00:02:47,000
让我们这个SFT的模型具备最基本的文本生成的能力

45
00:02:47,000 --> 00:02:50,000
接下来我们看看第二个阶段

46
00:02:50,000 --> 00:02:55,000
第二个阶段呢就是通过Reinforcement Human Feedback的这个思路呢

47
00:02:55,000 --> 00:02:58,000
来去奖励来去训练我们的Reward Model

48
00:02:58,000 --> 00:03:02,000
现在呢我们还是看一看右边的这个图

49
00:03:02,000 --> 00:03:05,000
右边这个图呢分开三个步骤

50
00:03:05,000 --> 00:03:09,000
首先第一步呢就是用刚才训练好的SFT网络模型呢

51
00:03:09,000 --> 00:03:12,000
去回答这里面提出的prompt的DataSet的问题

52
00:03:12,000 --> 00:03:17,000
然后呢收集四个输出1234ABCD四个输出

53
00:03:17,000 --> 00:03:22,000
接着呢员工呢对刚才SFT模型生成的四个回答呢

54
00:03:22,000 --> 00:03:25,000
进行好坏的判断并进行一个标注

55
00:03:25,000 --> 00:03:29,000
还有排列例如D的回答是最正确的CAB

56
00:03:29,000 --> 00:03:31,000
其实都没有D的回答好

57
00:03:31,000 --> 00:03:37,000
然后呢把这个排序的结果用来训练后面最后的奖励模型

58
00:03:37,000 --> 00:03:39,000
然后去学习排序的结果

59
00:03:39,000 --> 00:03:41,000
从而理解人类的偏好

60
00:03:41,000 --> 00:03:44,000
那这个呢就是RLHF里面呢

61
00:03:44,000 --> 00:03:46,000
之前讲到引入了人类的反馈

62
00:03:46,000 --> 00:03:50,000
然后去训练我们的网络模型去训练我们的Agent

63
00:03:50,000 --> 00:03:54,000
这个RL呢Reward Model就是我们对应的Agent呢

64
00:03:54,000 --> 00:03:58,000
而上面这个数据集呢就是我们对应的Environment

65
00:03:58,000 --> 00:04:01,000
而我们引入了人类的一个反馈一个评价

66
00:04:01,000 --> 00:04:04,000
这就是RLHF上一节里面讲过

67
00:04:04,000 --> 00:04:07,000
这里面呢用真实例子给大家再串一串

68
00:04:09,000 --> 00:04:11,000
接下来来到了第三个阶段

69
00:04:11,000 --> 00:04:13,000
第三个阶段呢是文中比较含糊的

70
00:04:13,000 --> 00:04:15,000
或者没讲太明白

71
00:04:15,000 --> 00:04:17,000
或者很多人没理解的太明白的

72
00:04:17,000 --> 00:04:19,000
但是呢也是非常重要的一个

73
00:04:19,000 --> 00:04:23,000
首先呢主要是通过训练好的Reward Model来去预测结果

74
00:04:23,000 --> 00:04:27,000
然后呢通过PPO算法来优化SFT的网络模型的策略

75
00:04:27,000 --> 00:04:29,000
所以这里面呢就出现了两个模型

76
00:04:29,000 --> 00:04:33,000
一个是PPO一个是FT来更新策略

77
00:04:33,000 --> 00:04:35,000
我们之前讲强化学习那一章节里面呢

78
00:04:35,000 --> 00:04:38,000
去讲了策略是怎么更新的如何更新

79
00:04:38,000 --> 00:04:40,000
那这里面呢还有一个V2model呢

80
00:04:40,000 --> 00:04:42,000
作为一个Critics

81
00:04:42,000 --> 00:04:43,000
因为我们之前讲了

82
00:04:43,000 --> 00:04:45,000
这两个呢是属于actor

83
00:04:45,000 --> 00:04:47,000
这个RM呢是属于Critics

84
00:04:47,000 --> 00:04:50,000
所以我们有一个C网络有个A网络

85
00:04:50,000 --> 00:04:52,000
A网络呢还有一个A一撇网络呢

86
00:04:52,000 --> 00:04:54,000
用来同步的更新我们的策略

87
00:04:54,000 --> 00:04:56,000
让我们的策略呢收敛的更快

88
00:04:56,000 --> 00:04:58,000
而且没有那么抖动

89
00:04:58,000 --> 00:04:59,000
而且差异没那么大

90
00:04:59,000 --> 00:05:02,000
下面我们回到右边的这个图来看一看

91
00:05:02,000 --> 00:05:06,000
首先第一步呢是我们先准备好prompt的问题

92
00:05:06,000 --> 00:05:09,000
然后呢让SFT就是我们的PPO模型呢

93
00:05:09,000 --> 00:05:10,000
去回答这个问题

94
00:05:10,000 --> 00:05:12,000
然后得到一个策略的输出

95
00:05:12,000 --> 00:05:14,000
得到这个策略的输出呢

96
00:05:14,000 --> 00:05:17,000
首先不要人工的去评价好坏

97
00:05:17,000 --> 00:05:19,000
而是要阶段2的Reward Model

98
00:05:19,000 --> 00:05:21,000
奖励函数或者奖励模型呢

99
00:05:21,000 --> 00:05:24,000
去给SFT呢生成的一个回答

100
00:05:24,000 --> 00:05:27,000
或者一个文本呢进行一个评价的打分

101
00:05:27,000 --> 00:05:29,000
就是这个art reward k 

102
00:05:29,000 --> 00:05:32,000
然后呢使用PPO的这个算法流程呢

103
00:05:32,000 --> 00:05:36,000
对SFT的这个网络模型进行反馈的更新

104
00:05:38,000 --> 00:05:39,000
哎基本上到现在为止呢

105
00:05:39,000 --> 00:05:42,000
我们已经简单的讲完Instruct GPT

106
00:05:42,000 --> 00:05:46,000
里面这篇文章所涉及到的一些模块了

107
00:05:46,000 --> 00:05:48,000
step1 step2 step3

108
00:05:48,000 --> 00:05:52,000
首先呢去训练一个SFT的一个gpt网络模型

109
00:05:52,000 --> 00:05:55,000
接着呢去用gpt这个网络模型呢

110
00:05:55,000 --> 00:05:58,000
生成相关的一些数据文本

111
00:05:58,000 --> 00:06:00,000
然后呢给Reward Model去学习

112
00:06:00,000 --> 00:06:02,000
引入了人类的反馈

113
00:06:02,000 --> 00:06:05,000
接着呢在第三步利用了PPO的算法

114
00:06:05,000 --> 00:06:08,000
去对我们整个过程进行学习

115
00:06:08,000 --> 00:06:10,000
但是可能这么讲了

116
00:06:10,000 --> 00:06:12,000
还是没讲太明白

117
00:06:12,000 --> 00:06:14,000
于是呢我们引入最后一个内容

118
00:06:14,000 --> 00:06:17,000
Instruct GPT的一个原理的深度剖析

119
00:06:17,000 --> 00:06:19,000
那后面的章节呢不会太多

120
00:06:19,000 --> 00:06:20,000
只有两页PPT

121
00:06:20,000 --> 00:06:22,000
我们再往下看一看

122
00:06:22,000 --> 00:06:24,000
实际上呢在阶段2啊

123
00:06:24,000 --> 00:06:26,000
我们主要是训练Reward Model嘛

124
00:06:26,000 --> 00:06:28,000
就是我们的奖励函数

125
00:06:28,000 --> 00:06:29,000
那Reward Model的核心呢

126
00:06:29,000 --> 00:06:33,000
就是人类对SFT的网络模型生成的

127
00:06:33,000 --> 00:06:36,000
多个答案进行排序

128
00:06:36,000 --> 00:06:38,000
那这里面的答案呢

129
00:06:38,000 --> 00:06:41,000
就是通过SFT网络模型进行生成的

130
00:06:41,000 --> 00:06:43,000
然后再来训练Reward Model

131
00:06:43,000 --> 00:06:45,000
整套流程是这样的

132
00:06:45,000 --> 00:06:47,000
那这里面呢我们可以看到

133
00:06:47,000 --> 00:06:49,000
我们假设现在已经有四个语句

134
00:06:49,000 --> 00:06:51,000
两两组和一组

135
00:06:51,000 --> 00:06:52,000
两两组一组

136
00:06:52,000 --> 00:06:53,000
A跟B进行组合

137
00:06:53,000 --> 00:06:54,000
A跟C进行组合

138
00:06:54,000 --> 00:06:56,000
A跟D进行组合

139
00:06:56,000 --> 00:06:57,000
C跟D组合

140
00:06:57,000 --> 00:06:58,000
B跟D组合

141
00:06:58,000 --> 00:07:01,000
于是呢就有C42个组合

142
00:07:01,000 --> 00:07:02,000
这里面的K呢

143
00:07:02,000 --> 00:07:06,000
就是对应于我们SFT生成的答案

144
00:07:06,000 --> 00:07:08,000
于是呢就变成CK2这种方式

145
00:07:08,000 --> 00:07:11,000
一共呢就有CK2个损失函数

146
00:07:11,000 --> 00:07:12,000
所以我们可以看到呢

147
00:07:12,000 --> 00:07:13,000
有了这个之后呢

148
00:07:13,000 --> 00:07:16,000
我们就可以通过Reward Model

149
00:07:16,000 --> 00:07:18,000
去算我们的损失函数

150
00:07:18,000 --> 00:07:20,000
两两进行组合

151
00:07:20,000 --> 00:07:23,000
最后呢再去加权平均值

152
00:07:23,000 --> 00:07:25,000
公式里面的aceta呢

153
00:07:25,000 --> 00:07:27,000
aceta呢就是我们的Reward Model

154
00:07:27,000 --> 00:07:29,000
最硬的就是im模型

155
00:07:29,000 --> 00:07:32,000
im模型呢进行更新的策略

156
00:07:32,000 --> 00:07:33,000
x呢就是我们的prompt

157
00:07:33,000 --> 00:07:36,000
y呢就是我们的SFT预测的结果

158
00:07:36,000 --> 00:07:40,000
从而去求出我们整个的损失函数

159
00:07:40,000 --> 00:07:42,000
接下来我们看一下

160
00:07:42,000 --> 00:07:44,000
第二个阶段的一个整体的图

161
00:07:44,000 --> 00:07:47,000
阶段二呢主要是训练Reward Model嘛

162
00:07:47,000 --> 00:07:48,000
那Reward Model呢

163
00:07:48,000 --> 00:07:49,000
主要是学会了

164
00:07:49,000 --> 00:07:51,000
给刚才D的这一类语句呢

165
00:07:51,000 --> 00:07:52,000
进行打分

166
00:07:52,000 --> 00:07:53,000
给ABC这类语句呢

167
00:07:53,000 --> 00:07:54,000
打低分

168
00:07:54,000 --> 00:07:56,000
从而模仿人类的偏好

169
00:07:56,000 --> 00:07:57,000
我们就引入了reinforcement

170
00:07:57,000 --> 00:07:58,000
human feedback

171
00:07:58,000 --> 00:08:00,000
这种方式由人类来充当reward

172
00:08:00,000 --> 00:08:02,000
人类来充当反馈的机制

173
00:08:02,000 --> 00:08:03,000
充当我们的环境

174
00:08:03,000 --> 00:08:04,000
下面这个图呢

175
00:08:04,000 --> 00:08:05,000
我们的首先的输呢

176
00:08:05,000 --> 00:08:06,000
就是pump的DataSet

177
00:08:06,000 --> 00:08:09,000
输到给我们的一个SFT的网络模型

178
00:08:09,000 --> 00:08:11,000
SFT的网络模型呢

179
00:08:11,000 --> 00:08:13,000
就会生成几个答案

180
00:08:13,000 --> 00:08:15,000
然后呢人类就进行打分

181
00:08:15,000 --> 00:08:16,000
打完分之后呢

182
00:08:16,000 --> 00:08:18,000
就把这些相关的数据呢

183
00:08:18,000 --> 00:08:21,000
去给Reward Model进行学习

184
00:08:21,000 --> 00:08:23,000
整体流程呢就是这么简单

185
00:08:23,000 --> 00:08:24,000
其实并不难

186
00:08:24,000 --> 00:08:26,000
但是呢大家要注意一下

187
00:08:26,000 --> 00:08:28,000
这里面呢就已经涉及到了

188
00:08:28,000 --> 00:08:30,000
整个强化学习的概念呢

189
00:08:30,000 --> 00:08:33,000
虽然呢它只是一个简单的概念

190
00:08:33,000 --> 00:08:35,000
但是呢我们接下来将会引入了

191
00:08:35,000 --> 00:08:37,000
更大的一个强化学习的内容

192
00:08:37,000 --> 00:08:38,000
就是ppo算法

193
00:08:38,000 --> 00:08:41,000
那我们现在马上就进入到了

194
00:08:41,000 --> 00:08:43,000
阶段三使用ppo算法呢

195
00:08:43,000 --> 00:08:46,000
去优化策略模型policy

196
00:08:46,000 --> 00:08:48,000
规点policy规点model

197
00:08:48,000 --> 00:08:49,000
这个内容

198
00:08:49,000 --> 00:08:51,000
我们可以看到在文中呢

199
00:08:51,000 --> 00:08:52,000
这条公式呢

200
00:08:52,000 --> 00:08:54,000
就是文中出现为数不多

201
00:08:54,000 --> 00:08:56,000
两套公式的其中第一条

202
00:08:56,000 --> 00:08:58,000
这个就是我们训练的目标函数

203
00:08:58,000 --> 00:08:59,000
一呢就是expectation

204
00:08:59,000 --> 00:09:01,000
我们的一个目标

205
00:09:01,000 --> 00:09:02,000
我们可以看到呢

206
00:09:02,000 --> 00:09:04,000
Rθ就是我们刚才讲的

207
00:09:04,000 --> 00:09:07,000
它是一个对应的RM的模型

208
00:09:07,000 --> 00:09:08,000
而后面的这个呢

209
00:09:08,000 --> 00:09:10,000
就是求角KL散度

210
00:09:10,000 --> 00:09:12,000
KL散度的简单的一个公式

211
00:09:12,000 --> 00:09:13,000
去对比RL模型

212
00:09:13,000 --> 00:09:19,000
还有SFT两个模型之间的一个差异

213
00:09:19,000 --> 00:09:20,000
这两个模型呢

214
00:09:20,000 --> 00:09:24,000
其实都是GPT-3的一个大规模的模型

215
00:09:24,000 --> 00:09:26,000
就呢就给出了一个偏正的修正项

216
00:09:26,000 --> 00:09:29,000
为了就是约束前面的一个值

217
00:09:29,000 --> 00:09:31,000
那下面呢我们看一看

218
00:09:31,000 --> 00:09:32,000
整体的流程

219
00:09:32,000 --> 00:09:34,000
整体呢它是一个因为强化学习的嘛

220
00:09:34,000 --> 00:09:36,000
它是一个迭代式的奖励的过程

221
00:09:36,000 --> 00:09:38,000
迭代式的通过我们的奖励模型

222
00:09:38,000 --> 00:09:40,000
还有策略SFT的模型

223
00:09:40,000 --> 00:09:41,000
让我们的奖励模型RM呢

224
00:09:41,000 --> 00:09:43,000
对PPO的模型输出的质量

225
00:09:43,000 --> 00:09:46,000
进行一个精准的刻画和修正引导

226
00:09:46,000 --> 00:09:48,000
使得我们输出的文本呢

227
00:09:48,000 --> 00:09:51,000
越来越符合人类的认知

228
00:09:51,000 --> 00:09:53,000
那这整个呢就是强化学习的

229
00:09:53,000 --> 00:09:54,000
学习的方式

230
00:09:54,000 --> 00:09:55,000
现在我们来看一下

231
00:09:55,000 --> 00:09:57,000
首先输入的是一个Prompt的DataSet

232
00:09:57,000 --> 00:09:59,000
这里面呢就有两个分支

233
00:09:59,000 --> 00:10:00,000
第一个分支呢

234
00:10:00,000 --> 00:10:02,000
就去把我们的网络模型处理化的

235
00:10:02,000 --> 00:10:04,000
就是SFT的网络模型

236
00:10:04,000 --> 00:10:05,000
进行一个计算

237
00:10:05,000 --> 00:10:07,000
得到我们的一个basic

238
00:10:07,000 --> 00:10:09,000
就是我们的basic light

239
00:10:09,000 --> 00:10:10,000
然后呢我们接下来

240
00:10:10,000 --> 00:10:12,000
用一个强化学习

241
00:10:12,000 --> 00:10:14,000
我们的一个RL model policy

242
00:10:14,000 --> 00:10:16,000
然后去生成另外一个内容

243
00:10:16,000 --> 00:10:18,000
就是我们的PPO模型

244
00:10:18,000 --> 00:10:21,000
接着呢去计算我们的KL散度

245
00:10:21,000 --> 00:10:22,000
送完KL散度之后呢

246
00:10:22,000 --> 00:10:24,000
Rθ就是给我们的Reward Model

247
00:10:24,000 --> 00:10:25,000
然后去计算

248
00:10:25,000 --> 00:10:27,000
给完Reward Model计算呢

249
00:10:27,000 --> 00:10:28,000
就返回回来

250
00:10:28,000 --> 00:10:30,000
去更新我们的θ

251
00:10:30,000 --> 00:10:32,000
这里面θ的更新公式呢

252
00:10:32,000 --> 00:10:34,000
就跟传统的深度学习的更新公式

253
00:10:34,000 --> 00:10:35,000
差不多

254
00:10:35,000 --> 00:10:37,000
去更新我们这里面的

255
00:10:37,000 --> 00:10:39,000
enforcement learning里面的policy

256
00:10:39,000 --> 00:10:41,000
就是PPO的网络模型

257
00:10:41,000 --> 00:10:43,000
通过这种方式呢

258
00:10:43,000 --> 00:10:46,000
不断的去更新PPO的网络模型

259
00:10:46,000 --> 00:10:47,000
而这个呢

260
00:10:47,000 --> 00:10:49,000
其实就是我们一开始训练好的

261
00:10:49,000 --> 00:10:50,000
SFT的网络模型

262
00:10:50,000 --> 00:10:53,000
那最后用的就是我们的PPO的模型

263
00:10:53,000 --> 00:10:55,000
通过PPO的模型呢

264
00:10:55,000 --> 00:10:57,000
使得他更加的智能

265
00:10:57,000 --> 00:10:59,000
更加的引入了员工的反馈

266
00:10:59,000 --> 00:11:02,000
从而完成了整体的学习

267
00:11:02,000 --> 00:11:03,000
那现在呢

268
00:11:03,000 --> 00:11:05,000
基本上我们已经讲完

269
00:11:05,000 --> 00:11:08,000
整个Instruct GPT的一个基本的原理

270
00:11:08,000 --> 00:11:10,000
下面呢我想引起一个思考

271
00:11:10,000 --> 00:11:11,000
提出一些疑问

272
00:11:11,000 --> 00:11:13,000
也希望大家能够参与进来

273
00:11:13,000 --> 00:11:14,000
首先呢

274
00:11:14,000 --> 00:11:16,000
这里面的问题分开三个

275
00:11:16,000 --> 00:11:17,000
第一个呢

276
00:11:17,000 --> 00:11:18,000
就是大模型

277
00:11:18,000 --> 00:11:20,000
其实已经通过了计算的方式

278
00:11:20,000 --> 00:11:21,000
去模拟人类的思考

279
00:11:21,000 --> 00:11:23,000
类似于ChatGPT这样的

280
00:11:23,000 --> 00:11:25,000
reinforcement human feedback的技术

281
00:11:25,000 --> 00:11:27,000
能否给整个世界带来新的

282
00:11:27,000 --> 00:11:29,000
技术的产业革命呢

283
00:11:29,000 --> 00:11:31,000
大家去搜b站或者youtube上面

284
00:11:31,000 --> 00:11:33,000
ChatGPT的东西呢

285
00:11:33,000 --> 00:11:35,000
确实已经有非常多的博主

286
00:11:35,000 --> 00:11:37,000
包括已经不是搞技术的博主呢

287
00:11:37,000 --> 00:11:39,000
都在研究这相关的技术

288
00:11:39,000 --> 00:11:40,000
那第二个呢

289
00:11:40,000 --> 00:11:42,000
就是我比较关心的

290
00:11:42,000 --> 00:11:43,000
像ChatGPT呢

291
00:11:43,000 --> 00:11:45,000
它使用了为这个框架

292
00:11:45,000 --> 00:11:47,000
作为细粒度的并行的计算

293
00:11:47,000 --> 00:11:48,000
还有异构的计算

294
00:11:48,000 --> 00:11:50,000
很好的去管理了分配强化学习

295
00:11:50,000 --> 00:11:52,000
深度学习的任务

296
00:11:52,000 --> 00:11:54,000
而且方便的利用强化学习

297
00:11:54,000 --> 00:11:57,000
对环境和计算任务进行控制

298
00:11:57,000 --> 00:11:58,000
那这个时候呢

299
00:11:58,000 --> 00:11:59,000
就对整个AI框架的

300
00:11:59,000 --> 00:12:01,000
分布式的能力的边界

301
00:12:01,000 --> 00:12:04,000
带来哪些新的挑战和新的冲击呢

302
00:12:04,000 --> 00:12:06,000
因为我们需要对环境

303
00:12:06,000 --> 00:12:07,000
还有对我们的任务

304
00:12:07,000 --> 00:12:09,000
对我们的深度学习的模型

305
00:12:09,000 --> 00:12:11,000
不断的进行交互

306
00:12:11,000 --> 00:12:12,000
那这个时候呢

307
00:12:12,000 --> 00:12:15,000
我们的分布式的能力的边界

308
00:12:15,000 --> 00:12:17,000
到底会怎么样的进一步拓充

309
00:12:17,000 --> 00:12:18,000
这也是希望大家

310
00:12:18,000 --> 00:12:20,000
能够进一步的思考的

311
00:12:20,000 --> 00:12:21,000
也是我之前分享的

312
00:12:21,000 --> 00:12:22,000
很多相关的视频

313
00:12:22,000 --> 00:12:24,000
AI系统里面

314
00:12:24,000 --> 00:12:26,000
需要认真去思考的

315
00:12:26,000 --> 00:12:27,000
这也是对我们昇腾

316
00:12:27,000 --> 00:12:29,000
带来的一个很大的挑战

317
00:12:29,000 --> 00:12:30,000
那第三个呢

318
00:12:30,000 --> 00:12:31,000
就是ChatGPT这种呢

319
00:12:31,000 --> 00:12:34,000
非常注重数据的交互

320
00:12:34,000 --> 00:12:36,000
对就是数据的交互

321
00:12:36,000 --> 00:12:37,000
对我们的环境

322
00:12:37,000 --> 00:12:38,000
不断的进行交互

323
00:12:38,000 --> 00:12:39,000
得到新的数据

324
00:12:39,000 --> 00:12:40,000
这个时候呢

325
00:12:40,000 --> 00:12:41,000
纯算一体的技术

326
00:12:41,000 --> 00:12:44,000
会不会因为ChatGPT等应用

327
00:12:44,000 --> 00:12:46,000
出现新的专用的芯片

328
00:12:46,000 --> 00:12:48,000
和新的AI的架构呢

329
00:12:48,000 --> 00:12:51,000
这是我比较关心的下一个问题

330
00:12:51,000 --> 00:12:53,000
也希望大家一起去思考

331
00:12:55,000 --> 00:12:56,000
好了看到这

332
00:12:56,000 --> 00:12:57,000
希望大家一键三连

333
00:12:57,000 --> 00:12:58,000
多多支持我

334
00:12:59,000 --> 00:13:00,000
一开始呢

335
00:13:00,000 --> 00:13:01,000
我们讲了BERT这种

336
00:13:01,000 --> 00:13:02,000
双向的语料模型呢

337
00:13:02,000 --> 00:13:03,000
跟GPT这种

338
00:13:03,000 --> 00:13:05,000
单向的语料模型的区别

339
00:13:05,000 --> 00:13:06,000
然后呢去分别介绍

340
00:13:06,000 --> 00:13:08,000
GPT家族整个系列

341
00:13:08,000 --> 00:13:09,000
接着呢

342
00:13:09,000 --> 00:13:10,000
我们去看看

343
00:13:10,000 --> 00:13:11,000
强化学习

344
00:13:11,000 --> 00:13:13,000
怎么加入人类的反馈

345
00:13:13,000 --> 00:13:14,000
然后呢

346
00:13:14,000 --> 00:13:15,000
我们了解了一下

347
00:13:15,000 --> 00:13:16,000
PG policy Gradient 

348
00:13:16,000 --> 00:13:17,000
还有PPO这个算法

349
00:13:17,000 --> 00:13:19,000
到底是怎么去运作的

350
00:13:19,000 --> 00:13:20,000
最后呢

351
00:13:20,000 --> 00:13:21,000
我们在第四个内容里面

352
00:13:21,000 --> 00:13:22,000
Instruct GPT

353
00:13:22,000 --> 00:13:24,000
深入的去把GPT系列

354
00:13:24,000 --> 00:13:26,000
还有PPO算法引进来

355
00:13:26,000 --> 00:13:27,000
当然了

356
00:13:27,000 --> 00:13:28,000
RLHF的算法

357
00:13:28,000 --> 00:13:29,000
也是用来去训练

358
00:13:29,000 --> 00:13:31,000
我们的Reward Model的

359
00:13:31,000 --> 00:13:32,000
而PPO算法呢

360
00:13:32,000 --> 00:13:33,000
就嵌到在Instruct GPT

361
00:13:33,000 --> 00:13:34,000
第三步

362
00:13:34,000 --> 00:13:35,000
这是第二步

363
00:13:35,000 --> 00:13:36,000
这是第一步

364
00:13:36,000 --> 00:13:37,000
一二三

365
00:13:37,000 --> 00:13:38,000
合起来就变成我们

366
00:13:38,000 --> 00:13:39,000
整个GPT的原理了

367
00:13:39,950 --> 00:13:42,279
好 今天的内容就到这里为之

368
00:13:42,521 --> 00:13:46,071
ChatGPT 这个内容也到这里为止

369
00:13:46,457 --> 00:13:50,057
卷的不行了 卷的不行了 记得一键三连 加关注哦

370
00:13:50,325 --> 00:13:53,125
所有的内容 都会开源在下面的这条链接里面

371
00:13:53,423 --> 00:13:54,948
拜了个拜

