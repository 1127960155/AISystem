1
00:00:05,350 --> 00:00:06,960
Hello 大家好

2
00:00:06,960 --> 00:00:08,880
我是那个月亮睡了

3
00:00:08,880 --> 00:00:09,760
我不睡

4
00:00:09,760 --> 00:00:12,800
我是秃头小宝贝的ZOMI

5
00:00:12,800 --> 00:00:14,590
今天我们来到

6
00:00:14,590 --> 00:00:18,240
ChatGPT狂飙原理普析的第二个内容

7
00:00:18,280 --> 00:00:19,000
在上一节

8
00:00:19,160 --> 00:00:22,240
其实我们已经讲了GPT整个系列

9
00:00:22,240 --> 00:00:25,240
包括之前的123系列里面的原理

10
00:00:25,240 --> 00:00:27,760
现在我们来到第二个内容

11
00:00:27,760 --> 00:00:29,960
主要是关心一下强化学习

12
00:00:29,960 --> 00:00:31,080
加入人类反馈

13
00:00:31,080 --> 00:00:32,160
就是reinforcement

14
00:00:32,160 --> 00:00:34,720
human feedback这个模式

15
00:00:35,200 --> 00:00:38,920
最后一个内容才是instructGPT

16
00:00:38,920 --> 00:00:41,080
如果大家想直接去了解

17
00:00:41,080 --> 00:00:41,760
instructGPT

18
00:00:41,760 --> 00:00:43,200
已经对强化学习

19
00:00:43,200 --> 00:00:45,120
还有GPT非常了解了

20
00:00:45,240 --> 00:00:47,240
你直接去跳到最后看就行了

21
00:00:47,240 --> 00:00:49,360
现在我们快速的回顾一下

22
00:00:49,360 --> 00:00:50,120
强化学习

23
00:00:50,120 --> 00:00:51,120
引入人类反馈

24
00:00:51,120 --> 00:00:53,240
还有RLHF这个模式

25
00:00:53,400 --> 00:00:55,040
现在我们来简单的去看一下

26
00:00:55,040 --> 00:00:56,560
强化学习到底是什么

27
00:00:56,560 --> 00:00:58,640
首先我们直接看下面这个图

28
00:00:58,640 --> 00:00:59,400
监督学习

29
00:00:59,400 --> 00:01:00,120
无监督学习

30
00:01:00,120 --> 00:01:00,840
强化学习

31
00:01:00,840 --> 00:01:03,640
是基金学习里面的三个范式

32
00:01:03,880 --> 00:01:05,400
现在我们经常用到的

33
00:01:05,400 --> 00:01:06,040
神经网络

34
00:01:06,040 --> 00:01:06,920
深度强化学习

35
00:01:07,160 --> 00:01:08,880
主要是在这个位置

36
00:01:08,880 --> 00:01:11,120
而后来出现了深度强化学习

37
00:01:11,120 --> 00:01:12,720
就把强化学习技术

38
00:01:12,720 --> 00:01:14,120
进行了一个交叉

39
00:01:14,120 --> 00:01:15,400
融合进来

40
00:01:15,520 --> 00:01:17,680
无论我们经常讲的XXX学习

41
00:01:17,680 --> 00:01:18,560
自监督学习

42
00:01:18,560 --> 00:01:19,480
还有无监督学习

43
00:01:19,480 --> 00:01:20,640
各种学习方法

44
00:01:20,640 --> 00:01:23,360
基本都囊括在这三个方法论

45
00:01:23,360 --> 00:01:25,440
或者这三个范式里面

46
00:01:25,480 --> 00:01:27,280
现在我们再来看看

47
00:01:27,280 --> 00:01:30,360
强化学习里面有什么东西

48
00:01:30,480 --> 00:01:33,360
强化学习主要有两个交互的对象

49
00:01:33,360 --> 00:01:34,800
第一个就是Agent

50
00:01:34,800 --> 00:01:36,160
我们的智能体

51
00:01:36,160 --> 00:01:37,560
第二个就是Environment

52
00:01:37,560 --> 00:01:39,200
我们的环境

53
00:01:39,400 --> 00:01:41,040
我们首先会从智能体

54
00:01:41,040 --> 00:01:41,640
开始出发

55
00:01:41,640 --> 00:01:43,120
智能体执行一个动作

56
00:01:43,120 --> 00:01:44,720
在环境当中执行一个动作

57
00:01:44,720 --> 00:01:47,320
那环境就会反馈一个新的状态

58
00:01:47,320 --> 00:01:48,920
和新的一个reward

59
00:01:48,920 --> 00:01:50,840
就是我们的反馈奖励

60
00:01:50,840 --> 00:01:52,160
给我们的Agent

61
00:01:52,160 --> 00:01:53,200
根据当前的状态

62
00:01:53,200 --> 00:01:53,800
新的状态

63
00:01:53,800 --> 00:01:54,920
还有新的一个奖励

64
00:01:55,200 --> 00:01:56,800
再去选择新的动作

65
00:01:57,000 --> 00:01:58,600
实际上我们虽然看上去

66
00:01:58,600 --> 00:02:00,120
这么简单的一个交互

67
00:02:00,120 --> 00:02:01,120
怎么去求解

68
00:02:01,120 --> 00:02:02,840
怎么变成我们的数学范式

69
00:02:03,360 --> 00:02:06,120
这个问题就非常的有意思了

70
00:02:07,720 --> 00:02:09,160
在强化学习里面

71
00:02:09,440 --> 00:02:12,320
主要是把刚才的一个交互的方式

72
00:02:12,320 --> 00:02:14,960
变成了一个马尔可夫链

73
00:02:15,000 --> 00:02:16,240
通过马尔可夫链

74
00:02:16,440 --> 00:02:18,000
就对强化学这个范式

75
00:02:18,200 --> 00:02:19,960
有了一个数学的表示

76
00:02:20,000 --> 00:02:21,760
最后就是怎么去求解

77
00:02:21,760 --> 00:02:23,800
马尔可夫链的最优质

78
00:02:24,000 --> 00:02:25,160
这个就完完全全

79
00:02:25,160 --> 00:02:27,000
变成数学等价的公式

80
00:02:27,000 --> 00:02:29,760
我们就可以求强化学习了

81
00:02:30,040 --> 00:02:32,040
下面我们继续往下看一下

82
00:02:32,040 --> 00:02:35,600
强化学习有哪些基本的概念

83
00:02:35,760 --> 00:02:37,480
其实有一些没有相关的概念

84
00:02:37,600 --> 00:02:38,880
我已经把提出了

85
00:02:38,880 --> 00:02:39,880
这些基本概念

86
00:02:40,040 --> 00:02:41,080
实际上到后面

87
00:02:41,240 --> 00:02:42,960
还是跟我们的chart GPT

88
00:02:42,960 --> 00:02:45,120
还有instructGPT相关的

89
00:02:45,120 --> 00:02:47,640
我们首先要了解一下策略

90
00:02:48,240 --> 00:02:49,280
策略主要是指

91
00:02:49,280 --> 00:02:50,760
智能体在特定的时间内

92
00:02:50,760 --> 00:02:52,800
选择的行为的方式

93
00:02:52,920 --> 00:02:53,880
简单的理解

94
00:02:54,000 --> 00:02:55,320
就是智能体

95
00:02:55,360 --> 00:02:56,760
它在某个时间段

96
00:02:56,760 --> 00:02:58,000
根据什么策略

97
00:02:58,000 --> 00:03:00,080
去执行它的动作

98
00:03:00,320 --> 00:03:01,280
奖励函数

99
00:03:01,440 --> 00:03:03,280
就是我执行完这个策略之后

100
00:03:03,640 --> 00:03:05,680
环境就会给我一个反馈

101
00:03:05,680 --> 00:03:06,200
告诉我

102
00:03:06,200 --> 00:03:07,160
我这一步做得好

103
00:03:07,160 --> 00:03:07,640
还是不好

104
00:03:07,640 --> 00:03:08,440
例如下棋

105
00:03:08,440 --> 00:03:10,560
我下这个位置到底好还是不好

106
00:03:10,560 --> 00:03:12,600
它会有一个奖励函数

107
00:03:12,960 --> 00:03:14,200
可能我下的比较好

108
00:03:14,200 --> 00:03:16,040
它就会给我做一个正的奖励

109
00:03:16,040 --> 00:03:17,200
如果我下的不好

110
00:03:17,200 --> 00:03:18,240
或者我要输棋了

111
00:03:18,240 --> 00:03:19,840
它就给我一个负的奖励

112
00:03:19,840 --> 00:03:20,360
这个时候

113
00:03:20,480 --> 00:03:22,160
我们就可以通过求解马尔可夫链

114
00:03:22,160 --> 00:03:24,080
或者求解我们的马尔可夫规则

115
00:03:24,320 --> 00:03:25,840
通过求解马尔可夫过程

116
00:03:25,840 --> 00:03:27,000
来得到最优的解

117
00:03:27,000 --> 00:03:27,960
或者最后的解

118
00:03:28,160 --> 00:03:30,600
下面还有一个价值函数

119
00:03:30,720 --> 00:03:31,480
这个价值函数

120
00:03:31,560 --> 00:03:32,560
也是在我们后面的

121
00:03:32,560 --> 00:03:34,840
chat gpt里面会用到的

122
00:03:35,040 --> 00:03:36,320
从长远的角度看

123
00:03:36,600 --> 00:03:38,480
刚才我们只是单单一步

124
00:03:38,480 --> 00:03:39,560
好还是不好

125
00:03:39,560 --> 00:03:41,680
价值函数是从长远角度看

126
00:03:41,680 --> 00:03:43,080
这个整体的状态

127
00:03:43,080 --> 00:03:44,240
或整体的策略

128
00:03:44,240 --> 00:03:46,080
到底有没有收益

129
00:03:46,280 --> 00:03:48,240
最后就是环境模型

130
00:03:48,240 --> 00:03:48,760
环境模型

131
00:03:48,760 --> 00:03:49,200
就是我们的

132
00:03:49,200 --> 00:03:50,400
Bandit environment

133
00:03:50,400 --> 00:03:51,760
就是我们的数据集

134
00:03:51,800 --> 00:03:53,960
就后面的POMD数据集

135
00:03:54,240 --> 00:03:56,560
我们接下来再往下看一看

136
00:03:56,560 --> 00:03:58,360
小新同学来问一问

137
00:03:59,160 --> 00:04:00,480
ZOMI老师你好

138
00:04:00,480 --> 00:04:02,080
在native强化学习里面

139
00:04:02,520 --> 00:04:04,840
我们有environment和reward model

140
00:04:04,840 --> 00:04:05,800
那reward model

141
00:04:05,800 --> 00:04:07,200
就是我们的奖励函数

142
00:04:07,880 --> 00:04:08,200
但是

143
00:04:08,200 --> 00:04:09,280
内强化学习里面

144
00:04:09,280 --> 00:04:10,360
没有奖励函数

145
00:04:10,760 --> 00:04:12,160
只有一些专家

146
00:04:12,160 --> 00:04:14,680
或者人类的一个示范和反馈

147
00:04:14,960 --> 00:04:16,560
怎么纳入到我们的

148
00:04:16,560 --> 00:04:18,160
强化学习这个体系里面

149
00:04:18,160 --> 00:04:19,480
或者马尔可夫链里面

150
00:04:20,480 --> 00:04:22,080
小新问的这个问题

151
00:04:22,080 --> 00:04:23,760
或者我类似的这个问题

152
00:04:23,760 --> 00:04:25,320
确实问的挺好的

153
00:04:26,640 --> 00:04:28,920
首先我们通过人类的标注

154
00:04:28,920 --> 00:04:30,520
去获得这个reward model

155
00:04:30,520 --> 00:04:32,160
就相当于我有了人类的标注

156
00:04:32,160 --> 00:04:33,040
然后我相信

157
00:04:33,040 --> 00:04:35,480
这个人或者专家标出来的数据

158
00:04:35,800 --> 00:04:36,640
是不错的

159
00:04:36,640 --> 00:04:37,440
比较好的

160
00:04:37,440 --> 00:04:39,240
然后反推人类

161
00:04:39,240 --> 00:04:40,280
就是我们的human

162
00:04:40,280 --> 00:04:43,080
为什么去设置这样的奖励函数

163
00:04:43,080 --> 00:04:44,480
就我去学习

164
00:04:44,920 --> 00:04:46,240
反向的过程

165
00:04:46,440 --> 00:04:47,800
既然学习完之后

166
00:04:47,880 --> 00:04:49,520
我们就有了奖励函数了

167
00:04:49,520 --> 00:04:52,520
就可以使用一般的强化学习的方法

168
00:04:52,520 --> 00:04:53,920
去找到最优的策略

169
00:04:53,920 --> 00:04:55,800
或者最优的动作

170
00:04:55,800 --> 00:04:58,840
下面我们看看下面这个图

171
00:04:59,000 --> 00:04:59,600
下面这个图

172
00:04:59,600 --> 00:05:01,360
我们简单的再放大一下

173
00:05:01,360 --> 00:05:03,240
它主要有三个模块

174
00:05:03,240 --> 00:05:04,560
首先有agent

175
00:05:04,560 --> 00:05:06,360
那agent就是我们刚才讲到的

176
00:05:06,360 --> 00:05:07,400
一个智能体

177
00:05:07,400 --> 00:05:08,400
那智能体实际上

178
00:05:08,400 --> 00:05:10,360
它是一个强化学习的算法

179
00:05:10,360 --> 00:05:11,960
你直接把它当成强化学习

180
00:05:11,960 --> 00:05:13,080
算法解好了

181
00:05:13,080 --> 00:05:15,440
然后我们还有一个Environment

182
00:05:15,440 --> 00:05:17,240
也就是我们的环境

183
00:05:17,240 --> 00:05:19,440
那实际上环境在chart gpt里面

184
00:05:19,480 --> 00:05:20,720
或者instruct gpd里面

185
00:05:21,000 --> 00:05:23,000
我们把它当成一个data set

186
00:05:23,000 --> 00:05:25,440
只是在里面不断的进行采样

187
00:05:25,440 --> 00:05:26,760
获得我们的数据

188
00:05:26,800 --> 00:05:28,760
然后我们有第三个内容

189
00:05:28,760 --> 00:05:30,360
就是reward predict

190
00:05:30,360 --> 00:05:31,960
人类的反馈

191
00:05:31,960 --> 00:05:33,200
我们通过人类的反馈

192
00:05:33,320 --> 00:05:35,320
去获得我们一个预测的奖励

193
00:05:35,320 --> 00:05:37,720
然后去给我们的算法进行学习

194
00:05:37,760 --> 00:05:38,520
学习完之后

195
00:05:38,640 --> 00:05:39,200
我的agent

196
00:05:39,200 --> 00:05:40,600
就可以在环境当中

197
00:05:40,600 --> 00:05:42,600
不断的跟环境进行交互

198
00:05:42,640 --> 00:05:43,800
执行一个动作

199
00:05:43,800 --> 00:05:46,320
然后得到当前的状态和观察值

200
00:05:46,320 --> 00:05:47,920
之前我们讲到的Environment

201
00:05:47,920 --> 00:05:49,080
返回的一个reward

202
00:05:49,360 --> 00:05:50,920
就可以通过reward predict

203
00:05:51,200 --> 00:05:53,760
返回给RL Algorithm

204
00:05:53,760 --> 00:05:54,880
我们的agent

205
00:05:54,880 --> 00:05:55,760
通过这种方式

206
00:05:55,840 --> 00:05:57,360
就把人类反馈

207
00:05:57,360 --> 00:06:00,720
加入到强化学习的整个体系里面了

208
00:06:00,720 --> 00:06:03,440
所以说并没有多难

209
00:06:03,440 --> 00:06:05,400
难的难在后面

210
00:06:05,400 --> 00:06:07,200
强化学习的算法

211
00:06:07,200 --> 00:06:08,960
策略梯度policy

212
00:06:08,960 --> 00:06:12,000
gradient到ppo算法

213
00:06:12,800 --> 00:06:16,280
下面在正式进入到算法之前

214
00:06:16,480 --> 00:06:20,400
我还是先讲讲一些简单的概念

215
00:06:20,520 --> 00:06:21,400
强化学习

216
00:06:21,600 --> 00:06:23,600
按照方法学习策略来分

217
00:06:23,800 --> 00:06:26,760
主要分为value base和policy base

218
00:06:26,760 --> 00:06:28,120
在chart gpt里面

219
00:06:28,240 --> 00:06:29,640
用的是policy base

220
00:06:29,640 --> 00:06:31,200
而两者之间的交集

221
00:06:31,400 --> 00:06:33,800
有一个actor跟Critic

222
00:06:33,960 --> 00:06:36,360
chart gpt更加准确的来说

223
00:06:36,600 --> 00:06:39,240
用的就是action跟critic

224
00:06:39,280 --> 00:06:42,080
这种方式或者这种求解的方法

225
00:06:42,080 --> 00:06:43,200
我有一个演员

226
00:06:43,200 --> 00:06:44,400
有一个评判家

227
00:06:44,400 --> 00:06:46,240
演员去跳一段舞蹈

228
00:06:46,240 --> 00:06:47,760
评判家去看一下

229
00:06:47,760 --> 00:06:49,160
他跳的好不好

230
00:06:49,160 --> 00:06:51,560
通过这种互相博弈的方式

231
00:06:51,560 --> 00:06:53,040
进行一个学习

232
00:06:54,360 --> 00:06:57,440
下面我们看一下两种学习方式

233
00:06:57,440 --> 00:06:58,480
一种是value base

234
00:06:58,480 --> 00:07:00,400
一种是policy base

235
00:07:00,400 --> 00:07:01,640
它到底有什么不同

236
00:07:01,800 --> 00:07:03,600
一种是学习一个值

237
00:07:03,640 --> 00:07:06,080
一种是学习一种策略

238
00:07:06,240 --> 00:07:08,320
在数学上其实我们表示很简单

239
00:07:08,360 --> 00:07:09,680
假设我现在的agent

240
00:07:09,800 --> 00:07:11,560
有三个动作

241
00:07:11,560 --> 00:07:13,720
a1 a2 a3三个动作

242
00:07:13,720 --> 00:07:14,400
那三个动作

243
00:07:14,520 --> 00:07:16,640
我选取q就是这个值

244
00:07:16,640 --> 00:07:17,440
三个值

245
00:07:17,440 --> 00:07:19,840
最大的作为本次选择的动作

246
00:07:19,840 --> 00:07:20,960
例如玩游戏的时候

247
00:07:20,960 --> 00:07:22,280
告诉我上下左右

248
00:07:22,280 --> 00:07:24,240
我按上左还是按下键

249
00:07:24,440 --> 00:07:26,240
这个就是value base

250
00:07:26,440 --> 00:07:28,840
下面就是policy base

251
00:07:28,960 --> 00:07:29,600
policy base

252
00:07:29,720 --> 00:07:31,800
就会有一个动作的函数actor

253
00:07:31,800 --> 00:07:32,640
这个动作函数

254
00:07:32,760 --> 00:07:35,440
会得到一个策略的概率

255
00:07:35,440 --> 00:07:36,600
就当前这个动作

256
00:07:36,600 --> 00:07:37,840
执行某个策略

257
00:07:37,840 --> 00:07:38,920
得到的一个概率

258
00:07:38,920 --> 00:07:40,720
我们叫psa

259
00:07:40,760 --> 00:07:42,080
最后根据这个策略

260
00:07:42,280 --> 00:07:44,640
选取对应的动作

261
00:07:46,680 --> 00:07:47,800
说的简单一点

262
00:07:48,280 --> 00:07:50,280
我的智能体决定我上下左右

263
00:07:50,400 --> 00:07:52,200
我直接用value base就行了

264
00:07:52,240 --> 00:07:54,160
但是在我下棋的时候

265
00:07:54,280 --> 00:07:55,600
如果有机器人辅助

266
00:07:55,600 --> 00:07:56,800
或有作弊机器人的时候

267
00:07:56,800 --> 00:07:57,800
我不喜欢智能体

268
00:07:57,960 --> 00:07:59,240
告诉我是下这一步

269
00:07:59,240 --> 00:08:00,440
还是下那一步

270
00:08:00,440 --> 00:08:02,040
我更希望智能体告诉我

271
00:08:02,040 --> 00:08:04,440
我下这一步赢的概率有多大

272
00:08:04,440 --> 00:08:05,520
我下这一步

273
00:08:05,520 --> 00:08:07,320
会得到什么不一样的结果

274
00:08:07,520 --> 00:08:10,160
这种就是一个policy base的方式

275
00:08:10,160 --> 00:08:11,360
能够更好的处理

276
00:08:11,360 --> 00:08:12,200
连续的动作

277
00:08:12,200 --> 00:08:13,400
给我一个预测的概率

278
00:08:13,400 --> 00:08:14,240
让我自主

279
00:08:14,240 --> 00:08:15,640
或者让agent再去决定

280
00:08:15,640 --> 00:08:17,080
我应该执行怎么动作

281
00:08:17,080 --> 00:08:18,880
我应该怎么进行探索

282
00:08:18,880 --> 00:08:20,880
这也是policy base跟value base

283
00:08:20,880 --> 00:08:22,480
最大的区别

284
00:08:23,400 --> 00:08:24,200
下一个话题

285
00:08:24,640 --> 00:08:26,080
谷歌之前很火的

286
00:08:26,080 --> 00:08:28,160
用强化显微镜做下棋

287
00:08:28,160 --> 00:08:30,760
我的alphaGo就是用了这种方式

288
00:08:30,960 --> 00:08:33,520
下面我们真实的来到了

289
00:08:33,960 --> 00:08:35,800
pg policy gradient

290
00:08:35,800 --> 00:08:37,240
策略梯度下降

291
00:08:37,240 --> 00:08:39,320
这一个算法里面

292
00:08:40,560 --> 00:08:41,720
首先我们看一下

293
00:08:41,720 --> 00:08:43,000
下面这一段话

294
00:08:43,000 --> 00:08:45,120
在实际的环境或者实验当中

295
00:08:45,200 --> 00:08:47,840
我们会让actor跟environment进行交互

296
00:08:47,840 --> 00:08:49,680
产生一系列的数据

297
00:08:49,680 --> 00:08:52,160
就是我given一个具体的policy

298
00:08:52,160 --> 00:08:54,960
然后产生一系列的simple的数据

299
00:08:54,960 --> 00:08:57,280
那x代表的是当前的状态

300
00:08:57,280 --> 00:08:59,320
a是代表执行这个policy

301
00:08:59,320 --> 00:09:01,160
得到的动作

302
00:09:01,160 --> 00:09:04,080
而奥鲁套就是当前的一个奖励

303
00:09:04,080 --> 00:09:05,240
通过跟环境的交互

304
00:09:05,400 --> 00:09:07,040
就得到很多sa的p

305
00:09:07,040 --> 00:09:08,360
就sa的对

306
00:09:08,360 --> 00:09:09,680
表示在某个状态下

307
00:09:09,680 --> 00:09:10,640
采取了某个动作

308
00:09:10,640 --> 00:09:12,800
得到什么一个奖励

309
00:09:13,040 --> 00:09:15,080
现在我们会将这些数据

310
00:09:15,080 --> 00:09:17,040
送入到训练的过程当中

311
00:09:17,040 --> 00:09:18,200
进行计算

312
00:09:18,200 --> 00:09:20,240
更新我们的网络模型

313
00:09:20,240 --> 00:09:22,160
而sita就是深度学习里面

314
00:09:22,160 --> 00:09:24,680
某个模型里面的所有的参数

315
00:09:24,680 --> 00:09:25,800
那参数的更新方式

316
00:09:25,960 --> 00:09:26,840
就是这一条

317
00:09:26,840 --> 00:09:28,720
而参数的更新的驱动方式

318
00:09:28,880 --> 00:09:30,320
就是算我们的RT

319
00:09:30,320 --> 00:09:32,040
下面我们清空所有的笔记验证

320
00:09:32,160 --> 00:09:34,440
看一下下面这条交互的方式

321
00:09:34,600 --> 00:09:36,840
那首先我们会有一个特定的策略

322
00:09:36,840 --> 00:09:38,280
在一个特定的策略里面

323
00:09:38,680 --> 00:09:41,240
我们的agent和环境进行交互

324
00:09:41,240 --> 00:09:43,640
得到很多sa的pairs的对

325
00:09:43,880 --> 00:09:45,680
这些都是我们的数据的样本

326
00:09:45,680 --> 00:09:46,840
我们的数据的样本

327
00:09:47,000 --> 00:09:48,640
会丢给我们的神经网络

328
00:09:48,640 --> 00:09:49,560
进行更新

329
00:09:49,560 --> 00:09:50,280
更新完之后

330
00:09:50,440 --> 00:09:51,720
我们就会update我们的model

331
00:09:51,720 --> 00:09:52,720
update我们的model之后

332
00:09:53,000 --> 00:09:55,240
就重新的给到我们新的policy

333
00:09:55,240 --> 00:09:56,080
去执行

334
00:09:56,080 --> 00:09:57,880
然后去判断什么时候好

335
00:09:57,880 --> 00:09:58,800
什么时候收敛

336
00:09:58,800 --> 00:10:00,760
什么时候达到我们的目标

337
00:10:00,760 --> 00:10:01,400
预期的目标

338
00:10:01,400 --> 00:10:03,880
就可以把迭代的方式停止下来

339
00:10:03,880 --> 00:10:05,440
我们就学习到了一个

340
00:10:05,440 --> 00:10:07,880
很好的强化学习的策略了

341
00:10:09,440 --> 00:10:11,480
现在我们正式的进入到

342
00:10:11,480 --> 00:10:13,000
PPO这个算法了

343
00:10:13,160 --> 00:10:13,880
PPO算法

344
00:10:14,040 --> 00:10:16,000
就是ChatGPT里面

345
00:10:16,000 --> 00:10:17,320
使用到的一个算法

346
00:10:17,320 --> 00:10:19,280
也是openai去提出来的

347
00:10:19,280 --> 00:10:20,680
在17年的时候

348
00:10:20,920 --> 00:10:22,200
对于PG算法来说

349
00:10:22,360 --> 00:10:23,400
其实最大的问题

350
00:10:23,400 --> 00:10:25,320
就是在策略更新之后

351
00:10:25,320 --> 00:10:27,680
还需要使用相同的环境互动

352
00:10:27,680 --> 00:10:28,840
去收集数据

353
00:10:28,840 --> 00:10:30,360
进行下轮的迭代

354
00:10:30,360 --> 00:10:32,280
也就是我们刚才说到的

355
00:10:32,280 --> 00:10:33,440
我收集完数据

356
00:10:33,440 --> 00:10:34,520
迭代更新

357
00:10:34,520 --> 00:10:35,200
收集数据

358
00:10:35,200 --> 00:10:36,280
迭代更新

359
00:10:36,280 --> 00:10:37,720
这种方式

360
00:10:38,280 --> 00:10:39,280
于是PPO算法

361
00:10:39,280 --> 00:10:41,800
就利用了重要性采样这种思想

362
00:10:41,840 --> 00:10:43,680
当智能体步入到当前策略的

363
00:10:43,680 --> 00:10:44,920
一个路径概率的情况下

364
00:10:45,160 --> 00:10:48,440
模拟一个近似的Q的分布

365
00:10:48,760 --> 00:10:49,880
这里很有意思

366
00:10:49,880 --> 00:10:51,000
模拟Q的分布

367
00:10:51,000 --> 00:10:52,760
只要P跟Q同分布

368
00:10:52,760 --> 00:10:53,760
差得不太远

369
00:10:53,760 --> 00:10:54,840
通过多轮迭代

370
00:10:55,000 --> 00:10:56,520
就可以快速的收敛

371
00:10:56,520 --> 00:10:58,240
使得我们的智能体策略

372
00:10:58,240 --> 00:11:00,120
学习的更快

373
00:11:00,160 --> 00:11:01,040
更爽

374
00:11:01,280 --> 00:11:02,800
我们现在正式的看看

375
00:11:02,800 --> 00:11:04,800
PPO算法里面的一些公式

376
00:11:04,800 --> 00:11:06,000
首先需要讲讲

377
00:11:06,440 --> 00:11:07,680
PPO是结合了一个

378
00:11:07,680 --> 00:11:09,480
actor跟critic的方式

379
00:11:09,480 --> 00:11:10,840
那actor是一个演员

380
00:11:10,840 --> 00:11:11,360
critic

381
00:11:11,360 --> 00:11:13,120
那就是一个评判家

382
00:11:13,400 --> 00:11:15,040
这里面的一个actor跟critics

383
00:11:15,040 --> 00:11:17,880
都是对应不同的网络模型

384
00:11:17,880 --> 00:11:19,320
这是个A模型

385
00:11:19,320 --> 00:11:20,840
这个是C模型

386
00:11:20,880 --> 00:11:22,200
A模型跟C模型

387
00:11:22,200 --> 00:11:24,800
是在环境当中不断的博弈的

388
00:11:24,800 --> 00:11:27,200
actor首先负责跟环境互动

389
00:11:27,200 --> 00:11:27,840
收集样本

390
00:11:27,840 --> 00:11:30,160
等同于刚才的一个PG

391
00:11:30,160 --> 00:11:32,000
然后用来更新PPO的

392
00:11:32,000 --> 00:11:32,600
另外的话

393
00:11:32,600 --> 00:11:33,800
添加了一个critics

394
00:11:33,920 --> 00:11:34,760
用于评判

395
00:11:34,760 --> 00:11:35,840
我的actor的动作

396
00:11:35,840 --> 00:11:37,440
到底好还是不好

397
00:11:37,560 --> 00:11:38,600
在实际算法的时候

398
00:11:38,760 --> 00:11:39,320
我的actor

399
00:11:39,440 --> 00:11:40,240
就会有一个旧的

400
00:11:40,240 --> 00:11:41,200
还有一个新的

401
00:11:41,200 --> 00:11:42,760
两个通过KL散度

402
00:11:42,760 --> 00:11:44,080
进行一个互相的学习

403
00:11:44,080 --> 00:11:46,160
也就是我们之前讲到的

404
00:11:46,160 --> 00:11:47,360
只要有两个同分布

405
00:11:47,360 --> 00:11:48,080
差得不太远

406
00:11:48,080 --> 00:11:49,440
我们就可以尽快去模拟

407
00:11:49,440 --> 00:11:50,800
其中的一个策略

408
00:11:51,080 --> 00:11:54,040
而AT就是advantage estimate

409
00:11:54,800 --> 00:11:56,880
对应强化学习概念里面的

410
00:11:56,880 --> 00:11:58,240
价值函数

411
00:11:58,240 --> 00:11:59,920
通过根据不同chattery

412
00:11:59,920 --> 00:12:01,240
就是我们整体的

413
00:12:01,240 --> 00:12:02,240
整一个环境里面

414
00:12:02,240 --> 00:12:03,280
获取的一个RT

415
00:12:03,280 --> 00:12:04,120
就是奖励

416
00:12:04,120 --> 00:12:06,440
然后去得到我们最终的

417
00:12:06,440 --> 00:12:07,800
或者我们预期的奖励

418
00:12:07,800 --> 00:12:09,000
通过计算预期的奖励

419
00:12:09,360 --> 00:12:10,960
就得到我们最终的

420
00:12:10,960 --> 00:12:12,760
一个整体的期望

421
00:12:12,960 --> 00:12:15,920
现在我们再看一下具体的算法

422
00:12:15,920 --> 00:12:16,760
现在我们看一下

423
00:12:16,760 --> 00:12:17,600
actor跟critics

424
00:12:17,600 --> 00:12:19,200
整个算法是怎么实现的

425
00:12:19,200 --> 00:12:20,920
首先我们有两轮迭代

426
00:12:21,120 --> 00:12:21,800
第一轮迭代

427
00:12:21,960 --> 00:12:23,840
是在我们的环境当中的一个交互

428
00:12:23,840 --> 00:12:25,680
第二个就是我的actor

429
00:12:25,680 --> 00:12:26,880
之间的一个互相学习

430
00:12:26,880 --> 00:12:29,160
或者在chattery里面去运行的

431
00:12:29,200 --> 00:12:29,840
proxy owner

432
00:12:29,840 --> 00:12:30,840
会在环境当中

433
00:12:30,840 --> 00:12:32,480
去不断的执行

434
00:12:32,480 --> 00:12:34,440
然后计算我们的advantage

435
00:12:34,440 --> 00:12:35,320
estimate

436
00:12:35,320 --> 00:12:36,520
对这个没读错

437
00:12:36,520 --> 00:12:38,520
就是说对应我们的价值的函数

438
00:12:38,520 --> 00:12:40,360
然后迭代完之后就停止了

439
00:12:40,360 --> 00:12:41,600
然后去更新

440
00:12:41,600 --> 00:12:44,240
我们的网络模型的策略

441
00:12:44,440 --> 00:12:45,360
最后更新完之后

442
00:12:45,480 --> 00:12:47,600
就把新的学习到的策略

443
00:12:47,600 --> 00:12:49,560
给到旧的网络模型当中

444
00:12:49,560 --> 00:12:51,640
然后重新的去跟环境

445
00:12:51,640 --> 00:12:52,720
进行交互迭代

446
00:12:52,720 --> 00:12:53,640
通过这种方式

447
00:12:53,840 --> 00:12:55,640
去不断的在环境当中

448
00:12:55,640 --> 00:12:56,800
学习到一个

449
00:12:56,800 --> 00:12:58,720
很好的算法的模式

450
00:12:58,800 --> 00:12:59,640
那好了

451
00:12:59,640 --> 00:13:00,600
今天的内容

452
00:13:00,880 --> 00:13:02,800
就基本上到这里为止了

453
00:13:03,040 --> 00:13:04,800
今天我们讲了强化学的基本概念

454
00:13:04,800 --> 00:13:05,760
还有policy Gradient

455
00:13:05,760 --> 00:13:07,520
还有PPO这个算法

456
00:13:07,760 --> 00:13:08,400
听不懂的

457
00:13:08,400 --> 00:13:09,600
其实没有太多关系

458
00:13:09,600 --> 00:13:10,480
你对这些概念

459
00:13:10,600 --> 00:13:12,160
有个大致的了解

460
00:13:12,160 --> 00:13:14,200
或者有些名词上的认知就好了

461
00:13:14,960 --> 00:13:16,480
我们将会在下一节当中

462
00:13:16,480 --> 00:13:17,360
instruct gdp

463
00:13:17,360 --> 00:13:17,960
chart gpt

464
00:13:17,960 --> 00:13:18,920
这个原理里面

465
00:13:19,240 --> 00:13:23,000
重新的去把刚才前面讲到的算法

466
00:13:23,000 --> 00:13:24,480
都融合进来

467
00:13:24,480 --> 00:13:27,280
然后再跟大家讲一遍

468
00:13:27,800 --> 00:13:29,200
这里面就是最重要

469
00:13:29,200 --> 00:13:30,400
最核心的内容了

470
00:13:30,400 --> 00:13:31,920
欢迎大家继续留意

471
00:13:31,920 --> 00:13:33,920
谢谢各位 拜了个拜

472
00:13:33,920 --> 00:13:37,020
记得一键三连加关注哦

473
00:13:37,283 --> 00:13:39,983
所有的内容都会开源在下面这条链接里面

474
00:13:40,268 --> 00:13:42,068
拜了个拜

