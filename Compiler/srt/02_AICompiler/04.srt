1
00:00:00,000 --> 00:00:06,600
Hello,大家好

2
00:00:06,600 --> 00:00:09,000
不知不觉我们又回到了AI编译器的

3
00:00:09,000 --> 00:00:11,600
这个围绕AI这个系列来展开

4
00:00:11,600 --> 00:00:14,400
那聊到AI可能很多人就不怎么关注了

5
00:00:14,400 --> 00:00:17,800
我发现传统的内容反而关注的人越来越多

6
00:00:17,800 --> 00:00:18,600
我是周米

7
00:00:18,600 --> 00:00:21,000
今天就跟大家去汇报一下

8
00:00:21,000 --> 00:00:23,400
关于AI编译器的一些挑战

9
00:00:23,400 --> 00:00:26,200
还有关于它的发展的一些未来的思考和洞察

10
00:00:26,200 --> 00:00:28,600
今天的这个内容也是我们AI编译器

11
00:00:28,600 --> 00:00:31,200
和传统编译器里面的最后一个小节

12
00:00:31,200 --> 00:00:34,600
那未来我希望能够跟大家去详细的展开

13
00:00:34,600 --> 00:00:36,600
还关于AI编译器通用架构

14
00:00:36,600 --> 00:00:38,800
就是它的架构里面的每一个parts

15
00:00:38,800 --> 00:00:40,000
每一个细节点

16
00:00:40,000 --> 00:00:41,800
那讲完这些技术点

17
00:00:41,800 --> 00:00:43,800
大家再回过头来去看一下

18
00:00:43,800 --> 00:00:45,600
我们整个AI编译器的通用架构

19
00:00:45,600 --> 00:00:47,200
你就可能会想得更明白

20
00:00:47,200 --> 00:00:50,000
或者有更好的新的思路也可以告诉我

21
00:00:50,000 --> 00:00:52,600
然后我也希望大家能够参与到Math4

22
00:00:52,600 --> 00:00:53,800
还有其他AI框架

23
00:00:53,800 --> 00:00:56,000
参与到真正的开源项目当中

24
00:00:56,000 --> 00:00:58,000
现在我们想看一些比较新的

25
00:00:58,000 --> 00:01:01,000
或者比较热门的一些AI编译器的一个对比

26
00:01:01,000 --> 00:01:02,000
下面这个图

27
00:01:02,000 --> 00:01:05,200
我是取自于之前给大家去安利过的一篇文章

28
00:01:05,200 --> 00:01:09,400
The Deep Learning Compiler A Comprehensive Surveys

29
00:01:09,400 --> 00:01:11,600
这篇文章是对AI编译器的综述

30
00:01:11,600 --> 00:01:13,000
那其中有一块table

31
00:01:13,000 --> 00:01:15,600
我们现在打开这篇文章来看看

32
00:01:15,600 --> 00:01:17,000
在这篇文章的第二刷页

33
00:01:17,000 --> 00:01:20,800
就横向的对比了TVM, NGREV, TCGROUP

34
00:01:20,800 --> 00:01:24,000
还有XLA五个AI编译器的区别

35
00:01:24,000 --> 00:01:25,800
XLA是谷歌推出的

36
00:01:26,600 --> 00:01:28,800
还有JAX作为它的底层编译层的

37
00:01:28,800 --> 00:01:31,000
它主要的特性是把我们的神经网络

38
00:01:31,000 --> 00:01:33,400
或者计算图打开变成小算子

39
00:01:33,400 --> 00:01:34,600
那基于这些小算子

40
00:01:34,600 --> 00:01:36,400
再做一些图算的融合

41
00:01:36,400 --> 00:01:38,600
然后一起去做一个编译的

42
00:01:38,600 --> 00:01:41,000
整体的设计主要是定义了几个IR

43
00:01:41,000 --> 00:01:44,000
例如HLO、LVM IR来去实现的

44
00:01:44,000 --> 00:01:45,600
在编译器里面的所有的Path

45
00:01:45,600 --> 00:01:47,800
都是手工的提前去指定的

46
00:01:47,800 --> 00:01:51,000
也就是通过人工去发现一些规律

47
00:01:51,000 --> 00:01:53,600
最后把这些规律汇总成为我们的代码

48
00:01:53,800 --> 00:01:56,200
TVM作为一个端到端的深度学习编译器

49
00:01:56,200 --> 00:01:57,800
主要是用在推理场景

50
00:01:57,800 --> 00:01:59,600
当然了训练场景其实也可以使用

51
00:01:59,600 --> 00:02:02,000
这个项目是由CMU的助理教授

52
00:02:02,000 --> 00:02:03,800
陈天琦发起的

53
00:02:03,800 --> 00:02:05,000
作为一个AI编译器

54
00:02:05,000 --> 00:02:07,600
它跟所有的框架都是松果核的关系

55
00:02:07,600 --> 00:02:09,400
就是它可以对接到多个框架

56
00:02:09,400 --> 00:02:11,200
最重要的目的就是想去解决

57
00:02:11,200 --> 00:02:12,400
人工写Kernel的问题

58
00:02:12,400 --> 00:02:14,600
然后对多个不同的硬件平台

59
00:02:14,600 --> 00:02:16,800
低级层面的代码进行优化

60
00:02:16,800 --> 00:02:18,200
它里面分为两层

61
00:02:18,200 --> 00:02:19,000
一层是BitLay层

62
00:02:19,000 --> 00:02:20,400
一层是TVM层

63
00:02:20,400 --> 00:02:21,800
而BitLay层关注于图

64
00:02:21,800 --> 00:02:23,000
TVM层关注于算

65
00:02:23,000 --> 00:02:26,000
而TVM里面就借鉴了海里德的一个想法

66
00:02:26,000 --> 00:02:27,200
把Compute和Schedule

67
00:02:27,200 --> 00:02:28,200
就是我的计算

68
00:02:28,200 --> 00:02:30,800
还有我的调度分离的这种方案

69
00:02:30,800 --> 00:02:33,600
当然我们还有Facebook早期的一个项目TC

70
00:02:33,600 --> 00:02:36,400
不过很可惜的就是随着PyTorch的兴起

71
00:02:36,400 --> 00:02:38,200
TC这个项目在Facebook里面

72
00:02:38,200 --> 00:02:39,600
没有得到一个很好的重视

73
00:02:39,600 --> 00:02:41,600
而这个项目已经没有持续的更新了

74
00:02:41,600 --> 00:02:43,400
但是这个项目的很多idea

75
00:02:43,400 --> 00:02:44,800
它的一个Polyhedron Model

76
00:02:44,800 --> 00:02:46,400
就是多面体的技术

77
00:02:46,400 --> 00:02:47,600
去解决编译器的问题

78
00:02:47,600 --> 00:02:49,800
这个思想其实现在被Mathbot所继承

79
00:02:49,800 --> 00:02:51,400
或者Mathbot基于这个idea

80
00:02:51,400 --> 00:02:53,400
做了很多自己的创新

81
00:02:53,400 --> 00:02:54,600
有了AI编译器

82
00:02:54,600 --> 00:02:56,800
AI编译器的输入是计算图

83
00:02:56,800 --> 00:03:00,200
而计算图是我们这些AI框架所产生的

84
00:03:00,200 --> 00:03:02,800
包括Mathbot、TensorFlow、PyTorch、PyTorch Battle

85
00:03:02,800 --> 00:03:04,400
等不同的AI框架

86
00:03:04,400 --> 00:03:06,400
都会去产生我们的计算图

87
00:03:06,400 --> 00:03:08,200
对计算图不了解的同学们

88
00:03:08,200 --> 00:03:10,400
可以回头看一下我们的计算图的系列

89
00:03:10,400 --> 00:03:11,800
有了计算图之后

90
00:03:11,800 --> 00:03:14,600
我们就去了解AI框架

91
00:03:14,600 --> 00:03:17,400
下面我们来看看AI编译器的挑战

92
00:03:17,400 --> 00:03:18,600
关于AI编译器的挑战

93
00:03:18,600 --> 00:03:19,800
其实有几条内容是

94
00:03:19,800 --> 00:03:22,400
载之于金雪芳老师在知乎上面的一篇文章

95
00:03:22,400 --> 00:03:23,800
这里面我根据个人的理解

96
00:03:23,800 --> 00:03:25,200
做了一些修改和补充

97
00:03:25,200 --> 00:03:27,400
第一个就是动态SHIP的问题

98
00:03:27,400 --> 00:03:29,600
这个问题在生腾硬件上面

99
00:03:29,600 --> 00:03:31,800
对接到PyTorch动态图的时候

100
00:03:31,800 --> 00:03:33,200
非常的突出

101
00:03:33,200 --> 00:03:34,800
也是非常难解决的

102
00:03:34,800 --> 00:03:36,400
第二个就是Mathbot底层的

103
00:03:36,400 --> 00:03:37,400
AI编译器的时候

104
00:03:37,400 --> 00:03:40,400
去解决动态SHIP也是花了不少的精力

105
00:03:40,400 --> 00:03:43,600
第二点就是PyTorch的编译的时候的静态化

106
00:03:43,600 --> 00:03:45,400
这个也是很难解决的问题

107
00:03:45,400 --> 00:03:47,600
第三点就是如何通过AI编译器

108
00:03:47,600 --> 00:03:51,000
去压榨更多或者去发挥更多的硬件的性能

109
00:03:51,000 --> 00:03:54,400
第四点就是如何处理神经网络的一些特性

110
00:03:54,400 --> 00:03:56,200
例如自动微分和自动并行

111
00:03:56,200 --> 00:03:59,600
第五点就是如何去兼顾应用性和性能

112
00:04:02,600 --> 00:04:04,400
下面我们来看看第一点

113
00:04:04,400 --> 00:04:07,200
就是动态SHIP和动态计算图的问题

114
00:04:07,200 --> 00:04:09,200
那比较突出的就是动态SHIP

115
00:04:09,200 --> 00:04:11,200
现在一些主流的AI编译器

116
00:04:11,200 --> 00:04:14,000
主要是针对我们的静态SHIP进行输入

117
00:04:14,000 --> 00:04:15,600
然后完成整个编译化的

118
00:04:15,600 --> 00:04:16,800
包括我们的TVM

119
00:04:16,800 --> 00:04:19,600
后来TVM其实也推出了一些解决动态SHIP的方案

120
00:04:19,600 --> 00:04:22,400
但一开始确实也只是面向于静态SHIP

121
00:04:22,400 --> 00:04:24,800
当计算图里面含有空字流的时候

122
00:04:24,800 --> 00:04:26,800
也是一个很大的挑战

123
00:04:26,800 --> 00:04:28,200
例如在NLP演物里面

124
00:04:28,200 --> 00:04:31,400
我输入给网络模型的一个序列的长度是不固定的

125
00:04:31,400 --> 00:04:32,400
有些句子长一点

126
00:04:32,400 --> 00:04:34,000
有些句子短一点

127
00:04:34,000 --> 00:04:36,800
这个时候就会引起大量的动态SHIP需求

128
00:04:36,800 --> 00:04:38,800
第三点就是有些AI演物

129
00:04:38,800 --> 00:04:42,200
例如检测模型的SSP金字塔的时候

130
00:04:42,200 --> 00:04:45,000
很难通过把动态SHIP改成静态SHIP

131
00:04:45,000 --> 00:04:46,600
然后去解决的

132
00:04:47,600 --> 00:04:48,800
第二个比较大的挑战

133
00:04:48,800 --> 00:04:50,800
就是Python数据语言的static

134
00:04:50,800 --> 00:04:52,000
就是它的静态化

135
00:04:52,000 --> 00:04:54,400
在AI编辑器业界最常用的一种方法

136
00:04:54,400 --> 00:04:56,400
就是通过JIT即时编译

137
00:04:56,400 --> 00:04:59,200
让Python程序执行一个静态的优化

138
00:04:59,200 --> 00:05:01,000
从而提升我们的性能

139
00:05:01,000 --> 00:05:03,400
而业界做一个JIT的通常的方法

140
00:05:03,400 --> 00:05:07,000
第一点就是提供一个Python的JIT的虚拟机

141
00:05:07,000 --> 00:05:10,000
第二种就是通过修饰符的方式

142
00:05:10,600 --> 00:05:14,000
下面我们来展开理解一下Python去执行的时候

143
00:05:14,000 --> 00:05:17,000
首先我们拿到一个Python的原代码.py

144
00:05:17,000 --> 00:05:19,000
然后Python的编译器

145
00:05:19,000 --> 00:05:21,200
会对原代码进行一个编译的处理

146
00:05:21,200 --> 00:05:22,600
所以我们会看到编译处理完

147
00:05:22,800 --> 00:05:25,400
经常会看到一些字节码.pyc

148
00:05:25,400 --> 00:05:28,000
在我们的文件夹里面去存着

149
00:05:28,000 --> 00:05:29,600
就是我运行一个.py的时候

150
00:05:29,600 --> 00:05:31,400
经常会出现一些.pyc

151
00:05:31,400 --> 00:05:33,200
一开始也是觉得莫名其妙的

152
00:05:33,200 --> 00:05:35,400
后来理解了Python的一个编译流程之后

153
00:05:35,600 --> 00:05:37,200
就搞懂了.pyc

154
00:05:37,400 --> 00:05:38,600
这是它的字节码

155
00:05:38,600 --> 00:05:40,400
然后产生完字节码之后

156
00:05:40,600 --> 00:05:42,600
就给我们的编译器进行编译处理

157
00:05:42,600 --> 00:05:44,800
最后程序再进行执行的

158
00:05:44,800 --> 00:05:47,000
而Python在执行的时候有两种方式

159
00:05:47,000 --> 00:05:48,400
第一种就是产生一个字节码

160
00:05:48,400 --> 00:05:49,800
然后通过Python的虚拟机

161
00:05:49,800 --> 00:05:51,400
给我们的硬件去执行

162
00:05:51,600 --> 00:05:54,600
第二种就是提供JIT即时编译的编译器

163
00:05:54,600 --> 00:05:55,800
然后产生一个机器码

164
00:05:55,800 --> 00:05:57,800
然后直接给我们的硬件去执行

165
00:05:57,800 --> 00:05:59,600
所以说Python在执行的时候

166
00:05:59,600 --> 00:06:00,600
有两种执行方式

167
00:06:00,600 --> 00:06:02,600
而左边的这种是最通用的

168
00:06:02,800 --> 00:06:04,400
刚才提到的一点就是

169
00:06:04,400 --> 00:06:07,000
Python它提供一个JIT的虚拟机

170
00:06:07,000 --> 00:06:09,600
好像Cpython它就混合了编译

171
00:06:09,600 --> 00:06:11,000
还有解析的功能

172
00:06:11,000 --> 00:06:12,600
将Python的原代码

173
00:06:12,600 --> 00:06:14,800
直接翻译成一系列的中间字节码

174
00:06:14,800 --> 00:06:16,000
就是.pyc

175
00:06:16,000 --> 00:06:18,400
然后由Cpython的一个内部虚拟机

176
00:06:18,400 --> 00:06:21,000
然后不断的去执行Python的一些指令

177
00:06:21,000 --> 00:06:22,800
第二种就是pypy

178
00:06:22,800 --> 00:06:24,600
直接利用JIT的即时编译器

179
00:06:24,600 --> 00:06:26,200
来去执行Python的代码

180
00:06:26,200 --> 00:06:28,000
假设我们现在有一些Python的原代码

181
00:06:28,000 --> 00:06:30,200
我按一些Python原代码的快

182
00:06:30,200 --> 00:06:31,200
进行一个编译

183
00:06:31,200 --> 00:06:32,600
编译成我们的字节码

184
00:06:32,600 --> 00:06:33,600
编译成字节码的时候

185
00:06:33,800 --> 00:06:35,200
JIT编译器去执行

186
00:06:35,200 --> 00:06:37,000
我们的Python的原代码的时候

187
00:06:37,000 --> 00:06:38,400
实际上程序执行的

188
00:06:38,400 --> 00:06:40,400
是我们已经编译好的字节码

189
00:06:41,000 --> 00:06:43,400
我们提到了Python静态化的第二种方案

190
00:06:43,400 --> 00:06:45,000
就是通过修饰符

191
00:06:45,000 --> 00:06:46,200
对于修饰符这种

192
00:06:46,200 --> 00:06:48,800
其实我们在计算图的系列里面

193
00:06:48,800 --> 00:06:51,400
其实已经简单的去提过了

194
00:06:51,400 --> 00:06:52,600
这里面有两种

195
00:06:52,600 --> 00:06:54,000
第一种就是trackbase

196
00:06:54,000 --> 00:06:56,400
类似于PyTorch的fs的功能

197
00:06:57,000 --> 00:06:58,800
第二种就是asttransfer

198
00:06:58,800 --> 00:07:00,200
基于原码的转换

199
00:07:00,200 --> 00:07:02,200
类似于PyTorch的JIT的功能

200
00:07:02,200 --> 00:07:04,600
可以看到这下面的左右

201
00:07:04,600 --> 00:07:06,000
都是PyTorch的代码

202
00:07:06,000 --> 00:07:10,200
而PyTorch fs就是tracing based的这种方式

203
00:07:10,200 --> 00:07:12,000
我前面加一个修饰符

204
00:07:12,000 --> 00:07:13,000
然后通过修饰符

205
00:07:13,000 --> 00:07:15,400
一条一条语句去做一个跟踪

206
00:07:15,400 --> 00:07:16,800
然后把它做一个翻译

207
00:07:16,800 --> 00:07:18,800
第二种就是asttransfer

208
00:07:18,800 --> 00:07:20,000
就是原码转换

209
00:07:20,000 --> 00:07:22,400
我现在先写了一个函数

210
00:07:22,400 --> 00:07:23,800
然后通过一个修饰符

211
00:07:23,800 --> 00:07:25,000
对我们刚才的函数

212
00:07:25,000 --> 00:07:26,600
进行一个原码转换

213
00:07:28,000 --> 00:07:30,200
PyTorch它是一个动态图表达

214
00:07:30,200 --> 00:07:32,200
非常灵活的一个AI框架

215
00:07:32,200 --> 00:07:33,600
在它的动态图方案里面

216
00:07:33,600 --> 00:07:36,200
它是没有任何AI编辑相关的功能

217
00:07:36,200 --> 00:07:38,000
但是为了提升执行效率

218
00:07:38,200 --> 00:07:40,400
所以PyTorch推出了FX、JIT

219
00:07:40,400 --> 00:07:42,000
不同的一些方案

220
00:07:42,000 --> 00:07:44,200
但这些方案多多少少都有些问题

221
00:07:44,200 --> 00:07:45,400
不是说非常纯粹

222
00:07:45,400 --> 00:07:47,400
而在最新的PyTorch 2.0里面

223
00:07:47,400 --> 00:07:49,600
他们说已经解决了这个问题了

224
00:07:49,600 --> 00:07:52,000
所以我希望能够后面有点时间

225
00:07:52,000 --> 00:07:53,600
去分析PyTorch 2.0

226
00:07:53,600 --> 00:07:56,000
这一块机制到底是怎么运作的

227
00:07:56,800 --> 00:07:58,800
下面我们看回PyTorch static

228
00:07:58,800 --> 00:08:00,800
就是静态化所遇到的一些挑战

229
00:08:00,800 --> 00:08:03,200
第一个就是类型的推导

230
00:08:03,200 --> 00:08:04,600
我们从动态语言

231
00:08:04,600 --> 00:08:06,000
去编译成一个静态语言

232
00:08:06,000 --> 00:08:08,200
肯定是会有很多类型的转换

233
00:08:08,200 --> 00:08:09,200
我们的静态类型

234
00:08:09,200 --> 00:08:11,000
不一定把所有的动态类型

235
00:08:11,000 --> 00:08:12,200
都能够表达出来

236
00:08:12,200 --> 00:08:14,200
第二个就是控制流的表达

237
00:08:14,200 --> 00:08:16,000
就是if else while for等控制流

238
00:08:16,000 --> 00:08:18,000
其实你很难通过静态图

239
00:08:18,000 --> 00:08:19,400
完完全全的表示的

240
00:08:19,400 --> 00:08:21,000
现在比较好的一种方案就是

241
00:08:21,000 --> 00:08:22,200
子图的展开

242
00:08:22,200 --> 00:08:24,200
第三种就是灵活的语言表达

243
00:08:24,200 --> 00:08:25,600
和数据类型的转换

244
00:08:25,600 --> 00:08:26,400
因为Python里面

245
00:08:26,400 --> 00:08:28,400
有非常多灵活的表示

246
00:08:28,400 --> 00:08:30,400
其实还是归根于第一个问题

247
00:08:30,400 --> 00:08:31,800
就是动态的类型

248
00:08:31,800 --> 00:08:33,000
怎么把它静态化

249
00:08:33,000 --> 00:08:35,800
第四个就是JIT的编译性能

250
00:08:35,800 --> 00:08:37,400
不管是Tracing Base

251
00:08:37,400 --> 00:08:38,800
还是STTransformer

252
00:08:38,800 --> 00:08:39,800
就是源码转换

253
00:08:39,800 --> 00:08:43,600
我们都需要额外增加编译的开销

254
00:08:43,600 --> 00:08:46,000
有编译肯定会损失性能

255
00:08:46,000 --> 00:08:47,600
我们既想提升性能

256
00:08:47,600 --> 00:08:49,600
但是又忍不住编译的开销

257
00:08:49,600 --> 00:08:51,400
这是个矛盾

258
00:08:51,400 --> 00:08:53,200
假设我们开发一个Metaspore的

259
00:08:53,200 --> 00:08:53,800
AI框架

260
00:08:53,800 --> 00:08:55,800
我们希望对接到不同的硬件上面

261
00:08:55,800 --> 00:08:57,400
但是不可能每个硬件

262
00:08:57,400 --> 00:08:59,200
我们自己都会去做一些

263
00:08:59,200 --> 00:09:00,600
开发和性能的优化

264
00:09:00,600 --> 00:09:02,800
所以这里面极度的去依赖于

265
00:09:02,800 --> 00:09:03,800
AI的编译性

266
00:09:04,000 --> 00:09:06,200
去发挥不同硬件的算力

267
00:09:06,200 --> 00:09:07,600
在面向专用硬件

268
00:09:07,600 --> 00:09:08,400
AI编译器

269
00:09:08,400 --> 00:09:11,200
其实也是遇到了非常大的挑战

270
00:09:11,200 --> 00:09:12,400
例如性能的优化了

271
00:09:12,400 --> 00:09:14,000
依赖于图算的融合

272
00:09:14,000 --> 00:09:16,000
就是我把小算子合成一个大算子

273
00:09:16,000 --> 00:09:17,000
那我算了起来

274
00:09:17,000 --> 00:09:18,800
可能更坏少了一些IO

275
00:09:18,800 --> 00:09:19,600
面向DSA

276
00:09:19,600 --> 00:09:21,400
第二个比较大的挑战

277
00:09:21,400 --> 00:09:22,800
就是优化的复杂度

278
00:09:23,000 --> 00:09:25,800
是随着我们的硬件的复杂度而提升的

279
00:09:25,800 --> 00:09:26,800
我们现在的硬件

280
00:09:26,800 --> 00:09:28,600
会有标量、向量、脏量

281
00:09:28,600 --> 00:09:30,000
还有一些加速的指令

282
00:09:30,000 --> 00:09:32,000
还有一些多级的存储结构

283
00:09:32,200 --> 00:09:34,600
所以说我们在做一些kernel的优化

284
00:09:34,600 --> 00:09:35,800
我们在写编辑器的时候

285
00:09:35,800 --> 00:09:37,800
或者底层code gene的时候

286
00:09:37,800 --> 00:09:40,400
其实我们的复杂度是从指数是增长的

287
00:09:40,400 --> 00:09:42,600
随着我们的硬件的复杂度提升

288
00:09:42,600 --> 00:09:44,600
专用的优化也会提出来

289
00:09:44,600 --> 00:09:45,800
而这些专用的优化

290
00:09:46,000 --> 00:09:47,400
又不能做好一个分化

291
00:09:47,400 --> 00:09:50,000
所以这里面其实是挺矛盾的

292
00:09:50,200 --> 00:09:52,800
第四点就是特殊的优化

293
00:09:52,800 --> 00:09:55,000
我们这里面以自动并行为例子

294
00:09:55,000 --> 00:09:57,000
现在大模型非常火

295
00:09:57,000 --> 00:09:58,200
包括我们Diffusion Model

296
00:09:58,200 --> 00:09:59,600
LM语言大模型

297
00:10:00,000 --> 00:10:01,800
其实你都离不开自动并行

298
00:10:01,800 --> 00:10:03,600
而自动并行你就会遇到一些

299
00:10:03,600 --> 00:10:05,400
内存墙、性能墙的问题

300
00:10:05,400 --> 00:10:07,000
所以业界面前Scaleup

301
00:10:07,000 --> 00:10:09,200
就提出了很多并行的方式

302
00:10:09,200 --> 00:10:10,200
就多维孔和并行

303
00:10:10,200 --> 00:10:12,400
数据并行、战略并行、流水线并行

304
00:10:12,400 --> 00:10:15,200
Scaleup会做一些重计算混合精度

305
00:10:15,400 --> 00:10:18,200
遇到最大的问题还是效率墙

306
00:10:18,200 --> 00:10:21,600
效率墙说起来感觉比较抽象

307
00:10:21,600 --> 00:10:23,200
其实我们简单的理解一下

308
00:10:23,200 --> 00:10:25,400
就是工程师需要去自动的

309
00:10:25,400 --> 00:10:27,400
去配置上面的这些算法

310
00:10:27,400 --> 00:10:29,000
那你要配置这些算法

311
00:10:29,000 --> 00:10:30,800
对我们的算法工程师来说

312
00:10:30,800 --> 00:10:32,000
可能会比较轻松

313
00:10:32,000 --> 00:10:33,600
但对于系统工程师来说

314
00:10:33,600 --> 00:10:34,800
可能会比较陌生

315
00:10:34,800 --> 00:10:36,600
我连这些算法听都没听过

316
00:10:37,000 --> 00:10:38,200
所以说算法工程师

317
00:10:38,200 --> 00:10:40,400
跟系统工程师之间有一个gap

318
00:10:40,400 --> 00:10:42,000
怎么去解决人的问题

319
00:10:42,000 --> 00:10:43,600
去把我们人释放出来

320
00:10:43,600 --> 00:10:46,200
真正的去依赖于我们的编译和优化

321
00:10:46,200 --> 00:10:47,800
去解决并行配置的问题

322
00:10:47,800 --> 00:10:49,200
未来我们的AI框架

323
00:10:49,200 --> 00:10:51,000
能不能自动微分做得更好

324
00:10:51,000 --> 00:10:52,800
我们现在所提的自动微分

325
00:10:52,800 --> 00:10:55,200
还是基于反向传播的一阶梯岛

326
00:10:55,200 --> 00:10:58,000
那未来HPC这种高性能场景

327
00:10:58,000 --> 00:11:00,800
我们怎么去解决我们的高阶微分

328
00:11:00,800 --> 00:11:03,400
能不能会有更高效的方式

329
00:11:03,400 --> 00:11:05,400
而不是通过我们现在去模拟

330
00:11:05,400 --> 00:11:07,400
海神矩阵的方式去求解呢

331
00:11:07,400 --> 00:11:08,800
第五点挑战就是

332
00:11:08,800 --> 00:11:10,800
应用性和性能的兼顾

333
00:11:10,800 --> 00:11:13,400
其实我们现在跟AI框架的边界

334
00:11:13,400 --> 00:11:14,800
是不同的

335
00:11:14,800 --> 00:11:18,000
接着就是对用户透明性的问题

336
00:11:18,000 --> 00:11:19,200
虽然透明性这个词

337
00:11:19,200 --> 00:11:21,400
还是比较隐晦比较学术的

338
00:11:21,400 --> 00:11:22,400
简单的来说

339
00:11:22,400 --> 00:11:24,600
就是现在部分的AI编译器

340
00:11:24,600 --> 00:11:25,400
我直接说了

341
00:11:25,400 --> 00:11:28,200
就是TVM它并不是真正完全自动的

342
00:11:28,200 --> 00:11:30,400
它现在的一些性能的优化

343
00:11:30,400 --> 00:11:32,800
还是基于我们的模板的实现

344
00:11:32,800 --> 00:11:34,000
那模板实现的越好

345
00:11:34,000 --> 00:11:35,200
它优化的越好

346
00:11:35,200 --> 00:11:37,800
后面TVM我们也会详细的去展开

347
00:11:37,800 --> 00:11:39,400
所以这里面听不懂也没关系

348
00:11:39,400 --> 00:11:40,600
我们大概了解一下

349
00:11:40,600 --> 00:11:42,000
后面回头再来看

350
00:11:42,000 --> 00:11:43,800
可能就会豁然开朗了

351
00:11:43,800 --> 00:11:45,200
关于透明性的第二点就是

352
00:11:45,200 --> 00:11:47,400
现在我们对AI编译器的

353
00:11:47,400 --> 00:11:48,400
一些抽象的pass

354
00:11:48,400 --> 00:11:49,600
或者抽象的优化

355
00:11:49,600 --> 00:11:52,000
没有办法很好的去满足我们新的硬件

356
00:11:52,000 --> 00:11:53,400
或者新的算子

357
00:11:53,400 --> 00:11:54,400
这个时候

358
00:11:54,400 --> 00:11:56,800
最近的Pytorch 2.0就推出一个特性

359
00:11:56,800 --> 00:11:59,200
就是我只支持250多个基础算子

360
00:11:59,200 --> 00:12:01,200
其他算子都使用Python

361
00:12:01,200 --> 00:12:02,600
去写底层的硬件

362
00:12:02,600 --> 00:12:04,400
我觉得这个特性还是很有意思的

363
00:12:04,400 --> 00:12:06,600
到时候可以跟大家一起解读一下

364
00:12:06,600 --> 00:12:08,800
后面就是关于性能的一个问题

365
00:12:08,800 --> 00:12:11,200
我们的编译会有开销

366
00:12:11,200 --> 00:12:12,000
想象一下

367
00:12:12,000 --> 00:12:15,200
其实我以前是一个算法的工程师

368
00:12:15,200 --> 00:12:17,400
但是我其实并不喜欢用AI编译器

369
00:12:17,400 --> 00:12:19,600
因为我希望能够我一按个回车

370
00:12:19,600 --> 00:12:21,200
像Pytorch一样没有编译器

371
00:12:21,200 --> 00:12:23,400
我在调试的阶段并不关心我的性能

372
00:12:23,400 --> 00:12:26,400
我更关心我的模型能不能够跑得通

373
00:12:26,400 --> 00:12:28,600
但是我的开发跟部署的情况

374
00:12:28,600 --> 00:12:30,400
不是在同一个模式下面的

375
00:12:30,400 --> 00:12:32,200
这时候就推出了新的功能

376
00:12:32,200 --> 00:12:34,600
就是我们之前讲的动静统一

377
00:12:34,600 --> 00:12:36,400
在我们的计算图系列里面

378
00:12:36,400 --> 00:12:38,400
在整体AI编译器结束之前

379
00:12:38,400 --> 00:12:40,200
我想再抛出几个问题

380
00:12:40,200 --> 00:12:43,200
跟大家一起去思考碰撞

381
00:12:43,200 --> 00:12:46,200
AI编译器现在处于一个高速发展的状态

382
00:12:46,200 --> 00:12:48,600
所以它有很多未知的问题

383
00:12:48,600 --> 00:12:50,400
等着我们一起去破解

384
00:12:50,400 --> 00:12:52,200
等着我们去发一些paper

385
00:12:52,400 --> 00:12:55,200
第一个问题就是我们说了stage 3的时候

386
00:12:55,200 --> 00:12:56,400
就在第三个阶段

387
00:12:56,400 --> 00:12:58,600
我们希望图算能够统一表达

388
00:12:58,600 --> 00:13:01,000
但是图和算子真的能够统一表达吗

389
00:13:01,000 --> 00:13:04,200
我们真的能够统一整个编译油画的流程吗

390
00:13:04,200 --> 00:13:07,000
而形成一个通用的AI编译器吗

391
00:13:07,000 --> 00:13:09,200
不管是训练大模型还是小模型

392
00:13:09,200 --> 00:13:11,600
我们都希望尽可能的去利用云

393
00:13:11,600 --> 00:13:13,400
去利用AI集群去训练

394
00:13:13,400 --> 00:13:15,000
让我们的训练时间更短

395
00:13:15,000 --> 00:13:16,200
训练的更快

396
00:13:16,200 --> 00:13:18,400
但是一涉及到集群

397
00:13:18,400 --> 00:13:20,800
我们真的能够做到并行吗

398
00:13:20,800 --> 00:13:22,200
机器跟机器之间

399
00:13:22,200 --> 00:13:23,600
机器跟机柜之间

400
00:13:23,600 --> 00:13:25,000
机柜跟机柜之间

401
00:13:25,000 --> 00:13:26,400
网段跟网段之间

402
00:13:26,400 --> 00:13:29,000
我们有大量的问题等着我们去解决

403
00:13:29,000 --> 00:13:31,400
真的自动并行可行不

404
00:13:31,400 --> 00:13:34,600
AI芯片真的需要AI编译器吗

405
00:13:34,600 --> 00:13:36,600
没有AI编译器又如何呢

406
00:13:36,600 --> 00:13:38,600
像英伟达它就没有自己的AI编译器

407
00:13:38,600 --> 00:13:40,000
但是它卖的非常火

408
00:13:40,000 --> 00:13:41,600
它现在是业界的标杆

409
00:13:41,600 --> 00:13:44,200
它只有一个传统的MVCC的编译器

410
00:13:44,200 --> 00:13:47,600
去把CUDA kernel编译成GPU能执行的代码

411
00:13:47,600 --> 00:13:50,000
我觉得这个是个革命性的问题

412
00:13:50,000 --> 00:13:53,200
说好不小心把自己给革命掉了

413
00:13:53,200 --> 00:13:54,200
最后的最后

414
00:13:54,200 --> 00:13:56,400
我们来看看AI编译器的未来

415
00:13:56,400 --> 00:13:58,600
虽然我们现在有充满着很多未知的问题

416
00:13:58,600 --> 00:14:00,400
但是未来还是很可观的

417
00:14:00,400 --> 00:14:02,200
我们未来的编译器的形态

418
00:14:02,200 --> 00:14:03,800
可能会分开推理和训练

419
00:14:03,800 --> 00:14:04,600
我们也讲了

420
00:14:04,600 --> 00:14:05,200
另外的话

421
00:14:05,200 --> 00:14:07,600
IR的形态可能会做一个统一

422
00:14:07,600 --> 00:14:08,200
另外的话

423
00:14:08,200 --> 00:14:09,800
我们希望能够做一个自动并行

424
00:14:09,800 --> 00:14:10,800
还有自动微分

425
00:14:10,800 --> 00:14:13,800
最后一个就是大家都很关心的

426
00:14:13,800 --> 00:14:15,800
kernel能不能够自动生成

427
00:14:15,800 --> 00:14:17,400
减少我的人工成本

428
00:14:17,400 --> 00:14:18,800
那这个就是我们未来

429
00:14:18,800 --> 00:14:21,800
希望去真正去解决的一些问题

430
00:14:21,800 --> 00:14:23,600
希望大家了解完这个系列之后

431
00:14:23,600 --> 00:14:25,400
能够参与到华为生腾

432
00:14:25,400 --> 00:14:28,000
华为图灵解决方案的建设当中

433
00:14:29,300 --> 00:14:29,700
好了

434
00:14:29,700 --> 00:14:30,300
谢谢各位

