1
00:00:00,000 --> 00:00:06,360
哈喽大家好

2
00:00:06,360 --> 00:00:08,840
我们来到AI编译器系列里面的

3
00:00:08,840 --> 00:00:11,440
AI编译器里面真正的第二个内容

4
00:00:11,440 --> 00:00:13,640
就是AI编译器的架构的发展

5
00:00:13,640 --> 00:00:17,600
我们会重点去讲讲AI编译器的一个具体的架构

6
00:00:18,600 --> 00:00:20,040
当然了在AI编译器里面

7
00:00:20,040 --> 00:00:22,040
它的架构并不是一成不变的

8
00:00:22,040 --> 00:00:23,680
而是随着时间的推移

9
00:00:23,680 --> 00:00:26,400
还有随着技术的发展而不断的演进的

10
00:00:26,400 --> 00:00:28,520
这里面我们分开三个阶段来看

11
00:00:28,560 --> 00:00:31,120
现在我们正式的进入我们的内容

12
00:00:31,120 --> 00:00:32,720
什么是AI编译器

13
00:00:39,560 --> 00:00:41,320
其实我抛出的这个问题

14
00:00:41,320 --> 00:00:43,440
其实想跟大家一起去探讨的

15
00:00:43,440 --> 00:00:47,000
我认为AI编译器主要分为两个场景

16
00:00:47,000 --> 00:00:49,640
两个场景的编译器的内容其实是不太一样的

17
00:00:49,640 --> 00:00:51,000
第一个就是推理了

18
00:00:51,000 --> 00:00:54,560
我们现在大部分应用场景都在推理里面的

19
00:00:54,680 --> 00:00:57,200
它的输入是AI框架训练出来的

20
00:00:57,200 --> 00:00:59,920
我们的模型文件或者模型权重

21
00:00:59,920 --> 00:01:03,640
输出是能够在不同硬件高效执行的程序

22
00:01:03,640 --> 00:01:06,560
编译器就是去链接我们的文件

23
00:01:06,560 --> 00:01:08,760
还有我们的硬件之间的一个关系

24
00:01:10,280 --> 00:01:13,040
第二个就是指训练场景

25
00:01:13,040 --> 00:01:15,080
训练场景跟推理场景不太一样

26
00:01:15,080 --> 00:01:19,400
训练场景输入的是高级语言表示的神经网络的代码

27
00:01:19,400 --> 00:01:20,920
这句话听上去很隐晦

28
00:01:20,920 --> 00:01:22,640
其实就是很简单

29
00:01:22,640 --> 00:01:26,160
我用python写了一些代码去表示神经网络

30
00:01:26,160 --> 00:01:28,320
然后我丢给我们的AI框架去处理

31
00:01:28,320 --> 00:01:31,160
然后AI框架就能够在不同的硬件上面

32
00:01:31,160 --> 00:01:33,600
高效的去执行我们的训练任务了

33
00:01:33,600 --> 00:01:36,760
这种情况就是我们训练场景的AI编译器

34
00:01:36,760 --> 00:01:39,160
去连接我们的高级语言的代码

35
00:01:39,160 --> 00:01:41,040
然后在不同硬件上面去执行

36
00:01:41,040 --> 00:01:44,440
所以我觉得主要是分为两个场景去探讨的

37
00:01:45,360 --> 00:01:47,360
在正式进入具体的内容之前

38
00:01:47,480 --> 00:01:49,240
我想刨除三个问题

39
00:01:49,280 --> 00:01:52,600
跟大家一起去学习和汇报一下

40
00:01:52,600 --> 00:01:54,760
第一个就是什么是训练场景

41
00:01:54,760 --> 00:01:57,240
我们刚才讲了有训练场景有推理场景

42
00:01:57,240 --> 00:01:58,880
它们之间有什么区别了

43
00:01:58,880 --> 00:02:00,640
什么是训练什么是推理

44
00:02:00,640 --> 00:02:03,400
如果不懂得AI的基础的同学

45
00:02:03,520 --> 00:02:07,560
可以翻看我们之前关于AI基础训练的一些内容

46
00:02:07,560 --> 00:02:10,440
也可以现在网上有非常多的相关的资料

47
00:02:10,440 --> 00:02:15,360
第二个就是我们去做AI编译器的人

48
00:02:15,360 --> 00:02:18,440
我们搞系统底层的人为什么要了解算法

49
00:02:18,760 --> 00:02:20,200
有没有必要了解算法

50
00:02:21,000 --> 00:02:24,000
第三个问题就是我是算子开发的人员

51
00:02:24,000 --> 00:02:27,040
我只是对我的kernel去做一些操作

52
00:02:27,040 --> 00:02:29,520
为什么我需要了解编译器

53
00:02:29,880 --> 00:02:31,960
或者我有必要去了解编译器吗

54
00:02:32,400 --> 00:02:34,840
带着这三个问题我们继续往下看

55
00:02:37,480 --> 00:02:39,200
对于什么是AI编译器

56
00:02:39,360 --> 00:02:42,000
我们这里总结了几个关键的特性

57
00:02:42,000 --> 00:02:45,400
这也是参考金雪峰老师的一个思考

58
00:02:45,640 --> 00:02:47,480
下面我们来看看第一个特性

59
00:02:47,600 --> 00:02:50,920
就是AI编译器主要是以python为主的

60
00:02:50,920 --> 00:02:53,160
动态解析语言作为前端

61
00:02:53,520 --> 00:02:56,840
第二个就是我们会有多层的IR的设计

62
00:02:57,520 --> 00:03:01,360
多层的IR可能跟LLVM的那种单一的IR不太一样

63
00:03:01,360 --> 00:03:03,080
因为这里面包括图的编译

64
00:03:03,200 --> 00:03:04,520
我们就会有图的IR

65
00:03:04,520 --> 00:03:06,280
我们需要对算子进行编译

66
00:03:06,280 --> 00:03:08,080
所以会出现算子的IR

67
00:03:08,080 --> 00:03:10,560
所以这里面就已经有两层IR了

68
00:03:10,560 --> 00:03:11,880
最后还有代码生成

69
00:03:12,000 --> 00:03:14,320
代码生成又是另外一套IR

70
00:03:14,320 --> 00:03:15,560
就CoreGene的那一套

71
00:03:16,160 --> 00:03:19,040
第三个特性主要是它是面向神经网络

72
00:03:19,040 --> 00:03:21,560
面向深度学习的特定的优化

73
00:03:21,560 --> 00:03:25,840
也就是Domain Specific DS面向特定领域的

74
00:03:26,120 --> 00:03:29,840
第四种就会去支持很多不同的芯片

75
00:03:30,200 --> 00:03:33,280
这个就是我们AI编译器的四个重要的特点

76
00:03:35,280 --> 00:03:38,040
下面这个图有没有点似曾相识的感觉

77
00:03:38,480 --> 00:03:41,720
其实这个图是取自于Mathball总体架构师

78
00:03:41,720 --> 00:03:42,720
我们叫总架

79
00:03:42,720 --> 00:03:45,360
然后金雪峰老师在知乎上面的一篇文章

80
00:03:45,560 --> 00:03:47,680
然后我对它加以了一些划分

81
00:03:47,680 --> 00:03:48,840
还有进行了一些

82
00:03:49,720 --> 00:03:51,720
我个人觉得可以修改的小地方

83
00:03:52,000 --> 00:03:54,040
然后我们看看主要是分为三个阶段

84
00:03:54,040 --> 00:03:56,320
Stage1就类似于TensorFlow的

85
00:03:56,320 --> 00:03:57,920
可能TensorFlow出现的比较早

86
00:03:57,920 --> 00:04:00,200
然后是第一个把AI编译器

87
00:04:00,200 --> 00:04:03,840
引入到AI框架里面的最早的一个框架了

88
00:04:04,120 --> 00:04:07,840
好像咖啡那种可能是游离在Stage1之外的

89
00:04:07,840 --> 00:04:09,840
或者在Stage1准备之前的

90
00:04:10,160 --> 00:04:12,320
后来我们现在到了Stage2之后

91
00:04:12,480 --> 00:04:16,680
我们可以发现有非常多不同的AI框架的涌现

92
00:04:16,680 --> 00:04:18,000
包括我们的PyTorch

93
00:04:18,000 --> 00:04:20,360
虽然PyTorch有点意思

94
00:04:20,360 --> 00:04:23,080
最近的出现的PyTorch2.0已经发布了

95
00:04:23,440 --> 00:04:24,760
然后到Stage3

96
00:04:24,880 --> 00:04:26,880
Stage3可能现在还没有达到

97
00:04:26,880 --> 00:04:28,640
而是在一个过渡的阶段

98
00:04:28,640 --> 00:04:30,200
那Stage3未来会怎么样

99
00:04:30,200 --> 00:04:32,120
我们做一些简单的畅想

100
00:04:32,240 --> 00:04:34,720
现在我们跟着这个图一起来去看一下

101
00:04:34,720 --> 00:04:37,520
Stage1,Stage2,Stage3各有什么不同

102
00:04:38,280 --> 00:04:39,960
首先我们叫做Stage1

103
00:04:40,000 --> 00:04:42,440
是一个朴素的AI编译器

104
00:04:42,840 --> 00:04:43,920
既然是朴素

105
00:04:43,920 --> 00:04:46,640
它存在于TensorFlow的一个早期的版本

106
00:04:46,640 --> 00:04:48,920
基于神经网络的编程模型

107
00:04:48,960 --> 00:04:51,200
进行了图、计算图

108
00:04:51,200 --> 00:04:53,520
还有算子两层按压的抽象

109
00:04:54,160 --> 00:04:56,520
第一层就是图层

110
00:04:56,760 --> 00:04:57,760
像TensorFlow早期

111
00:04:57,960 --> 00:05:00,520
它采用的是一个声明式的编程方式

112
00:05:01,280 --> 00:05:03,880
主要是以静态图的方式去执行

113
00:05:03,880 --> 00:05:04,840
然后在执行之前

114
00:05:04,920 --> 00:05:07,920
会做一些硬件无关和硬件相关的编译优化

115
00:05:08,120 --> 00:05:11,040
这里面的编译优化主要是针对图结构

116
00:05:11,040 --> 00:05:14,000
对我们的神经网络做一些优化和融合

117
00:05:14,520 --> 00:05:15,480
我们可以看一下

118
00:05:15,480 --> 00:05:17,680
假设现在这里面有非常多的算子

119
00:05:17,960 --> 00:05:19,200
每一次我下发的时候

120
00:05:19,200 --> 00:05:21,320
每一次让硬件去执行的时候

121
00:05:21,600 --> 00:05:22,520
每执行一个算子

122
00:05:22,520 --> 00:05:23,960
它就有它的输出

123
00:05:23,960 --> 00:05:25,480
就要占用我们的IO

124
00:05:25,840 --> 00:05:28,000
我们能不能做一些算子的融合

125
00:05:28,240 --> 00:05:29,800
做一些提前内存的分配

126
00:05:30,320 --> 00:05:32,520
所以这个就是对图层的优化

127
00:05:33,040 --> 00:05:36,400
第二个就是算子层面的优化

128
00:05:36,800 --> 00:05:37,800
在早期的版本

129
00:05:38,000 --> 00:05:40,160
算子层其实没有编译的概念

130
00:05:40,160 --> 00:05:42,760
主要是通过手写kernel的方式

131
00:05:42,760 --> 00:05:45,080
例如在英伟达的GPU上面

132
00:05:45,240 --> 00:05:47,320
可能会提供了一些CUDA

133
00:05:47,320 --> 00:05:49,160
自己写的一些.cu的算子

134
00:05:49,160 --> 00:05:51,440
或者依赖于cdnm的算子库

135
00:05:52,520 --> 00:05:53,960
从这里面我们可以看到

136
00:05:53,960 --> 00:05:56,800
实际上最开始的朴素的AI编译器

137
00:05:56,960 --> 00:05:58,760
它的概念也是比较简单

138
00:05:58,760 --> 00:06:01,920
我就是对神经网络所表示的计算图

139
00:06:02,200 --> 00:06:04,560
做一些编译相关的优化

140
00:06:04,800 --> 00:06:05,920
这些编译相关的优化

141
00:06:06,040 --> 00:06:08,840
其实我在进入minexport的前几年

142
00:06:09,360 --> 00:06:10,960
也就是199年的时候

143
00:06:11,160 --> 00:06:12,920
其实是亲自去写了这些

144
00:06:12,920 --> 00:06:14,400
大家不要觉得编译器

145
00:06:14,400 --> 00:06:16,880
或者编译底层的parts很难写

146
00:06:16,880 --> 00:06:18,520
它其实就是一个硬规则

147
00:06:18,800 --> 00:06:19,840
我拿到一个图

148
00:06:19,840 --> 00:06:20,880
拿到一个节点

149
00:06:20,880 --> 00:06:22,640
然后我们去自己控制

150
00:06:22,640 --> 00:06:24,800
最难的工作就是去思考

151
00:06:24,800 --> 00:06:27,680
去抽象计算图的基本的pattern

152
00:06:27,680 --> 00:06:29,640
去抽象计算图的模式

153
00:06:30,520 --> 00:06:31,200
刚才我们提到

154
00:06:31,200 --> 00:06:32,880
如果不了解生命是编程

155
00:06:32,880 --> 00:06:33,800
命令是编程

156
00:06:33,800 --> 00:06:35,040
静态图和动态图

157
00:06:35,160 --> 00:06:36,320
我们可以看一下

158
00:06:36,600 --> 00:06:39,440
AI框架基础的第4节和第5节的内容

159
00:06:41,000 --> 00:06:41,600
右边这个图

160
00:06:41,840 --> 00:06:43,520
就是我简单的总结了一下

161
00:06:43,520 --> 00:06:45,440
关于朴素AI编译器的一个

162
00:06:45,800 --> 00:06:46,760
简单的架构

163
00:06:46,760 --> 00:06:49,480
前端可能我们会有一些python的API

164
00:06:49,680 --> 00:06:51,160
这里面以TensorFlow为主

165
00:06:51,160 --> 00:06:52,440
它主要是写了自己

166
00:06:52,440 --> 00:06:54,000
关于计算图的一些表示

167
00:06:54,240 --> 00:06:55,120
用户用的时候

168
00:06:55,280 --> 00:06:58,840
就需要学它这一套python的API的前端

169
00:06:59,080 --> 00:07:00,320
这里面前端python的API

170
00:07:00,440 --> 00:07:02,880
只是借用了python去表达神经网络

171
00:07:03,200 --> 00:07:04,040
但实际上的编译

172
00:07:04,240 --> 00:07:05,760
是用TensorFlow自己的一个

173
00:07:05,760 --> 00:07:07,040
计算图的编译层

174
00:07:07,320 --> 00:07:08,560
所以大家用户用起来

175
00:07:08,680 --> 00:07:09,800
就会觉得很奇怪

176
00:07:09,800 --> 00:07:12,480
我要去学TensorFlow的一个python的解析

177
00:07:12,480 --> 00:07:14,040
跟我们平时写的python代码

178
00:07:14,280 --> 00:07:15,080
好像不太一样

179
00:07:15,440 --> 00:07:17,560
在算子层就直接是到one time

180
00:07:17,560 --> 00:07:18,800
然后去调用Qtn

181
00:07:18,800 --> 00:07:19,600
这些算子库

182
00:07:19,600 --> 00:07:22,120
最后执行在我们的eagle芯片上面

183
00:07:22,400 --> 00:07:25,080
这个就是最简单最朴素的AI编译器

184
00:07:26,600 --> 00:07:28,080
讲完朴素AI编译器之后

185
00:07:28,200 --> 00:07:30,120
我们看一下它具体遇到哪些问题

186
00:07:30,560 --> 00:07:31,880
第一个就是易用性

187
00:07:31,880 --> 00:07:34,440
易用性它在表达上是非python原生的

188
00:07:34,440 --> 00:07:36,720
也就是它不是真正的python的代码

189
00:07:36,720 --> 00:07:38,200
只是类python的代码

190
00:07:38,560 --> 00:07:40,640
这时候开发者就需要利用

191
00:07:40,640 --> 00:07:43,880
TensorFlow提供的API去显示的构图

192
00:07:44,240 --> 00:07:46,040
我必须要知道我构这个图

193
00:07:46,040 --> 00:07:47,400
我需要用哪些API

194
00:07:47,400 --> 00:07:48,800
这个时候是很头痛的

195
00:07:48,800 --> 00:07:50,600
所以大家用TensorFlow学TensorFlow

196
00:07:50,600 --> 00:07:52,640
觉得它难学就在于这一点

197
00:07:53,080 --> 00:07:55,120
第二点就是性能上

198
00:07:55,680 --> 00:07:57,960
越来越多的AI加速器的出现

199
00:07:57,960 --> 00:08:00,840
所以导致我们对性能的挑战很大

200
00:08:00,880 --> 00:08:03,160
我们可能在某些芯片上面跑得特别快

201
00:08:03,400 --> 00:08:05,880
在某些芯片上面跑得特别的慢

202
00:08:06,360 --> 00:08:08,600
第二个就是我们用的是QDN

203
00:08:08,600 --> 00:08:10,480
或者自己手工写的一些算子

204
00:08:10,480 --> 00:08:12,280
而且走的是一个静态图

205
00:08:12,280 --> 00:08:14,280
所以我们的算子的边界

206
00:08:14,280 --> 00:08:16,560
还有算子属性某些特定的情况

207
00:08:16,840 --> 00:08:18,320
是已经明确确定的

208
00:08:18,600 --> 00:08:20,600
例如我在LSTM这个算子里面

209
00:08:20,920 --> 00:08:23,880
发现我的NLP数的序列非常长

210
00:08:24,200 --> 00:08:26,400
这种情况可能超出了我手写Kernel

211
00:08:26,400 --> 00:08:29,240
或者提供的算子的一个边界

212
00:08:29,360 --> 00:08:31,720
这个时候我的执行就会变得非常慢

213
00:08:31,720 --> 00:08:34,000
甚至可能出现精度的问题

214
00:08:34,640 --> 00:08:38,200
第三点就是算子层它没有通过编译

215
00:08:38,200 --> 00:08:40,560
而是直接使用QDN的这种算子

216
00:08:40,560 --> 00:08:42,640
所以硬件厂商提供的优化库

217
00:08:43,200 --> 00:08:43,960
一定是最优的

218
00:08:44,200 --> 00:08:45,080
如果是最优的话

219
00:08:45,080 --> 00:08:47,320
就不会出现类似于PyTorch

220
00:08:47,320 --> 00:08:48,880
类似于PyTorch Atom里面

221
00:08:48,880 --> 00:08:51,360
大量的CUDA手写的算子

222
00:08:52,200 --> 00:08:54,280
所以说硬件厂商提供的算子库

223
00:08:54,280 --> 00:08:55,320
未必是最优的

224
00:08:55,320 --> 00:08:58,440
但是它给我们提供了一个方便的前提

225
00:09:00,240 --> 00:09:04,040
接下来我们就遇到了一个专用的AI编译器

226
00:09:04,040 --> 00:09:05,240
那专用的AI编译器

227
00:09:05,400 --> 00:09:07,600
我们会以PyTorch GX

228
00:09:07,600 --> 00:09:09,000
还有Maspore作为例子

229
00:09:09,320 --> 00:09:10,680
像PyTorch大家都觉得

230
00:09:10,680 --> 00:09:13,360
它没有一个计算图的概念

231
00:09:13,360 --> 00:09:15,200
它其实只是PyTorch的动态图

232
00:09:15,200 --> 00:09:16,600
它没有计算图的概念

233
00:09:16,600 --> 00:09:19,200
但是PyTorch后来又出现了

234
00:09:19,200 --> 00:09:20,400
PyTorch.fx

235
00:09:20,400 --> 00:09:21,880
PyTorch.git

236
00:09:21,880 --> 00:09:24,120
还有包括PyTorch.dynamic

237
00:09:24,120 --> 00:09:25,840
包括现在的PyTorch 2.0

238
00:09:26,040 --> 00:09:29,080
它其实已经出现了自己的一个AI编译器

239
00:09:29,160 --> 00:09:33,040
当然它没有一种说解决方案特别的完善

240
00:09:33,640 --> 00:09:35,640
如果大家有兴趣或者看的人比较多

241
00:09:35,800 --> 00:09:38,080
我们可以单独开一节去讲一讲

242
00:09:38,080 --> 00:09:41,200
PyTorch的一个PyTorch 2.0的一些新特性

243
00:09:41,200 --> 00:09:43,480
最重要的是PyTorch.comply

244
00:09:43,800 --> 00:09:46,080
这个新的重要的特点

245
00:09:46,760 --> 00:09:48,600
现在回过头来我们看看stage2

246
00:09:48,600 --> 00:09:50,960
一个专用AI编译器有什么特点

247
00:09:50,960 --> 00:09:54,320
首先表达上它是类似于PyTorch的灵活表达

248
00:09:54,320 --> 00:09:55,680
就现在不管哪个框架

249
00:09:55,680 --> 00:09:57,440
基本上我觉得大部分的API

250
00:09:57,880 --> 00:09:59,800
都是去参考PyTorch为主的

251
00:09:59,800 --> 00:10:02,440
因为PyTorch的应用性实在是太好了

252
00:10:02,640 --> 00:10:04,480
参考了PyTorch API之后

253
00:10:04,760 --> 00:10:07,440
图层的表达就是希望能够把类似于

254
00:10:07,440 --> 00:10:10,920
PyTorch的表达转换成为图的AI进行优化

255
00:10:11,160 --> 00:10:14,360
第二点就是AI专用的一个编译器的架构

256
00:10:14,360 --> 00:10:18,480
就希望能够打开图和算子的边界进行融合优化

257
00:10:18,960 --> 00:10:22,320
详细的内容我们将会在后面给大家一起去展开的

258
00:10:22,680 --> 00:10:27,160
第二个重要的特点就是性能上面做一个新的突破

259
00:10:27,440 --> 00:10:30,760
可以看到性能上面其实有很多不同的尝试

260
00:10:30,760 --> 00:10:31,920
包括我们的TBM

261
00:10:31,920 --> 00:10:33,880
还有Facebook推出的TC

262
00:10:33,880 --> 00:10:35,840
还有谷歌的XLA

263
00:10:35,840 --> 00:10:39,320
这里面主要是希望能够打开计算图和算子的边界

264
00:10:39,320 --> 00:10:41,240
进行重新的组合优化

265
00:10:41,240 --> 00:10:43,640
极度的去发挥我们芯片的算力

266
00:10:43,640 --> 00:10:45,120
就简单的来说

267
00:10:45,120 --> 00:10:47,080
你不要再给我分开什么计算图

268
00:10:47,080 --> 00:10:47,960
什么算子

269
00:10:47,960 --> 00:10:50,360
我能不能把它看成一个事情

270
00:10:50,720 --> 00:10:54,160
我能不能尽量的把它变成一个统一的IR

271
00:10:54,680 --> 00:10:57,920
把所有东西都变成最细腻度的子图

272
00:10:57,920 --> 00:10:58,960
然后进行边缘优化

273
00:10:58,960 --> 00:11:00,040
包括Buffer融合了

274
00:11:00,040 --> 00:11:00,800
水平融合了

275
00:11:00,800 --> 00:11:01,920
还有垂直融合

276
00:11:02,560 --> 00:11:06,560
这里面最大的一个挑战就是这些算子怎么去打开

277
00:11:06,560 --> 00:11:08,200
这些图怎么去打开

278
00:11:08,200 --> 00:11:09,560
我有了小算子之后

279
00:11:09,560 --> 00:11:12,520
我怎么去进行各种情况的融合

280
00:11:12,520 --> 00:11:15,240
这个点是对性能上最大的一个挑战

281
00:11:15,680 --> 00:11:16,520
总结一下

282
00:11:16,520 --> 00:11:18,680
表达上我们以PyTorch作为标杆

283
00:11:18,680 --> 00:11:21,480
然后进行一个图层或者算子层的IR

284
00:11:21,480 --> 00:11:22,640
第二层性能上

285
00:11:22,640 --> 00:11:25,400
我们希望能够打开计算图和算子的边界

286
00:11:25,400 --> 00:11:28,440
右边的这个就是中米来总结的一个图

287
00:11:28,440 --> 00:11:30,840
首先前端是有一个Python的代码

288
00:11:30,840 --> 00:11:32,840
用Python原生的数据语言

289
00:11:32,840 --> 00:11:34,760
然后对Python进行解析

290
00:11:34,760 --> 00:11:36,920
最后传给我们的图层的IR

291
00:11:36,920 --> 00:11:38,120
还有算子层的IR

292
00:11:38,120 --> 00:11:40,760
这一层是我们希望进行融合的

293
00:11:40,760 --> 00:11:42,600
接着我们传给我们的后端

294
00:11:42,600 --> 00:11:45,200
后端会根据不同的硬件进行一个编译

295
00:11:45,200 --> 00:11:46,440
例如CPU跟TPU

296
00:11:46,440 --> 00:11:48,120
我们可能会用LVM IR

297
00:11:48,120 --> 00:11:51,080
GPU可能会用NVCC去编译

298
00:11:51,080 --> 00:11:53,200
NPU可能我们会用去压压

299
00:11:53,200 --> 00:11:54,280
然后去编译的

300
00:11:54,280 --> 00:11:55,640
在真正执行的时候

301
00:11:55,640 --> 00:11:57,000
可能就没有编译过程了

302
00:11:57,000 --> 00:11:58,000
而要直接one time

303
00:11:58,000 --> 00:11:59,480
然后提供一些算子的库

304
00:11:59,480 --> 00:12:00,760
然后去执行

305
00:12:00,760 --> 00:12:02,600
对接到我们不同的硬件

306
00:12:02,600 --> 00:12:06,320
这个就是我们专用编译器的一个具体的架构图

307
00:12:06,320 --> 00:12:08,640
在阶段二专用AI编译器里面

308
00:12:08,640 --> 00:12:10,600
其实还有几个比较重要的问题

309
00:12:10,600 --> 00:12:13,680
我们其实在规避或者在努力的解决的

310
00:12:13,680 --> 00:12:15,480
第一种就是表达的分离

311
00:12:15,480 --> 00:12:17,080
因为计算图和算子层

312
00:12:17,080 --> 00:12:19,680
其实现在还是分开去表达的

313
00:12:19,680 --> 00:12:23,120
算子工程师主要是关心图层的表达

314
00:12:23,120 --> 00:12:24,320
就是我用Tensorflow

315
00:12:24,320 --> 00:12:25,000
我用PyTorch

316
00:12:25,000 --> 00:12:25,920
我用MindSpore

317
00:12:25,920 --> 00:12:28,880
怎么去实现我们的神经网络

318
00:12:28,880 --> 00:12:30,240
怎么实现我们的算法

319
00:12:30,240 --> 00:12:32,440
但是具体的算子的表达和实现

320
00:12:32,640 --> 00:12:35,360
是由框架开发者和芯片厂商提供的

321
00:12:35,360 --> 00:12:36,960
这里面就会遇到一个大问题

322
00:12:36,960 --> 00:12:39,200
如果我在底层做了一个算子的融合

323
00:12:39,200 --> 00:12:41,320
算法工程师不知道AI框架

324
00:12:41,320 --> 00:12:43,600
或者AI编译器给我们做了一个算子融合

325
00:12:43,600 --> 00:12:46,280
算法工程师对算子的定义

326
00:12:46,280 --> 00:12:48,240
跟我们AI框架的开发者

327
00:12:48,240 --> 00:12:50,520
和芯片厂商提供的算子不对等了

328
00:12:50,520 --> 00:12:51,200
这个时候

329
00:12:51,200 --> 00:12:54,360
返回给算法工程师的一个报错的赞

330
00:12:54,360 --> 00:12:55,640
可能就会不一样了

331
00:12:55,640 --> 00:12:57,120
算法工程师就觉得

332
00:12:57,120 --> 00:12:59,440
怎么我写的代码

333
00:12:59,440 --> 00:13:00,760
跟我的报错不一样

334
00:13:00,760 --> 00:13:02,040
我找不到我的报错点

335
00:13:02,040 --> 00:13:04,440
但是他告诉我这段代码执行错误

336
00:13:04,640 --> 00:13:06,120
这就是分离表达

337
00:13:06,120 --> 00:13:07,880
遇到比较典型的一个问题

338
00:13:07,880 --> 00:13:10,160
第2个就是功能的分化性

339
00:13:10,160 --> 00:13:11,920
我们在开发MindSpore的时候

340
00:13:11,920 --> 00:13:14,320
会遇到非常多的这种功能分化性的问题

341
00:13:14,320 --> 00:13:16,120
其实现在已经解决了大部分

342
00:13:16,120 --> 00:13:18,360
例如在动静态图的转换

343
00:13:18,360 --> 00:13:20,440
就是我从静态图转成动态图

344
00:13:20,440 --> 00:13:22,960
我再从动态图转成静态图

345
00:13:22,960 --> 00:13:24,800
那这个时候动态图的灵活表达

346
00:13:24,800 --> 00:13:26,440
是不是全部python的表达

347
00:13:26,440 --> 00:13:28,360
都能够在静态图里面去承载

348
00:13:28,560 --> 00:13:29,080
不一定

349
00:13:29,080 --> 00:13:30,440
它可能会有一个gap

350
00:13:30,440 --> 00:13:31,560
第3个就是动态shape

351
00:13:31,560 --> 00:13:33,520
动态shape这个事情特别头疼

352
00:13:33,520 --> 00:13:36,240
后面会单独分开一节来去介绍的

353
00:13:36,240 --> 00:13:38,920
那另外还有西数计算和分布式并行优化

354
00:13:38,920 --> 00:13:41,400
这些需求我们都需要对AI框架

355
00:13:41,400 --> 00:13:43,000
或者对AI编译器

356
00:13:43,000 --> 00:13:45,200
做一个充分的功能的分化

357
00:13:45,240 --> 00:13:47,880
最后一个就是平衡效率和性能

358
00:13:47,880 --> 00:13:49,920
效率和性能这个主要的体现

359
00:13:50,040 --> 00:13:51,520
就是在我们的颗垄层

360
00:13:51,520 --> 00:13:53,040
就是算子的底层

361
00:13:53,040 --> 00:13:54,080
怎么在scattering

362
00:13:54,080 --> 00:13:54,480
tooling

363
00:13:54,480 --> 00:13:57,040
quadrant上面去自动化的表示

364
00:13:57,360 --> 00:13:58,680
因为现在来说

365
00:13:59,600 --> 00:14:00,920
算子的实现工程师

366
00:14:00,920 --> 00:14:02,200
就颗垄的实现工程师

367
00:14:02,360 --> 00:14:04,720
他需要了解算子的一个逻辑

368
00:14:04,720 --> 00:14:07,920
同时他要了解硬件的一个架构体系

369
00:14:08,040 --> 00:14:09,240
你写算子的人

370
00:14:09,240 --> 00:14:10,440
你写你自己算法的人

371
00:14:10,440 --> 00:14:11,440
你不懂硬件架构

372
00:14:11,440 --> 00:14:12,240
然后随便写

373
00:14:12,240 --> 00:14:15,000
不能够充分发挥架构的一个优势

374
00:14:15,120 --> 00:14:16,480
那写算子的就会说

375
00:14:16,800 --> 00:14:18,000
你不懂我这个算法

376
00:14:18,000 --> 00:14:19,360
我这个算法不能随便的

377
00:14:19,360 --> 00:14:20,920
按你这套逻辑来实现的

378
00:14:20,920 --> 00:14:22,320
不然它的逻辑就不对了

379
00:14:22,320 --> 00:14:23,800
它的计算结果就不对了

380
00:14:27,120 --> 00:14:28,920
第三个就是我们的未来

381
00:14:28,920 --> 00:14:30,360
通用的AI编译器

382
00:14:30,360 --> 00:14:32,880
就是希望对图算的进行表达

383
00:14:32,880 --> 00:14:34,040
实现融合优化

384
00:14:34,040 --> 00:14:35,200
第二个就是kernel

385
00:14:35,200 --> 00:14:37,280
算子层面去实现的自动的

386
00:14:37,280 --> 00:14:37,760
scattering

387
00:14:37,760 --> 00:14:38,080
tooling

388
00:14:38,080 --> 00:14:38,720
还有quadrant

389
00:14:38,720 --> 00:14:40,960
降低整体的开发门槛

390
00:14:40,960 --> 00:14:43,200
第三个就是对我们神经网络

391
00:14:43,200 --> 00:14:44,880
对我们的AI的功能

392
00:14:44,880 --> 00:14:46,240
更进一步的泛化

393
00:14:46,240 --> 00:14:48,160
最后一个就是

394
00:14:48,160 --> 00:14:49,200
chris提出来的

395
00:14:49,200 --> 00:14:50,680
包括我们的编译器运行是

396
00:14:50,680 --> 00:14:52,640
eagle计算所有的东西

397
00:14:52,640 --> 00:14:55,280
都变成模块化的表示和组合

398
00:14:55,280 --> 00:14:57,480
专注于整体的可用性

399
00:14:57,480 --> 00:14:58,840
同样的我们前端

400
00:14:58,840 --> 00:15:00,480
还是用python进行解析

401
00:15:00,480 --> 00:15:02,160
然后对python的数据语言

402
00:15:02,160 --> 00:15:03,800
解析成我们的graph IR

403
00:15:03,800 --> 00:15:05,400
然后通过我们的graph IR

404
00:15:05,600 --> 00:15:08,880
去进行一个图算融合的编译优化

405
00:15:08,880 --> 00:15:10,960
最后传给我们的backend

406
00:15:10,960 --> 00:15:11,760
就是后端

407
00:15:11,800 --> 00:15:14,520
后端就可以在不同的芯片架构上面

408
00:15:14,520 --> 00:15:15,440
或者加速器上面

409
00:15:15,640 --> 00:15:17,280
去执行和编译的

410
00:15:17,280 --> 00:15:20,040
这个就是希望未来可以出现的

411
00:15:20,400 --> 00:15:21,840
现在我们又回到了

412
00:15:21,840 --> 00:15:24,040
金雪峰雪峰总里面的这个图

413
00:15:24,240 --> 00:15:26,040
这个图我加了一些小标

414
00:15:26,320 --> 00:15:26,960
我们可以看到

415
00:15:26,960 --> 00:15:27,840
在阶段一的时候

416
00:15:28,000 --> 00:15:29,000
主要是有TensorFlow

417
00:15:29,000 --> 00:15:30,280
但是在阶段二的时候

418
00:15:30,400 --> 00:15:31,960
就极度的去大量的

419
00:15:31,960 --> 00:15:34,680
爆发了我们不同的AI编译器的出现

420
00:15:34,680 --> 00:15:38,000
而我们现在主要是处在这一个阶段

421
00:15:38,960 --> 00:15:41,440
了解完AI编译器的一个发展情况之后

422
00:15:41,600 --> 00:15:43,480
或者什么是AI编译器

423
00:15:43,720 --> 00:15:45,600
我们现在抛出几个问题

424
00:15:45,600 --> 00:15:48,560
希望能够跟大家一起去思考和汇报的

425
00:15:48,920 --> 00:15:49,880
第一个问题就是

426
00:15:49,880 --> 00:15:52,560
AI编译器跟传统编译器有什么区别

427
00:15:52,880 --> 00:15:54,680
这个其实我们在上一节分享里面

428
00:15:54,840 --> 00:15:55,760
有回答了

429
00:15:55,760 --> 00:15:58,520
我们只是希望能够让大家去view一下

430
00:15:58,520 --> 00:16:00,440
AI编译器作为传统编译器的

431
00:16:00,440 --> 00:16:01,880
一个具体的补充

432
00:16:01,880 --> 00:16:04,640
而不是一个全新的替换

433
00:16:04,640 --> 00:16:06,240
另外它还借鉴了大量的

434
00:16:06,240 --> 00:16:08,040
传统编译器的一个思想

435
00:16:08,040 --> 00:16:08,800
第二个问题

436
00:16:09,160 --> 00:16:12,440
其实我觉得我自己也没有搞太清楚

437
00:16:12,440 --> 00:16:14,520
或者边界没有划分太明确了

438
00:16:14,720 --> 00:16:17,240
就是AI框架跟AI编译器什么关系吗

439
00:16:17,240 --> 00:16:20,200
到底是AI框架包含一个AI编译器

440
00:16:20,440 --> 00:16:23,440
还是AI框架就是一个大型移动的

441
00:16:23,440 --> 00:16:24,400
AI编译器呢

442
00:16:24,760 --> 00:16:26,600
这个问题我觉得是非常有意思

443
00:16:26,600 --> 00:16:29,520
也希望引起大家去思考和讨论的

444
00:16:30,800 --> 00:16:31,840
第三点就是

445
00:16:31,840 --> 00:16:34,160
AI领域真正需要编译器吗

446
00:16:34,600 --> 00:16:37,120
为什么像Pytorch的动态图模式

447
00:16:37,120 --> 00:16:37,960
这么多人用

448
00:16:38,080 --> 00:16:39,760
大家都觉得Pytorch很好

449
00:16:39,760 --> 00:16:42,240
但Pytorch的动态图模式是没有编译器的

450
00:16:42,920 --> 00:16:45,920
那现在AI领域真的是需要编译器吗

451
00:16:45,920 --> 00:16:49,120
第四个就是从投入的

452
00:16:49,360 --> 00:16:52,280
第四个就是从商业的角度去考虑的

453
00:16:52,280 --> 00:16:54,000
就是我们的技术的投入比

454
00:16:54,640 --> 00:16:58,040
我们现在的实现一个编译器的工作

455
00:16:58,240 --> 00:17:00,520
跟我们员工实现的编译器的工作来看

456
00:17:00,720 --> 00:17:02,080
哪个性价比更高

457
00:17:02,080 --> 00:17:03,440
从最终的结果来看

458
00:17:03,600 --> 00:17:05,960
不见得AI编译器编译出来的算子

459
00:17:06,000 --> 00:17:08,760
就比员工实现的算子的性能还要好

460
00:17:09,120 --> 00:17:10,080
抛出了四个问题

461
00:17:10,080 --> 00:17:13,040
我觉得更聚焦的是第二个问题和第三个问题

462
00:17:13,040 --> 00:17:15,120
AI框架跟AI编译器的关系

463
00:17:15,480 --> 00:17:17,560
AI领域真的需要AI编译器吗

464
00:17:17,560 --> 00:17:20,160
我们带着这两个问题继续往下走

465
00:17:21,400 --> 00:17:23,000
卷得不行了 卷得不行了

466
00:17:23,000 --> 00:17:24,720
记得一键三连加关注哦

467
00:17:24,960 --> 00:17:27,880
所有的内容都会开源在下面这条链接里面

468
00:17:28,480 --> 00:17:29,200
摆了个掰

