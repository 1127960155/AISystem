1
00:00:05,339 --> 00:00:06,360
哈喽大家好

2
00:00:06,360 --> 00:00:08,840
我们来到AI编译器系列里面的

3
00:00:08,840 --> 00:00:11,440
AI编译器里面真正的第二个内容

4
00:00:11,440 --> 00:00:13,640
就是AI编译器的架构的发展

5
00:00:13,640 --> 00:00:17,600
我们会重点去讲讲AI编译器的一个具体的架构

6
00:00:17,600 --> 00:00:20,040
当然了在AI编译器里面

7
00:00:20,040 --> 00:00:22,040
它的架构并不是一成不变的

8
00:00:22,040 --> 00:00:23,680
而是随着时间的推移

9
00:00:23,680 --> 00:00:26,400
还有随着技术的发展而不断的演进的

10
00:00:26,400 --> 00:00:28,520
这里面我们分开三个阶段来看

11
00:00:28,520 --> 00:00:31,120
现在我们正式的进入我们的内容

12
00:00:31,120 --> 00:00:32,800
什么是AI编译器

13
00:00:39,480 --> 00:00:41,320
其实我抛出的这个问题

14
00:00:41,320 --> 00:00:43,520
其实想跟大家一起去探讨的

15
00:00:43,520 --> 00:00:47,000
我认为AI编译器主要分为两个场景

16
00:00:47,000 --> 00:00:49,640
两个场景的编译器的内容其实是不太一样的

17
00:00:49,640 --> 00:00:51,000
第一个就是推理了

18
00:00:51,000 --> 00:00:54,600
我们现在大部分应用场景都在推理里面的

19
00:00:54,600 --> 00:00:57,200
它的输入是AI框架训练出来的

20
00:00:57,200 --> 00:00:59,840
我们的模型文件或者模型权重

21
00:00:59,840 --> 00:01:03,560
输出是能够在不同硬件高效执行的程序

22
00:01:03,560 --> 00:01:06,520
编译器就是去链接我们的文件

23
00:01:06,520 --> 00:01:08,880
还有我们的硬件之间的一个关系

24
00:01:10,200 --> 00:01:13,040
第二个就是指训练场景

25
00:01:13,040 --> 00:01:15,080
训练场景跟推理场景不太一样

26
00:01:15,080 --> 00:01:19,360
训练场景输入的是高级语言表示的神经网络的代码

27
00:01:19,360 --> 00:01:20,920
这句话听上去很隐晦

28
00:01:20,920 --> 00:01:22,560
其实就是很简单

29
00:01:22,560 --> 00:01:26,120
我用python写的一些代码去表示神经网络

30
00:01:26,120 --> 00:01:28,280
然后我丢给我们的AI框架去处理

31
00:01:28,280 --> 00:01:31,120
然后AI框架就能够在不同的硬件上面

32
00:01:31,120 --> 00:01:33,560
高效的去执行我们的训练任务了

33
00:01:33,560 --> 00:01:36,720
这种情况就是我们训练场景的AI编译器

34
00:01:36,720 --> 00:01:39,160
去连接我们的高级语言的代码

35
00:01:39,160 --> 00:01:41,000
然后在不同硬件上面去执行

36
00:01:41,000 --> 00:01:44,360
所以我觉得主要是分为两个场景去探讨的

37
00:01:45,360 --> 00:01:47,360
在正式进入具体的内容之前

38
00:01:47,480 --> 00:01:49,200
我想抛出三个问题

39
00:01:49,320 --> 00:01:52,640
跟大家一起去学习和汇报一下

40
00:01:52,640 --> 00:01:54,760
第一个就是什么是训练场景

41
00:01:54,760 --> 00:01:57,200
我们刚才讲了有训练场景有推理场景

42
00:01:57,200 --> 00:01:58,880
那它们之间有什么区别了

43
00:01:58,880 --> 00:02:00,680
什么是训练什么是推理

44
00:02:00,680 --> 00:02:03,400
如果不懂得AI的基础的同学

45
00:02:03,520 --> 00:02:07,560
可以翻看我们之前关于AI基础训练的一些内容

46
00:02:07,560 --> 00:02:10,520
也可以现在网上有非常多的相关的资料

47
00:02:10,520 --> 00:02:15,360
第二个就是我们去做AI编译器的人

48
00:02:15,360 --> 00:02:18,520
我们搞系统底层的人为什么要了解算法

49
00:02:18,760 --> 00:02:20,200
有没有必要了解算法

50
00:02:21,000 --> 00:02:24,000
第三个问题就是我是算子开发的人员

51
00:02:24,000 --> 00:02:27,015
我只是对我的kernel去做一些操作

52
00:02:27,015 --> 00:02:27,040
为什么我需要了解编译器呢

53
00:02:27,040 --> 00:02:29,495
为什么我需要了解编译器呢

54
00:02:29,880 --> 00:02:31,960
或者我有必要去了解编译器吗

55
00:02:32,400 --> 00:02:34,840
带着这三个问题我们继续往下看

56
00:02:37,480 --> 00:02:39,200
对于什么是AI编译器

57
00:02:39,360 --> 00:02:42,000
我们这里总结了几个关键的特性

58
00:02:42,000 --> 00:02:45,400
这也是参考金雪峰老师的一个思考

59
00:02:45,640 --> 00:02:47,480
下面我们来看看第一个特性

60
00:02:47,600 --> 00:02:50,920
就是AI编译器主要是以python为主的

61
00:02:50,920 --> 00:02:53,160
动态解析语言作为前端

62
00:02:53,520 --> 00:02:56,840
第二个就是我们会有多层的IR的设计

63
00:02:57,480 --> 00:03:01,360
多层的IR可能跟LLVM的那种单一的IR不太一样

64
00:03:01,360 --> 00:03:03,080
因为这里面包括图的编译

65
00:03:03,200 --> 00:03:04,520
我们就会有图的IR

66
00:03:04,520 --> 00:03:06,280
我们需要对算子进行编译

67
00:03:06,280 --> 00:03:08,080
所以会出现算子的IR

68
00:03:08,080 --> 00:03:10,560
所以这里面就已经有两层IR了

69
00:03:10,560 --> 00:03:11,880
最后还有代码生成

70
00:03:12,000 --> 00:03:14,320
代码生成又是另外一套IR

71
00:03:14,320 --> 00:03:15,560
就CodeGen的那一套

72
00:03:16,160 --> 00:03:19,040
第三个特性主要是它是面向神经网络

73
00:03:19,040 --> 00:03:21,560
面向深度学习的特定的优化

74
00:03:21,560 --> 00:03:25,840
也就是Domain Specific DS面向特定领域的

75
00:03:26,120 --> 00:03:29,840
第四种就会去支持很多不同的芯片

76
00:03:30,200 --> 00:03:33,280
这个就是我们AI编译器的四个重要的特点

77
00:03:35,280 --> 00:03:38,040
下面这个图有没有点似曾相识的感觉

78
00:03:38,480 --> 00:03:41,720
其实这个图是取自于MindSpore总体架构师

79
00:03:41,720 --> 00:03:42,720
我们叫总架

80
00:03:42,720 --> 00:03:45,360
然后金雪峰老师在知乎上面的一篇文章

81
00:03:45,560 --> 00:03:47,680
然后我对它加以了一些划分

82
00:03:47,680 --> 00:03:48,840
还有进行了一些

83
00:03:49,720 --> 00:03:51,720
我个人觉得可以修改的小地方

84
00:03:52,000 --> 00:03:54,040
然后我们看看主要是分为三个阶段

85
00:03:54,040 --> 00:03:56,320
Stage1就类似于TensorFlow的

86
00:03:56,320 --> 00:03:57,920
可能TensorFlow出现的比较早

87
00:03:57,920 --> 00:04:00,200
然后是第一个把AI编译器

88
00:04:00,200 --> 00:04:03,840
引入到AI框架里面的最早的一个框架了

89
00:04:04,120 --> 00:04:07,840
好像Caffe那种可能是游离在Stage1之外的

90
00:04:07,840 --> 00:04:09,840
或者在Stage1准备之前的

91
00:04:10,160 --> 00:04:12,320
后来我们现在到了Stage2之后

92
00:04:12,480 --> 00:04:16,680
我们可以发现有非常多不同的AI框架的涌现

93
00:04:16,680 --> 00:04:18,000
包括我们的PyTorch

94
00:04:18,000 --> 00:04:20,360
虽然PyTorch有点意思

95
00:04:20,360 --> 00:04:23,080
最近的出现的PyTorch2.0已经发布了

96
00:04:23,440 --> 00:04:24,760
然后到Stage3

97
00:04:24,880 --> 00:04:26,880
Stage3可能现在还没有达到

98
00:04:26,880 --> 00:04:28,640
而是在一个过渡的阶段

99
00:04:28,640 --> 00:04:30,200
那Stage3未来会怎么样

100
00:04:30,200 --> 00:04:32,120
我们做一些简单的畅想

101
00:04:32,240 --> 00:04:34,720
现在我们跟着这个图一起来去看一下

102
00:04:34,720 --> 00:04:37,520
Stage1,Stage2,Stage3各有什么不同

103
00:04:38,280 --> 00:04:39,960
首先我们叫做Stage1

104
00:04:40,000 --> 00:04:42,440
是一个朴素的AI编译器

105
00:04:42,840 --> 00:04:43,920
既然是朴素

106
00:04:43,920 --> 00:04:46,640
它存在于TensorFlow的一个早期的版本

107
00:04:46,640 --> 00:04:48,920
基于神经网络的编程模型

108
00:04:48,960 --> 00:04:51,200
进行了图、计算图

109
00:04:51,200 --> 00:04:53,520
还有算子两层IR的抽象

110
00:04:54,160 --> 00:04:56,520
第一层就是图层

111
00:04:56,760 --> 00:04:57,760
像TensorFlow早期

112
00:04:57,920 --> 00:05:00,520
它采用的是一个声明式的编程方式

113
00:05:01,280 --> 00:05:03,880
主要是以静态图的方式去执行

114
00:05:03,880 --> 00:05:04,840
然后在执行之前

115
00:05:04,920 --> 00:05:07,920
会做一些硬件无关和硬件相关的编译优化

116
00:05:08,120 --> 00:05:11,040
这里面的编译优化主要是针对图结构

117
00:05:11,040 --> 00:05:14,000
对我们的神经网络做一些优化和融合

118
00:05:14,520 --> 00:05:15,400
我们可以看一下

119
00:05:15,400 --> 00:05:17,680
假设现在这里面有非常多的算子

120
00:05:17,960 --> 00:05:19,200
每一次我下发的时候

121
00:05:19,200 --> 00:05:21,320
每一次让硬件去执行的时候

122
00:05:21,600 --> 00:05:22,520
每执行一个算子

123
00:05:22,520 --> 00:05:23,960
它就有它的输出

124
00:05:23,960 --> 00:05:25,480
就要占用我们的IO

125
00:05:25,840 --> 00:05:28,000
我们能不能做一些算子的融合

126
00:05:28,240 --> 00:05:29,800
做一些提前内存的分配

127
00:05:30,320 --> 00:05:32,520
所以这个就是对图层的优化

128
00:05:33,040 --> 00:05:36,400
第二个就是算子层面的优化

129
00:05:36,800 --> 00:05:37,800
在早期的版本

130
00:05:38,000 --> 00:05:40,160
算子层其实没有编译的概念

131
00:05:40,160 --> 00:05:42,760
主要是通过手写Kernel的方式

132
00:05:42,760 --> 00:05:45,080
例如在英伟达的GPU上面

133
00:05:45,240 --> 00:05:47,320
可能会提供了一些CUDA

134
00:05:47,320 --> 00:05:49,160
自己写的一些.cu的算子

135
00:05:49,160 --> 00:05:51,440
或者依赖于CuDNN的算子库

136
00:05:52,520 --> 00:05:53,960
从这里面我们可以看到

137
00:05:53,960 --> 00:05:56,800
实际上最开始的朴素的AI编译器

138
00:05:56,960 --> 00:05:58,760
它的概念也是比较简单

139
00:05:58,760 --> 00:06:01,880
我就是对神经网络所表示的计算图

140
00:06:02,160 --> 00:06:04,520
做一些编译相关的优化

141
00:06:04,800 --> 00:06:05,920
这些编译相关的优化

142
00:06:06,040 --> 00:06:08,840
其实我在进入MindSpore的前几年

143
00:06:09,360 --> 00:06:10,960
也就是199年的时候

144
00:06:11,160 --> 00:06:12,920
其实是亲自去写了这些

145
00:06:12,920 --> 00:06:14,400
大家不要觉得编译器

146
00:06:14,400 --> 00:06:16,880
或者编译底层的parts很难写

147
00:06:16,880 --> 00:06:18,520
它其实就是一个硬规则

148
00:06:18,800 --> 00:06:19,880
我拿到一个图

149
00:06:19,880 --> 00:06:20,920
拿到一个节点

150
00:06:20,920 --> 00:06:22,640
然后我们去自己控制

151
00:06:22,640 --> 00:06:24,800
最难的工作就是去思考

152
00:06:24,800 --> 00:06:27,680
去抽象计算图的基本的pattern

153
00:06:27,680 --> 00:06:29,640
去抽象计算图的模式

154
00:06:30,520 --> 00:06:31,200
刚才我们提到

155
00:06:31,200 --> 00:06:32,920
如果不了解声明式编程

156
00:06:32,920 --> 00:06:33,800
命令式编程

157
00:06:33,800 --> 00:06:35,040
静态图和动态图

158
00:06:35,160 --> 00:06:36,320
我们可以看一下

159
00:06:36,600 --> 00:06:39,440
AI框架基础的第4节和第5节的内容

160
00:06:41,000 --> 00:06:41,600
右边这个图

161
00:06:41,840 --> 00:06:43,520
就是我简单的总结了一下

162
00:06:43,520 --> 00:06:45,440
关于朴素AI编译器的一个

163
00:06:45,800 --> 00:06:46,760
简单的架构

164
00:06:46,760 --> 00:06:49,480
前端可能我们会有一些Python的API

165
00:06:49,680 --> 00:06:51,160
这里面以TensorFlow为主

166
00:06:51,160 --> 00:06:52,440
它主要是写了自己

167
00:06:52,440 --> 00:06:53,960
关于计算图的一些表示

168
00:06:54,240 --> 00:06:55,120
用户用的时候

169
00:06:55,280 --> 00:06:58,840
就需要学它这一套Python的API的前端

170
00:06:59,080 --> 00:07:00,320
这里面前端Python的API

171
00:07:00,440 --> 00:07:02,880
只是借用了Python去表达神经网络

172
00:07:03,200 --> 00:07:04,040
但实际上的编译

173
00:07:04,240 --> 00:07:05,760
是用TensorFlow自己的一个

174
00:07:05,760 --> 00:07:07,040
计算图的编译层

175
00:07:07,320 --> 00:07:08,560
所以大家用户用起来

176
00:07:08,680 --> 00:07:09,800
就会觉得很奇怪

177
00:07:09,800 --> 00:07:12,480
我要去学TensorFlow的一个Python的解析

178
00:07:12,480 --> 00:07:14,040
跟我们平时写的Python代码

179
00:07:14,280 --> 00:07:15,080
好像不太一样

180
00:07:15,440 --> 00:07:17,560
在算子层就直接是到RunTime

181
00:07:17,560 --> 00:07:18,800
然后去调用CuDNN

182
00:07:18,800 --> 00:07:19,600
这些算子库

183
00:07:19,600 --> 00:07:22,120
最后执行在我们的异构芯片上面

184
00:07:22,400 --> 00:07:25,080
这个就是最简单最朴素的AI编译器

185
00:07:26,600 --> 00:07:28,080
讲完朴素AI编译器之后

186
00:07:28,200 --> 00:07:30,120
我们看一下它具体遇到哪些问题

187
00:07:30,560 --> 00:07:31,880
第一个就是易用性

188
00:07:31,880 --> 00:07:34,440
易用性它在表达上是非Python原生的

189
00:07:34,440 --> 00:07:36,720
也就是它不是真正的Python的代码

190
00:07:36,720 --> 00:07:38,200
只是类Python的代码

191
00:07:38,560 --> 00:07:40,640
这时候开发者就需要利用

192
00:07:40,640 --> 00:07:43,880
TensorFlow提供的API去显示的构图

193
00:07:44,240 --> 00:07:46,040
我必须要知道我构这个图

194
00:07:46,040 --> 00:07:47,440
我需要用哪些API

195
00:07:47,440 --> 00:07:48,800
这个时候是很头痛的

196
00:07:48,800 --> 00:07:50,600
所以大家用TensorFlow学TensorFlow

197
00:07:50,600 --> 00:07:52,640
觉得它难学就在于这一点

198
00:07:53,080 --> 00:07:55,120
第二点就是性能上

199
00:07:55,680 --> 00:07:57,960
越来越多的AI加速器的出现

200
00:07:57,960 --> 00:08:00,840
所以导致我们对性能的挑战很大

201
00:08:00,880 --> 00:08:03,160
我们可能在某些芯片上面跑得特别快

202
00:08:03,400 --> 00:08:05,880
在某些芯片上面跑得特别的慢

203
00:08:06,360 --> 00:08:08,600
第二个就是我们用的是CuDNN

204
00:08:08,600 --> 00:08:10,480
或者自己手工写的一些算子

205
00:08:10,480 --> 00:08:12,280
而且走的是一个静态图

206
00:08:12,280 --> 00:08:14,280
所以我们的算子的边界

207
00:08:14,280 --> 00:08:16,560
还有算子属性某些特定的情况

208
00:08:16,840 --> 00:08:18,320
是已经明确确定的

209
00:08:18,600 --> 00:08:20,600
例如我在LSTM这个算子里面

210
00:08:20,920 --> 00:08:23,880
发现我的NLP输入的序列非常长

211
00:08:24,200 --> 00:08:26,400
这种情况可能超出了我手写Kernel

212
00:08:26,400 --> 00:08:29,240
或者提供的算子的一个边界

213
00:08:29,360 --> 00:08:31,720
这个时候我的执行就会变得非常慢

214
00:08:31,720 --> 00:08:34,000
甚至可能出现精度的问题

215
00:08:34,640 --> 00:08:38,200
第三点就是算子层它没有通过编译

216
00:08:38,200 --> 00:08:40,560
而是直接使用CuDNN的这种算子

217
00:08:40,560 --> 00:08:42,640
所以硬件厂商提供的优化库

218
00:08:43,200 --> 00:08:43,960
一定是最优的

219
00:08:44,200 --> 00:08:45,080
如果是最优的话

220
00:08:45,080 --> 00:08:47,320
就不会出现类似于PyTorch

221
00:08:47,320 --> 00:08:48,880
类似于PyTorch Atom里面

222
00:08:48,880 --> 00:08:51,360
大量的CUDA手写的算子

223
00:08:52,200 --> 00:08:54,280
所以说硬件厂商提供的算子库

224
00:08:54,280 --> 00:08:55,320
未必是最优的

225
00:08:55,320 --> 00:08:58,440
但是它给我们提供了一个方便的前提

226
00:09:00,240 --> 00:09:04,040
接下来我们就遇到了一个专用的AI编译器

227
00:09:04,040 --> 00:09:05,240
那专用的AI编译器

228
00:09:05,400 --> 00:09:07,600
我们会以PyTorch GX

229
00:09:07,600 --> 00:09:09,000
还有Mindspore作为例子

230
00:09:09,320 --> 00:09:10,680
像PyTorch大家都觉得

231
00:09:10,680 --> 00:09:13,360
它没有一个计算图的概念

232
00:09:13,360 --> 00:09:15,200
它其实只是PyTorch的动态图

233
00:09:15,200 --> 00:09:16,600
它没有计算图的概念

234
00:09:16,600 --> 00:09:19,200
但是PyTorch后来又出现了

235
00:09:19,200 --> 00:09:20,400
PyTorch.fx

236
00:09:20,400 --> 00:09:21,880
PyTorch.git

237
00:09:21,880 --> 00:09:24,120
还有包括PyTorch.dynamic

238
00:09:24,120 --> 00:09:25,840
包括现在的PyTorch 2.0

239
00:09:26,040 --> 00:09:29,080
它其实已经出现了自己的一个AI编译器

240
00:09:29,160 --> 00:09:33,040
当然它没有一种说解决方案特别的完善

241
00:09:33,640 --> 00:09:35,640
如果大家有兴趣或者看的人比较多

242
00:09:35,640 --> 00:09:37,920
我们可以单独开一节去讲一讲

243
00:09:38,080 --> 00:09:41,200
PyTorch的一个PyTorch 2.0的一些新特性

244
00:09:41,200 --> 00:09:43,480
最重要的是PyTorch.compile

245
00:09:43,800 --> 00:09:46,080
这个新的重要的特点

246
00:09:46,760 --> 00:09:48,600
现在回过头来我们看看stage2

247
00:09:48,600 --> 00:09:50,960
一个专用AI编译器有什么特点

248
00:09:50,960 --> 00:09:54,320
首先表达上它是类似于PyTorch的灵活表达

249
00:09:54,320 --> 00:09:55,680
就现在不管哪个框架

250
00:09:55,680 --> 00:09:57,440
基本上我觉得大部分的API

251
00:09:57,880 --> 00:09:59,800
都是去参考PyTorch为主的

252
00:09:59,800 --> 00:10:02,440
因为PyTorch的应用性实在是太好了

253
00:10:02,640 --> 00:10:04,480
参考了PyTorch API之后

254
00:10:04,760 --> 00:10:07,440
图层的表达就是希望能够把类似于

255
00:10:07,440 --> 00:10:10,920
PyTorch的表达转换成为图的IR进行优化

256
00:10:11,160 --> 00:10:14,360
第二点就是AI专用的一个编译器的架构

257
00:10:14,360 --> 00:10:18,480
就希望能够打开图和算子的边界进行融合优化

258
00:10:18,960 --> 00:10:22,320
详细的内容我们将会在后面给大家一起去展开的

259
00:10:22,680 --> 00:10:27,160
第二个重要的特点就是性能上面做一个新的突破

260
00:10:27,440 --> 00:10:30,760
可以看到性能上面其实有很多不同的尝试

261
00:10:30,760 --> 00:10:31,920
包括我们的TBM

262
00:10:31,920 --> 00:10:33,880
还有Facebook推出的TC

263
00:10:33,880 --> 00:10:35,840
还有谷歌的XLA

264
00:10:35,840 --> 00:10:39,320
这里面主要是希望能够打开计算图和算子的边界

265
00:10:39,320 --> 00:10:41,240
进行重新的组合优化

266
00:10:41,240 --> 00:10:43,640
极度的去发挥我们芯片的算力

267
00:10:43,640 --> 00:10:45,120
就简单的来说

268
00:10:45,120 --> 00:10:47,080
你不要再给我分开什么计算图

269
00:10:47,080 --> 00:10:47,960
什么算子

270
00:10:47,960 --> 00:10:50,360
我能不能把它看成一个事情

271
00:10:50,720 --> 00:10:54,160
我能不能尽量的把它变成一个统一的IR

272
00:10:54,760 --> 00:10:57,920
把所有东西都变成最细粒度的子图

273
00:10:57,920 --> 00:10:58,960
然后进行边界优化

274
00:10:58,960 --> 00:10:59,840
包括Buffer融合

275
00:11:00,040 --> 00:11:00,720
水平融合

276
00:11:00,800 --> 00:11:01,920
还有垂直融合

277
00:11:02,600 --> 00:11:06,560
这里面最大的一个挑战就是这些算子怎么去打开

278
00:11:06,560 --> 00:11:08,200
这些图怎么去打开

279
00:11:08,200 --> 00:11:09,520
我有了小算子之后

280
00:11:09,520 --> 00:11:12,520
我怎么去进行各种情况的融合

281
00:11:12,520 --> 00:11:15,240
这个点是对性能上最大的一个挑战

282
00:11:15,680 --> 00:11:16,520
总结一下

283
00:11:16,520 --> 00:11:18,680
表达上我们以PyTorch作为标杆

284
00:11:18,680 --> 00:11:21,480
然后进行一个图层或者算子层的IR

285
00:11:21,480 --> 00:11:22,640
第二层性能上

286
00:11:22,640 --> 00:11:25,400
我们希望能够打开计算图和算子的边界

287
00:11:25,400 --> 00:11:28,440
右边的这个就是ZOMI来总结的一个图

288
00:11:28,440 --> 00:11:30,840
首先前端是有一个Python的代码

289
00:11:30,840 --> 00:11:32,840
用Python原生的数据语言

290
00:11:32,840 --> 00:11:34,760
然后对Python进行解析

291
00:11:34,760 --> 00:11:36,920
最后传给我们的图层的IR

292
00:11:36,920 --> 00:11:38,120
还有算子层的IR

293
00:11:38,120 --> 00:11:40,760
这一层是我们希望进行融合的

294
00:11:40,760 --> 00:11:42,600
接着我们传给我们的后端

295
00:11:42,600 --> 00:11:45,200
后端会根据不同的硬件进行一个编译

296
00:11:45,200 --> 00:11:46,440
例如CPU跟TPU

297
00:11:46,440 --> 00:11:48,120
我们可能会用LLVM IR

298
00:11:48,120 --> 00:11:51,080
GPU可能会用NVCC去编译

299
00:11:51,080 --> 00:11:53,200
NPU可能我们会用去用GE IR

300
00:11:53,200 --> 00:11:54,280
然后去编译的

301
00:11:54,280 --> 00:11:55,640
在真正执行的时候

302
00:11:55,640 --> 00:11:57,000
可能就没有编译过程了

303
00:11:57,000 --> 00:11:58,000
而要直接Runtime

304
00:11:58,000 --> 00:11:59,480
然后提供一些算子的库

305
00:11:59,480 --> 00:12:00,760
然后去执行

306
00:12:00,760 --> 00:12:02,600
对接到我们不同的硬件

307
00:12:02,600 --> 00:12:06,320
这个就是我们专用编译器的一个具体的架构图

308
00:12:06,320 --> 00:12:08,640
在阶段二专用AI编译器里面

309
00:12:08,640 --> 00:12:10,600
其实还有几个比较重要的问题

310
00:12:10,600 --> 00:12:13,680
我们其实在规避或者在努力的解决的

311
00:12:13,680 --> 00:12:15,480
第一种就是表达的分离

312
00:12:15,480 --> 00:12:17,080
因为计算图和算子层

313
00:12:17,080 --> 00:12:19,680
其实现在还是分开去表达的

314
00:12:19,680 --> 00:12:23,120
算子工程师主要是关心图层的表达

315
00:12:23,120 --> 00:12:24,320
就是我用Tensorflow

316
00:12:24,320 --> 00:12:25,000
我用PyTorch

317
00:12:25,000 --> 00:12:25,920
我用MindSpore

318
00:12:25,920 --> 00:12:28,880
怎么去实现我们的神经网络

319
00:12:28,880 --> 00:12:30,240
怎么实现我们的算法

320
00:12:30,240 --> 00:12:32,440
但是具体的算子的表达和实现

321
00:12:32,640 --> 00:12:35,360
是由框架开发者和芯片厂商提供的

322
00:12:35,360 --> 00:12:36,960
这里面就会遇到一个大问题

323
00:12:36,960 --> 00:12:39,200
如果我在底层做了一个算子的融合

324
00:12:39,200 --> 00:12:41,320
算法工程师不知道AI框架

325
00:12:41,320 --> 00:12:43,600
或者AI编译器给我们做了一个算子融合

326
00:12:43,600 --> 00:12:46,280
算法工程师对算子的定义

327
00:12:46,280 --> 00:12:48,240
跟我们AI框架的开发者

328
00:12:48,240 --> 00:12:50,520
和芯片厂商提供的算子不对等了

329
00:12:50,520 --> 00:12:51,200
这个时候

330
00:12:51,200 --> 00:12:54,360
返回给算法工程师的一个报错的栈

331
00:12:54,360 --> 00:12:55,640
可能就会不一样了

332
00:12:55,640 --> 00:12:57,120
算法工程师就觉得

333
00:12:57,120 --> 00:12:59,440
怎么我写的代码

334
00:12:59,440 --> 00:13:00,760
跟我的报错不一样

335
00:13:00,760 --> 00:13:02,040
我找不到我的报错点

336
00:13:02,040 --> 00:13:04,440
但是他告诉我这段代码执行错误

337
00:13:04,640 --> 00:13:06,120
这就是分离表达

338
00:13:06,120 --> 00:13:07,880
遇到比较典型的一个问题

339
00:13:07,880 --> 00:13:10,160
第2个就是功能的泛化性

340
00:13:10,160 --> 00:13:11,920
我们在开发MindSpore的时候

341
00:13:11,920 --> 00:13:14,320
会遇到非常多的这种功能泛化性的问题

342
00:13:14,320 --> 00:13:16,120
其实现在已经解决了大部分

343
00:13:16,120 --> 00:13:18,360
例如在动静态图的转换

344
00:13:18,360 --> 00:13:20,440
就是我从静态图转成动态图

345
00:13:20,440 --> 00:13:22,960
我再从动态图转成静态图

346
00:13:22,960 --> 00:13:24,800
那这个时候动态图的灵活表达

347
00:13:24,800 --> 00:13:26,440
是不是全部python的表达

348
00:13:26,440 --> 00:13:28,360
都能够在静态图里面去承载

349
00:13:28,560 --> 00:13:29,080
不一定

350
00:13:29,080 --> 00:13:30,440
它可能会有一个gap

351
00:13:30,440 --> 00:13:31,560
第3个就是动态shape

352
00:13:31,560 --> 00:13:33,520
动态shape这个事情特别头疼

353
00:13:33,520 --> 00:13:36,240
后面会单独分开一节来去介绍的

354
00:13:36,240 --> 00:13:38,920
那另外还有稀疏计算和分布式并行优化

355
00:13:38,920 --> 00:13:41,400
这些需求我们都需要对AI框架

356
00:13:41,400 --> 00:13:43,000
或者对AI编译器

357
00:13:43,000 --> 00:13:45,200
做一个充分的功能的泛化

358
00:13:45,200 --> 00:13:47,880
最后一个就是平衡效率和性能

359
00:13:47,880 --> 00:13:49,920
效率和性能这个主要的体现

360
00:13:50,040 --> 00:13:51,520
就是在我们的kernel层

361
00:13:51,520 --> 00:13:53,040
就是算子的底层

362
00:13:53,040 --> 00:13:54,080
怎么在schedule

363
00:13:54,080 --> 00:13:54,480
tiling

364
00:13:54,480 --> 00:13:57,040
codegen上面去自动化的表示

365
00:13:57,320 --> 00:13:58,680
因为现在来说

366
00:13:59,560 --> 00:14:00,920
算子的实现工程师

367
00:14:00,920 --> 00:14:02,200
就kernel的实现工程师

368
00:14:02,360 --> 00:14:04,720
他需要了解算子的一个逻辑

369
00:14:04,720 --> 00:14:07,960
同时他要了解硬件的一个架构体系

370
00:14:08,040 --> 00:14:09,240
你写算子的人

371
00:14:09,240 --> 00:14:10,440
你写你自己算法的人

372
00:14:10,440 --> 00:14:11,440
你不懂硬件架构

373
00:14:11,440 --> 00:14:12,240
然后随便写

374
00:14:12,240 --> 00:14:15,000
不能够充分发挥架构的一个优势

375
00:14:15,120 --> 00:14:16,480
那写算子的就会说

376
00:14:16,800 --> 00:14:18,000
你不懂我这个算法

377
00:14:18,120 --> 00:14:19,360
这个算法不能随便的

378
00:14:19,360 --> 00:14:20,920
按你这套逻辑来实现的

379
00:14:20,920 --> 00:14:22,320
不然它的逻辑就不对了

380
00:14:22,320 --> 00:14:23,800
它的计算结果就不对了

381
00:14:27,120 --> 00:14:28,920
第三个就是我们的未来

382
00:14:28,920 --> 00:14:30,360
通用的AI编译器

383
00:14:30,360 --> 00:14:32,880
就是希望对图算的进行表达

384
00:14:32,880 --> 00:14:34,040
实现融合优化

385
00:14:34,040 --> 00:14:35,200
第二个就是kernel

386
00:14:35,200 --> 00:14:37,280
算子层面去实现的自动的

387
00:14:37,280 --> 00:14:37,760
schedule

388
00:14:37,760 --> 00:14:38,080
tiling

389
00:14:38,080 --> 00:14:38,760
还有codegen

390
00:14:38,760 --> 00:14:40,960
降低整体的开发门槛

391
00:14:40,960 --> 00:14:43,200
第三个就是对我们神经网络

392
00:14:43,200 --> 00:14:44,640
对我们的AI的功能

393
00:14:44,840 --> 00:14:46,240
更进一步的泛化

394
00:14:46,240 --> 00:14:48,120
最后一个就是

395
00:14:48,120 --> 00:14:49,200
Chris提出来的

396
00:14:49,200 --> 00:14:50,640
包括我们的编译器运行是

397
00:14:50,640 --> 00:14:51,520
异构计算

398
00:14:51,520 --> 00:14:53,960
所有的东西都变成模块化的

399
00:14:53,960 --> 00:14:55,280
表示和组合

400
00:14:55,280 --> 00:14:57,480
专注于整体的可用性

401
00:14:57,480 --> 00:14:58,840
同样的我们前端

402
00:14:58,840 --> 00:15:00,480
还是用python进行解析

403
00:15:00,480 --> 00:15:02,160
然后对python的数据语言

404
00:15:02,160 --> 00:15:03,760
解析成我们的graph IR

405
00:15:03,760 --> 00:15:05,360
然后通过我们的Graph IR

406
00:15:05,560 --> 00:15:08,880
去进行一个图算融合的编译优化

407
00:15:08,880 --> 00:15:10,920
最后传给我们的backend

408
00:15:10,920 --> 00:15:11,760
就是后端

409
00:15:11,800 --> 00:15:14,520
后端就可以在不同的芯片架构上面

410
00:15:14,520 --> 00:15:15,440
或者加速器上面

411
00:15:15,640 --> 00:15:17,280
去执行和编译的

412
00:15:17,280 --> 00:15:20,040
这个就是希望未来可以出现的

413
00:15:20,400 --> 00:15:21,840
现在我们又回到了

414
00:15:21,840 --> 00:15:24,040
金雪峰雪峰总里面的图

415
00:15:24,240 --> 00:15:26,040
这个图我加了一些小标

416
00:15:26,320 --> 00:15:26,960
我们可以看到

417
00:15:26,960 --> 00:15:27,840
在阶段一的时候

418
00:15:28,000 --> 00:15:29,000
主要是有TensorFlow

419
00:15:29,000 --> 00:15:30,280
但是在阶段二的时候

420
00:15:30,400 --> 00:15:31,960
就极度的去大量的

421
00:15:31,960 --> 00:15:34,680
爆发了我们不同的AI编译器的出现

422
00:15:34,680 --> 00:15:38,000
而我们现在主要是处在这一个阶段

423
00:15:38,960 --> 00:15:41,440
了解完AI编译器的一个发展情况之后

424
00:15:41,600 --> 00:15:43,440
或者什么是AI编译器

425
00:15:43,720 --> 00:15:45,600
我们现在抛出几个问题

426
00:15:45,600 --> 00:15:48,560
希望能够跟大家一起去思考和汇报的

427
00:15:48,920 --> 00:15:49,880
第一个问题就是

428
00:15:49,880 --> 00:15:52,560
AI编译器跟传统编译器有什么区别

429
00:15:52,880 --> 00:15:54,680
这个其实我们在上一节分享里面

430
00:15:54,840 --> 00:15:55,760
有回答了

431
00:15:55,760 --> 00:15:58,520
我们只是希望能够让大家去review一下

432
00:15:58,520 --> 00:16:00,440
AI编译器作为传统编译器的

433
00:16:00,440 --> 00:16:01,880
一个具体的补充

434
00:16:01,880 --> 00:16:04,640
而不是一个全新的替换

435
00:16:04,640 --> 00:16:06,240
另外它还借鉴了大量的

436
00:16:06,240 --> 00:16:08,040
传统编译器的一个思想

437
00:16:08,040 --> 00:16:08,800
第二个问题

438
00:16:09,160 --> 00:16:12,440
其实我觉得我自己也没有搞太清楚

439
00:16:12,440 --> 00:16:14,520
或者边界没有划分太明确了

440
00:16:14,720 --> 00:16:17,240
就是AI框架跟AI编译器什么关系吗

441
00:16:17,240 --> 00:16:20,200
到底是AI框架包含一个AI编译器

442
00:16:20,440 --> 00:16:23,440
还是AI框架就是一个大型移动的

443
00:16:23,440 --> 00:16:24,400
AI编译器呢

444
00:16:24,760 --> 00:16:26,600
这个问题我觉得是非常有意思

445
00:16:26,600 --> 00:16:29,480
也希望引起大家去思考和讨论的

446
00:16:30,840 --> 00:16:31,840
第三点就是

447
00:16:31,840 --> 00:16:34,120
AI领域真正需要编译器吗

448
00:16:34,640 --> 00:16:37,120
为什么像Pytorch的动态图模式

449
00:16:37,120 --> 00:16:37,960
这么多人用

450
00:16:38,080 --> 00:16:39,760
大家都觉得Pytorch很好

451
00:16:39,760 --> 00:16:42,240
但Pytorch的动态图模式是没有编译器的

452
00:16:42,920 --> 00:16:45,920
那现在AI领域真的是需要编译器吗

453
00:16:45,920 --> 00:16:49,120
第四个就是从投入的

454
00:16:49,360 --> 00:16:52,280
第四个就是从商业的角度去考虑的

455
00:16:52,280 --> 00:16:54,000
就是我们的技术的投入比

456
00:16:54,640 --> 00:16:58,040
我们现在的实现一个编译器的工作

457
00:16:58,240 --> 00:17:00,520
跟我们人工实现的编译器的工作来看

458
00:17:00,720 --> 00:17:02,080
哪个性价比更高

459
00:17:02,080 --> 00:17:03,440
从最终的结果来看

460
00:17:03,600 --> 00:17:05,960
不见得AI编译器编译出来的算子

461
00:17:05,960 --> 00:17:08,720
就比人工实现的算子的性能还要好

462
00:17:09,120 --> 00:17:10,080
抛出了四个问题

463
00:17:10,080 --> 00:17:13,040
我觉得更聚焦的是第二个问题和第三个问题

464
00:17:13,040 --> 00:17:15,120
AI框架跟AI编译器的关系

465
00:17:15,480 --> 00:17:17,560
AI领域真的需要AI编译器吗

466
00:17:17,560 --> 00:17:20,160
我们带着这两个问题继续往下走

467
00:17:21,400 --> 00:17:23,000
卷得不行了 卷得不行了

468
00:17:23,000 --> 00:17:24,720
记得一键三连加关注哦

469
00:17:24,960 --> 00:17:27,880
所有的内容都会开源在下面这条链接里面

470
00:17:28,480 --> 00:17:29,200
拜了个拜

