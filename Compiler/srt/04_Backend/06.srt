1
00:00:00,000 --> 00:00:10,200
大家好,我是那个月入两千八,拿命往里达的周米

2
00:00:10,200 --> 00:00:15,000
今天我们来到AI编辑后端优化的AutoTooling这个内容

3
00:00:15,000 --> 00:00:19,900
其实呢,在上一节里面我们讲了算子调度有哪些优化的方法

4
00:00:19,900 --> 00:00:24,200
但是这些优化方法要落到实处的靠的是我们的AutoTooling的方法

5
00:00:24,200 --> 00:00:29,500
今天我给大家汇报一下AutoTooling这里面的到底有哪些内容具体是怎么Tool的

6
00:00:30,300 --> 00:00:33,500
回到我们AI编译器的全站的一个架构图里面

7
00:00:33,500 --> 00:00:37,900
我们现在还是在OPS Optimizer这个模块里面

8
00:00:37,900 --> 00:00:41,800
其实AutoTooling这个概念不是因为AI编辑器而衍生的

9
00:00:41,800 --> 00:00:43,200
在传统的编译器里面

10
00:00:43,200 --> 00:00:45,500
我们已经早就有了这个概念

11
00:00:45,500 --> 00:00:48,000
我们现在看看传统编辑器是怎么做的

12
00:00:48,000 --> 00:00:50,900
首先传统编辑器它对AutoTooling的一个定义

13
00:00:50,900 --> 00:00:53,700
就是对于给定的程序和目标的一个架构

14
00:00:53,700 --> 00:00:56,400
找到一个最优的编译优化的方法

15
00:00:56,400 --> 00:01:00,000
所以这种是一个普适性通用性的一种方法

16
00:01:00,000 --> 00:01:03,200
具体可能会有几个问题我们一起去思考的

17
00:01:03,200 --> 00:01:06,300
就是我们需要使用哪些优化的方法

18
00:01:06,300 --> 00:01:09,100
选择什么样的一个优化的参数集

19
00:01:09,100 --> 00:01:13,200
接着用什么样的顺序去应用找到的一些优化的方法

20
00:01:13,200 --> 00:01:16,100
然后让我们的整个程序的性能最佳

21
00:01:16,100 --> 00:01:18,500
而传统编辑器里面的性能最佳

22
00:01:18,500 --> 00:01:20,800
主要是指我们程序的性能最佳

23
00:01:22,000 --> 00:01:24,600
因为传统编辑器也围绕着大量的计算

24
00:01:24,700 --> 00:01:26,700
其实我们有非常多的文章

25
00:01:26,700 --> 00:01:28,000
去对传统的编辑器

26
00:01:28,000 --> 00:01:31,800
例如一些 Blast 库去做一些性能的优化

27
00:01:31,800 --> 00:01:34,700
下面这个图的文章出自于这一篇文章

28
00:01:34,700 --> 00:01:38,600
这里面主要是对 GMM 矩阵层进行优化

29
00:01:38,600 --> 00:01:41,100
而这里面充分的去利用了整个

30
00:01:41,100 --> 00:01:42,900
现代处理器的多级缓存

31
00:01:42,900 --> 00:01:45,400
这里面的分为 1 2 3 4

32
00:01:45,400 --> 00:01:47,400
现代处理器里面的四级缓存

33
00:01:47,400 --> 00:01:48,400
那我们可以看一下

34
00:01:48,400 --> 00:01:51,700
其实主要是利用了 L3 L2 L1

35
00:01:51,700 --> 00:01:54,600
还有 Register 一些 ALU 旁边的一些

36
00:01:54,600 --> 00:01:56,800
寄存器进行一个加速的

37
00:01:56,800 --> 00:01:59,100
这种就是传统编辑器对于矩阵层

38
00:01:59,100 --> 00:02:01,200
对于我们的一些大量的计算

39
00:02:01,200 --> 00:02:02,900
进行一个优化的

40
00:02:02,900 --> 00:02:03,900
传统的编辑器

41
00:02:03,900 --> 00:02:05,700
其实发展了一段的时间之后

42
00:02:05,700 --> 00:02:08,300
主要分为两个主要的优化的方法

43
00:02:08,300 --> 00:02:10,200
第一个就是优化的选择

44
00:02:10,200 --> 00:02:13,100
选择使用哪些优化的算法

45
00:02:13,100 --> 00:02:15,100
第二个就是优化的顺序

46
00:02:15,100 --> 00:02:16,500
不同的优化的方法

47
00:02:16,500 --> 00:02:18,600
它们之间的顺序如何组成

48
00:02:18,600 --> 00:02:19,800
不同的优化的顺序

49
00:02:19,800 --> 00:02:21,700
可能会导致不同的优化的结果

50
00:02:23,300 --> 00:02:25,500
现在我们回到 AI 编辑器里面的

51
00:02:25,500 --> 00:02:27,700
Auto-Tuning in AI

52
00:02:27,700 --> 00:02:30,000
其实 AI 编辑器跟传统编辑器之间

53
00:02:30,000 --> 00:02:31,600
一个 Auto-Tuning 的最大的区别

54
00:02:31,600 --> 00:02:32,200
有两个

55
00:02:32,200 --> 00:02:34,400
第一个就是更低的实现开销

56
00:02:34,400 --> 00:02:37,800
第二个就是针对特定领域的一个结构

57
00:02:37,800 --> 00:02:39,200
我们来看一下第一个

58
00:02:39,200 --> 00:02:42,000
更低的实现开销指的是啥呢

59
00:02:42,000 --> 00:02:43,600
更低的实现开销主要是指

60
00:02:43,600 --> 00:02:45,700
我们现在更多的是聚焦于

61
00:02:45,700 --> 00:02:48,800
某个算子或者某个 Kernel 级别的一个优化

62
00:02:48,800 --> 00:02:50,300
而不是整个程序

63
00:02:50,300 --> 00:02:51,900
而传统的编辑器主要是指

64
00:02:51,900 --> 00:02:54,100
整个程序的一个性能的优化

65
00:02:55,300 --> 00:02:56,900
第二个点就是 Cost Model

66
00:02:56,900 --> 00:02:58,000
就是我们的成本函数

67
00:02:58,000 --> 00:02:59,100
我们后面会讲的

68
00:02:59,100 --> 00:03:00,600
在 CPU 上面去模拟

69
00:03:00,600 --> 00:03:02,500
我们在 MBU 在我们具体的

70
00:03:02,500 --> 00:03:05,000
AI 加速芯片上面去执行

71
00:03:05,000 --> 00:03:07,400
对于训练和推理的模拟的速度

72
00:03:07,400 --> 00:03:08,700
要求足够的快

73
00:03:08,700 --> 00:03:11,500
那这个就是我们更低的实现开销

74
00:03:11,500 --> 00:03:14,300
第二个就是特定的领域结构

75
00:03:14,300 --> 00:03:16,500
提到 Domain Specific 就特定了

76
00:03:16,500 --> 00:03:18,400
我们肯定知道主要是针对

77
00:03:18,400 --> 00:03:19,900
神经网络的算子

78
00:03:19,900 --> 00:03:22,400
或者神经网络的 Kernel 级别的优化

79
00:03:22,400 --> 00:03:25,600
那其实跟刚才上面那条是有点类似的

80
00:03:25,600 --> 00:03:27,300
但是这里面更多的是指

81
00:03:27,300 --> 00:03:29,300
我们真正的特定的内容

82
00:03:29,300 --> 00:03:31,800
就是我们的神经网络或者 Kernel 的算子

83
00:03:31,800 --> 00:03:34,000
大部分都是高度的循环化

84
00:03:34,000 --> 00:03:35,400
就是很多的 Loop

85
00:03:35,400 --> 00:03:37,500
第二个就是高度的张量化

86
00:03:37,500 --> 00:03:40,600
我们的数据的结构的维度是非常的高的

87
00:03:40,600 --> 00:03:43,200
另外一个有这么高的维度和循环化

88
00:03:43,200 --> 00:03:45,400
我们需要大量的并行的特性

89
00:03:45,400 --> 00:03:46,700
进行一个优化的

90
00:03:46,700 --> 00:03:47,300
那第二个呢

91
00:03:47,300 --> 00:03:50,600
就是大量相类似的算子的计算模式

92
00:03:50,600 --> 00:03:53,000
大量相类似的算子的计算模式呢

93
00:03:53,000 --> 00:03:54,500
我们可以回顾一下

94
00:03:54,500 --> 00:03:55,900
在神经网络里面呢

95
00:03:55,900 --> 00:03:59,400
我们有非常多的 Pooling 的算子

96
00:03:59,400 --> 00:04:01,100
Max Pooling, Average Pooling

97
00:04:01,100 --> 00:04:03,700
这些都是相类似的一些计算的模式

98
00:04:03,700 --> 00:04:06,100
还有我们的 Normalization

99
00:04:06,100 --> 00:04:07,100
Group Normalization

100
00:04:07,100 --> 00:04:08,000
Layer Normalization

101
00:04:08,000 --> 00:04:09,700
不同的 Normalization 的方式

102
00:04:09,700 --> 00:04:12,100
其实它有大量的相似的模式

103
00:04:12,100 --> 00:04:13,800
那这些部分相似的模式

104
00:04:13,800 --> 00:04:15,800
我们都可以凑像出来

105
00:04:15,800 --> 00:04:18,600
变成一个固定的优化的 Pattern 或者方式

106
00:04:18,600 --> 00:04:19,100
那这种呢

107
00:04:19,100 --> 00:04:21,400
就是特定领域的结构了

108
00:04:21,400 --> 00:04:23,500
在 AI 编辑器里面的 Auto-Tuning

109
00:04:23,500 --> 00:04:24,900
就是因为这两点

110
00:04:24,900 --> 00:04:27,700
而跟传统的 Auto-Tuning 所区分出来的

111
00:04:30,100 --> 00:04:30,600
下面呢

112
00:04:30,600 --> 00:04:32,100
我们一起来思考两个问题

113
00:04:32,100 --> 00:04:32,700
这个问题呢

114
00:04:32,700 --> 00:04:36,200
也是陈天琦去在一篇博客里面提到的一个问题

115
00:04:36,200 --> 00:04:37,200
我们来看一下

116
00:04:37,200 --> 00:04:39,300
其实我们 AI 编辑器里面呢

117
00:04:39,300 --> 00:04:40,400
做一个 Auto-Tuning

118
00:04:40,400 --> 00:04:42,400
最大的工作或者最大的目的呢

119
00:04:42,400 --> 00:04:45,600
就是使得如何让机器生成的 Kernel

120
00:04:45,600 --> 00:04:50,000
跟我们人工手写优化后的 Kernel 的性能相匹配

121
00:04:50,000 --> 00:04:50,700
而这里面呢

122
00:04:50,700 --> 00:04:53,500
它给出一个比较简单粗暴的回答

123
00:04:53,500 --> 00:04:53,900
第一个呢

124
00:04:53,900 --> 00:04:56,700
就是建立一个足够大的搜索空间

125
00:04:56,700 --> 00:04:58,200
那这个搜索空间的保证

126
00:04:58,200 --> 00:05:01,700
我们可以把人工手写优化的全部的内容

127
00:05:01,700 --> 00:05:04,500
都包含在这个搜索空间里面

128
00:05:04,500 --> 00:05:05,000
接着呢

129
00:05:05,000 --> 00:05:07,700
对这个搜索空间快速的去搜索

130
00:05:07,700 --> 00:05:11,200
找到最好的一个优化的实现的方式

131
00:05:11,200 --> 00:05:14,300
但实际上面对第一个和第二个问题的步骤

132
00:05:14,300 --> 00:05:16,400
有非常多大的挑战

133
00:05:16,400 --> 00:05:18,400
第一个就是这个搜索空间这么大

134
00:05:18,400 --> 00:05:21,700
保证所有的人工优化的方法都包含在里面

135
00:05:21,700 --> 00:05:24,700
本来就不一定能够完完全全的实现

136
00:05:24,700 --> 00:05:27,600
第二个如何快速的搜索这个空间

137
00:05:27,600 --> 00:05:30,100
也是一个很大的一个挑战

138
00:05:30,100 --> 00:05:31,200
面对这个问题呢

139
00:05:31,200 --> 00:05:33,200
越来越多的计算机科学家

140
00:05:33,200 --> 00:05:34,500
或者 AI 科学家呢

141
00:05:34,500 --> 00:05:37,100
或者工程师投身到这个领域里面

142
00:05:37,100 --> 00:05:39,200
然后把我们的一些大量的循环

143
00:05:39,200 --> 00:05:41,100
或者大量的一些 Kernel 的算子

144
00:05:41,100 --> 00:05:43,300
进行一些优化

145
00:05:43,300 --> 00:05:45,300
去探索 auto-tuning 的方式

146
00:05:45,300 --> 00:05:46,000
那现在呢

147
00:05:46,000 --> 00:05:46,800
一般来说呢

148
00:05:46,800 --> 00:05:48,400
总结成为三个步骤

149
00:05:48,400 --> 00:05:50,800
那第一个步骤就是参数化

150
00:05:50,800 --> 00:05:53,100
primitivization 就是参数化

151
00:05:53,100 --> 00:05:53,600
第二个呢

152
00:05:53,600 --> 00:05:55,300
就是成本函数

153
00:05:55,300 --> 00:05:56,500
建立一个 cost model 呢

154
00:05:56,500 --> 00:05:59,600
去评价我们的参数化到底合不合理

155
00:05:59,600 --> 00:06:01,300
有了参数化成本函数之后呢

156
00:06:01,300 --> 00:06:03,700
就需要建立一个搜索算法

157
00:06:03,700 --> 00:06:05,000
search algorithm

158
00:06:05,000 --> 00:06:06,500
在搜索空间里面呢

159
00:06:06,500 --> 00:06:08,600
去找到一个比较好的算法

160
00:06:08,600 --> 00:06:10,300
比较好的参数化的方式

161
00:06:10,300 --> 00:06:12,700
让我们的成本函数最小化

162
00:06:12,700 --> 00:06:15,800
那接下来我们逐个的打开每一个步骤

163
00:06:17,000 --> 00:06:17,800
第一个步骤呢

164
00:06:17,800 --> 00:06:20,400
就是参数化的内容

165
00:06:20,400 --> 00:06:21,500
参数化这个概念呢

166
00:06:21,500 --> 00:06:22,700
听上去比较实在

167
00:06:22,700 --> 00:06:23,400
但实际上呢

168
00:06:23,400 --> 00:06:25,500
我们主要是对调度优化的问题呢

169
00:06:25,500 --> 00:06:26,800
进行建模

170
00:06:26,800 --> 00:06:27,900
参数化优化的空间

171
00:06:27,900 --> 00:06:29,400
或者我们的搜索空间呢

172
00:06:29,400 --> 00:06:31,000
主要是由我们在上一节里面

173
00:06:31,000 --> 00:06:33,800
给大家去介绍的一些 loop 的一些方法

174
00:06:33,800 --> 00:06:35,600
不管是循环优化

175
00:06:35,600 --> 00:06:36,200
指令优化

176
00:06:36,200 --> 00:06:37,500
还是内存优化

177
00:06:37,500 --> 00:06:39,100
它们都可以组成一些

178
00:06:39,100 --> 00:06:40,700
相对固定的组合结构

179
00:06:41,800 --> 00:06:44,500
变成一些实际上可以调度的一些言语

180
00:06:44,500 --> 00:06:45,600
有了这些言语呢

181
00:06:45,600 --> 00:06:48,500
我们就可以对它进行参数化的表示

182
00:06:48,500 --> 00:06:49,200
那海里德呢

183
00:06:49,200 --> 00:06:51,100
就将算法和调度结合出来

184
00:06:51,100 --> 00:06:52,500
变成一个调度术

185
00:06:52,500 --> 00:06:53,000
TBM呢

186
00:06:53,000 --> 00:06:54,600
就提供了调度模板

187
00:06:54,600 --> 00:06:55,700
通过调度模板呢

188
00:06:55,700 --> 00:06:58,100
对我们的调度术进行一个表达

189
00:06:58,100 --> 00:07:00,600
我们来看一下下面这条公式

190
00:07:00,600 --> 00:07:01,100
我们呢

191
00:07:01,100 --> 00:07:02,800
有 bxts 里面呢

192
00:07:02,800 --> 00:07:05,900
就声明我的 xc 这一个数据的内容

193
00:07:05,900 --> 00:07:08,000
进行 split 循环切分

194
00:07:08,000 --> 00:07:09,500
因子呢是 64

195
00:07:09,500 --> 00:07:10,800
而 factor 等于 64 呢

196
00:07:10,800 --> 00:07:13,800
就是我们可能的一个参数取值的空间

197
00:07:13,800 --> 00:07:14,700
或者范围

198
00:07:14,700 --> 00:07:17,000
我们的 factor 可以等于各种内容

199
00:07:17,000 --> 00:07:17,800
或者各种参数

200
00:07:17,800 --> 00:07:20,000
让我们整个空间进行一个搜索的

201
00:07:20,000 --> 00:07:22,200
这个就是参数化的作用

202
00:07:22,200 --> 00:07:22,700
接着呢

203
00:07:22,700 --> 00:07:23,700
有了参数化之后

204
00:07:23,700 --> 00:07:25,900
我们需要建立一个成本函数

205
00:07:25,900 --> 00:07:27,000
就是 cost model

206
00:07:27,000 --> 00:07:28,100
这个 cost model 呢

207
00:07:28,100 --> 00:07:29,500
主要是用来评价

208
00:07:29,500 --> 00:07:32,000
在某一个参数化的调度性能下面呢

209
00:07:32,000 --> 00:07:34,300
去评估这个调子策略

210
00:07:34,300 --> 00:07:35,800
到底好还是不好

211
00:07:35,800 --> 00:07:36,800
那怎么评价呢

212
00:07:36,900 --> 00:07:38,200
从哪几个维度呢

213
00:07:38,200 --> 00:07:41,000
更多的我们有几种评价的手段

214
00:07:41,000 --> 00:07:41,400
第一种呢

215
00:07:41,400 --> 00:07:42,700
就是运行的时间

216
00:07:42,700 --> 00:07:44,000
真正的运行时间

217
00:07:44,000 --> 00:07:44,400
第二个呢

218
00:07:44,400 --> 00:07:47,200
就是内存的占用的大小

219
00:07:47,200 --> 00:07:47,800
第三个呢

220
00:07:47,800 --> 00:07:50,300
就是编译后指令的数量来去评价的

221
00:07:50,300 --> 00:07:51,900
但是现在来说呀

222
00:07:51,900 --> 00:07:54,300
为了让我们的成本函数计算的越快

223
00:07:54,300 --> 00:07:57,400
我们一般的更关注的是运行的时间

224
00:07:57,400 --> 00:08:00,200
运行的越快肯定是越好的

225
00:08:00,200 --> 00:08:01,000
实现方式呢

226
00:08:01,000 --> 00:08:02,000
主要分为三种

227
00:08:02,000 --> 00:08:02,400
第一种呢

228
00:08:02,400 --> 00:08:04,700
是大家能够想到的最简单的

229
00:08:04,700 --> 00:08:07,800
就是基于我们的NPU的硬件的一个黑盒模式

230
00:08:07,800 --> 00:08:09,700
就真正的我们的成本函数

231
00:08:09,700 --> 00:08:11,800
跑在我们的AI加速芯片上面

232
00:08:11,800 --> 00:08:12,500
那但是呢

233
00:08:12,500 --> 00:08:14,000
这种方式是最耗时

234
00:08:14,000 --> 00:08:15,800
也是最慢的

235
00:08:15,800 --> 00:08:18,100
好处就是非常的准确

236
00:08:18,100 --> 00:08:18,600
第二种呢

237
00:08:18,600 --> 00:08:20,100
就是模拟的预定义模型

238
00:08:20,100 --> 00:08:22,500
就我们现在所谓的模拟

239
00:08:22,500 --> 00:08:23,200
模拟什么呢

240
00:08:23,200 --> 00:08:24,800
模拟我们的NPU

241
00:08:24,800 --> 00:08:25,700
那这种方式呢

242
00:08:25,700 --> 00:08:26,600
也是非常耗时

243
00:08:26,600 --> 00:08:29,600
而且建模的手段非常的复杂

244
00:08:29,600 --> 00:08:30,200
第三种呢

245
00:08:30,200 --> 00:08:33,700
就是基于机器学习的ML-based的一种模型

246
00:08:33,800 --> 00:08:37,300
通过机器学习来对调度性能进行预测

247
00:08:37,300 --> 00:08:38,400
那这第三种呢

248
00:08:38,400 --> 00:08:39,800
就是TBM使用的方式

249
00:08:39,800 --> 00:08:43,400
也是现在来说相对成熟的一种方案

250
00:08:43,400 --> 00:08:44,900
有了成本函数之后呢

251
00:08:44,900 --> 00:08:47,400
第三步就是进行一个搜索

252
00:08:47,400 --> 00:08:49,400
对我们的搜索空间进行搜索

253
00:08:49,400 --> 00:08:50,800
对我们的参数化的内容

254
00:08:50,800 --> 00:08:53,800
对我们参数化的调度方式进行搜索

255
00:08:53,800 --> 00:08:57,100
以找到一个性能最好的参数的配置的方案

256
00:08:57,100 --> 00:09:00,300
那现在的搜索方法或者搜索算法有非常多

257
00:09:00,300 --> 00:09:01,200
有一生算法了

258
00:09:01,200 --> 00:09:02,100
模拟退货算法了

259
00:09:02,100 --> 00:09:03,100
还有强化学习

260
00:09:03,100 --> 00:09:04,300
非常多的算法

261
00:09:04,300 --> 00:09:08,000
我们现在来看看TBM的一个站里面是怎么去实现的

262
00:09:08,000 --> 00:09:08,600
首先呢

263
00:09:08,600 --> 00:09:09,400
TBM里面呢

264
00:09:09,400 --> 00:09:10,900
分为非常多的站

265
00:09:10,900 --> 00:09:11,300
但是呢

266
00:09:11,300 --> 00:09:13,200
我们基本上从这个框开始

267
00:09:13,200 --> 00:09:15,700
就是我们AutoTuning的内容

268
00:09:15,700 --> 00:09:18,100
AutoTuning的第一个内容就对应于Section

269
00:09:18,100 --> 00:09:19,600
是我们的参数化

270
00:09:19,600 --> 00:09:21,100
我们把Tensor Expression

271
00:09:21,100 --> 00:09:23,200
就是把我们Tensor的表达方式

272
00:09:23,200 --> 00:09:24,000
处理出来

273
00:09:24,000 --> 00:09:25,200
类似于海力德的

274
00:09:25,200 --> 00:09:28,700
把我们调度和计算的逻辑分离出来

275
00:09:28,700 --> 00:09:30,200
右边这个灰色的框呢

276
00:09:30,200 --> 00:09:32,200
就是硬件杆子的优化的一些

277
00:09:32,200 --> 00:09:33,600
元语Permittive

278
00:09:33,600 --> 00:09:34,800
有了参数化之后呢

279
00:09:34,800 --> 00:09:36,700
我们就可以很好的对我们的计算

280
00:09:36,700 --> 00:09:37,900
还有对我们的硬件呢

281
00:09:37,900 --> 00:09:39,400
进行一个表示

282
00:09:39,400 --> 00:09:40,500
有了这种表示之后呢

283
00:09:40,500 --> 00:09:43,200
就建立了我们的成本的模型

284
00:09:43,200 --> 00:09:44,100
就Cost Model

285
00:09:44,100 --> 00:09:45,100
这个成本模型呢

286
00:09:45,100 --> 00:09:47,800
就是使用了机器学习的一个算法

287
00:09:47,800 --> 00:09:49,300
进行去评价的

288
00:09:49,300 --> 00:09:50,100
至于搜索呢

289
00:09:50,100 --> 00:09:51,400
我们就在这里面呢

290
00:09:51,400 --> 00:09:54,300
对Loop Programming进行一个不断的迭代搜索

291
00:09:54,300 --> 00:09:55,700
搜索我们的整个空间

292
00:09:55,700 --> 00:09:58,000
使得我们的成本函数最优

293
00:09:58,000 --> 00:09:59,300
找到对应的策略之后呢

294
00:09:59,300 --> 00:10:00,900
就利用LVM

295
00:10:00,900 --> 00:10:01,900
或者其他编程语言

296
00:10:01,900 --> 00:10:03,300
或者底层的编译器呢

297
00:10:03,300 --> 00:10:07,100
去实现或者生成对应的指令代码

298
00:10:07,100 --> 00:10:09,900
然后去给我们真正的硬件去部署使用

299
00:10:09,900 --> 00:10:10,500
那这种呢

300
00:10:10,500 --> 00:10:12,600
就是TVM的方式

301
00:10:12,600 --> 00:10:15,400
当然了TVM也迭代了好几个版本

302
00:10:15,400 --> 00:10:16,200
像Enso呢

303
00:10:16,200 --> 00:10:18,400
就是TVM最新的一个版本

304
00:10:18,400 --> 00:10:18,800
里面呢

305
00:10:18,800 --> 00:10:20,900
就避免了使用的模板的方式

306
00:10:20,900 --> 00:10:21,600
那这里面呢

307
00:10:21,600 --> 00:10:22,900
主要是有几个步骤

308
00:10:22,900 --> 00:10:24,900
那第一个步骤就是我们深度学习

309
00:10:24,900 --> 00:10:26,600
用AI框架进行表达之后呢

310
00:10:26,600 --> 00:10:28,400
我们就拿到它的一个子图

311
00:10:28,400 --> 00:10:29,000
那接着呢

312
00:10:29,000 --> 00:10:30,400
我们会对子图呢

313
00:10:30,400 --> 00:10:32,000
去做一个切分

314
00:10:32,000 --> 00:10:34,300
获取一些比较重要的一些子图

315
00:10:34,300 --> 00:10:36,100
拿到重要的一个子图之后呢

316
00:10:36,100 --> 00:10:38,100
我们就需要走到Session 4

317
00:10:38,100 --> 00:10:39,100
或者Session 5

318
00:10:39,100 --> 00:10:40,300
这个步骤里面的

319
00:10:40,300 --> 00:10:40,900
这里面呢

320
00:10:40,900 --> 00:10:41,900
主要有两个

321
00:10:41,900 --> 00:10:42,300
第一个呢

322
00:10:42,300 --> 00:10:43,700
Sketch Generation

323
00:10:43,700 --> 00:10:45,900
确定优化的主要的结构

324
00:10:45,900 --> 00:10:46,600
那第二个呢

325
00:10:46,600 --> 00:10:48,700
就是Rhythm Annotation

326
00:10:48,700 --> 00:10:49,700
随机注释呢

327
00:10:49,700 --> 00:10:51,100
主要是对应于右边

328
00:10:51,100 --> 00:10:52,300
我们看到这里面呢

329
00:10:52,300 --> 00:10:54,600
产生非常多的不同的循环

330
00:10:54,600 --> 00:10:56,400
或者不同的方式

331
00:10:56,400 --> 00:10:58,600
这里面会进行一些随机的初始化

332
00:10:58,600 --> 00:10:59,600
随机的Tooling

333
00:10:59,600 --> 00:11:01,300
或者随机的Unload

334
00:11:01,300 --> 00:11:04,200
优化的方式随机的进行Annotation

335
00:11:04,200 --> 00:11:06,900
产生了非常多的搜索的配置的方式

336
00:11:06,900 --> 00:11:07,700
接着下一步呢

337
00:11:07,700 --> 00:11:09,700
就是Performance Tooling

338
00:11:09,700 --> 00:11:10,200
这里面呢

339
00:11:10,200 --> 00:11:11,900
就使用了启发式的搜索方法

340
00:11:11,900 --> 00:11:13,800
还有我们的CrossModel去配合

341
00:11:13,800 --> 00:11:16,100
CrossModel也是一个可学习的CrossModel

342
00:11:16,100 --> 00:11:17,500
互相配合学习迭代

343
00:11:17,500 --> 00:11:20,400
最后学习到了一个比较好的策略之后

344
00:11:20,400 --> 00:11:23,400
就利用我们右边的Simple Programming Theory

345
00:11:23,400 --> 00:11:25,400
学习到一个比较明确的策略之后

346
00:11:25,400 --> 00:11:26,400
或者比较好的

347
00:11:26,400 --> 00:11:27,900
Kernels的优化方法之后呢

348
00:11:27,900 --> 00:11:31,800
就变成我们实际在硬件上面可以执行的

349
00:11:31,800 --> 00:11:32,500
一些指令

350
00:11:32,500 --> 00:11:34,000
然后进行一个黑盒测试

351
00:11:34,000 --> 00:11:35,200
黑盒测试完之后呢

352
00:11:35,200 --> 00:11:37,700
再迭代回来不断的进行优化

353
00:11:37,700 --> 00:11:38,300
那这种呢

354
00:11:38,300 --> 00:11:40,100
就是TBM最新的一种方法

355
00:11:41,700 --> 00:11:42,100
好了

356
00:11:42,100 --> 00:11:43,000
今天的内容呢

357
00:11:43,000 --> 00:11:44,000
就到这里为止

358
00:11:44,000 --> 00:11:44,800
谢谢各位

359
00:11:44,800 --> 00:11:45,700
摆了个掰

360
00:11:45,700 --> 00:11:46,600
卷的不行了

361
00:11:46,600 --> 00:11:47,500
卷的不行了

362
00:11:47,500 --> 00:11:49,300
记得一键三连加关注哦

363
00:11:49,300 --> 00:11:50,900
所有的内容都会开源在

364
00:11:50,900 --> 00:11:52,900
下面这条链接里面

365
00:11:52,900 --> 00:11:53,600
摆了个掰

