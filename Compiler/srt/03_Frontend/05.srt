1
00:00:00,000 --> 00:00:07,600
哈喽大家好,我是周米

2
00:00:07,600 --> 00:00:11,800
今天我们来到AI编译器系列里面的前端优化

3
00:00:11,800 --> 00:00:14,800
前端优化这里面我们会讲一个新的内容

4
00:00:14,800 --> 00:00:16,800
就是内存的分配

5
00:00:16,800 --> 00:00:19,800
内存分配是在我们前端优化里面

6
00:00:19,800 --> 00:00:23,100
基于计算图去杆子去做一个具体的计算

7
00:00:23,100 --> 00:00:26,300
但实际上内存分配的执行不是在这里面

8
00:00:26,300 --> 00:00:28,500
而是在具体的backend里面去执行的

9
00:00:28,500 --> 00:00:31,100
现在我们来到内存分配的内容里面

10
00:00:31,100 --> 00:00:34,100
今天我们要给大家带来三个内容

11
00:00:34,100 --> 00:00:38,500
第一个就看看模型和硬件关于内存相关的一个演进

12
00:00:38,500 --> 00:00:42,700
接着第二个内容我们去看看内存的划分和附用的好处

13
00:00:42,700 --> 00:00:46,100
AI的内存跟传统的内存有哪些区别

14
00:00:46,100 --> 00:00:49,700
第三个就是节省内存的具体的算法

15
00:00:49,700 --> 00:00:51,500
第三个可能是重点

16
00:00:51,500 --> 00:00:53,900
而第二个是概念的澄清

17
00:00:53,900 --> 00:00:58,200
接下来我们看一下我们现在在整个AI编译器的前端里面

18
00:00:58,300 --> 00:01:00,600
还是在图优化层里面

19
00:01:00,600 --> 00:01:03,300
内存的分配更多的是对我们的神经网络

20
00:01:03,300 --> 00:01:07,800
或者对我们的计算图进行杆子做一个预分配的工作

21
00:01:09,400 --> 00:01:10,200
来到第一个内容

22
00:01:10,200 --> 00:01:14,100
我们看看模型和硬件对内存的一个具体的需求

23
00:01:14,100 --> 00:01:16,000
可以看到其实最近这几年

24
00:01:16,000 --> 00:01:19,800
随着我们的AI或者神经网络的模型越来越大

25
00:01:19,800 --> 00:01:23,100
我们也是需要非常大量的显存空间的

26
00:01:23,200 --> 00:01:24,900
例如像一个Bot

27
00:01:24,900 --> 00:01:30,500
就Bot已经非常出名的一个椒麻鸡的鸡麻尖里面的一个小娃娃

28
00:01:30,500 --> 00:01:33,700
它里面有768个隐藏层Hidden Layer

29
00:01:33,700 --> 00:01:36,400
那Batch Size我们设置为64的时候

30
00:01:36,400 --> 00:01:39,300
就需要73个GB的显存空间

31
00:01:39,300 --> 00:01:42,500
73个GB一般的一些消费卡是塞不进去的

32
00:01:42,500 --> 00:01:46,500
训练一个Bot大部分都需要一个单机多卡的情况

33
00:01:46,500 --> 00:01:48,900
就利用分布式的功能了

34
00:01:48,900 --> 00:01:52,600
接下来我们看一下一个硬件的相关的能力

35
00:01:52,600 --> 00:01:56,600
这里面我就总结了一个英伟大的一个系列

36
00:01:56,600 --> 00:02:00,100
可以看到CUDA的核心数确实是不断的去增长

37
00:02:00,100 --> 00:02:04,100
而单浮减精度的性能确实也是增长的很厉害

38
00:02:04,100 --> 00:02:06,000
但是内存的容量可以看到

39
00:02:06,000 --> 00:02:10,500
其实内存的容量并不是说增加的非常的夸张

40
00:02:10,500 --> 00:02:14,000
而且基本上不同的型号有不同的内存的配置

41
00:02:14,000 --> 00:02:18,000
这个时候我们可以看到模型的参数量越来越大

42
00:02:18,000 --> 00:02:22,800
但是我们的硬件的内存的容量却没有变得特别的大

43
00:02:22,800 --> 00:02:26,300
那这个时候我们回顾一下深度学习的整个训练流程里面

44
00:02:26,300 --> 00:02:29,400
哪些流程需要用到大量的内存空间

45
00:02:29,400 --> 00:02:31,600
那第一个就是我们的Data

46
00:02:31,600 --> 00:02:34,900
我们的数的数据都要塞到我们的内存里面哦

47
00:02:34,900 --> 00:02:36,400
在具体的训练过程当中

48
00:02:36,400 --> 00:02:39,300
我们还要塞进去我们的神经网络

49
00:02:39,300 --> 00:02:42,600
那神经网络有包括正向的神经网络的图

50
00:02:42,600 --> 00:02:45,100
还有反向的神经网络的图

51
00:02:45,200 --> 00:02:48,800
正反向加起来我们的内存空间就变大了

52
00:02:48,800 --> 00:02:52,200
所以说大部分我们的内存的主要是消耗在数据

53
00:02:52,200 --> 00:02:54,200
还有我们的网络模型里面

54
00:02:56,200 --> 00:03:01,000
第二个内容就是具体在我们AI在我们神经网络里面的内存的划分

55
00:03:01,000 --> 00:03:02,000
到底分什么呢

56
00:03:02,000 --> 00:03:03,200
主要分为两个

57
00:03:03,200 --> 00:03:04,400
一个是静态的内存

58
00:03:04,400 --> 00:03:06,500
一个是动态的内存

59
00:03:06,500 --> 00:03:08,400
那我们现在看一下静态的内存

60
00:03:08,400 --> 00:03:10,100
静态的内存的主要有三个

61
00:03:10,100 --> 00:03:11,300
第一个是Parameter

62
00:03:11,300 --> 00:03:14,300
就是我们的网络模型当中的一些权重参数

63
00:03:14,400 --> 00:03:16,900
权重参数一本上就是不变的

64
00:03:16,900 --> 00:03:17,900
我申请了这么多

65
00:03:17,900 --> 00:03:21,000
就塞在内存里面不断的去训练学习更新

66
00:03:21,000 --> 00:03:22,500
接着第二个就是Value to know

67
00:03:22,500 --> 00:03:24,800
就是网络模型中的常量

68
00:03:24,800 --> 00:03:27,000
那有一些常量的不能够被直叠的

69
00:03:27,000 --> 00:03:30,700
这些常量的在训练的过程当中不断的去用到

70
00:03:30,700 --> 00:03:34,100
然后我们也是作为一个静态内存塞在我们的内存里面

71
00:03:34,100 --> 00:03:37,400
那第三个就是Output网络模型的输出

72
00:03:37,400 --> 00:03:39,800
那这个时候我们一般对静态内存

73
00:03:39,800 --> 00:03:42,200
就是一次过在初始化的时候

74
00:03:42,300 --> 00:03:46,200
申请完之后就不再在推理或者训练的场景里面

75
00:03:46,200 --> 00:03:47,500
频繁的申请

76
00:03:47,500 --> 00:03:50,300
从而提高我们整个系统的一个性能

77
00:03:50,300 --> 00:03:53,400
那我们现在以一个具体的图来看一下

78
00:03:53,400 --> 00:03:55,000
大部分这些静态内存

79
00:03:55,000 --> 00:03:57,000
主要是指我们的权重

80
00:03:57,000 --> 00:04:01,800
还有我们的优化器所对应到的一些内存空间

81
00:04:03,000 --> 00:04:04,400
下面我们看第二个内容

82
00:04:04,400 --> 00:04:06,200
就是动态内存

83
00:04:06,200 --> 00:04:08,400
那动态内存主要分开两个

84
00:04:08,400 --> 00:04:10,000
一个是Output Tensor

85
00:04:10,000 --> 00:04:11,200
这是我们算子

86
00:04:11,200 --> 00:04:12,300
每一层的算子哦

87
00:04:12,300 --> 00:04:13,300
不是总体哦

88
00:04:13,300 --> 00:04:15,500
是每一层的算子输出的Tensor

89
00:04:15,500 --> 00:04:17,100
我们作为一个动态的内存

90
00:04:17,100 --> 00:04:19,700
因为这里面有很多它可以复用的

91
00:04:19,700 --> 00:04:22,600
我们后面具体讲具体的算法时候就讲到了

92
00:04:22,600 --> 00:04:24,800
那第二个就是Workspace的Tensor

93
00:04:24,800 --> 00:04:26,800
如果有用户或者有开发者

94
00:04:26,800 --> 00:04:29,100
自己去写过一些CUDA的代码的时候

95
00:04:29,100 --> 00:04:30,800
我们就会发现很多时候

96
00:04:30,800 --> 00:04:32,100
我们在写CUDA之前

97
00:04:32,100 --> 00:04:34,900
我们就需要单独的去申请一个Workspace

98
00:04:34,900 --> 00:04:36,800
这种就是在我们的网络模型

99
00:04:36,800 --> 00:04:39,800
特别是在我们的Kernel计算的过程当中

100
00:04:39,800 --> 00:04:41,600
要申请的一些临时的Buffer

101
00:04:43,400 --> 00:04:46,600
动态内存是在我们整个神级网络里面

102
00:04:46,600 --> 00:04:48,400
占了非常大的一个大头

103
00:04:48,400 --> 00:04:50,200
我们看一看这个图

104
00:04:50,200 --> 00:04:52,500
动态内存的红色的框框

105
00:04:52,500 --> 00:04:55,200
它的内容确实比静态内存要多很多

106
00:04:55,200 --> 00:04:57,800
例如像这个就是我们卷机的时候

107
00:04:57,800 --> 00:04:59,800
额外的申请的一些额外的空间

108
00:04:59,800 --> 00:05:01,500
那我们网络模型的输出

109
00:05:01,500 --> 00:05:04,400
其实也是对应于我们的动态的内存的

110
00:05:07,100 --> 00:05:08,600
我们看一下下面这个图

111
00:05:08,600 --> 00:05:10,000
就是MobileNet VR

112
00:05:10,000 --> 00:05:12,300
那上面就是没有经过内存优化的

113
00:05:12,300 --> 00:05:15,400
一个具体的内存空间开辟的一个图

114
00:05:15,400 --> 00:05:18,300
下面就是经过内存优化之后的一个图

115
00:05:18,300 --> 00:05:20,100
可以看到我们以颜色来看

116
00:05:20,100 --> 00:05:22,800
这个是神经网络MobileNet VR的一个图结构

117
00:05:22,800 --> 00:05:24,500
那可以看到橙色的这一块

118
00:05:24,500 --> 00:05:27,300
就是做了内存优化之后的

119
00:05:27,300 --> 00:05:29,200
整体的内存的消耗量

120
00:05:29,200 --> 00:05:32,500
可以看到一个MobileNet VR的网络模型

121
00:05:32,500 --> 00:05:34,900
进行了一个内存优化之后

122
00:05:34,900 --> 00:05:36,200
它整体的网络模型

123
00:05:36,900 --> 00:05:39,300
内存空间的数是非常的少

124
00:05:39,300 --> 00:05:40,600
但是没有做优化之前

125
00:05:40,600 --> 00:05:42,300
就非常多倍了

126
00:05:42,300 --> 00:05:44,200
那现在怎么做优化呢

127
00:05:44,200 --> 00:05:45,700
我们继续往下看

128
00:05:47,700 --> 00:05:50,900
就是我们节省内存的算法

129
00:05:55,700 --> 00:05:57,700
在进行节省内存的算法之前

130
00:05:57,700 --> 00:06:00,500
我要讲四个算法

131
00:06:00,500 --> 00:06:01,400
这四个算法

132
00:06:01,400 --> 00:06:04,200
我们今天的重点就是内存的复用

133
00:06:04,200 --> 00:06:05,700
就是利用AI编译器

134
00:06:05,700 --> 00:06:08,900
对我们的计算图Graph IR的数据流进行分析

135
00:06:08,900 --> 00:06:11,500
复用内存是我们今天的重点

136
00:06:11,500 --> 00:06:14,200
但是我这里面也给大家介绍一下

137
00:06:14,200 --> 00:06:16,000
几个不同的算法

138
00:06:16,000 --> 00:06:18,900
那第一个就是空间换内存

139
00:06:18,900 --> 00:06:23,300
那这种其实在我们的传统的一些程序里面

140
00:06:23,300 --> 00:06:24,700
用的也很多

141
00:06:24,700 --> 00:06:27,200
那在AI里面或者在深度学习里面

142
00:06:27,200 --> 00:06:30,100
我们一般都会做一些CPU的offload

143
00:06:30,100 --> 00:06:31,500
就是把我们的内存空间

144
00:06:31,500 --> 00:06:35,100
把NPU的内存空间丢给CPU

145
00:06:35,100 --> 00:06:37,700
可能它临时用不到了这一个模块

146
00:06:37,700 --> 00:06:40,100
那就直接卸载到CPU里面

147
00:06:40,100 --> 00:06:41,300
那这种算法

148
00:06:41,300 --> 00:06:44,400
更多的是针对MOE的一些算法结构

149
00:06:44,400 --> 00:06:46,300
就是我们有很多个ESPR

150
00:06:46,300 --> 00:06:48,200
Multi of ESPR的这种算法

151
00:06:48,200 --> 00:06:50,900
或者模型结构去做一个优化的

152
00:06:52,300 --> 00:06:54,700
第二个就是计算换内存

153
00:06:54,700 --> 00:06:56,000
在计算换内存里面

154
00:06:56,000 --> 00:06:57,100
最重要的一个算法

155
00:06:57,100 --> 00:06:58,400
就是Gradient Checkpoint

156
00:06:58,400 --> 00:07:00,100
我们叫做重计算

157
00:07:00,100 --> 00:07:01,700
那基本上我们很多时候

158
00:07:01,800 --> 00:07:05,200
如果大家去把一些计算的中间结果

159
00:07:05,200 --> 00:07:07,400
存在我们的内存空间

160
00:07:07,400 --> 00:07:09,000
我们需要把从ALU

161
00:07:09,000 --> 00:07:10,600
就是我们的计算单元里面

162
00:07:10,600 --> 00:07:13,400
算出来的结果存到我们的内存空间

163
00:07:13,400 --> 00:07:16,000
这里面传输的速率就很重要了

164
00:07:16,000 --> 00:07:18,300
假设我重新再算一遍的速率

165
00:07:18,300 --> 00:07:21,200
都比我读取数据的效率要高

166
00:07:21,200 --> 00:07:23,800
那我肯定会选择重新算一把

167
00:07:23,800 --> 00:07:25,500
因为重新算一把的效率

168
00:07:25,500 --> 00:07:28,100
可能会比我直接存起来更快

169
00:07:28,100 --> 00:07:30,600
那这个时候我们叫做计算换内存

170
00:07:30,700 --> 00:07:33,000
在我们做一些大模型的时候

171
00:07:33,000 --> 00:07:35,200
这一个计算换内存的算法优化

172
00:07:35,200 --> 00:07:36,500
也是非常常用的

173
00:07:38,000 --> 00:07:40,400
第三点就是模型压缩

174
00:07:40,400 --> 00:07:42,600
模型压缩这个在端次推理的时候

175
00:07:42,600 --> 00:07:44,400
特别常用

176
00:07:44,400 --> 00:07:47,300
例如会做一些低匹德量化的一些工作

177
00:07:47,300 --> 00:07:48,300
还有模型减资

178
00:07:48,300 --> 00:07:50,900
还有模型征流相关的一些算法

179
00:07:50,900 --> 00:07:53,100
这些都是内存节省的算法

180
00:07:53,100 --> 00:07:55,800
而今天的主角就是我们真正的内存附用

181
00:07:55,800 --> 00:08:00,600
内存附用里面有几个操作

182
00:08:00,600 --> 00:08:02,900
我现在给大家简单的讲一讲

183
00:08:02,900 --> 00:08:06,300
那第一种就是替换的操作

184
00:08:06,300 --> 00:08:08,200
我们叫做Embrace Operation

185
00:08:08,200 --> 00:08:09,600
就是一个内存

186
00:08:09,600 --> 00:08:12,400
假设这里面是一个左边是一个计算图

187
00:08:12,400 --> 00:08:14,200
每个方框是一个节点

188
00:08:14,200 --> 00:08:16,800
三角形代表的是我们的内存

189
00:08:16,800 --> 00:08:18,700
那如果有一块的内存

190
00:08:18,700 --> 00:08:22,100
算完这一块就这个B算子里面不用了

191
00:08:22,100 --> 00:08:24,600
而且下一个算子也是Elementwise

192
00:08:24,600 --> 00:08:26,400
就跟它的操作是一模一样的

193
00:08:26,400 --> 00:08:28,200
我们可以原地的覆盖掉

194
00:08:28,200 --> 00:08:29,500
就这个内存空间

195
00:08:29,500 --> 00:08:32,700
可以直接用下一个内存空间覆盖掉

196
00:08:32,700 --> 00:08:33,800
就不需要了

197
00:08:33,800 --> 00:08:36,700
那这种我们叫做Embrace Operation

198
00:08:36,700 --> 00:08:38,100
但是有种情况

199
00:08:38,100 --> 00:08:40,200
我们由右边的这个图所示

200
00:08:40,200 --> 00:08:42,400
像这个B算完之后

201
00:08:42,400 --> 00:08:44,400
如果没有另外一个分子

202
00:08:44,400 --> 00:08:45,700
绿色的这个分子

203
00:08:45,700 --> 00:08:48,800
其实我C可以做一个Embrace Operation

204
00:08:48,800 --> 00:08:50,300
直接把它覆盖掉的

205
00:08:50,300 --> 00:08:53,600
但是因为我们需要一个F等于B加2

206
00:08:53,600 --> 00:08:56,700
就是我需要B的这个计算的结果去加一个2

207
00:08:56,700 --> 00:08:59,000
那这个时候B的结果其实是有用的

208
00:08:59,000 --> 00:09:01,600
它就不能做一个Embrace Operation了

209
00:09:01,600 --> 00:09:03,100
就这块内存还是有用的

210
00:09:03,100 --> 00:09:04,100
你不能把它改写

211
00:09:04,100 --> 00:09:05,400
你不能把它复用掉

212
00:09:07,700 --> 00:09:09,000
第二种内存优化的方式

213
00:09:09,000 --> 00:09:11,000
我们叫做内存的共享

214
00:09:11,000 --> 00:09:12,300
Memory Sharing

215
00:09:13,500 --> 00:09:15,300
在我们的计算图里面

216
00:09:15,300 --> 00:09:16,900
这个就是我们的计算图

217
00:09:16,900 --> 00:09:18,900
如果有两个数据的内存

218
00:09:18,900 --> 00:09:21,100
使用的大小空间是一样

219
00:09:21,100 --> 00:09:22,300
那这个时候

220
00:09:22,400 --> 00:09:24,600
上面的不需要参与计算了

221
00:09:24,600 --> 00:09:27,700
那后面的其实可以重新的去覆盖掉

222
00:09:27,700 --> 00:09:29,600
去共享这一块的内存

223
00:09:29,600 --> 00:09:30,700
当然了这一块的内存

224
00:09:30,700 --> 00:09:33,200
我们不用重复的去申请和释放

225
00:09:33,200 --> 00:09:36,100
只需要进行一个共享内存空间就可以了

226
00:09:36,100 --> 00:09:37,900
那如下面这个图所示

227
00:09:37,900 --> 00:09:39,500
我们以一个例子

228
00:09:39,500 --> 00:09:41,300
A是一个上面的一个数据

229
00:09:41,300 --> 00:09:43,800
W是下面的一个权重的数据

230
00:09:43,800 --> 00:09:45,500
B是一个卷积的算子

231
00:09:45,500 --> 00:09:46,800
那我们的内存空间

232
00:09:46,800 --> 00:09:48,400
已经开辟出了一个三角形

233
00:09:48,400 --> 00:09:49,500
C去执行的时候

234
00:09:49,500 --> 00:09:51,300
因为这个三角形的内存空间

235
00:09:51,300 --> 00:09:53,300
跟上面的内存空间大小是不一样的

236
00:09:53,300 --> 00:09:55,000
他们的计算也是不一样的

237
00:09:55,000 --> 00:09:55,800
这个时候

238
00:09:55,800 --> 00:09:58,800
他们之间的内存是不能做一个sharing的

239
00:09:58,800 --> 00:10:00,500
下面又有一个E的算子

240
00:10:00,500 --> 00:10:02,700
那这个算子的内存空间的大小

241
00:10:02,700 --> 00:10:05,000
假设相关的参数是相同的

242
00:10:05,000 --> 00:10:07,600
这个时候它的内存空间大小相同

243
00:10:07,600 --> 00:10:10,200
那这种情况就可以直接去复用掉了

244
00:10:10,200 --> 00:10:11,900
因为上面跟下面之间

245
00:10:11,900 --> 00:10:13,700
没有太多的依赖关系

246
00:10:13,700 --> 00:10:15,800
而且在算E算子的时候

247
00:10:15,800 --> 00:10:16,900
B算子已经算完了

248
00:10:16,900 --> 00:10:17,600
不需要了

249
00:10:17,600 --> 00:10:20,300
我们就可以对B算子的内存空间

250
00:10:20,300 --> 00:10:21,800
进行一个共享

251
00:10:23,200 --> 00:10:24,600
下面我们总体了

252
00:10:24,600 --> 00:10:26,800
来看一下内存空间的优化方法

253
00:10:26,800 --> 00:10:28,500
就是我们刚才讲到的两种

254
00:10:28,500 --> 00:10:30,600
第一种就是in-place operation

255
00:10:30,600 --> 00:10:31,700
内存替换

256
00:10:31,700 --> 00:10:33,600
第二种就是memory sharing

257
00:10:33,600 --> 00:10:35,600
内存的一个共享

258
00:10:35,600 --> 00:10:38,000
那可以看到左边的这是正向图

259
00:10:38,000 --> 00:10:39,500
像我们刚才举的例子

260
00:10:39,500 --> 00:10:41,200
只是一个简单的正向图

261
00:10:41,200 --> 00:10:44,900
右边的是正向图跟反向图都有的

262
00:10:44,900 --> 00:10:48,100
那这个时候我们的模型就变得复杂了

263
00:10:48,100 --> 00:10:50,900
如果我们要对正反向图都有的模型

264
00:10:50,900 --> 00:10:53,400
这么复杂的模型做一个内存的优化

265
00:10:53,400 --> 00:10:55,600
实际上它并不简单

266
00:10:55,600 --> 00:11:00,100
这时候就涉及到了很多新的算法的提出

267
00:11:00,100 --> 00:11:04,500
怎么样才能够正确的去分配具体的内存

268
00:11:04,500 --> 00:11:06,900
去分配我们的AI计算图的内存

269
00:11:08,100 --> 00:11:11,100
那这个它跟我们的传统的编译器的

270
00:11:11,100 --> 00:11:14,200
寄存器的一个内存的分配非常相似

271
00:11:14,200 --> 00:11:16,800
我们可以借鉴很多的思想

272
00:11:19,000 --> 00:11:21,400
我们现在简单的去解读一下

273
00:11:21,400 --> 00:11:22,900
那这个内存的优化方法

274
00:11:22,900 --> 00:11:24,500
其实跟我们传统的编译器的

275
00:11:24,500 --> 00:11:26,300
内存的优化方法有点像

276
00:11:26,300 --> 00:11:27,700
我们简单来看一下

277
00:11:27,700 --> 00:11:30,300
左边的这一个就是我们的计算图

278
00:11:30,300 --> 00:11:31,300
中间的虚线

279
00:11:31,300 --> 00:11:33,600
就是我们计算图的一个数据的流布

280
00:11:33,600 --> 00:11:35,000
我们在初始化的时候

281
00:11:35,000 --> 00:11:37,500
去感知到我们每一个算子

282
00:11:37,500 --> 00:11:39,300
被使用被调用到多少次

283
00:11:39,300 --> 00:11:41,900
像这个B算子我就被调用到两次

284
00:11:41,900 --> 00:11:44,500
它有两个使用有两个依赖

285
00:11:44,500 --> 00:11:46,300
那其他算子基本上都是1

286
00:11:46,300 --> 00:11:49,200
那这个算子G算子就是最后的输出

287
00:11:49,200 --> 00:11:50,400
这是第零部

288
00:11:50,400 --> 00:11:52,100
我们需要做一个标记

289
00:11:54,400 --> 00:11:56,300
接着我们在step1的时候

290
00:11:56,300 --> 00:11:58,100
我们会分配一个tag

291
00:11:58,100 --> 00:12:00,200
对我们的B算子分配一个tag

292
00:12:00,200 --> 00:12:02,300
就像红色的这个小模块

293
00:12:02,300 --> 00:12:06,200
在分配一个新的tag的时候

294
00:12:06,200 --> 00:12:07,700
我们可以发现C的算子

295
00:12:07,700 --> 00:12:08,600
跟我们的B算子

296
00:12:08,600 --> 00:12:11,200
其实不能做一个impressed的操作的

297
00:12:11,200 --> 00:12:14,100
因为B的算子它的生命周期还在

298
00:12:14,100 --> 00:12:15,000
它没有变零

299
00:12:15,000 --> 00:12:16,100
它只是变成1了

300
00:12:16,100 --> 00:12:17,800
就是我们下面已经用了一次

301
00:12:17,800 --> 00:12:18,900
它的生命周期了

302
00:12:18,900 --> 00:12:20,300
但是这两个不能impressed

303
00:12:20,300 --> 00:12:22,200
因为它没有变零

304
00:12:22,200 --> 00:12:25,200
接着我们再往下看第三步

305
00:12:25,200 --> 00:12:28,100
第三步我们去计算F算子的时候

306
00:12:28,100 --> 00:12:29,100
我分配一个tag

307
00:12:29,100 --> 00:12:32,300
那这个时候F算子已经计算完了

308
00:12:32,300 --> 00:12:34,000
上面B算子可以释放出来

309
00:12:34,000 --> 00:12:36,500
所以我们释放到对应的一个对列里面

310
00:12:36,500 --> 00:12:39,900
对列里面我们就有一个B算子的空间

311
00:12:39,900 --> 00:12:42,600
那第四步我们继续往下去执行

312
00:12:42,600 --> 00:12:43,400
执行的时候

313
00:12:43,400 --> 00:12:46,300
其实这个C算子已经执行完了

314
00:12:46,300 --> 00:12:47,500
那C算子执行完了

315
00:12:47,500 --> 00:12:48,300
执行完之后

316
00:12:48,300 --> 00:12:50,400
它的内存空间也会放进来

317
00:12:50,400 --> 00:12:52,700
我们的内存对列

318
00:12:52,700 --> 00:12:55,000
但这个时候我要执行一个E算子

319
00:12:55,000 --> 00:12:56,900
所以我们会把刚才的内存对列的

320
00:12:56,900 --> 00:12:58,900
那个红色的小方块

321
00:12:58,900 --> 00:13:00,300
把它push出来

322
00:13:00,300 --> 00:13:02,600
给到我们的一个E算子

323
00:13:02,600 --> 00:13:06,600
去进行一个内存的memory sharing

324
00:13:06,600 --> 00:13:09,000
而这里面对应的标记

325
00:13:09,000 --> 00:13:11,600
也会去进行修改的

326
00:13:11,600 --> 00:13:12,500
那在这个时候

327
00:13:12,500 --> 00:13:15,200
我们可以看到在进行下一个计算的时候

328
00:13:15,200 --> 00:13:17,600
假设它的内存空间都是相同的

329
00:13:17,600 --> 00:13:20,000
我们就可以用相关的内存模块

330
00:13:20,000 --> 00:13:21,900
进行一个impaste的操作

331
00:13:21,900 --> 00:13:25,900
那我们最后通过这种方式进行迭代下来

332
00:13:25,900 --> 00:13:28,900
看到最后的final memory的plane

333
00:13:28,900 --> 00:13:29,800
为啥叫plane

334
00:13:29,800 --> 00:13:31,400
因为它不是实际执行

335
00:13:31,400 --> 00:13:34,200
而是一个预执行预分配的工作

336
00:13:34,200 --> 00:13:36,400
可以看到这里面的不同的颜色

337
00:13:36,400 --> 00:13:39,400
代表不同的内存的分配的模块

338
00:13:39,400 --> 00:13:40,800
和内存的分配的空间

339
00:13:40,800 --> 00:13:42,400
这里面三个红色的

340
00:13:42,400 --> 00:13:45,100
就可以各自的去进行一个复用

341
00:13:45,100 --> 00:13:46,500
而中间的那两块

342
00:13:46,500 --> 00:13:48,300
确实就没办法进行复用了

343
00:13:48,300 --> 00:13:51,000
这个就是最原始的一个内存优化的方法

344
00:13:53,800 --> 00:13:55,500
诶 钟鸣老师你好啊

345
00:13:55,500 --> 00:13:56,600
我有一个问题

346
00:13:56,600 --> 00:13:59,900
就是你刚才讲到的一些内存分配的方法

347
00:13:59,900 --> 00:14:01,600
都是串行的

348
00:14:01,600 --> 00:14:04,100
并行的时候怎么办呢

349
00:14:04,100 --> 00:14:05,400
诶 小新同学

350
00:14:05,400 --> 00:14:07,800
这个还是个灵魂问题啊

351
00:14:07,800 --> 00:14:12,200
确实我们会遇到很多并行的操作

352
00:14:12,200 --> 00:14:14,500
假设我们现在有一个计算图啊

353
00:14:14,500 --> 00:14:15,400
这个计算图呢

354
00:14:15,400 --> 00:14:17,400
有1 2 3 4 5 6 7 8

355
00:14:17,400 --> 00:14:18,400
8个算子

356
00:14:18,400 --> 00:14:20,100
就从A1算到A8

357
00:14:20,100 --> 00:14:22,100
如果我只是以串行的方式去运行

358
00:14:22,100 --> 00:14:23,700
我就左边运行一个

359
00:14:23,700 --> 00:14:24,500
右边运行一个

360
00:14:24,500 --> 00:14:25,100
左边运行一个

361
00:14:25,100 --> 00:14:26,200
右边运行一个

362
00:14:26,200 --> 00:14:27,100
这种方式呢

363
00:14:27,100 --> 00:14:28,900
这两种分配的方案呢

364
00:14:28,900 --> 00:14:30,000
或者我们刚才讲到

365
00:14:30,000 --> 00:14:31,100
in place和memory

366
00:14:31,100 --> 00:14:32,400
shareway两种方案

367
00:14:32,400 --> 00:14:34,500
都是能够去执行的

368
00:14:34,500 --> 00:14:35,000
但是呢

369
00:14:35,000 --> 00:14:37,000
这种方式引入了非常多的依赖

370
00:14:37,000 --> 00:14:38,300
例如我们可以看到

371
00:14:38,300 --> 00:14:38,900
基本上啊

372
00:14:38,900 --> 00:14:41,200
我们有很多的并行的依赖

373
00:14:41,200 --> 00:14:43,100
就左边跟右边是相同的

374
00:14:43,100 --> 00:14:44,700
我们有内存相关的依赖

375
00:14:44,700 --> 00:14:46,600
以刚才的这种算法去分配呢

376
00:14:46,600 --> 00:14:50,700
就很难做到一个并行的运行操作

377
00:14:50,700 --> 00:14:52,200
就假设我左边这条分支

378
00:14:52,200 --> 00:14:54,500
给一个NPU核去运行

379
00:14:54,500 --> 00:14:55,500
右边的这个分支呢

380
00:14:55,500 --> 00:14:57,400
给另外一个NPU核去运行

381
00:14:57,400 --> 00:14:58,300
那这种方式呢

382
00:14:58,300 --> 00:14:59,900
其实是没办法去做的

383
00:14:59,900 --> 00:15:02,400
因为它有内存的依赖

384
00:15:02,400 --> 00:15:02,900
于是呢

385
00:15:02,900 --> 00:15:06,200
我们可以向右边的这种分配的方式

386
00:15:06,200 --> 00:15:07,500
我左边的分配

387
00:15:07,500 --> 00:15:09,400
以左边的这条分支作为分配

388
00:15:09,400 --> 00:15:10,000
右边的

389
00:15:10,000 --> 00:15:11,900
我以右边的这条分支

390
00:15:11,900 --> 00:15:14,700
自己做一个具体的分配

391
00:15:14,700 --> 00:15:15,300
那这种呢

392
00:15:15,300 --> 00:15:16,800
就是以左边的这条分支

393
00:15:16,800 --> 00:15:18,200
作为一个单独的内存分配

394
00:15:18,200 --> 00:15:19,400
右边的内存分支呢

395
00:15:19,400 --> 00:15:20,800
作为单独的分配

396
00:15:20,800 --> 00:15:24,600
就可以进行一个并行的操作了

397
00:15:24,600 --> 00:15:25,500
那在这里面呢

398
00:15:25,500 --> 00:15:27,600
我没有提到某一种具体的算法

399
00:15:27,600 --> 00:15:29,600
这里面我更希望大家去创造

400
00:15:29,600 --> 00:15:31,200
更多的内存的分配的算法

401
00:15:31,200 --> 00:15:33,900
和发表更多相关的论文

402
00:15:33,900 --> 00:15:34,700
在这里面呢

403
00:15:34,700 --> 00:15:36,400
我们有一条最基本的原则

404
00:15:36,400 --> 00:15:39,000
就是尽可能的允许更多的并行

405
00:15:39,000 --> 00:15:39,600
毕竟啊

406
00:15:39,600 --> 00:15:40,900
我们的MPU或者GPU

407
00:15:40,900 --> 00:15:42,500
一些AI的加速芯片

408
00:15:42,500 --> 00:15:44,900
是有大量的并行的一些能力的

409
00:15:44,900 --> 00:15:47,000
所以我们希望引入更多的并行

410
00:15:47,000 --> 00:15:49,000
而不是简单的串行的依赖

411
00:15:49,000 --> 00:15:49,800
那这个时候呢

412
00:15:49,800 --> 00:15:51,300
我们怎么去分配呢

413
00:15:51,300 --> 00:15:52,800
一般的分配的过程当中呢

414
00:15:52,800 --> 00:15:55,900
我们就会去尝试找到每个图里面的

415
00:15:55,900 --> 00:15:57,400
最长的路径

416
00:15:57,400 --> 00:15:59,300
然后根据这个最长的路径

417
00:15:59,300 --> 00:16:02,800
进行一个简单的in place和shared memory的操作

418
00:16:05,100 --> 00:16:07,500
针对另外的独立的路径呢

419
00:16:07,500 --> 00:16:10,900
我们再独立的进行in place和memory的操作

420
00:16:10,900 --> 00:16:11,800
那这种方式呢

421
00:16:11,800 --> 00:16:13,500
就可以最大程度的利用到

422
00:16:13,500 --> 00:16:15,000
我们系统的并行能力

423
00:16:16,900 --> 00:16:17,600
最后这个图呢

424
00:16:17,600 --> 00:16:19,400
就是M3light的一个对比里面呢

425
00:16:19,400 --> 00:16:22,000
就用了in place和shared memory的一个操作

426
00:16:22,000 --> 00:16:22,800
那shared memory呢

427
00:16:22,800 --> 00:16:25,000
它这里面叫做co-share

428
00:16:25,000 --> 00:16:26,200
其实都是一样的

429
00:16:26,200 --> 00:16:27,500
就是一个概念

430
00:16:27,500 --> 00:16:28,400
然后可以看到啊

431
00:16:28,400 --> 00:16:31,000
把两个算法结合起来

432
00:16:31,000 --> 00:16:32,800
对我们的内存的优化

433
00:16:32,800 --> 00:16:34,600
和对我们的内存的消耗啊

434
00:16:34,600 --> 00:16:36,700
确实有一个比较大的提升

435
00:16:36,700 --> 00:16:39,500
对我们原始的就naive的这种方式呢

436
00:16:39,500 --> 00:16:41,200
都有一个比较好的提升

437
00:16:41,200 --> 00:16:42,400
对不同的网络模型

438
00:16:46,200 --> 00:16:46,700
好了

439
00:16:46,700 --> 00:16:48,500
今天我们来一个总结

440
00:16:48,500 --> 00:16:50,900
就是我们现在的AI编译器啊

441
00:16:50,900 --> 00:16:51,600
大部分呢

442
00:16:51,600 --> 00:16:54,200
都是用Graph IR进行巧妙的

443
00:16:54,200 --> 00:16:56,600
对我们的内存进行分配

444
00:16:56,600 --> 00:16:57,500
那今天的内容呢

445
00:16:57,500 --> 00:16:58,500
就到这里为止

446
00:16:58,500 --> 00:16:59,400
谢谢各位

447
00:16:59,400 --> 00:17:01,100
卷的不行了卷的不行了

448
00:17:01,100 --> 00:17:02,900
记得一键三连加关注哦

449
00:17:03,000 --> 00:17:04,500
所有的内容都会开源在

450
00:17:04,500 --> 00:17:06,500
下面这条链接里面

451
00:17:06,500 --> 00:17:07,200
拜拜

