1
00:00:00,000 --> 00:00:07,120
嗨,大家好,我是周明

2
00:00:07,120 --> 00:00:11,800
今天我们来到AI编译器里面的前端优化的常量直迭

3
00:00:11,800 --> 00:00:14,800
那常量直迭我们叫做ConstantFold

4
00:00:15,480 --> 00:00:18,080
这常量直迭里面主要是有两个概念

5
00:00:18,080 --> 00:00:19,280
今天的内容不会很多

6
00:00:19,280 --> 00:00:22,680
第一个就是传统编译器里面的常量直迭这个概念

7
00:00:22,680 --> 00:00:27,240
接着我们去看看AI编译器里面的常量直迭具体是怎么实现的

8
00:00:28,240 --> 00:00:33,400
现在我们来了解一下传统编译器里面一个比较通用的概念

9
00:00:33,400 --> 00:00:35,680
也就是常量直迭ConstantFolding

10
00:00:35,680 --> 00:00:38,080
常量直迭这个概念是比较简单

11
00:00:38,080 --> 00:00:40,320
通过对编译是常量或者常量表达式

12
00:00:40,320 --> 00:00:43,800
进行计算来简化整体的代码

13
00:00:43,800 --> 00:00:46,400
那我们简单的去看一下第三行的代码

14
00:00:46,400 --> 00:00:50,360
我们i等于320×200×32

15
00:00:50,360 --> 00:00:53,760
在传统的编译器里面我们把它变成一个i

16
00:00:53,760 --> 00:00:55,880
首先我会产生一个变量的a

17
00:00:55,880 --> 00:00:57,920
那这个a去加载我的320

18
00:00:57,920 --> 00:01:00,160
另外一个变量b去加载200

19
00:01:00,160 --> 00:01:04,240
接着这个x就是我的计算a×b

20
00:01:04,240 --> 00:01:07,800
然后我再有一个变量去加载32

21
00:01:07,800 --> 00:01:11,400
接着有一个变量y等于x×c

22
00:01:11,400 --> 00:01:14,840
也就是x×c完成了整一个操作

23
00:01:14,840 --> 00:01:17,920
这里面我们有三次的内存读取

24
00:01:17,920 --> 00:01:21,080
首先a b c三次内存读取的load

25
00:01:21,080 --> 00:01:23,000
然后有两个指令的计算

26
00:01:23,000 --> 00:01:25,480
就是x和y两个都是相乘

27
00:01:25,480 --> 00:01:26,600
所以总结一下

28
00:01:26,600 --> 00:01:28,080
我们有三次的内存读取

29
00:01:28,080 --> 00:01:29,480
两次的指令相乘

30
00:01:29,480 --> 00:01:32,080
但实际上的第三行代码

31
00:01:32,080 --> 00:01:33,880
320×200×32

32
00:01:33,880 --> 00:01:36,560
实际上我们可以直接从内存里面

33
00:01:36,560 --> 00:01:39,880
直接把这个数加载进来204800

34
00:01:39,880 --> 00:01:41,720
就是我在编译的时候

35
00:01:41,720 --> 00:01:45,480
直接把320×200×32这个数

36
00:01:45,480 --> 00:01:47,360
提前的算出来

37
00:01:47,360 --> 00:01:50,080
这个算出来的工作就交给我们的编译器

38
00:01:50,080 --> 00:01:52,680
而不用到我们真正执行的时候再去算

39
00:01:53,680 --> 00:01:56,120
我们现在来看看下一个概念

40
00:01:56,120 --> 00:01:58,120
就是常量的传播

41
00:01:58,120 --> 00:01:59,560
constant propagation

42
00:01:59,560 --> 00:02:00,440
常量传播

43
00:02:00,440 --> 00:02:02,680
这也是我们传统编译器优化里面的

44
00:02:02,680 --> 00:02:03,800
其中一个手段

45
00:02:03,800 --> 00:02:05,200
可以在一段代码里面

46
00:02:05,200 --> 00:02:06,720
将表达式的常量

47
00:02:06,720 --> 00:02:08,960
替换成为相关表达式

48
00:02:08,960 --> 00:02:10,160
然后再使用常量字叠

49
00:02:10,160 --> 00:02:12,000
来进行一个代码的简化

50
00:02:12,000 --> 00:02:15,600
我们来看看两个比较典型的一个概念

51
00:02:15,600 --> 00:02:17,880
首先我们有一个函数-

52
00:02:17,880 --> 00:02:19,600
现在定义一个

53
00:02:19,640 --> 00:02:21,280
备量x等于14

54
00:02:21,280 --> 00:02:23,520
y等于7-x除以2

55
00:02:23,520 --> 00:02:26,160
其实我们的x可以直接代进来

56
00:02:26,160 --> 00:02:30,040
然后我们return y×28除以x加2

57
00:02:30,040 --> 00:02:32,280
那这个时候我x等于14

58
00:02:32,280 --> 00:02:33,720
可以直接代进来

59
00:02:33,720 --> 00:02:34,600
那代进来之后

60
00:02:34,600 --> 00:02:36,480
y我们已经知道了

61
00:02:36,480 --> 00:02:39,440
我把常量s进行往下传播

62
00:02:39,440 --> 00:02:41,800
最后会使用一个常量值

63
00:02:41,800 --> 00:02:42,960
叠这个字数

64
00:02:42,960 --> 00:02:44,360
来简化我们的代码

65
00:02:44,360 --> 00:02:46,120
那常量值叠体现在哪了

66
00:02:46,120 --> 00:02:47,960
就是我最后x已经知道了

67
00:02:48,000 --> 00:02:49,720
我y其实可以通过编译器

68
00:02:49,720 --> 00:02:50,960
来提前算出来

69
00:02:50,960 --> 00:02:52,200
算出来这个y之后

70
00:02:52,200 --> 00:02:53,600
我这个y已经有了

71
00:02:53,600 --> 00:02:55,160
那这个y已经求得到

72
00:02:55,160 --> 00:02:55,880
我return的时候

73
00:02:55,880 --> 00:02:56,960
再通过编译器

74
00:02:56,960 --> 00:02:59,440
把这个数提前预算出来

75
00:02:59,440 --> 00:03:01,080
那我们的函数-

76
00:03:01,080 --> 00:03:02,560
就已经得到它的值了

77
00:03:02,560 --> 00:03:05,240
就不需要通过我们的硬件再去计算

78
00:03:05,240 --> 00:03:07,040
直接在编译的过程当中

79
00:03:07,040 --> 00:03:09,400
输出我们-的一个返回值

80
00:03:10,720 --> 00:03:13,680
这种方式就是先通过常量传播

81
00:03:13,680 --> 00:03:15,040
再使用常量值叠

82
00:03:15,040 --> 00:03:16,720
来去简化我们的代码

83
00:03:17,120 --> 00:03:18,920
是不是觉得我们的编译器很有意思

84
00:03:18,920 --> 00:03:20,760
我们不用去真正的去执行

85
00:03:20,760 --> 00:03:21,720
我们在编译阶段

86
00:03:21,720 --> 00:03:24,520
已经把这些数提前的算好了

87
00:03:27,840 --> 00:03:29,320
现在我们看一个新的概念

88
00:03:29,320 --> 00:03:31,720
就是AI编译器和常量值叠

89
00:03:31,720 --> 00:03:32,880
常量值叠怎么

90
00:03:32,880 --> 00:03:34,680
幻化到我们的AI编译器

91
00:03:34,680 --> 00:03:37,240
怎么使用图层按压进行表达

92
00:03:38,320 --> 00:03:40,960
我们看一下这里面有个非常有意思的概念

93
00:03:40,960 --> 00:03:41,840
就是常量值叠

94
00:03:42,080 --> 00:03:44,400
虽然我们是用在传统的编译器的

95
00:03:44,400 --> 00:03:46,520
但是这里面我们会将计算图

96
00:03:46,520 --> 00:03:49,200
可以预先确定的输出值的节点

97
00:03:49,200 --> 00:03:50,800
替换成为常量

98
00:03:50,800 --> 00:03:53,960
就是把我们的计算图某些点变成一个常量

99
00:03:53,960 --> 00:03:57,360
然后对计算图进行一些结构的优化

100
00:03:57,360 --> 00:03:59,640
这个就是实际的过程

101
00:03:59,640 --> 00:04:01,720
我们往下看一些具体的例子

102
00:04:01,720 --> 00:04:03,680
假设我们现在以addN

103
00:04:03,680 --> 00:04:05,320
作为一个具体的例子

104
00:04:05,320 --> 00:04:07,920
对于两个形状大小为NCHW

105
00:04:07,920 --> 00:04:09,880
视为的张量Tensor

106
00:04:09,880 --> 00:04:11,720
这个张量它是一个常量

107
00:04:11,720 --> 00:04:14,720
就是NCHW我所有数都是这样知道的

108
00:04:14,720 --> 00:04:16,200
add的结果也是一定的

109
00:04:16,320 --> 00:04:18,560
这个结果我们就可以通过AI编译器

110
00:04:18,560 --> 00:04:20,040
提前的算出来

111
00:04:21,360 --> 00:04:22,760
提前算出来有什么好处

112
00:04:22,920 --> 00:04:24,640
就是不需要给add节点的

113
00:04:24,640 --> 00:04:26,640
额外的分配存储资源的

114
00:04:26,640 --> 00:04:27,880
在AI编辑器里面

115
00:04:28,080 --> 00:04:29,640
直接基于计算图

116
00:04:29,640 --> 00:04:30,720
直接算出来

117
00:04:30,720 --> 00:04:33,440
不需要反复的去计算add这个操作

118
00:04:33,440 --> 00:04:35,600
可以进行直接的内存访问

119
00:04:35,600 --> 00:04:36,920
这个所谓的好处

120
00:04:37,080 --> 00:04:38,680
其实跟我们传统编译器是一样的

121
00:04:38,680 --> 00:04:40,120
我们看看一个具体的例子

122
00:04:40,400 --> 00:04:43,800
左边的图是我们计算图其中的一部分

123
00:04:43,800 --> 00:04:45,680
我们addN这个算子

124
00:04:46,000 --> 00:04:48,680
它的输入是一个常量的张量

125
00:04:48,680 --> 00:04:50,680
B也是一个常量的张量

126
00:04:50,880 --> 00:04:53,680
这个时候我可以先把Tensor A

127
00:04:53,680 --> 00:04:56,720
跟Tensor B在编译器里面进行相加

128
00:04:56,720 --> 00:04:58,960
得到我的Tensor X的时候

129
00:04:59,240 --> 00:05:02,480
Tensor X再给我们卷积进行计算

130
00:05:02,720 --> 00:05:05,600
这种方式就可以简化了我们的计算图

131
00:05:05,600 --> 00:05:07,480
计算图里面的内容少了

132
00:05:07,480 --> 00:05:08,960
我就可以提前编译出来

133
00:05:08,960 --> 00:05:09,960
提前算好

134
00:05:09,960 --> 00:05:11,720
然后真正在硬件执行的时候

135
00:05:11,840 --> 00:05:14,360
就不需要再次调度我们的addA

136
00:05:14,400 --> 00:05:15,640
也不需要从内存里面

137
00:05:15,800 --> 00:05:18,440
再去加载我们的A和B这个张量

138
00:05:18,440 --> 00:05:20,000
直接加载X这个张量

139
00:05:20,000 --> 00:05:22,000
然后进行去计算就完了

140
00:05:23,000 --> 00:05:25,680
下面我们再看一个比较经典的例子

141
00:05:25,680 --> 00:05:27,640
就是Bn折叠

142
00:05:28,400 --> 00:05:30,360
这个Bn折叠也是ZOMI

143
00:05:30,480 --> 00:05:31,520
最开始的时候

144
00:05:31,520 --> 00:05:33,800
写AI编译器里面实现的一个功能

145
00:05:34,240 --> 00:05:36,080
我们看一下Batch Normalization

146
00:05:36,360 --> 00:05:38,080
这个算子的作用

147
00:05:38,200 --> 00:05:39,280
它最主要的作用

148
00:05:39,400 --> 00:05:42,320
是对我们各层的输入进行一个规划

149
00:05:42,320 --> 00:05:44,640
然后再给我们下一层训练学习的

150
00:05:44,640 --> 00:05:46,560
使得我们训练学习的目的

151
00:05:46,720 --> 00:05:48,360
是希望能够在训练过程当中

152
00:05:48,520 --> 00:05:49,560
数据更加稳定

153
00:05:49,560 --> 00:05:50,320
更加收敛

154
00:05:50,640 --> 00:05:51,720
在实践的过程当中

155
00:05:51,840 --> 00:05:53,480
它是一个额外的一层

156
00:05:53,480 --> 00:05:55,080
通常会放在卷机

157
00:05:55,080 --> 00:05:55,600
MathML

158
00:05:55,720 --> 00:05:57,800
全数码码等计算之后

159
00:05:57,800 --> 00:06:00,160
在一些非线性激活之前

160
00:06:00,160 --> 00:06:01,880
就是卷机Bn比录

161
00:06:02,040 --> 00:06:04,360
我们可以看到卷机跟非线性层

162
00:06:04,360 --> 00:06:06,160
之间就会插一个Bn

163
00:06:06,160 --> 00:06:07,160
Batch Normalization

164
00:06:07,440 --> 00:06:09,040
主要分为两个步骤

165
00:06:09,560 --> 00:06:12,320
第一步是先减去平均值

166
00:06:12,320 --> 00:06:14,560
然后再除以我们的标准差

167
00:06:15,080 --> 00:06:16,560
接着去计算我们的

168
00:06:16,560 --> 00:06:18,880
γ缩放和β的偏移

169
00:06:19,640 --> 00:06:21,760
具体的公式就是下面这两个

170
00:06:21,760 --> 00:06:23,440
首先我计算我的均值

171
00:06:23,440 --> 00:06:24,720
计算我的方差

172
00:06:25,040 --> 00:06:26,080
对我们的数的数据

173
00:06:26,280 --> 00:06:27,360
减去我们的均值

174
00:06:27,360 --> 00:06:29,200
然后除以我们的标准差

175
00:06:29,600 --> 00:06:31,440
最后通过γ和β

176
00:06:31,440 --> 00:06:33,440
进行一个缩放和偏移

177
00:06:33,880 --> 00:06:36,160
得到Bn的计算结果

178
00:06:36,880 --> 00:06:38,680
我们现在看看

179
00:06:38,880 --> 00:06:40,280
一旦我们训练结束之后

180
00:06:40,480 --> 00:06:42,160
每一个边层都有β

181
00:06:42,160 --> 00:06:43,960
γ还有均值和方差

182
00:06:44,320 --> 00:06:45,800
其实在训练的过程当中

183
00:06:45,960 --> 00:06:48,160
这两个是可学习的参数

184
00:06:48,160 --> 00:06:50,440
这两个也是可学习的参数

185
00:06:50,440 --> 00:06:52,080
但是在推理场景

186
00:06:52,080 --> 00:06:54,160
我们这些参数都已经固定了

187
00:06:54,160 --> 00:06:55,480
就它不需要学习了

188
00:06:55,480 --> 00:06:57,080
这个时候我们就可以

189
00:06:57,080 --> 00:06:59,040
这个时候我们就可以对上面的公式

190
00:06:59,040 --> 00:07:00,840
进行一个简单的先行转换

191
00:07:00,840 --> 00:07:02,360
我们现在看一下

192
00:07:02,360 --> 00:07:04,800
由于Bn经常放在卷机后面

193
00:07:04,840 --> 00:07:07,080
而卷机是一个线性的变换

194
00:07:07,080 --> 00:07:09,000
这个时候意味着我们两个操作

195
00:07:09,120 --> 00:07:11,280
可以合成一个简单的操作

196
00:07:11,760 --> 00:07:13,640
这个我们叫做常量值迭

197
00:07:13,640 --> 00:07:15,800
需要跟算子融合区分一下

198
00:07:15,800 --> 00:07:17,320
因为这里面最主要的作用

199
00:07:17,440 --> 00:07:18,720
就是把我们的一些常量

200
00:07:19,040 --> 00:07:21,440
在A边系里面提前算出来

201
00:07:22,680 --> 00:07:23,560
在推理场景

202
00:07:23,760 --> 00:07:26,160
这个就是我们简单的卷机的操作

203
00:07:26,320 --> 00:07:27,560
由于这里面的超差

204
00:07:27,720 --> 00:07:29,400
其实我们在训练的过程当中

205
00:07:29,400 --> 00:07:30,280
已经得到了

206
00:07:30,280 --> 00:07:32,760
所以我们会对它进行一个常量的值迭

207
00:07:33,040 --> 00:07:33,760
这个常量值迭

208
00:07:33,760 --> 00:07:35,640
我们又叫做Bn值迭

209
00:07:35,640 --> 00:07:38,600
我们的Wflow可以通过提前预算出来

210
00:07:38,600 --> 00:07:41,320
我们的Bflow也可以通过提前预算出来

211
00:07:41,480 --> 00:07:43,600
我们AI编译器在编译的阶段

212
00:07:43,760 --> 00:07:44,800
就把这些超差

213
00:07:44,800 --> 00:07:46,320
跟我们的权重参数

214
00:07:46,320 --> 00:07:47,920
算成我们的Wflow

215
00:07:47,920 --> 00:07:49,120
把我们的Bflow的

216
00:07:49,280 --> 00:07:51,840
也是在AI编译器里面提前算好

217
00:07:51,880 --> 00:07:54,160
然后在真正推理的时候

218
00:07:54,360 --> 00:07:56,440
就没有了Bn这一层了

219
00:07:56,640 --> 00:07:58,720
直接把Bn层算出来的超差

220
00:07:59,040 --> 00:08:00,760
放在我们的权重里面

221
00:08:00,880 --> 00:08:03,480
最后只执行这一条卷机的操作

222
00:08:03,480 --> 00:08:07,120
就完成了整个Bn加卷机的计算了

223
00:08:07,680 --> 00:08:09,120
所以我们理解一下

224
00:08:09,120 --> 00:08:10,400
就是Wflow Bflow

225
00:08:10,640 --> 00:08:13,200
这个都是在我们AI编译器里面去实现的

226
00:08:15,600 --> 00:08:16,560
下面这个表

227
00:08:16,720 --> 00:08:19,520
就是Bn值迭之后的一个性能的对比

228
00:08:19,520 --> 00:08:21,120
可以看到我们以GPU为例子

229
00:08:21,480 --> 00:08:22,600
在Wesnet50

230
00:08:22,600 --> 00:08:24,200
就是我们Bn值迭之前

231
00:08:24,520 --> 00:08:28,200
它运算速率执行一个Wesnet50是11.03

232
00:08:28,200 --> 00:08:29,360
但是Bn值迭之后

233
00:08:29,480 --> 00:08:32,480
它整体的运算速率是7.3毫秒

234
00:08:32,480 --> 00:08:34,960
可以看到性能提升了51%

235
00:08:34,960 --> 00:08:37,200
还是有一个非常高的性能的提升的

236
00:08:37,200 --> 00:08:39,120
而我们的编译器最重要的作用

237
00:08:39,320 --> 00:08:41,680
就是帮助我们提前算好一些工作

238
00:08:41,680 --> 00:08:44,280
免得我们在真正执行的时候再去算

239
00:08:44,280 --> 00:08:46,640
为了提升我们的运行效率

240
00:08:49,200 --> 00:08:50,360
最后我们看一下

241
00:08:50,360 --> 00:08:51,520
在AI编译器里面

242
00:08:51,680 --> 00:08:53,840
怎么去实现常量折叠的

243
00:08:53,840 --> 00:08:56,640
刚才其实我们只是举了两个例子

244
00:08:56,640 --> 00:08:57,320
一个是add

245
00:08:57,320 --> 00:08:58,720
一个是Bn折叠

246
00:08:58,720 --> 00:09:01,480
实际上AI编译器会通过一些

247
00:09:01,480 --> 00:09:05,120
人工提前预设的规则去做一些折叠的

248
00:09:05,120 --> 00:09:07,440
那我们看一下常量折叠有几个分类

249
00:09:07,440 --> 00:09:10,280
第一个就是传统编辑器里面的常量折叠

250
00:09:10,280 --> 00:09:13,520
找到输入节点均为常量的一些节点

251
00:09:13,520 --> 00:09:15,640
然后去提前算出来

252
00:09:15,640 --> 00:09:16,600
例如add

253
00:09:16,600 --> 00:09:18,160
第二种就是常量折叠

254
00:09:18,160 --> 00:09:20,640
与数据的类型shape有关系

255
00:09:20,640 --> 00:09:22,560
我们通过计算图已有的信息

256
00:09:22,760 --> 00:09:24,320
去推断出这个shape之后

257
00:09:24,600 --> 00:09:27,400
然后再来替换掉原来的节点

258
00:09:27,680 --> 00:09:29,480
这种最典型的例子

259
00:09:29,640 --> 00:09:30,280
有shape

260
00:09:30,400 --> 00:09:31,000
transformer

261
00:09:31,080 --> 00:09:31,480
size

262
00:09:31,640 --> 00:09:33,040
这种这些节点

263
00:09:33,040 --> 00:09:35,040
我们可以提前把它算好

264
00:09:35,040 --> 00:09:36,840
那第三个就是常量折叠

265
00:09:36,840 --> 00:09:39,800
跟已有的常量的代数关系相关的

266
00:09:39,800 --> 00:09:40,880
这种最经典的

267
00:09:40,880 --> 00:09:43,960
就是我们刚才讲到的卷积Bn这些

268
00:09:43,960 --> 00:09:46,120
我们已经人工提前算好

269
00:09:47,680 --> 00:09:49,160
下面这个算法

270
00:09:49,160 --> 00:09:50,720
虽然我是1234列出来

271
00:09:50,720 --> 00:09:52,000
但它其实是一个算法

272
00:09:52,000 --> 00:09:53,720
就是TensorFlow常量折叠的parts

273
00:09:53,720 --> 00:09:54,600
是怎么处理的

274
00:09:54,600 --> 00:09:56,520
首先TensorFlow先去处理

275
00:09:56,520 --> 00:09:56,840
shape

276
00:09:56,840 --> 00:09:57,200
size

277
00:09:57,200 --> 00:09:57,760
length

278
00:09:57,760 --> 00:09:59,360
这三类的节点

279
00:09:59,360 --> 00:10:00,560
这三类节点的输出

280
00:10:00,720 --> 00:10:02,640
都取决于输入Tensor的形状

281
00:10:02,800 --> 00:10:05,160
跟具体的数字值是没有关系的

282
00:10:05,160 --> 00:10:07,360
所以可以提前把它算出来

283
00:10:07,920 --> 00:10:10,720
算出来之后就转换成为constant节点

284
00:10:10,720 --> 00:10:12,800
就是转换成为一些常量的节点

285
00:10:13,600 --> 00:10:15,840
接着去折叠计算图里面的

286
00:10:15,840 --> 00:10:17,200
一些常量的操作

287
00:10:17,200 --> 00:10:19,080
如果在变异整个计算图的时候

288
00:10:19,200 --> 00:10:21,520
看到的节点的数都是常量

289
00:10:21,600 --> 00:10:23,040
那么这个节点的输出

290
00:10:23,240 --> 00:10:25,040
也都可以提前算出来的

291
00:10:26,160 --> 00:10:27,200
至于这个原理

292
00:10:27,400 --> 00:10:29,360
去叠到我们的整个计算图

293
00:10:29,640 --> 00:10:30,520
变异计算图

294
00:10:30,520 --> 00:10:32,320
直到没有任何常量的节点

295
00:10:32,320 --> 00:10:33,200
可以替换为止

296
00:10:33,440 --> 00:10:35,040
第4步就是去处理

297
00:10:35,040 --> 00:10:37,520
sum min max这种point max的节点

298
00:10:38,160 --> 00:10:39,520
将这些节点替换成

299
00:10:39,520 --> 00:10:40,960
我们的占位服务的节点

300
00:10:41,160 --> 00:10:44,200
这种就是TensorFlow常量折叠的一个parts

301
00:10:44,200 --> 00:10:46,400
而我们其实在真正实现的时候

302
00:10:46,960 --> 00:10:48,040
更重要的是发现

303
00:10:48,040 --> 00:10:50,800
类似于bn折叠这类型的规则

304
00:10:51,040 --> 00:10:51,480
好了

305
00:10:51,480 --> 00:10:52,920
今天的内容到这里为止

306
00:10:52,960 --> 00:10:53,640
谢谢各位

307
00:10:53,640 --> 00:10:54,160
拜了

308
00:10:54,160 --> 00:10:54,680
拜拜

