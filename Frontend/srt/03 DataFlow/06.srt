1
00:00:00,000 --> 00:00:02,000
哈喽大家好,我是钟咪

2
00:00:02,000 --> 00:00:04,000
昨天录的太晚

3
00:00:04,000 --> 00:00:06,000
所以今天上班的时候

4
00:00:06,000 --> 00:00:08,000
经常在摸鱼

5
00:00:10,000 --> 00:00:11,000
废话不多说了

6
00:00:11,000 --> 00:00:13,000
我们来看看今天一个很重要的内容

7
00:00:13,000 --> 00:00:15,000
也是我觉得计算图里面

8
00:00:15,000 --> 00:00:17,000
除了自动微分之外

9
00:00:17,000 --> 00:00:19,000
第二个最重要的内容了

10
00:00:19,000 --> 00:00:21,000
就是我们的控制流

11
00:00:22,000 --> 00:00:23,000
那控制流里面

12
00:00:23,000 --> 00:00:25,000
我们今天主要是讲几个概念

13
00:00:25,000 --> 00:00:27,000
第一个就是我们的控制流

14
00:00:27,000 --> 00:00:29,000
第二个就是我们的控制流

15
00:00:29,000 --> 00:00:31,000
我们今天主要是讲几个概念

16
00:00:31,000 --> 00:00:33,000
第一个去看看什么是控制流

17
00:00:33,000 --> 00:00:35,000
控制流跟计算图是怎么表示的

18
00:00:35,000 --> 00:00:37,000
还有它们之间的关系

19
00:00:37,000 --> 00:00:39,000
接着去讲讲动态图

20
00:00:39,000 --> 00:00:41,000
引起的控制流的问题

21
00:00:41,000 --> 00:00:43,000
还有静态图跟控制流的关系

22
00:00:43,000 --> 00:00:45,000
最后我们看看

23
00:00:45,000 --> 00:00:47,000
现在我们经常去讲

24
00:00:47,000 --> 00:00:49,000
TensorFu也好,Pytorch也好

25
00:00:49,000 --> 00:00:51,000
都会做一些动静统一的结合和技术

26
00:00:51,000 --> 00:00:53,000
它到底是个什么样子的

27
00:00:59,000 --> 00:01:04,000
最后我们来看看动态图转换成为静态图

28
00:01:04,000 --> 00:01:06,000
这也是MindSpot这个AI框架

29
00:01:06,000 --> 00:01:10,000
首次提出动静态图统一的概念

30
00:01:10,000 --> 00:01:13,000
后来有了TensorFu推出了AutoGrade

31
00:01:13,000 --> 00:01:15,000
Pytorch推出了JIT

32
00:01:15,000 --> 00:01:17,000
都是使用这种方式

33
00:01:17,000 --> 00:01:19,000
也是希望能够把动态图转换成为静态图

34
00:01:19,000 --> 00:01:23,000
这个概念是MindSpot头包第一次提出的

35
00:01:24,000 --> 00:01:27,000
下面我们来看看动态图转静态图

36
00:01:27,000 --> 00:01:28,000
主要有两种方式

37
00:01:28,000 --> 00:01:30,000
第一种就是基于追踪

38
00:01:30,000 --> 00:01:31,000
就是Trace的方式

39
00:01:31,000 --> 00:01:34,000
第二种就是基于原码转换AST

40
00:01:34,000 --> 00:01:36,000
抽象语法术的方式

41
00:01:36,000 --> 00:01:38,000
不要着急

42
00:01:38,000 --> 00:01:40,000
下面我们来逐个算法去展开

43
00:01:40,000 --> 00:01:42,000
具体是怎么实现的

44
00:01:42,000 --> 00:01:45,000
首先第一个就是基于追踪的方式

45
00:01:45,000 --> 00:01:47,000
基于追踪的方式比较直接

46
00:01:47,000 --> 00:01:49,000
就是直接执行用户的代码

47
00:01:49,000 --> 00:01:50,000
直接执行Python代码

48
00:01:50,000 --> 00:01:52,000
记录下算子的调用的序列

49
00:01:52,000 --> 00:01:55,000
把这些序列的保存成为一个静态图

50
00:01:56,000 --> 00:01:59,000
然后在问探的时候去执行静态图就好了

51
00:01:59,000 --> 00:02:03,000
优点就是非常广泛的去支持数字语言的各种控制流

52
00:02:03,000 --> 00:02:06,000
这个其实更像PyTorch的方式

53
00:02:06,000 --> 00:02:08,000
就PyTorch原生的方式

54
00:02:08,000 --> 00:02:11,000
但是问题就是场景非常受限

55
00:02:11,000 --> 00:02:15,000
只能保存有限次执行的轨迹并且先行化

56
00:02:15,000 --> 00:02:19,000
静态图就失去了原程序的完整的控制流

57
00:02:19,000 --> 00:02:23,000
就是我们可以这么简单的理解

58
00:02:23,000 --> 00:02:26,000
我基于用户的代码进行一个追踪

59
00:02:26,000 --> 00:02:31,000
但是我不可能把用户的所有的程序都执行一遍

60
00:02:31,000 --> 00:02:32,000
既然不能执行一遍

61
00:02:32,000 --> 00:02:34,000
那我可能只能执行一两遍

62
00:02:34,000 --> 00:02:37,000
去做一个检测代码或者做一个便利

63
00:02:37,000 --> 00:02:39,000
知道它的代码结构

64
00:02:39,000 --> 00:02:41,000
然后变成一个静态图

65
00:02:41,000 --> 00:02:47,000
所以这种方式其实在实际的AI框架或者AI系统里面用的比较少

66
00:02:47,000 --> 00:02:49,000
用的更多的我们这里面

67
00:02:49,000 --> 00:02:52,000
我们这里面以Pytorch的JIT为例子

68
00:02:52,000 --> 00:02:54,000
就Just in time

69
00:02:54,000 --> 00:02:57,000
那天我在群里面跟一些同事去聊

70
00:02:57,000 --> 00:03:00,000
他们说你一听到JIT

71
00:03:00,000 --> 00:03:04,000
十个用户有九个用户都不知道JIT到底是什么

72
00:03:04,000 --> 00:03:07,000
另外一个同事又跳出来补充了一句

73
00:03:07,000 --> 00:03:12,000
十个用户有9.999个用户不知道JIT是什么

74
00:03:12,000 --> 00:03:14,000
JIT就是Jaxed in time

75
00:03:14,000 --> 00:03:17,000
也就是在运行时动态编译

76
00:03:17,000 --> 00:03:19,000
计算机实际运行的是机器码

77
00:03:19,000 --> 00:03:23,000
我们通过把前端的这些高级语言变成我们的汇编语言

78
00:03:23,000 --> 00:03:28,000
然后每一条汇编都对应一条对应一串机器码

79
00:03:28,000 --> 00:03:34,000
而JIT Just in time就是在内存里面生成和运行一段代码

80
00:03:34,000 --> 00:03:37,000
就是边执行边运行这么理解就好了

81
00:03:37,000 --> 00:03:40,000
下面我们来看看基于原码解析的方式

82
00:03:40,000 --> 00:03:44,000
基于原码解析的方式就是我们把数字语言

83
00:03:44,000 --> 00:03:50,000
就是把Python的语言通过抽象语法数把它变成一个内部的语法数

84
00:03:50,000 --> 00:03:52,000
也就是这种内部的语法数

85
00:03:52,000 --> 00:03:54,000
Internal AST数

86
00:03:54,000 --> 00:03:58,000
然后经过SSA等各种编译优化之后

87
00:03:58,000 --> 00:04:01,000
再做一些类型的推导等不同的pass

88
00:04:01,000 --> 00:04:04,000
然后最终转换成为一个计算图

89
00:04:04,000 --> 00:04:07,000
这种方式就是基于原码的方式

90
00:04:07,000 --> 00:04:11,000
下面这个左边就是Pytorch的一个JIT script

91
00:04:11,000 --> 00:04:13,000
右边就是它的一个编译过程

92
00:04:13,000 --> 00:04:15,000
首先输入的是一个Python的代码

93
00:04:15,000 --> 00:04:20,000
然后通过一些解析器把它变成一个Python的AST抽象语法数

94
00:04:20,000 --> 00:04:24,000
然后再对一些latest进行解析语法进行解析

95
00:04:24,000 --> 00:04:26,000
然后转换成为内部的语法数

96
00:04:26,000 --> 00:04:30,000
然后再对它进行一个IR的转换、IR的优化

97
00:04:30,000 --> 00:04:33,000
最后就变成一个可执行的IR

98
00:04:33,000 --> 00:04:35,000
然后给sqlter进行执行

99
00:04:35,000 --> 00:04:41,000
这个就是动态图转换成为静态图的一个最典型的方式

100
00:04:41,000 --> 00:04:43,000
那我们看看它有什么优缺点

101
00:04:43,000 --> 00:04:46,000
因为每一个解决方案它背后都有优缺点

102
00:04:46,000 --> 00:04:49,000
其实就在于我们怎么去选择

103
00:04:49,000 --> 00:04:51,000
小孩子才判断对错

104
00:04:51,000 --> 00:04:53,000
但人只权衡利弊

105
00:04:53,000 --> 00:04:56,000
那在AI框架或者AI系统的方案选择

106
00:04:56,000 --> 00:04:58,000
我们也只看利弊

107
00:05:03,000 --> 00:05:08,000
那第一个优点就是非常能够广泛的去支持数数语言的各种动态控制理由

108
00:05:09,000 --> 00:05:11,000
就各种if else while大部分是支持的

109
00:05:11,000 --> 00:05:15,000
缺点可能跟MindSpot的缺点其实差不多

110
00:05:15,000 --> 00:05:20,000
就是后端的实现的硬件和软件对静态图还是有一定的约束的

111
00:05:20,000 --> 00:05:22,000
就是它不能灵活的去表达

112
00:05:22,000 --> 00:05:29,000
那第二个就是数字语言的控制理由并不是完全一定能够映射过来的

113
00:05:29,000 --> 00:05:33,000
那第三个就是遇到过度灵活的动态控制理由

114
00:05:33,000 --> 00:05:37,000
运行的时候就会回退到由前端语言和后端的调用

115
00:05:37,000 --> 00:05:39,000
去不断的去轮回执行

116
00:05:39,000 --> 00:05:42,000
这个就类似于MindSpot的方式

117
00:05:42,000 --> 00:05:45,000
执行流跟控制流还会不断的跳转

118
00:05:45,000 --> 00:05:47,000
带来运行时的开销

119
00:05:47,000 --> 00:05:50,000
动态图转换成为静态图这种方式

120
00:05:50,000 --> 00:05:56,000
真的是为了兼顾灵活性应用性还有性能所提出的一种新的方案

121
00:05:56,000 --> 00:05:59,000
也被各个主流的AI框架所采用

122
00:05:59,000 --> 00:06:02,000
也被各个主流的AI框架所采用

123
00:06:03,000 --> 00:06:10,000
这一节课我们讲了控制流在计算图里面的表达和控制流跟计算图的关系

124
00:06:10,000 --> 00:06:14,000
这个是计算图里面非常非常核心的一个概念

125
00:06:14,000 --> 00:06:18,000
控制流采用不同的事迹就决定了我们的AI框架

126
00:06:18,000 --> 00:06:23,000
分为了声明式编程的静态图或者是命令式编程的动态图

127
00:06:23,000 --> 00:06:26,000
静态图就统一了深度学习的一个表示

128
00:06:26,000 --> 00:06:29,000
非常利于编译优化和执行加速

129
00:06:29,000 --> 00:06:32,000
但是应用性实在非常难堪

130
00:06:32,000 --> 00:06:35,000
动态图非常灵活的应用数字语言

131
00:06:35,000 --> 00:06:38,000
但是优化起来也是很痛苦

132
00:06:38,000 --> 00:06:42,000
所以现在Tensorful很难部署或者部署起来了

133
00:06:42,000 --> 00:06:45,000
又要转成Onlyt或者Mobile或者Cafe

134
00:06:45,000 --> 00:06:48,000
就它在部署的时候还是很大问题

135
00:06:48,000 --> 00:06:53,000
最后就提出了基于追踪的Traced或者基于圆满的方式

136
00:06:53,000 --> 00:06:57,000
把动静态图统一起来兼顾应用性和性能

137
00:06:57,000 --> 00:06:59,000
好了,谢谢各位

138
00:06:59,000 --> 00:07:01,000
卷的不行了,卷的不行了

139
00:07:01,000 --> 00:07:03,000
记得一键三连加关注哦

140
00:07:03,000 --> 00:07:06,000
所有的内容都会开源在下面这条链接里面

141
00:07:06,000 --> 00:07:08,000
拜了个拜

