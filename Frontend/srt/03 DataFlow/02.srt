1
00:00:00,000 --> 00:00:05,760
哈喽大家好

2
00:00:05,760 --> 00:00:09,400
在上一节里面我们新挖了一个坑叫做计算图

3
00:00:09,400 --> 00:00:11,800
计算图看着好像很简单

4
00:00:11,800 --> 00:00:15,040
其实里面还有很多内容等我们去一起去挖掘的

5
00:00:15,040 --> 00:00:19,080
计算图在某些场景里面又叫数据流图

6
00:00:19,080 --> 00:00:20,320
为了统一概念

7
00:00:20,320 --> 00:00:23,560
我们在这里面统一叫它叫做计算图就好了

8
00:00:23,560 --> 00:00:25,400
后面聊到数据流图啊

9
00:00:25,400 --> 00:00:26,160
DAG图啊

10
00:00:26,160 --> 00:00:29,200
其实我们都把它当做计算图去看待就好了

11
00:00:29,520 --> 00:00:32,760
那计算图的出现其实是为了解决AI系统

12
00:00:32,760 --> 00:00:35,160
挖过程当中遇到的很多问题

13
00:00:35,560 --> 00:00:38,400
那下面我们来看看定义一个神经网络

14
00:00:38,400 --> 00:00:40,160
或者深度学习之前

15
00:00:40,160 --> 00:00:42,440
我们一般有那几种步骤

16
00:00:42,440 --> 00:00:46,320
之前我们在AI框架系统里面已经简单的讲过了

17
00:00:46,320 --> 00:00:48,200
首先就是定一个神经网络

18
00:00:48,200 --> 00:00:51,000
从输入马东梅到预测马什么梅

19
00:00:51,400 --> 00:00:52,600
马东梅

20
00:00:53,160 --> 00:00:54,080
什么东梅啊

21
00:00:55,120 --> 00:00:57,560
接着去定义我们的优化目标

22
00:00:57,560 --> 00:00:58,960
就是我们的优化目标

23
00:00:59,080 --> 00:01:01,760
接着呢自动的去更新我们的梯度

24
00:01:02,040 --> 00:01:03,360
那这里面问题就来了

25
00:01:03,760 --> 00:01:05,840
怎么样去实现进入自动求导吗

26
00:01:05,840 --> 00:01:08,400
自动求导这个工作其实是很复杂的

27
00:01:08,680 --> 00:01:13,040
那第二个我们怎么在系统里面统一的表示神经网络了

28
00:01:13,040 --> 00:01:16,480
就是这个神经网络到底怎么去用程序去表达呢

29
00:01:17,320 --> 00:01:19,520
除了上乘表示上的问题呢

30
00:01:19,520 --> 00:01:22,160
其实我们还会遇到更多的问题

31
00:01:22,160 --> 00:01:26,120
例如我们假设已经有了对程序的表达

32
00:01:26,200 --> 00:01:30,000
现在怎么对这个神经网络进行简化合并变化

33
00:01:30,000 --> 00:01:31,880
让它在硬件上面

34
00:01:31,880 --> 00:01:34,360
就是底层这些英伟达或者华为

35
00:01:34,360 --> 00:01:39,000
阿特拉斯800这些训练服务器上面更高效的去执行呢

36
00:01:40,600 --> 00:01:42,920
假设我刚才只有一台服务器

37
00:01:42,920 --> 00:01:45,240
现在我有多台服务器的时候

38
00:01:45,240 --> 00:01:47,680
怎么对内存进行预分配合管理了

39
00:01:48,000 --> 00:01:49,640
如果我只是一堆程序

40
00:01:49,640 --> 00:01:51,640
我可能依赖于编译器是吧

41
00:01:51,640 --> 00:01:53,960
LVM等传统的编译器去执行

42
00:01:53,960 --> 00:01:55,480
但是深度学习

43
00:01:55,640 --> 00:01:57,080
他用什么编译器呢

44
00:01:57,920 --> 00:02:01,160
如果我用编译器编译成机器指令之后

45
00:02:01,160 --> 00:02:03,320
我怎么去适配到不同的后端

46
00:02:03,320 --> 00:02:05,200
不同的硬件去执行呢

47
00:02:06,240 --> 00:02:07,760
我太难了

48
00:02:07,760 --> 00:02:11,600
所以AI框架在系统化或者工程化当中呢

49
00:02:11,600 --> 00:02:13,880
会遇到非常非常多的问题

50
00:02:14,160 --> 00:02:16,800
如果其他同学看到有这么多问题

51
00:02:16,800 --> 00:02:19,040
或者如果你能想到更多的问题

52
00:02:19,040 --> 00:02:20,680
或者更核心的问题

53
00:02:20,680 --> 00:02:22,440
也非常欢迎给点弹幕

54
00:02:22,920 --> 00:02:25,720
为了解决我们刚才所说的很多问题

55
00:02:25,720 --> 00:02:26,600
所以呢

56
00:02:26,600 --> 00:02:27,600
我们在这里面

57
00:02:27,600 --> 00:02:31,800
在AI框架和这个就是AI框架的最原始的图里面

58
00:02:31,800 --> 00:02:34,600
我们就提出了一个统一表示

59
00:02:34,600 --> 00:02:37,720
去把各种各样的神经网络的模型

60
00:02:37,720 --> 00:02:40,800
然后通过一个计算图对它进行表达

61
00:02:40,800 --> 00:02:42,360
包括我们的Transformer

62
00:02:42,360 --> 00:02:43,080
CNN

63
00:02:43,080 --> 00:02:44,360
LSTMIN

64
00:02:44,360 --> 00:02:46,960
都可以通过这个计算图进行表达

65
00:02:46,960 --> 00:02:48,640
那对上呢就是前端

66
00:02:48,640 --> 00:02:50,520
对下呢就是一堆的层了

67
00:02:51,520 --> 00:02:54,880
计算图最基本的组成主要是由

68
00:02:54,880 --> 00:02:55,440
张亮

69
00:02:55,440 --> 00:02:56,240
Tensor

70
00:02:56,240 --> 00:02:57,080
还有算子

71
00:02:57,080 --> 00:02:58,440
Operator来组成的

72
00:02:58,440 --> 00:03:01,080
Tensor呢就作为最基本的数据结构

73
00:03:01,080 --> 00:03:03,680
主要是对高维的数字进行表示

74
00:03:03,680 --> 00:03:06,920
它也是对标量向量矩阵的一种推广

75
00:03:06,920 --> 00:03:08,800
那可能一开始呢

76
00:03:08,800 --> 00:03:11,640
很多人没有接触计算机体系的时候呢

77
00:03:11,640 --> 00:03:14,000
一弹标量可能不知道是什么

78
00:03:14,000 --> 00:03:16,520
标量呢简单的理解就是一个数

79
00:03:16,520 --> 00:03:19,440
123567这些叫做数

80
00:03:19,480 --> 00:03:21,680
在高等数学的先行代数里面

81
00:03:21,680 --> 00:03:23,520
其实我们已经学到大量的

82
00:03:23,520 --> 00:03:25,760
下量和矩阵的运算了

83
00:03:25,760 --> 00:03:27,560
那高维的数组

84
00:03:27,560 --> 00:03:28,480
也就是张亮

85
00:03:28,480 --> 00:03:29,880
它只是一个表示

86
00:03:29,880 --> 00:03:32,480
那通常呢我们会用张亮来代表

87
00:03:32,480 --> 00:03:34,520
对更高维的数组

88
00:03:34,520 --> 00:03:36,640
例如4维3维5维

89
00:03:36,640 --> 00:03:38,560
甚至到6 7维

90
00:03:38,560 --> 00:03:40,320
例如在华为生腾的

91
00:03:40,320 --> 00:03:43,760
Atlas服务器或者Atlas这个芯片里面呢

92
00:03:43,760 --> 00:03:45,680
我们可能一般对数据的

93
00:03:45,680 --> 00:03:48,120
会直接把它表示成6维的数据

94
00:03:48,160 --> 00:03:49,520
然后进行运算的

95
00:03:49,520 --> 00:03:51,480
不过大家不用深度的去理解

96
00:03:51,480 --> 00:03:54,080
现在我们去讲的张亮呢

97
00:03:54,080 --> 00:03:55,600
主要是稠密的张亮

98
00:03:55,600 --> 00:03:56,520
稀疏的张亮呢

99
00:03:56,520 --> 00:03:58,080
我们暂时不去谈

100
00:03:58,080 --> 00:03:59,360
那张亮的形状呢

101
00:03:59,360 --> 00:04:02,120
像这个图可能这个张亮的shape呢

102
00:04:02,120 --> 00:04:03,560
就是325

103
00:04:03,560 --> 00:04:06,000
那我们从后面往前看

104
00:04:06,000 --> 00:04:07,520
是从后面往前看

105
00:04:07,520 --> 00:04:08,080
第一个呢

106
00:04:08,080 --> 00:04:08,760
是5呢

107
00:04:08,760 --> 00:04:11,520
就是指我有5维的一个数组

108
00:04:11,520 --> 00:04:11,960
2呢

109
00:04:11,960 --> 00:04:13,880
就是这个数组有两排

110
00:04:13,880 --> 00:04:14,520
那3呢

111
00:04:14,520 --> 00:04:17,680
就是有三组这种2乘以5的数组

112
00:04:17,680 --> 00:04:22,120
这样呢就组成了一个3维shape为325的一个张亮

113
00:04:22,120 --> 00:04:25,680
那张亮它主要里面的元素的基本数据类型

114
00:04:25,680 --> 00:04:26,920
也会进行表示的

115
00:04:26,920 --> 00:04:29,000
就是Data Type等于Internet Floater

116
00:04:29,000 --> 00:04:31,040
还是其他数据类型

117
00:04:31,040 --> 00:04:33,320
就是代表里面每一个元素

118
00:04:33,320 --> 00:04:35,840
具体存的是哪一种数据类型

119
00:04:37,440 --> 00:04:40,280
为什么AI框架里面的计算图

120
00:04:40,280 --> 00:04:42,800
使用张亮这个数据结构呢

121
00:04:42,800 --> 00:04:44,960
作为它最基础的数据结构呢

122
00:04:44,960 --> 00:04:46,200
其实我们可以看到啊

123
00:04:46,240 --> 00:04:47,960
它优点有两个

124
00:04:47,960 --> 00:04:49,640
这是我能够想得到的

125
00:04:49,640 --> 00:04:50,120
第一个呢

126
00:04:50,120 --> 00:04:52,880
就是在上层用户表示的时候

127
00:04:52,880 --> 00:04:56,160
其实我们可以把它表示成一个张亮的数字

128
00:04:56,160 --> 00:04:58,400
那在底层做物理运算的时候

129
00:04:58,400 --> 00:04:59,640
或者计算的时候

130
00:04:59,640 --> 00:05:01,960
就有了一个统一的数据结构

131
00:05:01,960 --> 00:05:04,240
可以对物理的地址进行映射

132
00:05:04,240 --> 00:05:05,840
非常方便我们对物理啊

133
00:05:05,840 --> 00:05:07,520
或者对内存进行切片

134
00:05:07,520 --> 00:05:09,040
切分操作

135
00:05:09,040 --> 00:05:13,240
那第二个优点就是在作为最基本的运算的时候呢

136
00:05:13,240 --> 00:05:15,800
我们可以把数据变成一个整体

137
00:05:15,800 --> 00:05:17,400
做P操作处理

138
00:05:17,400 --> 00:05:19,520
特别适合与单指令多数据

139
00:05:19,520 --> 00:05:21,760
就是SIMD的这种并行加速

140
00:05:23,280 --> 00:05:26,240
作为计算的最基本的单元

141
00:05:26,240 --> 00:05:28,240
对计算肯定是有帮助的

142
00:05:28,240 --> 00:05:29,800
所以我们叫做张亮呢

143
00:05:29,800 --> 00:05:33,360
是作为计算图的最基本的数据结构

144
00:05:33,360 --> 00:05:37,640
那下面我们来看一看张亮的几个不同维度的数据

145
00:05:37,640 --> 00:05:38,240
首先呢

146
00:05:38,240 --> 00:05:39,360
是一维的张亮

147
00:05:39,360 --> 00:05:39,960
一维的张亮

148
00:05:39,960 --> 00:05:40,960
其实我们一般呢

149
00:05:40,960 --> 00:05:43,880
都把它叫做象亮或者举赞就完了

150
00:05:43,920 --> 00:05:44,440
二维呢

151
00:05:44,440 --> 00:05:46,240
可能就是一个举赞

152
00:05:46,240 --> 00:05:47,040
那三维呢

153
00:05:47,040 --> 00:05:49,200
可能我们更多的热肠呢

154
00:05:49,200 --> 00:05:51,640
会更多的代表一张图片

155
00:05:51,640 --> 00:05:52,760
因为一张图片呢

156
00:05:52,760 --> 00:05:53,960
就有长和宽

157
00:05:53,960 --> 00:05:55,240
当然它有一个圈楼

158
00:05:55,240 --> 00:05:58,120
就由三个不同的通道组成一张图片

159
00:05:58,120 --> 00:06:00,000
才能够变成一个彩色的图片

160
00:06:01,240 --> 00:06:02,960
那这个时候在图像里面呢

161
00:06:02,960 --> 00:06:04,320
我们刚才已经用了三维

162
00:06:04,320 --> 00:06:05,760
就是CHW了

163
00:06:05,760 --> 00:06:07,600
C就是圈楼

164
00:06:07,600 --> 00:06:09,080
实际上图像呢

165
00:06:09,080 --> 00:06:12,760
作为张亮丢进去我们AI框架进行计算之前呢

166
00:06:12,760 --> 00:06:15,120
我们前面还会加一个维度

167
00:06:15,120 --> 00:06:16,760
叫做N就是Batch

168
00:06:16,760 --> 00:06:17,720
Batch Size

169
00:06:17,720 --> 00:06:20,760
就是把多张图片打包成一起

170
00:06:20,760 --> 00:06:24,560
然后丢给我们的AI框架进行处理的

171
00:06:24,560 --> 00:06:26,840
这个时候图像张亮化处理

172
00:06:26,840 --> 00:06:28,760
丢给我们的AI系统之前

173
00:06:28,760 --> 00:06:31,800
就会把数据组成一个四维的张亮

174
00:06:31,800 --> 00:06:33,560
当然了自然语言处理呢

175
00:06:33,560 --> 00:06:35,040
可能会比较特别

176
00:06:35,040 --> 00:06:36,240
因为自然语言处理

177
00:06:36,240 --> 00:06:38,360
首先第一个就是我们的词象量

178
00:06:38,360 --> 00:06:39,600
就是我的Worst

179
00:06:39,600 --> 00:06:40,840
有了单词之后

180
00:06:40,840 --> 00:06:42,680
我可能还会用sentenced

181
00:06:42,680 --> 00:06:44,520
就是把它拼成一个句子

182
00:06:44,520 --> 00:06:48,040
就是不同的单词组成句子

183
00:06:48,040 --> 00:06:48,560
然后呢

184
00:06:48,560 --> 00:06:51,320
我有很多个句子或者很多个段落

185
00:06:51,320 --> 00:06:53,600
那所以我们前面再加一个N

186
00:06:53,600 --> 00:06:56,160
这样就变成一个自然语言处理的张亮了

187
00:06:57,520 --> 00:06:59,800
我们刚才谈到的图像啊

188
00:06:59,800 --> 00:07:02,400
自然语言处理是我们最基础的任务

189
00:07:02,400 --> 00:07:04,720
实际上我们还有一些稀疏张亮

190
00:07:04,720 --> 00:07:06,040
特别是到后期

191
00:07:06,040 --> 00:07:08,080
或者我们现在大模型计算的时候

192
00:07:08,080 --> 00:07:10,400
会引起大量稀疏的问题

193
00:07:10,400 --> 00:07:12,080
那最新的一个研究热点

194
00:07:12,080 --> 00:07:13,000
就是图

195
00:07:13,000 --> 00:07:14,400
图和点语这两个了

196
00:07:14,400 --> 00:07:15,800
也是我们稀疏张亮

197
00:07:15,800 --> 00:07:17,120
可能我们在这里面呢

198
00:07:17,120 --> 00:07:19,800
就暂时不要去展开稀疏的关系

199
00:07:19,800 --> 00:07:22,800
我们在AI框架基础里面的历史环节

200
00:07:22,800 --> 00:07:25,080
或者历史那一小节里面呢

201
00:07:25,080 --> 00:07:28,480
已经讲过了最基础的计算图有什么概念

202
00:07:28,480 --> 00:07:29,040
这里面呢

203
00:07:29,040 --> 00:07:30,280
我们重复一下

204
00:07:30,280 --> 00:07:34,040
或者做一个更加详细一丢丢的一个理解

205
00:07:34,040 --> 00:07:34,920
那计算图呢

206
00:07:34,920 --> 00:07:36,560
它是一个有像无环图

207
00:07:36,560 --> 00:07:39,120
但是我们现在不要去理解那个图

208
00:07:39,120 --> 00:07:40,880
而是先去理解一下

209
00:07:40,920 --> 00:07:44,800
计算图里面有两种最重要的元素

210
00:07:44,800 --> 00:07:45,560
那第一种呢

211
00:07:45,560 --> 00:07:47,120
就是最基本的数据结构

212
00:07:47,120 --> 00:07:48,360
叫我们的张亮

213
00:07:48,760 --> 00:07:49,440
那第二个呢

214
00:07:49,440 --> 00:07:51,120
就是基本的运算单元

215
00:07:51,120 --> 00:07:52,360
我们叫做算子

216
00:07:52,360 --> 00:07:53,560
所以大家提到算子

217
00:07:53,560 --> 00:07:56,480
不要觉得算子只是一些加减乘除啊

218
00:07:56,480 --> 00:07:58,120
其实卷机BatchLong

219
00:07:58,120 --> 00:07:58,880
Sigmoor

220
00:07:58,880 --> 00:08:01,200
还有很多我们后来提出的

221
00:08:01,200 --> 00:08:02,240
包括Transformer

222
00:08:02,240 --> 00:08:04,120
后来已经成为一个大的算子

223
00:08:04,120 --> 00:08:05,480
所以算子有大有小

224
00:08:05,480 --> 00:08:07,400
小的算子可能加减乘除

225
00:08:07,400 --> 00:08:09,400
Q跟号或3N、3N

226
00:08:09,400 --> 00:08:11,040
那复杂的神经网络的算子

227
00:08:11,040 --> 00:08:13,600
可能就是类似于CudaNN这种

228
00:08:13,600 --> 00:08:15,080
提供了很多卷机

229
00:08:15,080 --> 00:08:18,440
SSTM,INN,Transformer这一类的大算子

230
00:08:19,960 --> 00:08:20,600
下面呢

231
00:08:20,600 --> 00:08:22,520
我们了解了两个基本的概念

232
00:08:22,520 --> 00:08:23,840
或者基本的元素之后呢

233
00:08:23,840 --> 00:08:26,440
我们现在就来看看数据流图了

234
00:08:26,440 --> 00:08:28,880
或者DAG图或者计算图

235
00:08:28,880 --> 00:08:31,360
那我们后面都叫做计算图就好了

236
00:08:31,360 --> 00:08:32,080
里面呢

237
00:08:32,080 --> 00:08:35,720
主要是表示神经网络的一个运算逻辑和状态

238
00:08:35,720 --> 00:08:36,120
这个呢

239
00:08:36,120 --> 00:08:38,480
就是最基础的正向的计算图

240
00:08:38,480 --> 00:08:39,280
比较好看

241
00:08:39,280 --> 00:08:40,640
比较容易理解

242
00:08:40,640 --> 00:08:41,280
其他点呢

243
00:08:41,280 --> 00:08:45,400
就代表我们刚才所说的一些算子

244
00:08:45,400 --> 00:08:47,000
包括我们的卷机VLU

245
00:08:48,080 --> 00:08:48,720
那边呢

246
00:08:48,720 --> 00:08:49,640
就代表张量

247
00:08:49,640 --> 00:08:51,080
就是我们的数据流

248
00:08:51,080 --> 00:08:54,320
所以我们有时候叫做数据流图或者计算图

249
00:08:54,320 --> 00:08:55,120
那计算图呢

250
00:08:55,120 --> 00:08:57,440
更聚焦于我们中间的这个计算

251
00:08:57,440 --> 00:08:58,520
计算很重要

252
00:08:58,520 --> 00:08:59,360
数据流图呢

253
00:08:59,360 --> 00:09:00,800
就代表我们这个数据呢

254
00:09:00,800 --> 00:09:02,680
是通过不断的去流动的

255
00:09:02,680 --> 00:09:05,120
然后其实他们都是代表DAG图

256
00:09:05,120 --> 00:09:06,600
都是同一个概念

257
00:09:06,600 --> 00:09:07,080
然后呢

258
00:09:07,120 --> 00:09:09,480
张量就从这里面不断的流进去

259
00:09:09,480 --> 00:09:10,800
我们的计算的算子

260
00:09:10,800 --> 00:09:11,600
然后流出来

261
00:09:11,600 --> 00:09:12,720
作为我们的输出

262
00:09:12,720 --> 00:09:13,640
然后输出呢

263
00:09:13,640 --> 00:09:15,480
作为下一个算子的输入

264
00:09:15,480 --> 00:09:19,280
这么一个简单的表示神经网络的计算逻辑

265
00:09:19,280 --> 00:09:23,080
for while这种控制流相关的一些语句

266
00:09:23,080 --> 00:09:23,880
那这个时候呢

267
00:09:23,880 --> 00:09:24,920
我们的计算图呢

268
00:09:24,920 --> 00:09:26,720
可能就会变得更加复杂

269
00:09:26,720 --> 00:09:29,320
或者我们循环的时候就嵌套一套套的

270
00:09:29,320 --> 00:09:32,240
嵌套很多个往下的计算图

271
00:09:32,240 --> 00:09:33,560
在后面的章节里面呢

272
00:09:33,560 --> 00:09:35,360
我们就会详细的去展开

273
00:09:35,360 --> 00:09:38,880
当数据流图遇到这些控制流的时候怎么办

274
00:09:40,800 --> 00:09:43,280
那下面我们来回顾一下

275
00:09:43,280 --> 00:09:48,720
刚才我们了解了AI实际上在系统挂共诚挂当中呢

276
00:09:48,720 --> 00:09:50,800
会遇到非常多的问题

277
00:09:50,800 --> 00:09:53,120
怎么去表示我的神经网络

278
00:09:53,120 --> 00:09:54,840
表示网友的神经网络呢

279
00:09:54,840 --> 00:09:57,200
怎么去适配到不同的硬件上面

280
00:09:57,200 --> 00:09:58,800
这都有大量的问题

281
00:09:58,800 --> 00:10:00,400
那为了解决这些问题呢

282
00:10:00,400 --> 00:10:02,440
所以我们就提出了计算图

283
00:10:02,440 --> 00:10:04,200
能够对AI系统

284
00:10:04,200 --> 00:10:04,880
对下呢

285
00:10:04,880 --> 00:10:09,320
对AI的硬件执行或者计算进行了一个统一的表示

286
00:10:09,320 --> 00:10:10,080
对上呢

287
00:10:10,080 --> 00:10:12,320
承接各种各样的AI的算法

288
00:10:12,320 --> 00:10:14,400
或者AI的程序的表达

289
00:10:14,400 --> 00:10:15,080
那另外呢

290
00:10:15,080 --> 00:10:17,040
我们还了解了计算图呢

291
00:10:17,040 --> 00:10:19,400
主要是由张量和基本的算子

292
00:10:19,400 --> 00:10:23,000
两个最基础的元素所组成的

293
00:10:23,000 --> 00:10:25,080
好了今天的课程比较简单

294
00:10:25,080 --> 00:10:25,880
谢谢各位

295
00:10:25,880 --> 00:10:26,760
白了个白

296
00:10:27,680 --> 00:10:28,480
卷的不行了

297
00:10:28,480 --> 00:10:29,360
卷的不行了

298
00:10:29,360 --> 00:10:31,160
记得一键三连加关注哦

299
00:10:31,160 --> 00:10:34,160
所有的内容都会开源在下面这条链接里面

300
00:10:34,160 --> 00:10:35,520
白了个白

