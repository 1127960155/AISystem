1
00:00:00,000 --> 00:00:06,375
哈喽大家好,今天我们换了一个新的桌面,

2
00:00:06,375 --> 00:00:13,000
我觉得稍微比之前好看一点,或者说时尚一点。

3
00:00:13,000 --> 00:00:17,000
这里面这个是MindSpore,MindSpore我把它调整过来了这个图。

4
00:00:17,000 --> 00:00:24,725
废话就不多说了,今天我们来讲讲一个比较难或者在

5
00:00:24,725 --> 00:00:29,000
计算图里面最核心的一个概念,就是计算图与自动微分。 

6
00:00:32,000 --> 00:00:39,000
在这一节课里面我们可能讲的内容有三个,第一个是深度学习和微分之间的关系。

7
00:00:39,000 --> 00:00:51,000
我们之前在自动微分系列里面其实只是讲了自动微分自身的一个概念, 怎么通过计算机去实现自动微分,但是没有讲深度学习跟微分的关系。 

8
00:00:51,000 --> 00:00:55,920
然后我们就去回顾一下自动微分的一个原理,

9
00:00:55,920 --> 00:01:00,000
搞清楚为什么需要自动微分,自动微分的形式或者实现方式。

10
00:01:00,000 --> 00:01:04,670
最后我们就是讲讲今天我们主要的课程,

11
00:01:04,670 --> 00:01:07,000
计算图怎么去表达我们的自动微分。

12
00:01:07,000 --> 00:01:13,275
首先我们回顾一下深度学习训练的一个流程,

13
00:01:13,275 --> 00:01:15,000
训练其实我们主要是分三个。

14
00:01:15,000 --> 00:01:21,000
第一个是前向计算,然后我们还是以马东梅这个例子做一个前向的计算。

15
00:01:21,000 --> 00:01:27,000
接着可能通过自动微分或者我们的AI框架实现一个反向的计算。

16
00:01:27,000 --> 00:01:33,000
有了这个反向图之后就可以第三步更新可学习的权重参数。

17
00:01:33,000 --> 00:01:36,000
这里面可学习的权重参数就是这个W。

18
00:01:37,000 --> 00:01:41,000
然后通过自动微分求导,然后不断的更新。

19
00:01:41,000 --> 00:01:43,185
因为大家都知道导数是求函数的顺势的变化趋势,

20
00:01:43,185 --> 00:01:51,000
拿到这个导数的趋势之后,我们就会迭代式的去求解优化我们的损失值了。

21
00:01:51,000 --> 00:01:59,400
所以在数学里面这个神经网络就是刚才马东梅这个非常复杂的神经网络,

22
00:01:59,400 --> 00:02:05,000
其实它是一个复杂的带有参数的高度的非凸函数。

23
00:02:05,000 --> 00:02:09,000
数据高为平面可以是平的,那更多的是凹的。

24
00:02:09,000 --> 00:02:17,000
我们希望能够找到凹下去的最低点,找到最低点就是我们的损失值,最小的损失值。

25
00:02:17,000 --> 00:02:22,840
那中间我们就是通过这些带参数的进行一阶梯度求导,

26
00:02:22,840 --> 00:02:24,000
然后迭代的更新我们的导数。

27
00:02:24,000 --> 00:02:29,880
通过优化器去更新我们的参数,使得我们整个模型找到最小的点,

28
00:02:29,880 --> 00:02:32,000
也去找到我们的数据的鞍点。

29
00:02:32,000 --> 00:02:37,000
那这个就是深度学习训练流程所代表的数学含义。

30
00:02:37,000 --> 00:02:43,000
我们回到求导里面,其实求导是一个经典的问题。

31
00:02:43,000 --> 00:02:48,440
深度学习的核心就是计算网络模型的参数,

32
00:02:48,440 --> 00:02:54,000
我们刚才讲了权重的参数,然后不断的更新权重参数的梯度。

33
00:02:54,000 --> 00:02:59,000
我们最后以损失函数,输入的是X,然后我们的label就是Y。

34
00:02:59,000 --> 00:03:05,000
最后通过更新我们的梯度,然后求得我们的L(W),就是我们的损失值。

35
00:03:05,000 --> 00:03:10,506
在这里面,自动微分就是通过一些原子的操作,

36
00:03:10,506 --> 00:03:14,000
构造一个比较复杂的前向计算的程序,也就是我们的神经网络。

37
00:03:14,000 --> 00:03:21,000
自动微分关注的一个点就是高效的自动的去生成一个反向的计算程序。

38
00:03:21,000 --> 00:03:26,000
就是我们有一个前向,我们还构建一个反向,就跟刚才的这个图一样。

39
00:03:26,000 --> 00:03:31,370
我们这个前向是人工的去构造的,反向这个非常复杂,

40
00:03:31,370 --> 00:03:34,000
我们希望系统自动的帮我们去构建。

41
00:03:38,000 --> 00:03:42,000
左边的这个程序就是LeNet5这个网络模型。

42
00:03:42,000 --> 00:03:48,000
首先我们人工的去定义了一些算子,就是我们的神经网络的算子。

43
00:03:48,000 --> 00:03:53,000
接着把这些算子串起来,然后拼成一个正向的网络模型,就是我们的forward。

44
00:03:53,000 --> 00:03:58,000
这个是PyTorch的代码,里面用forward,而MindSpore用construct。

45
00:03:58,000 --> 00:04:03,000
基本上这个网络模型的构建的过程或者其实是差不多的。

46
00:04:03,000 --> 00:04:11,000
那这个网络模型只是一个正向,就是我的forward只是一个正向,也就是构建了我右边的正向的模型。

47
00:04:11,000 --> 00:04:13,000
那反向这个怎么办呢?

48
00:04:13,000 --> 00:04:20,000
反向这个我们希望通过AI框架的自动微分,帮我们自动的去构建反向的程序。

49
00:04:20,000 --> 00:04:26,000
那反向的程序可能并不是像我们左边看到的这种形式上的代码。

50
00:04:26,000 --> 00:04:32,000
无论是正向还是反向,我们构建的是一个DAG的有向无还图。

51
00:04:32,000 --> 00:04:39,000
通过这个有向无还图或者我们叫做计算图去表示我们的正向,表示我们的反向。

52
00:04:39,000 --> 00:04:45,570
接下来我们回顾一下在自动微分系列里面我们讲到的几个

53
00:04:45,570 --> 00:04:47,000
最简单的计算机去实现微分的方式。

54
00:04:47,000 --> 00:04:50,000
首先第一个就是符号微分。

55
00:04:50,000 --> 00:04:54,850
那符号微分最简单的下面两条式就是

56
00:04:54,850 --> 00:04:58,000
念式求导法则的展开或者导数表,这两个字打错了。

57
00:04:58,000 --> 00:05:05,000
通过求导法则或者导数的变换公式,然后精确的去计算函数所对应的导数。

58
00:05:05,000 --> 00:05:12,290
那优势就是精确值非常高,缺点就是表达式不断的膨胀,

59
00:05:12,290 --> 00:05:14,000
因为我要人工或者计算机系统自动的去展开。

60
00:05:14,000 --> 00:05:18,000
而这个缺点在深度学习里面会引起比较大的一些问题。

61
00:05:18,000 --> 00:05:23,000
首先我们深度学习的网络模型非常大,层数非常深。

62
00:05:23,000 --> 00:05:29,253
这会就导致我们的导数会变得非常复杂,

63
00:05:29,253 --> 00:05:32,000
就是我们的函数f(x)可能是急剧性的膨胀急剧的变大。

64
00:05:32,000 --> 00:05:35,000
最后就衍生一个问题,就是难以高效的求解。

65
00:05:35,000 --> 00:05:43,860
另外还有第二个缺点就是有部分的算子没办法通过求导去解决的,

66
00:05:43,860 --> 00:05:46,000
例如Relu,Switch这些比较特殊的神经网络所对应的算子。

67
00:05:46,000 --> 00:05:53,000
那我们还有第二种实现方式,就是数值微分。数值微分就是用有限差分方式。

68
00:05:53,000 --> 00:06:01,000
这个就是差分方式进行近似,优势就是比较好实现,缺点就是结果不精确,复杂度高。

69
00:06:01,000 --> 00:06:06,300
那针对这个问题呢,在深度学习里面或者在AI框架里面,

70
00:06:06,300 --> 00:06:12,000
就会因为数值的阶段和近似问题,导致没办法得到一个精确的导数。

71
00:06:12,000 --> 00:06:19,350
因为我们去求导数的时候经常都要用浮点运算,最低我们可能要用float16进行运算,

72
00:06:19,350 --> 00:06:25,000
可能部分算子float16或者混合精度都不达标,只能用FP32执行。

73
00:06:25,000 --> 00:06:29,000
那这个时候阶段和近似的问题就得不到解了。

74
00:06:29,000 --> 00:06:33,800
所以就有很多研究学者去研究自动微分,

75
00:06:33,800 --> 00:06:38,000
怎么通过数值计算把有限的导数进行表示出来。

76
00:06:38,000 --> 00:06:44,280
那这个时候就推出一个表达式追逐Evaluation Trace

77
00:06:44,280 --> 00:06:46,000
去跟踪计算的整个中间变量的过程。

78
00:06:47,000 --> 00:06:54,000
那下面我们看看刚才我们提到一个很重要的概念,就是中间变量。

79
00:06:54,000 --> 00:07:02,400
那中间变量我们希望引入这个中间变量,把这些中间变量分解成一系列的基本的函数,

80
00:07:02,400 --> 00:07:05,000
将这些基本的函数构造成一个计算图。

81
00:07:05,000 --> 00:07:15,000
那下面我们来看看一个熟悉的例子,这个例子呢,贯穿我们自动微分系列了, 还有AI框架了,基础系列了,现在来到我们的计算图系列。

82
00:07:15,000 --> 00:07:23,145
那这个例子还是这个例子,我们所谓的中间变量就是我的x1s2是原始的输入,

83
00:07:23,145 --> 00:07:31,000
那最后的输出呢是这个fx1s2,那中间变量就是我们这些圈圈 ,从v-1到v5都是我们的中间变量。

84
00:07:31,000 --> 00:07:42,000
每经过一条边一条运算我们都会产生一个中间变量,我们把中间变量保存起来,然后不断的运算往下执行就可以看到。

85
00:07:43,000 --> 00:07:48,000
这个呢就是FORWARD TRACE,就是我们正向的一个计算公式,从输入到输出。

86
00:07:48,000 --> 00:08:00,000
那接下来有个很重要的概念也是我们在自动微分里面提过的,我们引入的中间变量除了正向,其实这些灰色的也是我们的中间变量。

87
00:08:00,000 --> 00:08:08,725
这些中间变量就对应于这个v5的delta,v4的delta,v2的delta,v3的delta,

88
00:08:08,725 --> 00:08:16,125
你要不断的把这些中间变量全都保存起来,通过我们这些虚线立向的去求得到我们整个图,

89
00:08:16,125 --> 00:08:22,000
这个叫做计算图的导数,通过这种方式去构造我们正向和反向的自动微分。

90
00:08:23,000 --> 00:08:30,000
那下面我们更加详细的去展开一下这个图,跟我们神经网络的图其实是一样的。

91
00:08:30,000 --> 00:08:34,075
我们把这些圈叫做神经圆,当作它是神经圆,

92
00:08:34,075 --> 00:08:38,000
那这些中间的边我们把它当成一个计算单元。

93
00:08:38,000 --> 00:08:43,825
现在我们看一下其中有一层,我们把神经网络的一层拿出来,那

94
00:08:43,825 --> 00:08:51,000
这一层它的计算是G(x),我们不管它是加减乘除也好,你把它当成一个函数。

95
00:08:51,000 --> 00:08:55,450
我的输入是X输出是Y,也就代表左边输进去的,

96
00:08:55,450 --> 00:09:00,000
这个是X就是V1V2,它是一个X的向量或者张量。

97
00:09:00,000 --> 00:09:06,000
输出就是V4V3,这等于Y我们的输出的张量。

98
00:09:06,000 --> 00:09:10,575
为了在数学上更好的去表达Y对X的导数,

99
00:09:10,575 --> 00:09:17,000
我们这里面引入了一个雅可比矩阵,Y对X,Y对所有X的偏导。

100
00:09:18,000 --> 00:09:26,000
这个是正向的计算,反向的时候我们计算完的FX1的导数,然后不断的往后传。

101
00:09:26,000 --> 00:09:33,775
那我们在V4V3的时候,反向的时候其实我们传的是一个,反向传的时候其实是一个Vdelta

102
00:09:33,775 --> 00:09:40,000
,我们现在暂且叫它Vdelta,后面我们把它叫做Vector-Jacobian的乘积。

103
00:09:41,000 --> 00:09:47,950
那往下反向传的时候其实是DeltaF到V3的导数,DeltaF对V4的导数,

104
00:09:47,950 --> 00:09:54,025
也就是我从L这个L这个损失函数,对每一个反向的反向的输入的导数,

105
00:09:54,025 --> 00:09:57,000
就是我们的Vector-Jacobian的乘积。

106
00:09:57,000 --> 00:10:02,700
最后呢,通过去计算里面的每个运算,然后乘以我们的雅可比乘积,

107
00:10:02,700 --> 00:10:06,000
之后呢,就得到我们下一次的导数的输出。

108
00:10:06,000 --> 00:10:12,075
AI框架里面我们并不会去存我们整个雅可比矩阵,因为雅可比矩阵太大了,

109
00:10:12,075 --> 00:10:17,000
而我们需要的,用到的是我们的Vector,雅可比的乘积。

110
00:10:17,000 --> 00:10:21,025
所以每一次我们都是在AI框架里面存的是雅可比的乘积,

111
00:10:21,025 --> 00:10:27,000
当做我们的权重参数,然后通过计算得到我们的雅可比,然后最后成为我们的Vector。

112
00:10:27,000 --> 00:10:33,650
这个就是自动微分的一个概念,那其实我们再展开,再展开细一点,

113
00:10:33,650 --> 00:10:37,100
就是AI框架跟自动微分的关系,

114
00:10:37,100 --> 00:10:41,000
刚才已经讲了把它当做一个神经网络的层。

115
00:10:41,000 --> 00:10:48,400
那其实呢,我们在做AI框架的时候,首先需要去注册前向的计算节点和导数的计算节点,

116
00:10:48,400 --> 00:10:54,000
就是正向的计算节点V4和反向的计算节点deltaf除以V4。

117
00:10:54,000 --> 00:11:00,300
那注册完这个呢,就是我们的系统已经有了这两个节点之后呢,这个节点呢,

118
00:11:00,300 --> 00:11:04,000
我们就会接受输入计算我们的这个节点的输出。

119
00:11:04,000 --> 00:11:10,425
那反向也是一样的,反向我接受刚才的向量雅可比的乘积作用的输入,

120
00:11:10,425 --> 00:11:14,575
然后计算输出,每次呢计算这个输入的成绩,

121
00:11:14,575 --> 00:11:18,000
然后就得到我们上一层的导数,就这个。  

122
00:11:18,000 --> 00:11:25,900
其实刚才我们讲的都是在当前的这个图,我们在上一节不是已经说过了吗,

123
00:11:25,900 --> 00:11:32,150
实际上计算图是一个有向无环图,图里面的展开是这样的,

124
00:11:32,150 --> 00:11:37,000
就是这个呢,黑色的这个正向,其实是正向的。

125
00:11:37,000 --> 00:11:43,250
但是我反向的时候,其实我是产生新的节点的,反向就是红色的新的节点,

126
00:11:43,250 --> 00:11:50,200
但是我反向的时候,我还会用到红就橙色的这一条,我还会用到正向的这些节点的变量,

127
00:11:50,200 --> 00:11:53,000
所以我们需要把中间变量都存起来。

128
00:11:53,000 --> 00:11:58,950
这个时候为什么我们经常说非常吃内存,非常吃显存,就是因为这个概念,

129
00:11:58,950 --> 00:12:06,075
我们的神经网络的图存下了大量的中间变量,而这个图呢,就是对应于我们反向的时候,

130
00:12:06,075 --> 00:12:11,025
我们有V5,V5,然后V5的时候可能还会,求V2的时候,

131
00:12:11,025 --> 00:12:15,000
V0的时候,我们还可能需要V0,是吧,V-0。

132
00:12:16,000 --> 00:12:22,075
其实我们会不断的去用正向的相关的一些计算,你要通过一个有向无环图,

133
00:12:22,075 --> 00:12:25,000
去把我们整个正向的反向的记录下来。

134
00:12:26,000 --> 00:12:33,450
这里面呢,我想提出几个问题,跟大家一起去思考一下,就是正向的模式和反向的模式,

135
00:12:33,450 --> 00:12:38,000
这两个模式都是自动微分的模式,计算量是否相同呢?

136
00:12:38,000 --> 00:12:44,000
那第二个呢,就是AI框架或者深度学习用户,为什么大部分都用反向模式呢?

137
00:12:45,000 --> 00:12:51,245
其实我们之前在自动微分系列里面去讲的时候,已经简单的通过一个手把手,

138
00:12:51,245 --> 00:12:56,000
教大家实现一个Pytorch AI框架,已经初步的展示了如何去实现的。

139
00:12:57,000 --> 00:13:00,000
具体的实现方式有三种,我们这里面总结了一下。

140
00:13:00,000 --> 00:13:05,875
首先呢,就是通过前向的计算,在前向计算的时候保留我们的中间的结果,

141
00:13:05,875 --> 00:13:10,375
那接着呢,根据反向模式的原理呢,一次计算中间的导数,

142
00:13:10,375 --> 00:13:15,000
就是我们刚才讲的每次计算中间的导数,记录正向的一个结果。

143
00:13:16,000 --> 00:13:20,975
接着呢,通过表达式追踪,那中间的这一等号右边的这个呢,

144
00:13:20,975 --> 00:13:28,000
就是我们的表达式,去跟踪记录每一个表达式,每一条表达式,最后把这些过程存起来。

145
00:13:28,000 --> 00:13:33,695
问题就是需要保存大量的计算结果和中间变量结果,

146
00:13:33,695 --> 00:13:39,000
但是呢,好处就是方便跟踪整个计算过程,就每一条每个过程我都有,它不会偷工减料。

147
00:13:40,000 --> 00:13:46,000
使用这种方式,最出名的一个AI框架呢,就是Pytouch大家用的,可能听的也比较多了。

148
00:13:47,000 --> 00:13:53,025
那第二种呢,就是无论是MathBoard和TensorFlow都是使用这种方式,

149
00:13:53,025 --> 00:13:58,000
将导数的计算表示成为一个计算图,那计算图就是我们这个系列里面的重点。

150
00:13:59,000 --> 00:14:07,090
然后通过这个计算图,我们叫做IR,GraphIR,对计算图进行一个统一的表示,

151
00:14:07,090 --> 00:14:12,000
非常方便计算机去进行操作,但是呢,它可能并不是非常符合人去看,

152
00:14:12,000 --> 00:14:17,775
因为它变成一个图,只是计算机能理解的图,这个图随着神经网络的构建,

153
00:14:17,775 --> 00:14:23,000
它会急剧的膨胀,包括反向它也会膨胀的非常厉害,非常不利于看。

154
00:14:24,000 --> 00:14:29,225
所以呢,它的缺点就是不利于调试跟踪和数学表达过程的一个表示,

155
00:14:29,225 --> 00:14:33,000
那优点就是非常方便全局的优化,非常节省内存。

156
00:14:33,000 --> 00:14:38,375
所以现在在部署的阶段呢,还是有很多人去用TensorFlow,

157
00:14:38,375 --> 00:14:42,275
而现在要做一些又可以训练又可以部署的情况下呢,

158
00:14:42,275 --> 00:14:45,000
可能会有越来越多的用户去采用学习。 

159
00:14:45,950 --> 00:14:47,950
MindSpore头包。

160
00:14:49,000 --> 00:14:55,200
刚才只是讲了具体的一个实现的概念,就是concept,其实在具体的实现呢,

161
00:14:55,200 --> 00:15:00,000
我们是做一个优化的pass,做pass呢是编辑器的一个概念,

162
00:15:00,000 --> 00:15:05,000
通过优化的pass去实现的,就在编辑器里面去实现的。

163
00:15:06,000 --> 00:15:12,000
大家搞清楚基本概念和关系,后面在实现的过程当中就很好理解了。

164
00:15:13,000 --> 00:15:17,000
那首先呢就要给进正向的数据流图,那正向的数据流图比较好构建。

165
00:15:18,000 --> 00:15:21,800
正向的数据流图呢比较好构建,就通过我们这个forward,

166
00:15:21,800 --> 00:15:26,000
然后对这个forward进行解析,就可以把它变成一个正向的数据流图。

167
00:15:26,000 --> 00:15:31,225
然后呢我们有了正向的数据流图之后呢,就会以损失函数作为根节点,

168
00:15:31,225 --> 00:15:34,000
就是我们的这个loss作为根节点。

169
00:15:35,000 --> 00:15:39,300
然后通过广度优先遍历前向的数据流图,通过队伍的关系,

170
00:15:39,300 --> 00:15:44,000
我们可以看到基本上都是队伍的关系,求出我们的反向的计算图。

171
00:15:45,000 --> 00:15:49,900
现在我们来review一下,首先呢我们的模型表示就是通过我们的前端的代码,

172
00:15:49,900 --> 00:15:54,000
然后变成一个计算图,我们叫做computation graph。

173
00:15:54,000 --> 00:16:01,225
表示完之后呢,后面呢就会去实行自动微分,然后呢有了这个正向的计算图之后呢,

174
00:16:01,225 --> 00:16:05,000
就会基于反向模式的原理构建反向的计算图。

175
00:16:06,000 --> 00:16:13,000
有了正反向的计算图,才能够成为一个完整的深度学习训练之前执行的计算图。

176
00:16:14,000 --> 00:16:22,000
今天我们回顾一下,我们了解了神经网络或者AI系统当中,训练流程跟微分之间的一个关系。

177
00:16:22,000 --> 00:16:27,775
第二个呢就是回顾了自动微分的正反向模式和计算图的自动微分,

178
00:16:27,775 --> 00:16:32,000
就计算图跟自动微分的关系,我们已经搞清楚了。

179
00:16:33,775 --> 00:16:40,775
那最后呢我们还了解了自动微分在深度学习是 通过一个优化的pass在编译器里面去实现的。

