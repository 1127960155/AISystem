1
00:00:00,000 --> 00:00:07,520
Hello 大家好,我是钟眉将。

2
00:00:07,520 --> 00:00:16,080
我们又来到了没什么观看,但是我依然在坚持的一节自动微分的系列的课堂当中。

3
00:00:16,080 --> 00:00:21,760
这一节主要是讲反向操作服重载去实现自动微分。

4
00:00:21,960 --> 00:00:30,320
那这个自动微分的方式更类似于Pytorch这个AI框架,就是使用反向操作服重载的自动微分。

5
00:00:30,760 --> 00:00:34,240
那我们一起来回顾一下什么叫做操作服重载。

6
00:00:34,240 --> 00:00:42,800
下面这个操作服重载的这句话其实是我在Wiki或者百度上面粘过来的,具体在哪粘我已经忘了。

7
00:00:43,160 --> 00:00:49,040
简单的来说,其实它只是利用了语言的多态性,然后进行了一个重载。

8
00:00:50,040 --> 00:01:01,120
下面这一段反倒是没什么用,但是依旧在那放着的一句话,就是讲操作服自动重载的微分的方式的一些过去的AI框架。

9
00:01:01,120 --> 00:01:05,520
那最典型的一个代表就是我们经常用到的Pytorch。

10
00:01:05,520 --> 00:01:13,960
其中最重要的就是使用数据结构TAP来记录整个计算流程,也就是我们理解的计算图。

11
00:01:13,960 --> 00:01:17,640
但是在Pytorch里面,它没有一个现实的计算图。

12
00:01:18,040 --> 00:01:23,920
然后在反向求解T度的时候去replay,去重放我的操作这么一种方式。

13
00:01:23,920 --> 00:01:29,080
现在我们来简单的去回顾一下操作服重载的基本流程。

14
00:01:29,080 --> 00:01:39,840
首先就是需要用语言的多态性对操作服进行重载,定一个特殊的数据结构,并且对每个计算进行重载的操作。

15
00:01:40,040 --> 00:01:47,560
第二个就是有一个TAP的一个数据结构,对我们的数据输出和我们的计算进行记录。

16
00:01:47,560 --> 00:01:56,160
接着我们记录了每一次操作之后,需要对每次操作进行变例,然后计算它的微分方式。

17
00:01:56,160 --> 00:02:05,040
最后就是使用练试求到法则,然后通过练试法则把刚才变例得到的微分的结果进行累积。

18
00:02:05,040 --> 00:02:09,440
这个就完成了整个操作服重载的流程了。

19
00:02:10,040 --> 00:02:17,840
操作服重载我们其实已经多次讲到了,它的优点就是实现起来只需要语言去提供多态的性能。

20
00:02:17,840 --> 00:02:24,240
第二个就是它的应用性非常高,操作服重载之后跟原生语言的编程方式是类似的。

21
00:02:24,240 --> 00:02:35,240
所以大家都会说,极度的模仿操作服重载的PyTorch的方式,非常容易去使用,非常方便去理解,跟理解Python代码一样。

22
00:02:35,240 --> 00:02:39,240
这个就是PyTorch的优点,但它的缺点也是非常明显。

23
00:02:39,240 --> 00:02:52,640
上面我们用了一个Tab去记录大量的操作,这个时候就需要对特殊的数据结构进行大量的读和写了,便利等操作了,非常不利于高阶的微分实现。

24
00:02:52,640 --> 00:03:02,840
高阶微分我们可能会在动力学、生物分子建模、物理方程模拟等非常常见的一些科学计算场景经常用到。

25
00:03:02,840 --> 00:03:07,040
这个时候这种自动微分的方式非常不利于求解。

26
00:03:07,040 --> 00:03:14,240
第二个就是类似于Wire,If Else这些控制表达,其实很难通过操作服去重载的。

27
00:03:14,240 --> 00:03:17,440
下面我们来看看反向模式。

28
00:03:17,440 --> 00:03:22,240
反向模式一般来说是比较简单好理解的。

29
00:03:22,240 --> 00:03:45,840
又回到我们熟悉的图里面,正向模式假设我现在有一个X的输入,然后我正向的就是每一次去计算每一个节点,然后去计算中间变量的导数,最后一个个计算,然后得到我们最终输出的FSES2这个输出对于X的导数,这个就是每次正向计算的。

30
00:03:46,040 --> 00:04:05,640
那反向计算就是我从最后一个,每个中间变量关于最初的一个导数,那从反向开始就是从后面开始计算每一条路径关于逆向输入的一个导数,最后我就求得了DeltaF关于S2和DeltaF关于S1的所有的导出形式。

31
00:04:05,640 --> 00:04:25,440
那在机器学习里面呢,因为我的输入神经元非常的大量,而我的输出类别有限,在机器学习里面呢,所以我们一般都会用到反向模式的自动微分的方式去实现,那这个也是反向传播的一个最原始的idea或者数学原理。

