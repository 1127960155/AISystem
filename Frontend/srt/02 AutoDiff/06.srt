1
00:00:00,000 --> 00:00:07,520
Hello 大家好,我是钟眉将。

2
00:00:07,520 --> 00:00:16,080
我们又来到了没什么观看,但是我依然在坚持的一节自动微分的系列的课堂当中。

3
00:00:16,080 --> 00:00:21,760
这一节主要是讲反向操作服重载去实现自动微分。

4
00:00:21,960 --> 00:00:30,320
那这个自动微分的方式更类似于Pytorch这个AI框架,就是使用反向操作服重载的自动微分。

5
00:00:30,760 --> 00:00:34,240
那我们一起来回顾一下什么叫做操作服重载。

6
00:00:34,240 --> 00:00:42,800
下面这个操作服重载的这句话其实是我在Wiki或者百度上面粘过来的,具体在哪粘我已经忘了。

7
00:00:43,160 --> 00:00:49,040
简单的来说,其实它只是利用了语言的多态性,然后进行了一个重载。

8
00:00:50,040 --> 00:01:01,160
下面这一段反倒是没什么用,但是依旧在那放着的一句话,就是讲操作服自动重载的微分的方式的一些过去的AI框架。

9
00:01:01,160 --> 00:01:05,520
那最典型的一个代表就是我们经常用到的Pytorch。

10
00:01:05,520 --> 00:01:13,960
其中最重要的就是使用数据结构Tap来记录整个计算流程,也就是我们理解的计算图。

11
00:01:13,960 --> 00:01:17,680
但是在Pytorch里面,它没有一个现实的计算图。

12
00:01:18,000 --> 00:01:23,880
然后在反向求解T度的时候去replay,去重放我的操作,这么一种方式。

13
00:01:24,240 --> 00:01:29,080
现在我们来简单的去回顾一下操作服重载的基本流程。

14
00:01:29,080 --> 00:01:39,840
首先就是需要用语言的多态性对操作服进行重载,定一个特殊的数据结构,并且对每个计算进行重载的操作。

15
00:01:40,120 --> 00:01:47,560
第二个就是有一个Tap的一个数据结构,对我们的数据输出和我们的计算进行记录。

16
00:01:48,080 --> 00:01:56,160
接着我们记录了每一次操作之后,需要对每次操作进行变例,然后计算它的微分方式。

17
00:01:56,440 --> 00:02:05,000
最后就是使用练试求到法则,然后通过练试法则把刚才变例得到的微分的结果进行累积。

18
00:02:05,720 --> 00:02:09,400
这个就完成了整个操作服重载的流程了。

19
00:02:10,120 --> 00:02:17,800
操作服重载我们其实已经多次讲到了,它的优点就是实现起来只需要语言去提供多态的性能。

20
00:02:17,800 --> 00:02:24,200
第二个就是它的应用性非常高,操作服重载之后跟原生语言的编程方式是类似的。

21
00:02:24,680 --> 00:02:35,120
所以大家都会说,极度的模仿操作服重载的PyTorch的方式,非常容易去使用,非常方便去理解,跟理解Python代码一样。

22
00:02:35,320 --> 00:02:37,080
这个就是PyTorch的优点。

23
00:02:37,400 --> 00:02:43,680
它的缺点也是非常明显,上面我们用了一个Tab去记录大量的操作,

24
00:02:44,200 --> 00:02:52,480
这个时候就需要对特殊的数据结构进行大量的读和写了,便利等操作了,非常不利于高阶的微分实现。

25
00:02:52,840 --> 00:03:02,840
高阶微分我们可能会在动力学、生物分子建模、物理方程模拟等非常常见的一些科学计算场景经常用到。

26
00:03:02,920 --> 00:03:07,120
这个时候这种自动微分的方式非常不利于求解。

27
00:03:07,440 --> 00:03:14,200
第二个就是类似于Wire,If Else这些控制表达,其实很难通过操作服去重载的。

28
00:03:15,080 --> 00:03:17,480
下面我们来看看反向模式。

29
00:03:18,800 --> 00:03:22,360
反向模式一般来说是比较简单好理解的。

30
00:03:22,600 --> 00:03:28,760
又回到我们熟悉的图里面,正向模式,假设我现在有一个X的输入,

31
00:03:28,920 --> 00:03:35,840
然后我正向的就是每一次去计算每一个节点,然后去计算中间变量的导数,

32
00:03:36,000 --> 00:03:45,720
最后一个个计算,然后得到我们最终输出的FSES2这个输出对于X的导数,这个就是每次正向计算的。

33
00:03:46,120 --> 00:03:52,320
那反向计算就是我从最后一个,每个中间变量关于最初的一个导数,

34
00:03:52,480 --> 00:03:58,960
那从反向开始就是从后面开始,计算每一条路径关于逆向输入的一个导数,

35
00:03:59,160 --> 00:04:05,680
最后我就求得了DeltaF关于S2和DeltaF关于SE的所有的导出形式。

36
00:04:06,160 --> 00:04:13,200
那在机器学习里面,因为我的输入神经元非常的大量,而我的输出类别有限。

37
00:04:13,600 --> 00:04:20,040
在机器学习里面,所以我们一般都会用到反向模式的自动微分的方式去实现。

38
00:04:20,280 --> 00:04:25,720
那这个也是反向传播的一个最原始的idea或者数学原理。

39
00:04:25,960 --> 00:04:30,840
后面我们了解了这一点,看反向传播这个算法可能会更有感觉。

40
00:04:31,960 --> 00:04:41,720
下面我想通过简单的几分钟的了解去跟着大家一起去回顾或者学习一下Pytorch的AutoDiff是怎么去实现的。

41
00:04:41,960 --> 00:04:51,240
这里面的所有的操作方式都是根据Pytorch的最核心的框架的一个原始理念然后去复现的。

42
00:04:51,880 --> 00:05:01,960
首先我们需要FromTyping,ImportList,Line,Tupo,Callable,Disk,Operation这一些简单的操作方便我们下面去一个加载的。

43
00:05:02,200 --> 00:05:08,360
那这个FlashName有什么用呢?FlashName这个函数是用来打印跟Tape相关的变量。

44
00:05:08,520 --> 00:05:15,320
这是我这个V呢FV然后GunName这个Name就是记录我们下面的每一条Tape。

45
00:05:15,480 --> 00:05:26,600
假设我SE等于V-1,S2等于V0,V-1又通过一个计算,每一行每一次计算都有一个Tape去记录的。

46
00:05:27,000 --> 00:05:33,000
所以我这里面通过FlashName去记录我每一次Tape到底是第几个。

47
00:05:33,160 --> 00:05:42,040
然后GunName第一个就是1,我们从1开始不断的去累积然后返回我们的V等于多少个。

48
00:05:42,840 --> 00:05:52,920
为了更加好的理解Pytorch里面的反向模式自动微分的实现,我们实现的代码过程当中完全不依赖于Pytorch的AutoGrid的方式,

49
00:05:53,080 --> 00:06:01,480
反倒是我们引入了一个新的Lay,这个Lay叫做Variable,也就是类似于Pytorch里面的Tensor。

50
00:06:01,720 --> 00:06:08,840
我们在计算的时候,实际上我们是从最后的损失函数或者L来去进行一个计算的。

51
00:06:09,080 --> 00:06:16,280
程序当中每算一个张量X的值,就是它的T度的时候,都会去计算DL到DS的一个导数。

52
00:06:16,680 --> 00:06:23,720
然后反向模式就是从DL对DL自身的导数开始,也就是DL对DL的导数等于1。

53
00:06:23,960 --> 00:06:30,120
我们回头看看上面这条公式,V5就是我的Y对Y对自身的导数是1。

54
00:06:30,280 --> 00:06:39,080
我们可以从这个讲式开始,然后使用偏导数和练习法则进行传播,也就是下面这条公式,然后一步步的去算的。

55
00:06:39,320 --> 00:06:46,040
下面我们的代码实现可能还是比较简单,我的Variable我们可以理解为简单的张量。

56
00:06:46,440 --> 00:06:53,720
对于张量,一开始我们会初始化一个值叫做Value,通过这个值变成张量的成员变量。

57
00:06:53,800 --> 00:07:07,480
然后SelfName就是我们刚才的中间变量,如果一开始没有输入Name,它可能就直接使用FastName,就是我们刚才上面的一个函数,FastName,然后不断的去累加1。

58
00:07:08,760 --> 00:07:15,960
接着下面这几个就比较有意思了,Constant其实是比较方便我们去打印查看的一个过程。

59
00:07:16,120 --> 00:07:22,840
我们会通过Constant然后上下文去把当前的一个值打印出来,还有当前的Name打印出来。

60
00:07:23,160 --> 00:07:33,720
下面这几个就是回到我们一开始去实现或者上两节分享内容里面的一个实现,只有一条简单的公式,这里面有5个操作。

61
00:07:33,880 --> 00:07:46,840
第一个就是成,加,解,Scient和Log,我们一开始并没有去实现这几个函数,而是返回了OPSMOD,OPSAD,OPSSUB。

62
00:07:47,480 --> 00:07:58,360
在反向自动微分的时候,其实最核心的就是一个Type,用来跟踪Variable的所有的计算,以便于后面用链式求打法则的。

63
00:07:58,680 --> 00:08:04,760
这里面就出现了一个Type的类,Type的类的数就是NameTupo,它是一个String。

64
00:08:05,400 --> 00:08:14,760
我的输入或者我的记录的类有两个,第一个是Input,第二个是Output,那Propagation就是应用我们的链式求打法则的,

65
00:08:14,840 --> 00:08:26,680
告诉我的输入是什么,输出是什么,值得注意的是这里面的输入是我的DeltaL到DeltaOutput,输出是DeltaL除以DeltaInput。

66
00:08:27,000 --> 00:08:41,160
Type所有原始计算的累计的List列表,就是我要把所有的计算逆向的过程记录下来,最终通过便利的方式求的每一次反向的自动微分的操作。

67
00:08:41,880 --> 00:08:56,200
下面我们有另外一个函数,叫做ResetTable,这个函数很简单,就是重新初始化我们整个GradientTape,把GradientTape重新初始化一遍。

68
00:08:57,160 --> 00:09:17,160
下面我们来看看具体的每个原子操作怎么去实现,刚才我们在Variable或者我们的Tensor里面,从载MODE、ADD、SUB这些原始操作的时候,返回的是一个OPSSUB这个原子操作,我们看看现在这个原子操作具体实现了哪些功能。

69
00:09:17,160 --> 00:09:43,000
正向的时候的计算比较简单,首先第一个传进来的Addes它也是一个Variable或者一个张量,我们这里面自身其实它是一个张量,所以我们两个张量相乘,需要通过Variable把它们包起来,最后返回一个X,这个X正向计算的时候我们直接返回出去,中间的这一坨就是为了我们在反向的时候去计算的,反向的时候我们先不要去看反向的计算,而是去看反向的计算。

70
00:09:47,160 --> 00:10:17,160
然后我们可以看一下我们的Tape具体做了哪些工作,这个就是我们的Tape,Tape就是记录我们的输入输出还有我们反向操作的一个必保函数,Tape我们刚才也是重新声明了,只是记录我们输入输出还有对应的操作,对应的反向操作就是这个,然后通过Gradient的Tape把当前的TapeAppend进去,就通过一个列表List来记录我所有的操作,

71
00:10:17,160 --> 00:10:47,160
然后我们就会去便利这个Gradient的Tape去把每一次的操作逆向的求出来,就把所有的正向的计算操作求一遍,把反向的计算操作求一遍,就求得了最终的DeltaL对DeltaS1,S2,S3,反向的微分的时候我们有一个函数叫做Propagation,它的输入是Dout对Doutput的一个值,这个反向就是我的损失函数对输出的一个导数,那Dout对Ds就是我当前的一个计算,

72
00:10:47,160 --> 00:11:04,160
接着就是Dx对Dself的一个值就是Others,Dx对Douts的值就是我当前的数,藏法里面我们可以看到根据乘数的除道法则,就是这两个,然后我们再求Del对Dself还有Del对Douts的一个值,

73
00:11:04,160 --> 00:11:19,160
最后就把我的输出扔出来,因为这里面有两个输出,所以我们会把两个输出都同时反送出去,那同样的,我们的Add操作也是相同的方式去处理,我的Sub操作也是相同的。

74
00:11:19,160 --> 00:11:35,160
那加减乘除里面可能会简单一点的,就是加和减,加和减无论你怎么算,它里面就是对自身的数进行求导等于1,如果你对另外一个数进行求导,那就是减号,保留减号就等于-1。

75
00:11:35,160 --> 00:11:42,160
Science还有Lock这两个也是比较简单的,Lock就是1除以Self Value就可以了。

76
00:11:42,160 --> 00:11:47,160
然后如果你对另外一个数进行求导,就是对Dx乘以Dself。

77
00:11:51,160 --> 00:12:02,160
在Pytorch,Tenso 4或者MindSpore里面,如果不显示的去试字Set AutoGrid或者实现一个自动微分的时候,其实只是做了一个正向的计算,

78
00:12:02,160 --> 00:12:08,160
在实际上需要反向计算的时候,就需要去声明我这个函数需要进行反向。

79
00:12:08,160 --> 00:12:11,160
那我们这里面的反向模式也是一样的。

80
00:12:11,160 --> 00:12:17,160
首先通过一个函数Grid,然后去声明我需要进行一个反向梯度的求解。

81
00:12:17,160 --> 00:12:21,160
那输入有两个,第一个是L,第二个是Results。

82
00:12:21,160 --> 00:12:31,160
输出Results它是一个X,代表它是一个List,里面就对应于我们的需要求导的所有的函数。

83
00:12:31,160 --> 00:12:41,160
L就是我最后的V0,我们可以看到这里面的公式,对应的是这个Results,从下往上求。

84
00:12:41,160 --> 00:12:50,160
我的L就是V5,我的Results就是V1和X1和S2,对应的V-1和V0。

85
00:12:52,160 --> 00:12:57,160
回到我们最核心的这个里面,我们首先创建一个字典,Di-D。

86
00:12:57,160 --> 00:13:07,160
Di-D它是一个字典,里面就记录了每个Di对Ds或者D中间的变量的所有的名字和数值。

87
00:13:07,160 --> 00:13:15,160
然后我们最后一个Rubble等于1,所以把最后一个L的name拿出来,然后丢给它作为1。

88
00:13:15,160 --> 00:13:20,160
我们可以看到这里面最后一个假设是1,这个是所有的前提。

89
00:13:21,160 --> 00:13:32,160
Gathering Grid这个类型函数主要是去把所有的Entity,就是我的Grid里面的所有的Tape的数值都记录下来,丢给我的Di和D,

90
00:13:32,160 --> 00:13:40,160
也就是把所有的数值或者我的计算的过程放在我的Di-D里面,为的就是方便我进行打印的时候操作。

91
00:13:41,160 --> 00:13:51,160
这个时候我们可以看到Dl-D主要是去记录所有的Dl,D0,这个具体的计算公式。

92
00:13:51,160 --> 00:14:00,160
这段代码是最核心的,里面用了Python的reversed函数,这意味着我们需要对Gradient Tape,

93
00:14:00,160 --> 00:14:05,160
就是刚才去计算初始化的时候所有的Tape进行一个立项的操作。

94
00:14:05,160 --> 00:14:12,160
我们在计算的时候主要是正向的,例如这条公式,我的Forward Model是正向的计算一遍的,

95
00:14:12,160 --> 00:14:21,160
Reverse Model的时候在我的计算图需要反过来进行纠结,所以我们这个时候通过Reverse把每一个计算操作反过来,

96
00:14:21,160 --> 00:14:33,160
然后把Entity打印出来,这个时候Dl对Di-Post,然后Gathering Grid出来,放在我的Dl跟D里面,接着去把Dl跟Di-Impost计算出来。

97
00:14:33,160 --> 00:14:41,160
这个函数就是正式的计算了,还记得我们的Entity里面刚才放入了哪些内容吗?

98
00:14:41,160 --> 00:14:47,160
Entity里面我们刚才放入了有几个内容,一个是Inputs,一个是Outputs,一个是Propagation。

99
00:14:47,160 --> 00:14:54,160
我们现在把Propagation引入进来,然后把DlPost丢给它,然后就得到我们的Inputs了。

100
00:14:54,160 --> 00:15:01,160
我们可以反向的去计算的,所有的Output输进去我的函数,反向过来得到我的Inputs。

101
00:15:01,160 --> 00:15:10,160
对我们的球解的T度进行累积,所以在牙科比矩阵里面去计算的时候,并不是每个牙科比矩阵算一遍,

102
00:15:10,160 --> 00:15:19,160
而是对牙科比矩阵计算出来的最后一个逐渐的往上累积,得到我们最终的每一条计算输出的导数。

103
00:15:19,160 --> 00:15:28,160
那这个就是把刚才的Dl便利一遍,然后把Dl的Name出现,Dl的Name所对应的值打印出来。

104
00:15:28,160 --> 00:15:36,160
下面我们以这条公式为例,看不懂,上面的没有关系,我们来看看实际代码里面具体是怎么呈现的。

105
00:15:36,160 --> 00:15:45,160
或者回头你再看看我评论里面的开源代码委托在哪里,你要重新深入的去看一看具体代码是怎么的一个过程。

106
00:15:45,160 --> 00:15:53,160
公式还是那个熟悉的味道,fx1s2等于lin的se加上se乘以s2,减去science的s2。

107
00:15:53,160 --> 00:16:00,160
这里面由于基于的是操作副存在的计算方式,所以我们在初始化的时候需要两个边量,

108
00:16:00,160 --> 00:16:12,160
第一个就是x,第二个就是y,x代表x1,y代表x2,然后初始化是2和5,它的名字是从-1到0开始。

109
00:16:12,160 --> 00:16:22,160
下面我们来一个正向的计算,正向的计算的结果是一个f,f它也是个variable的对象,这里面代表它是一个张量或者一个variable。

110
00:16:22,160 --> 00:16:30,160
我们可以看到它的计算就是variable,lnln加乘减再求个science,所有的操作都是一模一样的。

111
00:16:30,160 --> 00:16:42,160
正向的时候,因为我们刚才对每一个数进行打印,所以我们可以看到从-1,0等于5,然后v等于lnv1,v2等于v的-1乘以v0,

112
00:16:42,160 --> 00:16:54,160
然后这么一步步过来,我们可以得到最终的输出结果等于11.6520.7,那这个跟我们正向的是一模一样的,正向的求答方式是一模一样的。

113
00:16:54,160 --> 00:17:02,160
从v-0到v5,然后每个计算v4-v3,v5等于v4-v3,所有的操作都是一模一样的。

114
00:17:02,160 --> 00:17:13,160
这个就是得益于我们刚才在操作符重载的时候,把我们的操作打印出来,把每一个需要进行哪一步,做什么动作都打印出来。

115
00:17:13,160 --> 00:17:19,160
这个s就是正向的计算,通过type记录下来每个反向的操作。

116
00:17:19,160 --> 00:17:27,160
然后再到变力的时候逆过来去求每一个立项,然后进行一个累积。

117
00:17:28,160 --> 00:17:33,160
那下面我们来看看最后的一个反向的时候怎么操作。

118
00:17:33,160 --> 00:17:46,160
反向的时候我们去上面我的gradient,然后L等于我的f,results等于x和y,意味着我需要对x和y进行求答,然后把dx打印出来,把dy打印出来。

119
00:17:46,160 --> 00:17:52,160
我们其实去调用这个时候,下面这一坨内容就会不断的打印出来。

120
00:17:52,160 --> 00:18:00,160
dl,d实际上存的是一个字点,那这个字点存的就是v5,我们的第几个,然后它的值是多少。

121
00:18:00,160 --> 00:18:05,160
然后type就是记录我的input有多少个,我的output是多少个。

122
00:18:05,160 --> 00:18:15,160
我们从v5开始然后去求v4和v3,因为v4和v3同时都利用到v5,那我们可以看到基本上是一样的。

123
00:18:15,160 --> 00:18:26,160
然后propagation,这个就是我的propagation的一个对象,然后一步步的逆向的求解,dx等于5.5,dy等于1.7,那这个就有意思了。

124
00:18:26,160 --> 00:18:36,160
基本上我们从逆向过来求解的时候,就可以求得了v1,v2就是v-1,v0等于5.5,等于1.1,1.7,1.6。

125
00:18:37,160 --> 00:18:46,160
好了,今天我们主要是手把手的,大家去实现一个类似于pyTorch的核心机制,反向操作服自动存载的微分。

126
00:18:46,160 --> 00:18:57,160
在这过程当中,我们学习了操作服存载,我们回顾了操作服存载的具体的原理和方式,然后又回顾了反向模式的方式。

127
00:18:57,160 --> 00:19:06,160
接着通过python这个高级语言去模拟了一片pyTorch的AutoGrid核心机制原理具体是什么实现的。

128
00:19:06,160 --> 00:19:15,160
其中最核心的就是我的tape的操作,我们需要把tape的inputs,outputs搞清楚tape具体做了哪些工作,记录了哪些工作。

129
00:19:15,160 --> 00:19:20,160
然后在存载的过程当中,具体的运算方式是什么样的。

130
00:19:20,160 --> 00:19:26,160
这里面运算方式绝对不能写错,写错了,可能最后所有的计算都是错误的。

131
00:19:26,160 --> 00:19:38,160
那在具体的AutoGrid里面,我们又实现了一个微分的自动累加,通过微分的自动累加,实现了牙和鼻矩子里面每一行的计算的数据累加。

132
00:19:41,160 --> 00:19:43,160
就得到反向自动微分具体的实现。

133
00:19:43,160 --> 00:19:47,160
卷的不行了,卷的不行了,记得一键三连加关注哦。

134
00:19:47,160 --> 00:19:50,160
所有的内容都会开源在下面这条链接里面。

135
00:19:50,160 --> 00:19:52,160
拜了,拜拜。

