1
00:00:00,000 --> 00:00:04,600
BBABABAAABABAAABABABABABA

2
00:00:05,880 --> 00:00:06,800
Hello 大家好

3
00:00:06,800 --> 00:00:07,600
我是中米醬

4
00:00:07,600 --> 00:00:09,600
欢迎来到中米的课堂

5
00:00:09,600 --> 00:00:12,960
那这一节可能是我给大家分享的

6
00:00:12,960 --> 00:00:15,720
所有的知识里面最复杂最深奥的

7
00:00:15,720 --> 00:00:19,880
所以我尽可能希望能够简的明白简单一点

8
00:00:19,880 --> 00:00:22,560
WOW

9
00:00:22,560 --> 00:00:23,480
EW

10
00:00:23,480 --> 00:00:24,800
EW

11
00:00:24,800 --> 00:00:25,760
What the f***

12
00:00:25,760 --> 00:00:28,840
这里面我们要讲讲自动微分的模式

13
00:00:29,200 --> 00:00:30,280
什么

14
00:00:30,280 --> 00:00:32,360
自动微分还分不同模式

15
00:00:32,360 --> 00:00:34,000
不就搞个微分吗

16
00:00:34,000 --> 00:00:35,840
搞那么多干嘛

17
00:00:35,840 --> 00:00:37,040
哎 不要急

18
00:00:37,040 --> 00:00:38,440
我们来看看自动微分

19
00:00:38,440 --> 00:00:39,880
实际上分两种模式

20
00:00:39,880 --> 00:00:41,720
只有两种没有太复杂

21
00:00:41,720 --> 00:00:43,320
一种叫做前向微分

22
00:00:43,320 --> 00:00:45,240
一种叫做后向微分

23
00:00:45,240 --> 00:00:48,320
前向很明显就是从前往后算

24
00:00:48,320 --> 00:00:50,440
后向微分比较直接了

25
00:00:50,440 --> 00:00:52,160
就是从后往前算

26
00:00:52,160 --> 00:00:55,680
虽然我们前向微分在英文里面叫做Forward Model

27
00:00:55,680 --> 00:00:58,000
后向叫做Backward Model

28
00:00:58,000 --> 00:01:00,640
为了方便我们去对数学表示

29
00:01:00,640 --> 00:01:02,800
所以我们引入了牙科比矩阵

30
00:01:02,800 --> 00:01:06,480
通过牙科比矩阵去表示我们前向微分

31
00:01:06,480 --> 00:01:09,120
或者后向微分的一个数学表现

32
00:01:09,120 --> 00:01:11,240
我太难了

33
00:01:13,320 --> 00:01:18,000
现在我们继续回到上一节里面去给大家分享的

34
00:01:18,000 --> 00:01:19,720
什么是自动微分

35
00:01:19,720 --> 00:01:23,560
自动微分上一节其实没有太多的去展开去讲

36
00:01:23,560 --> 00:01:26,080
只是给大家介绍了一个概念

37
00:01:26,120 --> 00:01:30,440
它是有很多有限的基本运算去组成的

38
00:01:30,440 --> 00:01:32,640
也就是加减乘除

39
00:01:32,640 --> 00:01:35,360
通过这么简单的一些自动微分的方式

40
00:01:35,360 --> 00:01:38,200
或者面试求答法则记录下来

41
00:01:38,200 --> 00:01:40,080
然后进行组合

42
00:01:40,080 --> 00:01:45,080
这个公式Fx1s2就会贯穿整一个系列的

43
00:01:45,080 --> 00:01:46,760
那这里面很简单

44
00:01:46,760 --> 00:01:51,560
它是link s1加上s1xs2-science x2

45
00:01:51,560 --> 00:01:55,120
我们正向去计算Forward Primer Traced

46
00:01:55,120 --> 00:01:59,080
是表示我对这个公式进行正向的计算

47
00:01:59,080 --> 00:02:01,920
跟我们正向自动微分没有半点关系

48
00:02:01,920 --> 00:02:04,600
我们看一下现在我有两个输入

49
00:02:04,600 --> 00:02:05,520
一个输入是x1

50
00:02:05,520 --> 00:02:06,760
一个输入是x2

51
00:02:06,760 --> 00:02:08,000
1是2

52
00:02:08,000 --> 00:02:09,480
x2是5

53
00:02:09,480 --> 00:02:12,680
然后带进这个公式不断的往下计算

54
00:02:12,680 --> 00:02:14,760
我们可以看到我们先转link2

55
00:02:14,760 --> 00:02:15,880
然后算2x5

56
00:02:15,880 --> 00:02:17,280
再算science5

57
00:02:17,280 --> 00:02:18,600
然后再算加号

58
00:02:18,600 --> 00:02:20,200
再算减号

59
00:02:20,200 --> 00:02:24,320
最后得到我的Fx1s2是我的y

60
00:02:24,360 --> 00:02:26,040
等于10.652

61
00:02:26,040 --> 00:02:27,960
这个就是正向的一个计算

62
00:02:30,320 --> 00:02:31,840
有了正向的计算

63
00:02:31,840 --> 00:02:36,240
我们希望能够把它变成一个更加简单的方式

64
00:02:36,240 --> 00:02:37,920
或者变成一个图

65
00:02:37,920 --> 00:02:41,440
那图在计算机里面是很容易去实现的

66
00:02:41,440 --> 00:02:44,000
我们叫做DAG图

67
00:02:44,000 --> 00:02:46,160
有像无还图

68
00:02:46,160 --> 00:02:49,280
因为这个图我们从s1s2输入

69
00:02:49,280 --> 00:02:52,280
然后到Fx1s2输出

70
00:02:52,280 --> 00:02:53,880
都是没有回还的

71
00:02:53,880 --> 00:02:55,680
也就是不会往后走

72
00:02:55,680 --> 00:02:56,920
返回前面的

73
00:02:56,920 --> 00:02:58,640
所以叫做DAG图

74
00:02:58,640 --> 00:03:01,720
那每一个节点就是我们的一个中间结果

75
00:03:01,720 --> 00:03:03,760
或者叫做中间变量

76
00:03:03,760 --> 00:03:06,400
而边就是我的每一个计算

77
00:03:06,400 --> 00:03:07,760
包括我的加减乘除

78
00:03:07,760 --> 00:03:09,360
求根号science coscience

79
00:03:09,360 --> 00:03:11,800
不同的计算方式

80
00:03:11,800 --> 00:03:13,240
代表我的边

81
00:03:13,240 --> 00:03:15,480
我们现在假设我要对这个公式

82
00:03:15,480 --> 00:03:17,600
对s1进行求导

83
00:03:17,600 --> 00:03:20,760
那这个时候根据链式求导法则

84
00:03:20,760 --> 00:03:23,840
就展开变成这一条公式了

85
00:03:23,920 --> 00:03:25,920
公式还是这一条公式

86
00:03:25,920 --> 00:03:29,960
不过现在我们正式的进入到第一个内容

87
00:03:29,960 --> 00:03:33,000
就是正向自动微分

88
00:03:33,000 --> 00:03:35,600
叫做Fourth or Tangent model

89
00:03:35,600 --> 00:03:37,600
现在我们有两个概念

90
00:03:37,600 --> 00:03:39,720
第一个就是vi一点

91
00:03:39,720 --> 00:03:42,560
第二个就是yi一点

92
00:03:42,560 --> 00:03:46,640
那vi一点代表我对s1进行求导

93
00:03:46,640 --> 00:03:50,240
yj就代表我的y对s1进行求导

94
00:03:50,240 --> 00:03:52,680
可以看到现在左边的这一列

95
00:03:52,720 --> 00:03:54,200
都是加了一点的

96
00:03:54,200 --> 00:03:57,800
代表我左边的是对每一次小的操作

97
00:03:57,800 --> 00:03:59,280
进行求导

98
00:03:59,280 --> 00:04:00,840
正向的时候很简单

99
00:04:00,840 --> 00:04:03,800
首先我现在是对x1进行求导

100
00:04:03,800 --> 00:04:06,480
所有的东西都是对x1没有对s2

101
00:04:06,480 --> 00:04:09,360
对s1求导的时候我还是1

102
00:04:09,360 --> 00:04:12,080
s2这个时候已经变成我们的常量了

103
00:04:12,080 --> 00:04:14,520
对常量进行求导是我的0

104
00:04:14,520 --> 00:04:18,240
这个就是最简单link s1的一个导数

105
00:04:18,240 --> 00:04:21,280
下面就是x1乘以s2的导数

106
00:04:21,280 --> 00:04:23,760
我们每一次小的展开

107
00:04:23,760 --> 00:04:26,840
都是一个最简单最原始的计算

108
00:04:26,840 --> 00:04:31,680
最后就得到了我的y对s1的导数是5.5

109
00:04:31,680 --> 00:04:34,760
现在一个正向只是算了一遍

110
00:04:34,760 --> 00:04:37,600
算了我对x1的导数

111
00:04:37,600 --> 00:04:39,480
还没有算对x2的导数

112
00:04:42,000 --> 00:04:46,920
那刚才我们讲了简单的正向的求导方式

113
00:04:46,920 --> 00:04:49,680
下面我们来讲讲反向怎么求

114
00:04:49,680 --> 00:04:50,960
正向很简单的

115
00:04:50,960 --> 00:04:52,320
图还是那个图

116
00:04:52,320 --> 00:04:57,480
现在我们在刚才的图加了很多灰色的一些计算

117
00:04:57,480 --> 00:05:01,800
这里面每一个是单节点的一个影节点

118
00:05:01,800 --> 00:05:03,560
在反向的时候

119
00:05:03,560 --> 00:05:06,160
我是从y去计算

120
00:05:06,160 --> 00:05:09,480
然后算的我每一个输入的导数

121
00:05:09,480 --> 00:05:12,800
根据念式求导法则

122
00:05:12,800 --> 00:05:14,040
我们进行展开

123
00:05:14,040 --> 00:05:15,960
我对fx进行求导

124
00:05:15,960 --> 00:05:20,040
实际上是对每一个中间变量的一个偏导

125
00:05:20,080 --> 00:05:21,760
然后累积起来的

126
00:05:23,520 --> 00:05:28,520
那reverse model就是完全根据我们刚才讲的那条公式来去计算的

127
00:05:28,520 --> 00:05:32,440
这里面的vi就是我的yi对vi进行求导

128
00:05:32,440 --> 00:05:35,160
那下面我们从下面往上看起

129
00:05:35,160 --> 00:05:37,280
这里面有一个假式

130
00:05:37,280 --> 00:05:40,040
就是yi等于1

131
00:05:40,040 --> 00:05:41,680
这个假式很重要

132
00:05:41,680 --> 00:05:44,240
任何在计算的时候或者计算机里面

133
00:05:44,240 --> 00:05:46,160
我们都会把它变成一个1

134
00:05:46,160 --> 00:05:47,800
然后再往上去计算

135
00:05:47,840 --> 00:05:51,160
后面进行实际的一个代码编写的时候

136
00:05:51,160 --> 00:05:52,920
也会这么去做

137
00:05:52,920 --> 00:05:56,200
v4的一个导数实际上是我的v我的导数

138
00:05:56,200 --> 00:05:58,440
然后v5对v4的一个偏导

139
00:05:58,440 --> 00:06:01,120
然后这里面很简单就是v5乘以1

140
00:06:01,120 --> 00:06:03,520
因为这个的偏导其实是1

141
00:06:03,520 --> 00:06:06,680
然后不断的去往上计算

142
00:06:06,680 --> 00:06:11,080
算到我的x1的导数和x2的导数

143
00:06:12,560 --> 00:06:13,720
有点神奇

144
00:06:13,720 --> 00:06:15,760
x1的导数是5.5

145
00:06:15,800 --> 00:06:18,120
跟刚才的计算是一模一样的

146
00:06:18,120 --> 00:06:22,160
就是我的这一个5.5

147
00:06:22,160 --> 00:06:26,320
而我的那个x2的导数也算出来了

148
00:06:26,320 --> 00:06:28,880
是我的1.167x2的导数

149
00:06:28,880 --> 00:06:31,040
reverse model有点不一样的就是

150
00:06:31,040 --> 00:06:32,840
我一次输入

151
00:06:32,840 --> 00:06:36,640
通过刚才的炼射求导法则展开

152
00:06:36,640 --> 00:06:41,200
然后去计算得所有x的一个导数结果

153
00:06:41,200 --> 00:06:42,400
哎

154
00:06:42,400 --> 00:06:45,360
在计算机里面或者在神经网络里面

155
00:06:45,360 --> 00:06:48,160
当我的输入不断的增大的时候

156
00:06:48,160 --> 00:06:49,640
只有一个输出

157
00:06:49,640 --> 00:06:53,240
这个时候就非常适合用reverse model了

158
00:06:53,240 --> 00:06:55,400
这也是我们为什么去讲

159
00:06:55,400 --> 00:06:58,160
会有一个reverse model去进行求导

160
00:06:58,160 --> 00:07:00,120
而不是像高等数学一样

161
00:07:00,120 --> 00:07:03,120
只讲一个正向的求导方式

162
00:07:03,120 --> 00:07:07,160
下面我们来看看这个叫做亚可比矩阵

163
00:07:07,160 --> 00:07:09,240
亚可比矩阵很简单

164
00:07:09,240 --> 00:07:11,680
我们是y关于x的一个t度

165
00:07:11,680 --> 00:07:13,600
然后表示成为亚可比矩阵

166
00:07:13,600 --> 00:07:15,920
我们用jf来表示

167
00:07:15,920 --> 00:07:17,040
我们可以看到

168
00:07:17,040 --> 00:07:21,320
这里面我对每一个输出对x1进行求导

169
00:07:21,320 --> 00:07:23,920
我有多个y的时候怎么办

170
00:07:23,920 --> 00:07:27,200
所以我们就会把它变成一个大的矩阵

171
00:07:27,200 --> 00:07:29,800
那行就是我只有一个输出

172
00:07:29,800 --> 00:07:31,880
然后从x1到xn

173
00:07:31,880 --> 00:07:33,840
每个s对它进行的求导

174
00:07:33,840 --> 00:07:36,800
打数就是我从y1到ym

175
00:07:36,800 --> 00:07:38,960
但是每一次只对我的x1

176
00:07:38,960 --> 00:07:41,760
就是一个单的输入进行求导

177
00:07:41,800 --> 00:07:43,360
那通过亚可比矩阵

178
00:07:43,360 --> 00:07:45,040
我们就可以去表示

179
00:07:45,040 --> 00:07:48,800
刚才的正向维分和反向维分了

180
00:07:48,800 --> 00:07:51,920
刚才讲的说那么多正向维分反向维分的

181
00:07:51,920 --> 00:07:53,720
这只是一个过程

182
00:07:53,720 --> 00:07:56,680
过程也很难用一个矩阵去表示的

183
00:07:56,680 --> 00:07:59,160
迎入了亚可比矩阵之后

184
00:07:59,160 --> 00:08:01,160
我们不是很方便的去表示的吗

185
00:08:11,760 --> 00:08:13,760
实际上正向的自动维分模式

186
00:08:13,760 --> 00:08:16,680
我们叫做亚可比向量的g

187
00:08:16,680 --> 00:08:18,440
那我们现在来看一下公式

188
00:08:18,440 --> 00:08:19,520
还是这条公式

189
00:08:19,520 --> 00:08:21,920
jf这个是我们的亚可比矩阵

190
00:08:21,920 --> 00:08:23,920
那我们现在去试试v

191
00:08:23,920 --> 00:08:26,720
是关于函数l等于gy的一个梯度

192
00:08:26,720 --> 00:08:29,920
就是把我们的y损失函数扩展我们的输出

193
00:08:29,920 --> 00:08:31,920
通过g去表示

194
00:08:31,920 --> 00:08:33,360
多了一个数字

195
00:08:33,360 --> 00:08:36,080
我们可以用这个数字来表达

196
00:08:36,080 --> 00:08:38,080
我们可以用这个数字来表达

197
00:08:38,080 --> 00:08:40,080
我们可以用这个数字来表达

198
00:08:40,160 --> 00:08:42,960
去表示多了一个函数之后

199
00:08:42,960 --> 00:08:44,640
我们对y进行求导

200
00:08:44,640 --> 00:08:46,640
这个v就对y进行求导

201
00:08:46,640 --> 00:08:48,840
为什么要多这么一个函数呢

202
00:08:48,840 --> 00:08:51,240
因为我们实际上在机器学习里面

203
00:08:51,240 --> 00:08:52,800
我们会有一个损失函数

204
00:08:52,800 --> 00:08:54,800
就是最后一个多一个导数

205
00:08:54,800 --> 00:08:55,840
那如果它是1

206
00:08:55,840 --> 00:08:57,520
它是一个最后已经得到的数

207
00:08:57,520 --> 00:09:00,240
对它进行多一次求导是没有关系的

208
00:09:00,240 --> 00:09:04,400
我们现在来去看一下对l这个函数

209
00:09:04,400 --> 00:09:05,520
关于x1的导数

210
00:09:05,520 --> 00:09:08,000
也就是j乘以我的v

211
00:09:08,080 --> 00:09:10,960
所以我们叫做牙可比向量的g

212
00:09:10,960 --> 00:09:12,960
然后去得到输出

213
00:09:12,960 --> 00:09:15,360
对我的第1个输入x进行求导

214
00:09:15,360 --> 00:09:17,360
就我们的正向的表示了

215
00:09:18,160 --> 00:09:20,480
那反向的时候怎么办呢

216
00:09:20,480 --> 00:09:21,760
反向的时候

217
00:09:21,760 --> 00:09:25,280
我们叫做vector牙可比乘g

218
00:09:25,280 --> 00:09:27,600
这个反向反过来是不一样的

219
00:09:27,600 --> 00:09:30,560
因为我们反向反过来去算的时候

220
00:09:30,560 --> 00:09:32,240
这里面加了一个t

221
00:09:32,240 --> 00:09:34,160
反向加了一个t有什么不一样

222
00:09:34,160 --> 00:09:36,560
我们看一下最后一个乘g之后

223
00:09:36,560 --> 00:09:38,400
最后的一个l

224
00:09:38,400 --> 00:09:42,800
我们是对l对每一个输入都进行求导

225
00:09:42,800 --> 00:09:44,400
所以反向的时候

226
00:09:44,400 --> 00:09:48,720
我们就变成了vector jacobin production

227
00:09:50,240 --> 00:09:53,520
现在我们来看看正向跟反向模式之间的比较

228
00:09:53,520 --> 00:09:56,240
牙可比矩阵还是那个牙可比矩阵

229
00:09:56,240 --> 00:09:59,280
只是我每一次去计算的时候

230
00:09:59,280 --> 00:10:03,280
都是去算牙可比矩阵的不同的内容

231
00:10:03,280 --> 00:10:06,320
对于一个输入的时候特别多

232
00:10:06,320 --> 00:10:07,520
就我的输入非常多

233
00:10:07,520 --> 00:10:09,040
我的x有非常多

234
00:10:09,040 --> 00:10:13,040
那这个时候我们只需要进行一次反向叠代

235
00:10:13,040 --> 00:10:17,600
就可以算出牙可比矩阵的每一行了

236
00:10:17,600 --> 00:10:19,760
就我的x特别多的时候

237
00:10:19,760 --> 00:10:22,480
那如果我的x特别小的少的时候

238
00:10:22,480 --> 00:10:24,240
但是我的y特别多

239
00:10:24,240 --> 00:10:25,680
输出特别多的时候

240
00:10:25,680 --> 00:10:27,520
我通过一个正向的模式

241
00:10:27,520 --> 00:10:31,680
就可以算出牙可比矩阵的每一列了

242
00:10:31,680 --> 00:10:34,800
这个就是牙可比矩阵所表示的内容

243
00:10:34,960 --> 00:10:36,400
回到我们机器学习里面

244
00:10:36,400 --> 00:10:38,960
或者回到我们的自动微分里面

245
00:10:38,960 --> 00:10:40,160
有个最大的区别

246
00:10:40,160 --> 00:10:42,000
就是当我的m大于n

247
00:10:42,000 --> 00:10:44,960
我的输出大于我的输的时候

248
00:10:44,960 --> 00:10:46,560
适用于前向模式

249
00:10:46,560 --> 00:10:48,400
当我的n大于m

250
00:10:48,400 --> 00:10:51,760
就是我的输入绝对的大于我的输出

251
00:10:51,760 --> 00:10:54,160
那我们适合用反向自动微分

252
00:10:54,160 --> 00:10:58,400
回到跟我们的机器学习或者跟我们的神经网络相结合

253
00:10:58,400 --> 00:11:00,800
我们可以看到神经网络

254
00:11:00,800 --> 00:11:04,240
输入特别特别的多

255
00:11:04,560 --> 00:11:08,080
输入可能是几百万张图片或者几百万个神经元

256
00:11:08,080 --> 00:11:10,640
假设我的输入有非常多

257
00:11:10,640 --> 00:11:14,480
我的输出就是去猜我是你东梅

258
00:11:14,480 --> 00:11:16,160
我的输入有可能是李红梅

259
00:11:16,160 --> 00:11:16,720
李小梅

260
00:11:16,720 --> 00:11:17,520
李大梅

261
00:11:17,520 --> 00:11:19,200
李的神经网络里面

262
00:11:19,200 --> 00:11:21,520
输入绝对的大于输出

263
00:11:21,520 --> 00:11:24,160
所以我们更倾向于用反向模式

264
00:11:24,160 --> 00:11:29,040
一次过去算出我们输入不同x的一个值

265
00:11:29,040 --> 00:11:30,080
通过这种方式

266
00:11:30,080 --> 00:11:32,320
我们就可以迭代式的去求得

267
00:11:32,400 --> 00:11:34,880
我们牙科毕举正的每一行

268
00:11:34,880 --> 00:11:36,400
算得出来每一行

269
00:11:36,400 --> 00:11:39,680
就知道每一个神经元的导数

270
00:11:39,680 --> 00:11:40,880
有了每一个神经元

271
00:11:40,880 --> 00:11:44,080
我们后面就会介绍我算这个导数

272
00:11:44,080 --> 00:11:46,640
对我们的神经网络有什么用

273
00:11:46,640 --> 00:11:49,120
这个不会在自动微分里面去展开

274
00:11:49,120 --> 00:11:53,600
而是在我们的AI框架基础里面去展开

275
00:11:53,600 --> 00:11:54,480
所以不要急

276
00:11:54,480 --> 00:11:57,120
我们还是去看看我们的数学概念

277
00:11:58,640 --> 00:12:01,680
自动微分就是通过电视求导法则

278
00:12:01,760 --> 00:12:05,280
把所有的最简单的计算组合起来

279
00:12:05,280 --> 00:12:07,360
最后我们通过牙科毕举正

280
00:12:07,360 --> 00:12:09,600
去对我们的数学进行表示

281
00:12:09,600 --> 00:12:12,560
那优势就是数值的精度非常高

282
00:12:12,560 --> 00:12:16,560
因为它是真正的通过我们的电视求导法则去计算出来的

283
00:12:16,560 --> 00:12:20,000
另外一个优势就是没有表达式膨胀

284
00:12:20,000 --> 00:12:21,920
因为它不会不断的去表达

285
00:12:21,920 --> 00:12:24,960
它不会无限制的去膨胀我们的公式

286
00:12:24,960 --> 00:12:30,080
那缺点就是我需要去存储很多中间变量

287
00:12:30,160 --> 00:12:33,520
这里面每条公式我们都要存起来

288
00:12:33,520 --> 00:12:35,680
每个V0、V2、V1

289
00:12:35,680 --> 00:12:37,760
每个中间变量都要存起来

290
00:12:37,760 --> 00:12:42,960
存起来方便上一次或者下一次调用的时候去使用

291
00:12:42,960 --> 00:12:48,000
所以它的坏处就是需要很多中间变量去求导结果

292
00:12:48,000 --> 00:12:51,840
那这就会导致占用大量的计算机内存

293
00:12:53,120 --> 00:12:57,120
这个就是我们自动微分的一个基本概念

294
00:12:57,120 --> 00:12:58,640
可能这里面有点难

295
00:12:58,640 --> 00:13:01,440
今天的课稍微长了一点点

296
00:13:01,440 --> 00:13:03,440
今天我们来总结一下

297
00:13:03,440 --> 00:13:05,040
我们学会了自动微分

298
00:13:05,040 --> 00:13:08,640
实际上是分成前向微分和反向微分

299
00:13:08,640 --> 00:13:11,680
我们还了解了亚克比矩阵的基本原理

300
00:13:11,680 --> 00:13:14,400
前向和反向的亚克比的表示

301
00:13:14,400 --> 00:13:16,800
我们还了解了自动微分的优缺点

302
00:13:16,800 --> 00:13:19,600
还有在AI框架最常用的模式

303
00:13:19,600 --> 00:13:21,360
好了

304
00:13:21,360 --> 00:13:24,000
欢乐的时光过得特别快

305
00:13:24,000 --> 00:13:26,160
又是时候说拜拜了

