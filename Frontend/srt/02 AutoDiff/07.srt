1
00:00:00,000 --> 00:00:06,720
Hello,大家好,我是多米

2
00:00:06,720 --> 00:00:09,520
今天又是一节没什么人来观看

3
00:00:09,520 --> 00:00:13,320
但是我依然在坚持的一节分享课里面

4
00:00:13,320 --> 00:00:18,320
今天主要是给大家去聊一聊自动微分的一个挑战和未来

5
00:00:18,320 --> 00:00:22,880
自动微分这个话题其实我们已经过了非常多的内容了

6
00:00:22,880 --> 00:00:26,080
包括从自动微分的一个基础的概念

7
00:00:26,080 --> 00:00:27,840
那有哪几种微分呢?

8
00:00:27,840 --> 00:00:31,520
在计算机表达里面就有三种的微分模式

9
00:00:31,520 --> 00:00:35,960
然后实际上模式又有前向微分和后向微分

10
00:00:35,960 --> 00:00:40,760
具体的实现过程又分别有表达式图、操作幅重载

11
00:00:40,760 --> 00:00:42,520
还有源码转换

12
00:00:42,520 --> 00:00:45,040
那在具体的实现或者代码实现

13
00:00:45,040 --> 00:00:48,040
其实我们用了操作幅重载的这种方式

14
00:00:48,040 --> 00:00:51,240
去表达了正向和后向怎么实现的

15
00:00:51,240 --> 00:00:54,920
既然讲完所有关于自动微分的具体实现

16
00:00:55,040 --> 00:00:56,480
面向这些

17
00:00:57,240 --> 00:00:59,600
面向自动微分的实现

18
00:00:59,600 --> 00:01:03,080
其实我们还有很多挑战和未来可以聊一聊的

19
00:01:03,360 --> 00:01:06,600
首先第一个最大的就是易用性问题

20
00:01:07,200 --> 00:01:11,840
我们可以看到左边这个就是一些数学的表示

21
00:01:11,840 --> 00:01:15,520
我们需要对数学的表示通过计算机系统

22
00:01:15,520 --> 00:01:17,880
变成我们可编程的语言

23
00:01:17,880 --> 00:01:19,520
就是右边的这一坨

24
00:01:19,800 --> 00:01:24,040
但是我们看到左边的这个左边的是理想的情况

25
00:01:24,040 --> 00:01:27,240
理想的情况就是对数学进行表达

26
00:01:27,240 --> 00:01:31,880
对数学的公式进行分解微分组合的这么一个过程

27
00:01:32,200 --> 00:01:33,720
但是右边我们可以看到

28
00:01:33,720 --> 00:01:36,240
其实我们已经把数学过程变成一个便利

29
00:01:36,240 --> 00:01:37,200
变成一个for

30
00:01:37,200 --> 00:01:39,840
变成一个实际的程序表达

31
00:01:39,840 --> 00:01:43,240
所以自动微分系统或者AI框架

32
00:01:43,240 --> 00:01:46,160
实际上并不是对数学进行表达

33
00:01:46,160 --> 00:01:50,120
而是对程序或者我们的代码进行表达

34
00:01:50,160 --> 00:01:54,200
这个时候就会引起大量的编程的易用性问题

35
00:01:54,200 --> 00:01:57,400
或者程序员编写代码习惯性的问题

36
00:01:59,600 --> 00:02:01,680
说到编程的习惯问题

37
00:02:02,160 --> 00:02:05,120
下面我们就继续详细的展开一下

38
00:02:05,120 --> 00:02:07,560
第一个就是控制流表达

39
00:02:07,560 --> 00:02:10,520
这个是作为我们易用性的第一条

40
00:02:10,520 --> 00:02:12,480
可以看到左边的公式

41
00:02:12,640 --> 00:02:14,680
还是我们刚才熟悉的味道

42
00:02:15,800 --> 00:02:18,000
分别是ln等于4的ln

43
00:02:18,040 --> 00:02:20,560
就是做一个自闭包的一个函数

44
00:02:20,560 --> 00:02:23,840
那右边实际上我们对程序进行表达

45
00:02:24,200 --> 00:02:26,320
变成一个for i等于1

46
00:02:26,320 --> 00:02:30,160
to 3这么一个简单的三次循环

47
00:02:30,160 --> 00:02:32,400
然后再把v返回出来

48
00:02:32,760 --> 00:02:36,960
这个时候AI框架就要求程序去识别

49
00:02:36,960 --> 00:02:41,600
除了if else while这些控制流部分的程序

50
00:02:41,800 --> 00:02:45,000
然后将其排除在微分的过程当中

51
00:02:45,000 --> 00:02:46,640
才能进行很好的微分

52
00:02:46,640 --> 00:02:47,160
对吧

53
00:02:47,160 --> 00:02:49,360
所以控制流进行表达的时候

54
00:02:49,360 --> 00:02:51,920
就会引起各种副作用的问题

55
00:02:54,240 --> 00:02:58,320
第二点就是我们的数据类型非常复杂

56
00:02:58,480 --> 00:03:00,080
在计算机系统里面

57
00:03:00,360 --> 00:03:03,000
除了我们经常看到的数据类型

58
00:03:03,000 --> 00:03:06,040
int float double这些数据类型以外

59
00:03:06,040 --> 00:03:06,840
还有true

60
00:03:09,000 --> 00:03:11,920
第二点就是复杂的数据类型

61
00:03:12,280 --> 00:03:14,480
在我们实际语言表达的时候

62
00:03:14,720 --> 00:03:15,800
我们还有Bertz

63
00:03:15,800 --> 00:03:16,480
还有List

64
00:03:16,480 --> 00:03:17,600
Emil tuple

65
00:03:17,600 --> 00:03:18,080
Dist

66
00:03:18,360 --> 00:03:19,120
还有list

67
00:03:19,120 --> 00:03:22,640
很多不同的一些复杂的数据结构

68
00:03:22,800 --> 00:03:25,480
如何对这些数据结构进行微分呢

69
00:03:25,920 --> 00:03:27,840
我们在前面的章节里面

70
00:03:27,840 --> 00:03:30,200
根本没有去介绍面向不同

71
00:03:30,200 --> 00:03:32,600
复杂的数据类型进行微分

72
00:03:32,720 --> 00:03:34,760
我们只是把它当成一个数值

73
00:03:34,760 --> 00:03:36,600
然后对数值或者对我们的

74
00:03:36,600 --> 00:03:38,880
基本的数学公式进行微分

75
00:03:39,000 --> 00:03:41,360
所以我们同样AI框架

76
00:03:41,560 --> 00:03:44,240
还要排除掉复杂的数据类型

77
00:03:44,280 --> 00:03:47,760
然后对最原始的数据表达进行微分

78
00:03:48,880 --> 00:03:51,480
这就是应用性引起的第二个问题

79
00:03:51,480 --> 00:03:52,840
复杂数据类型

80
00:03:54,520 --> 00:03:57,840
第三个应用性问题就是语言特性了

81
00:03:57,960 --> 00:03:59,200
我们刚才也

82
00:03:59,280 --> 00:04:01,280
我们在上一节里面也说了

83
00:04:01,280 --> 00:04:02,760
实现自动微分

84
00:04:02,800 --> 00:04:05,360
大部分现在最常用的一种方式

85
00:04:05,360 --> 00:04:07,040
就是操作服从在

86
00:04:07,080 --> 00:04:08,920
利用语言的高级特性

87
00:04:09,240 --> 00:04:11,600
既然利用到语言的高级特性

88
00:04:11,640 --> 00:04:12,920
语言的多态性

89
00:04:12,920 --> 00:04:13,960
异常处理

90
00:04:14,280 --> 00:04:16,120
try watch error

91
00:04:16,120 --> 00:04:17,240
这些报错的

92
00:04:17,280 --> 00:04:18,920
还有我的调试的手段

93
00:04:18,960 --> 00:04:21,240
还有我的一些IO的处理

94
00:04:21,280 --> 00:04:23,200
还有程序里面的继承

95
00:04:23,400 --> 00:04:25,560
这一些高级的语言特性

96
00:04:25,880 --> 00:04:28,000
同样没办法被我们的

97
00:04:28,640 --> 00:04:30,840
自动微分系统所识别

98
00:04:30,920 --> 00:04:34,520
这个时候就引起了很多高级语言的特性

99
00:04:34,560 --> 00:04:36,080
在我的AI框架里面

100
00:04:36,120 --> 00:04:38,160
没办法去使用的问题

101
00:04:40,000 --> 00:04:42,480
第四点就是需求重写

102
00:04:42,480 --> 00:04:44,480
我们现在用到Pytorch

103
00:04:44,480 --> 00:04:45,120
Tensorflow

104
00:04:45,120 --> 00:04:45,760
MathSport

105
00:04:45,760 --> 00:04:46,920
这些AI框架

106
00:04:46,920 --> 00:04:48,240
其实用的更多

107
00:04:48,280 --> 00:04:51,120
是聚焦于CVNLP通用领域

108
00:04:51,920 --> 00:04:54,480
但是如果我要利用到HPC的

109
00:04:54,480 --> 00:04:55,280
模拟模拟

110
00:04:55,280 --> 00:04:56,440
我还是用这些框架

111
00:04:56,680 --> 00:04:57,360
不一定

112
00:04:57,600 --> 00:04:58,960
不同受众群体

113
00:04:58,960 --> 00:05:01,520
它对应用性的定义是不一样的

114
00:05:01,560 --> 00:05:03,320
例如我做游戏开发的时候

115
00:05:03,320 --> 00:05:05,640
可能用习惯的是utility

116
00:05:05,760 --> 00:05:07,440
还有Halo的系统

117
00:05:07,720 --> 00:05:08,280
这个时候

118
00:05:08,280 --> 00:05:10,280
如果你让我去用AI框架

119
00:05:10,280 --> 00:05:11,560
可编程的方式

120
00:05:11,560 --> 00:05:13,400
去做游戏引擎的开发

121
00:05:13,400 --> 00:05:14,600
去做物理仿真

122
00:05:14,600 --> 00:05:16,120
或者光线追踪

123
00:05:16,440 --> 00:05:18,920
可能用Pytorch就不是很适合了

124
00:05:18,960 --> 00:05:21,120
这个时候就要求我们去做一个

125
00:05:21,120 --> 00:05:22,240
需求重写

126
00:05:22,280 --> 00:05:24,280
特还有一些气候模拟

127
00:05:24,280 --> 00:05:26,320
具有DSL属性的一些

128
00:05:26,360 --> 00:05:28,600
特殊领域的子系统里面

129
00:05:28,640 --> 00:05:30,920
使用现在的AI框架

130
00:05:30,920 --> 00:05:32,440
就非常不合适了

131
00:05:33,680 --> 00:05:34,840
关于自动位分

132
00:05:34,840 --> 00:05:36,760
第二个比较大的问题

133
00:05:36,760 --> 00:05:38,440
就是性能问题

134
00:05:38,480 --> 00:05:41,000
性能也是除了应用性以外

135
00:05:41,000 --> 00:05:42,800
用户比较关心的

136
00:05:42,840 --> 00:05:45,440
我们现在在打市场的时候

137
00:05:45,440 --> 00:05:46,280
说实话

138
00:05:46,320 --> 00:05:49,400
应用性实际上真的很难去比你

139
00:05:49,400 --> 00:05:51,560
Pytorch加英伟达

140
00:05:51,600 --> 00:05:53,240
但是我们做性能

141
00:05:53,240 --> 00:05:55,760
或者国内一些芯片厂商

142
00:05:55,800 --> 00:05:58,440
它最好的发力点就是性能

143
00:05:58,480 --> 00:06:01,360
如何在性能优先的前提下去

144
00:06:01,400 --> 00:06:04,520
保证我们的应用性也能跟得上

145
00:06:04,560 --> 00:06:06,760
这是一个很大的商业挑战

146
00:06:06,760 --> 00:06:08,240
和技术挑战的问题

147
00:06:08,280 --> 00:06:09,920
现在我们有一条公式

148
00:06:09,920 --> 00:06:13,240
第一条公式是F等于X的三次方

149
00:06:13,280 --> 00:06:14,120
第二条公式

150
00:06:14,280 --> 00:06:16,720
就是对第一条公式进行求导

151
00:06:16,760 --> 00:06:19,840
DS等于3乘以S的平方

152
00:06:19,880 --> 00:06:23,120
现在我们简单的把这两条公式

153
00:06:23,160 --> 00:06:26,080
变成一个计算机可识别的公式

154
00:06:28,040 --> 00:06:29,800
这个时候第一条公式

155
00:06:29,800 --> 00:06:31,400
我假设有个中间变量

156
00:06:31,400 --> 00:06:33,960
T等于X乘以X再乘以X

157
00:06:34,000 --> 00:06:35,920
你要把T返回出来

158
00:06:35,960 --> 00:06:38,720
这个对应我们上面的第一条公式

159
00:06:39,000 --> 00:06:39,680
第二条公式

160
00:06:39,920 --> 00:06:41,760
就是对我们上一个function

161
00:06:41,800 --> 00:06:42,920
进行一个求导的

162
00:06:42,920 --> 00:06:44,040
我们叫D function

163
00:06:44,080 --> 00:06:45,840
里面就变成DS等于3

164
00:06:45,840 --> 00:06:47,160
乘以S

165
00:06:47,200 --> 00:06:49,120
然后we turn DS出来

166
00:06:49,160 --> 00:06:51,640
但实际上这只是我们程序的

167
00:06:51,640 --> 00:06:52,960
基本的表达

168
00:06:53,000 --> 00:06:54,760
如果我们要做性能优化

169
00:06:54,800 --> 00:06:56,440
我做一个三次的乘

170
00:06:56,440 --> 00:06:58,200
然后做两次的乘

171
00:06:58,520 --> 00:07:00,480
我每一次都要去计算

172
00:07:00,480 --> 00:07:01,480
X乘以X

173
00:07:01,480 --> 00:07:03,000
也就是X的平方

174
00:07:03,040 --> 00:07:05,240
能不能把X的平方提取出来

175
00:07:05,640 --> 00:07:07,920
我们想是有个中间变量

176
00:07:07,920 --> 00:07:08,720
等于T

177
00:07:08,720 --> 00:07:10,720
这个就是X的平方

178
00:07:10,960 --> 00:07:12,520
接着我对正向的输出

179
00:07:12,680 --> 00:07:14,240
就等于X乘以T

180
00:07:14,280 --> 00:07:15,640
我对反向的输出

181
00:07:15,800 --> 00:07:17,040
就是3乘以T

182
00:07:17,040 --> 00:07:18,960
这个时候就可以极大的

183
00:07:18,960 --> 00:07:21,400
去减少计算机的运算量

184
00:07:21,440 --> 00:07:23,040
可能单单看这么一条

185
00:07:23,040 --> 00:07:24,680
简单的公式没多少

186
00:07:24,720 --> 00:07:26,880
但是等我们整个系统搭起来

187
00:07:26,880 --> 00:07:29,000
等我们整个神经网络搭起来

188
00:07:29,240 --> 00:07:31,120
这种中间变量的计算

189
00:07:31,120 --> 00:07:32,160
会非常的多

190
00:07:32,720 --> 00:07:35,560
就会引起一系列的性能问题

191
00:07:35,840 --> 00:07:37,320
下面我们来看看

192
00:07:37,320 --> 00:07:39,760
性能损耗的一个具体的例子

193
00:07:39,760 --> 00:07:42,000
特别是产生非常多的

194
00:07:42,040 --> 00:07:44,080
额外的中间变量

195
00:07:44,320 --> 00:07:46,960
这个VI就是代表我对求导

196
00:07:47,040 --> 00:07:49,960
我对每个中间变量的导数

197
00:07:49,960 --> 00:07:50,680
可以看到

198
00:07:51,000 --> 00:07:52,640
在反向模式里面

199
00:07:52,640 --> 00:07:54,000
进行求导的时候

200
00:07:54,000 --> 00:07:56,800
里面会引申大量的中间变量

201
00:07:56,840 --> 00:07:59,400
我的V4 V3的导数

202
00:07:59,400 --> 00:08:01,480
同样都会用到V5的导数

203
00:08:01,480 --> 00:08:03,520
我的V1 V2可能会用到

204
00:08:03,520 --> 00:08:04,880
我的V4的导数

205
00:08:04,880 --> 00:08:06,840
这个时候大量的中间变量

206
00:08:06,840 --> 00:08:07,960
都要存起来

207
00:08:07,960 --> 00:08:09,000
不能销毁

208
00:08:09,040 --> 00:08:10,200
如果一旦销毁

209
00:08:10,200 --> 00:08:11,920
我就要重新计算了

210
00:08:11,960 --> 00:08:13,680
这个时候中间变量

211
00:08:13,680 --> 00:08:15,760
就会影响我们的性能

212
00:08:15,760 --> 00:08:17,880
我们到底是要存储更多

213
00:08:17,880 --> 00:08:19,560
还是要计算更多呢

214
00:08:20,320 --> 00:08:23,320
假设我们现在有一个二阶微分方程

215
00:08:23,360 --> 00:08:25,080
其中x是自变量

216
00:08:25,080 --> 00:08:26,680
y是位置函数

217
00:08:26,680 --> 00:08:28,760
我对y进行二次求导

218
00:08:28,760 --> 00:08:29,440
可以看到

219
00:08:29,760 --> 00:08:33,160
我首先需要求一个dx对s的一阶导

220
00:08:33,200 --> 00:08:35,800
接着求dy对xdx的导数

221
00:08:35,840 --> 00:08:38,920
然后再求最终的dy对dy的

222
00:08:38,920 --> 00:08:40,240
一个第二次导

223
00:08:40,280 --> 00:08:41,960
这个时候每一次

224
00:08:41,960 --> 00:08:45,600
我都需要去利用最小的中间变量

225
00:08:45,640 --> 00:08:46,960
然后一次又一次

226
00:08:46,960 --> 00:08:49,000
产生更多的中间变量

227
00:08:49,040 --> 00:08:50,440
当我的系统一大

228
00:08:50,440 --> 00:08:51,960
当我的模型一大

229
00:08:52,000 --> 00:08:54,200
大量的中间变量就引起了

230
00:08:54,720 --> 00:08:57,720
这个时候针对性能优化问题

231
00:08:57,720 --> 00:09:00,560
又衍生了一些新的研究

232
00:09:00,600 --> 00:09:03,840
就是重计算和边缘优化问题

233
00:09:03,880 --> 00:09:06,400
下面这个图就非常好的反映了

234
00:09:06,400 --> 00:09:08,320
重计算和边缘优化的问题了

235
00:09:08,360 --> 00:09:10,520
左边向右的这个箭头

236
00:09:10,680 --> 00:09:12,720
就代表我们正向的计算

237
00:09:12,760 --> 00:09:14,280
右边的这个箭头

238
00:09:14,280 --> 00:09:15,840
这个小箭头右边的

239
00:09:15,840 --> 00:09:19,240
就代表我们反向模式去计算的

240
00:09:19,240 --> 00:09:21,000
中间的白色圆圈

241
00:09:21,480 --> 00:09:22,880
白色的这些圆圈

242
00:09:23,200 --> 00:09:25,720
代表的就是中间变量

243
00:09:25,720 --> 00:09:28,160
可以看到我们第一种模式

244
00:09:28,360 --> 00:09:31,120
就是我往前做一个前向计算之后

245
00:09:31,160 --> 00:09:32,520
我往后的时候

246
00:09:32,560 --> 00:09:35,640
产生的小白圆圈比较少

247
00:09:35,680 --> 00:09:38,280
所以我每一次都会重新去计算

248
00:09:38,640 --> 00:09:40,800
这些中间变量存少一点

249
00:09:40,840 --> 00:09:43,480
每一次我都重新计算一把

250
00:09:43,520 --> 00:09:45,960
这个时候我就需要大量的计算了

251
00:09:46,000 --> 00:09:47,960
箭头越长越多

252
00:09:47,960 --> 00:09:50,800
就代表我们消耗的计算量越大

253
00:09:51,120 --> 00:09:52,800
可以看到右边的这个图

254
00:09:53,000 --> 00:09:54,440
就是每一次

255
00:09:54,440 --> 00:09:56,520
我都把中间变量

256
00:09:56,520 --> 00:09:58,280
这些小圆圈存起来

257
00:09:58,280 --> 00:10:00,160
用到的时候我直接去用

258
00:10:00,160 --> 00:10:01,160
所以反向的时候

259
00:10:01,160 --> 00:10:02,440
我非常方便

260
00:10:02,880 --> 00:10:04,680
减少我重计算的问题

261
00:10:04,680 --> 00:10:06,440
直接从内存来取了

262
00:10:06,720 --> 00:10:09,840
于是我们可能就需要用到编译器

263
00:10:09,880 --> 00:10:11,560
如何通过编译器

264
00:10:11,560 --> 00:10:12,920
自动的感知到

265
00:10:12,920 --> 00:10:15,120
我什么时候什么情况

266
00:10:15,120 --> 00:10:17,560
哪些参数需要进行优化

267
00:10:17,560 --> 00:10:19,160
哪些需要重计算

268
00:10:19,160 --> 00:10:21,840
然后在中间找到一个最好的gap

269
00:10:21,960 --> 00:10:25,080
使得我在比较优的计算量的情况下

270
00:10:25,080 --> 00:10:27,280
也比较优的内存的情况下

271
00:10:27,280 --> 00:10:29,840
能够使整个系统的效率最大化

272
00:10:29,840 --> 00:10:30,800
下面两个图

273
00:10:31,080 --> 00:10:32,560
就是通过编译器

274
00:10:32,560 --> 00:10:35,200
进行一个优化选择之后的一个图

275
00:10:35,200 --> 00:10:38,800
怎么样才能够让系统达到最优化

276
00:10:39,040 --> 00:10:41,040
这个就是性能问题

277
00:10:41,440 --> 00:10:43,200
刚才我们已经讲了

278
00:10:43,200 --> 00:10:45,680
AI框架面临的两大挑战

279
00:10:45,680 --> 00:10:47,000
第一个就是应用性

280
00:10:47,000 --> 00:10:48,840
第二个就是性能效率

281
00:10:48,840 --> 00:10:50,440
现在我们来看看

282
00:10:50,760 --> 00:10:54,000
自动微分AI框架的一个未来

283
00:10:54,200 --> 00:10:55,120
自动微分

284
00:10:55,880 --> 00:10:59,880
其实它已经变成了一个可谓变成的表达

285
00:10:59,880 --> 00:11:02,160
未来可能我们的程序

286
00:11:02,160 --> 00:11:03,960
或者我们写程序的时候

287
00:11:03,960 --> 00:11:07,240
非常多的应用到了微分的功能

288
00:11:07,560 --> 00:11:10,720
于是我们可能会把微分和语言设计

289
00:11:10,720 --> 00:11:11,360
编译器

290
00:11:11,360 --> 00:11:12,160
解析器

291
00:11:12,160 --> 00:11:15,080
甚至ID的工具深度的融合

292
00:11:15,080 --> 00:11:18,480
就把微分作为第一的语言特性

293
00:11:18,680 --> 00:11:20,960
有可能吹牛逼的

294
00:11:20,960 --> 00:11:22,240
或者我幻想的

295
00:11:22,240 --> 00:11:24,360
未来C++21

296
00:11:24,360 --> 00:11:26,920
或者C++更新的标准里面

297
00:11:27,160 --> 00:11:29,280
有没有可能可谓变成

298
00:11:29,280 --> 00:11:31,440
我们的第一语言特性呢

299
00:11:31,440 --> 00:11:33,800
或者类似于JAX一样

300
00:11:34,000 --> 00:11:36,920
未来我做一个微分编程的时候

301
00:11:36,920 --> 00:11:39,200
就像我写普通语言那么简单

302
00:11:41,160 --> 00:11:42,960
现在我们来总结一下

303
00:11:43,400 --> 00:11:44,600
自动微分的挑战

304
00:11:44,840 --> 00:11:47,920
主要是在应用性和性能两方面

305
00:11:48,200 --> 00:11:50,920
性用性我们就受制于控制流

306
00:11:50,920 --> 00:11:51,720
数据类型

307
00:11:51,720 --> 00:11:53,200
还有语言的高级特性

308
00:11:53,200 --> 00:11:56,160
另外还受制于领域的特殊需求

309
00:11:56,160 --> 00:11:58,520
所以应用性并不是那么好做的

310
00:11:58,520 --> 00:12:01,520
很多时候大家都说PyTorch非常好用

311
00:12:01,520 --> 00:12:03,480
是因为PyTorch它解决了

312
00:12:03,480 --> 00:12:06,000
应用性的某一类型的问题

313
00:12:06,000 --> 00:12:08,960
解决了大部分用户场景需求的问题

314
00:12:08,960 --> 00:12:12,040
但是仍然有小部分用户的需求没有解决

315
00:12:12,320 --> 00:12:14,720
这个时候可能又有另外一个框架

316
00:12:14,720 --> 00:12:15,720
例如Mathball

317
00:12:15,720 --> 00:12:17,680
JAX这种新的框架

318
00:12:17,800 --> 00:12:20,560
去解决特殊领域需求的问题

319
00:12:21,040 --> 00:12:22,600
第二点就是性能

320
00:12:22,600 --> 00:12:26,000
性能主要是程序表达和微分相结合的

321
00:12:26,000 --> 00:12:28,080
甚至需要利用到编译

322
00:12:28,080 --> 00:12:29,680
或者更高阶微分的时候

323
00:12:29,680 --> 00:12:33,040
引起各种各样的问题

324
00:12:33,040 --> 00:12:36,440
这个时候可能我们就需要引入整个系统

325
00:12:36,440 --> 00:12:38,600
去做一个性能的最优化

326
00:12:38,600 --> 00:12:42,280
我们没办法去保证某一点性能最优

327
00:12:42,280 --> 00:12:43,440
它就是最好的

328
00:12:43,440 --> 00:12:46,120
可能需要整个系统的一起去考虑

329
00:12:47,120 --> 00:12:47,600
好了

330
00:12:47,600 --> 00:12:49,400
自动微分的课程到此为止

331
00:12:49,400 --> 00:12:50,160
谢谢各位

332
00:12:50,160 --> 00:12:51,160
摆了个拜

333
00:12:52,440 --> 00:12:53,240
卷的不行了

334
00:12:53,240 --> 00:12:54,080
卷的不行了

335
00:12:54,080 --> 00:12:55,880
记得一键三连加关注哦

336
00:12:55,880 --> 00:12:59,080
所有的内容都会开源在下面这条链接里面

337
00:12:59,480 --> 00:13:00,360
摆了个拜

