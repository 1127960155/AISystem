1
00:00:00,000 --> 00:00:11,520
嗨,大家好,我是周咪。今天我们来到分波士训练系列里面的大模型算法结构。

2
00:00:11,520 --> 00:00:18,400
那聊到大模型的算法结构,我们主要是去看看大模型算法的一个整体的发展。

3
00:00:18,400 --> 00:00:29,200
从没有到有,从小到大。然后呢,我想到那个耶来。接着我们来看看大模型算法的最重要的或者最著名的两个结构。

4
00:00:29,200 --> 00:00:34,240
第一个呢,就是我们现在基本上经常会用到的Transformer这个结构。

5
00:00:34,240 --> 00:00:39,520
那第二个呢,我们去看看能够让我们的网络模型上万一规模的MOE结构。

6
00:00:40,160 --> 00:00:44,000
可以看到网络模型的规模越大,一台机器是放不下。

7
00:00:44,000 --> 00:00:48,240
所以面向大模型呢,我们要进行大规模分布式训练。

8
00:00:48,240 --> 00:00:52,080
但是呢,今天我们主要是聊聊大模型的一些结构。

9
00:00:52,160 --> 00:01:00,720
而在后面的分享里面呢,我们才去看看怎么把这些模型并行的切分到不同的机器上面去做一个训练的加速。

10
00:01:00,720 --> 00:01:06,720
从2011年到2022年,我们的模型的参数量其实是不断的增加的。

11
00:01:06,720 --> 00:01:13,920
而在2016年到2017年的时候,Transformer的出现呢,就形成了大模型红色的这条线。

12
00:01:13,920 --> 00:01:17,520
相当于这大模型里面我们已经有了一个断层。

13
00:01:17,520 --> 00:01:19,760
那大模型对我们来说意味着什么?

14
00:01:19,920 --> 00:01:23,440
大模型又能帮助我们解决哪些问题呢?

15
00:01:23,440 --> 00:01:25,440
我们看看左边的三个。

16
00:01:25,440 --> 00:01:28,480
首先呢,大模型需要大量的数据。

17
00:01:28,480 --> 00:01:32,800
既然数据量非常大,就不可能每个数据都进行标注。

18
00:01:32,800 --> 00:01:39,920
所以呢,大模型就有了一个很重要的工作,就是引入了自监督学习或者无监督学习的方法。

19
00:01:39,920 --> 00:01:44,160
第二个优点就是大模型,我们可以看到为什么要叫大。

20
00:01:44,160 --> 00:01:47,680
因为模型的参数量非常多,参数量多。

21
00:01:47,840 --> 00:01:51,840
网络模型的精度就有了进一步的突破。

22
00:01:51,840 --> 00:01:56,400
第三个就是大模型可以解决很多下游任务的问题。

23
00:01:56,400 --> 00:02:05,120
在Bird和Transformer出现之后,大模型就提供了预训链或者Cell swap的这种方式,解决了模型碎片化。

24
00:02:05,120 --> 00:02:10,960
我解决十几个NLP的任务,可能只需要用一个大模型就可以解决了。

25
00:02:10,960 --> 00:02:15,120
不需要开发十个大模型,每个大模型对应一个任务。

26
00:02:16,080 --> 00:02:21,520
下面我们看看,在分布式训练里面,我们要解决训练耗时的问题,

27
00:02:21,520 --> 00:02:25,600
可能会跟训练的规模还有单部的计算量相关。

28
00:02:25,600 --> 00:02:29,200
单部的计算量又跟我们的网络模型相关。

29
00:02:29,200 --> 00:02:32,240
这几天我看到一些非常派言听闻的标题,

30
00:02:32,240 --> 00:02:36,880
更具将于网络模型和设计和领域相关的网络模型。

31
00:02:36,880 --> 00:02:42,240
2022年10月24号,2022年10月24号的这些标题,我给大家去念一念。

32
00:02:42,720 --> 00:02:50,880
第一个就是谷歌Friend T5诞生了1800种语言,超大规模微调,1800种语言。

33
00:02:50,880 --> 00:03:01,680
第二个就是多语言图像描述,最强评估基准,XM3600来了,还应该36种语言,36种语言。

34
00:03:02,160 --> 00:03:07,440
第三个就是小扎亲自演示首个闽南语翻译系统,

35
00:03:07,440 --> 00:03:12,800
主攻3000多种无文字语言,3000多种无文字语言。

36
00:03:12,800 --> 00:03:18,400
现在的大模型都已经这么牛逼了吗?一下子就解决了这么多下游任务。

37
00:03:21,360 --> 00:03:27,120
于是在这一节分享里面,我想给大家去聊一聊大模型的整个结构的眼镜,

38
00:03:27,120 --> 00:03:28,800
是相关结构的眼镜。

39
00:03:28,800 --> 00:03:35,200
我们从一开始的Transformer取代了INN,让我们迈进了整个大模型的时代。

40
00:03:35,200 --> 00:03:42,880
其实有了Transformer之后,我们的模型的规模参数可能还停留在一个亿或者千万的级别上面。

41
00:03:42,880 --> 00:03:51,200
在2017年的时候,谷歌针对Hitten的MOE模型又提出了稀疏混合专家的结构,

42
00:03:51,200 --> 00:03:54,400
让我们的模型量可以进一步的突破了百亿。

43
00:03:54,560 --> 00:04:00,480
那像Bird网络模型就是首个稠密的突破十亿规模的NLP大模型,

44
00:04:00,480 --> 00:04:09,440
所以它对大模型的贡献是非常大的,直到GPT-3的出现一下子刷新了人们对大模型的认知。

45
00:04:09,440 --> 00:04:15,520
原来大模型还可以去到千亿规模,千亿规模的参数量是非常大的,

46
00:04:15,520 --> 00:04:20,960
可能一个GDP-3的网络模型的权重就已经快接近一个G了。

47
00:04:21,360 --> 00:04:26,000
在2021年的时候谷歌又发明了Switch Transformer,

48
00:04:26,000 --> 00:04:30,640
这个Switch Transformer已经突破了首个万亿的大模型,

49
00:04:30,640 --> 00:04:37,440
刚才我们聊到了GTP-3还是千亿,千亿之后又来到了万亿的大模型,

50
00:04:37,440 --> 00:04:41,360
这个还是很牛逼的,后来又有了GLAM,

51
00:04:42,240 --> 00:04:48,640
在保持相同万亿规模的参数量的时候,谷歌推出了GLAM网络模型,

52
00:04:49,040 --> 00:04:52,800
让大规模语言模型进一步提升它的精度,

53
00:04:52,800 --> 00:04:56,480
下面我们谷歌来展开一下我们的大模型的结构,

54
00:04:56,480 --> 00:05:00,640
大模型的参数量是怎么一步一步往上走的。

55
00:05:05,920 --> 00:05:10,400
Transformer这篇文章原文叫做Attention is all you need,

56
00:05:10,400 --> 00:05:13,760
就是你只需要注意力机制就行了。

57
00:05:13,760 --> 00:05:18,560
这篇文章更多的是一个设立和公式讲的没那么详细,

58
00:05:18,560 --> 00:05:22,480
后面我希望用一个简单的图去给大家讲示的,

59
00:05:22,480 --> 00:05:25,600
我们现在来粗略的浏览一下这篇文章,

60
00:05:25,600 --> 00:05:29,760
首先文章里面这个就代表我们Transformer的网络结构,

61
00:05:29,760 --> 00:05:32,880
网络模型从一开始的Input in Bending,

62
00:05:32,880 --> 00:05:34,960
然后再加一个Position in Cord,

63
00:05:34,960 --> 00:05:37,040
然后输给我们的网络模型,

64
00:05:37,040 --> 00:05:40,800
输进去的时候第一个会遇到Multi-head Attention,

65
00:05:40,800 --> 00:05:44,480
就是多头的注意力,接着有一个Normalize,

66
00:05:44,480 --> 00:05:46,240
然后再做一个Fit Forward,

67
00:05:46,240 --> 00:05:49,120
然后就传给我们的Incorder成,

68
00:05:49,120 --> 00:05:51,440
我们可以理解为左边的是Incorder,

69
00:05:51,440 --> 00:05:54,000
右边的是Decoder的这种模式。

70
00:05:54,000 --> 00:05:58,720
3.2节开始就去讲讲Attention的机制,

71
00:05:58,720 --> 00:06:02,880
这里面就通过QKV去实现我们Attention的机制,

72
00:06:02,880 --> 00:06:06,560
3.3就去讲讲Fit Forward到底是个什么东西,

73
00:06:06,560 --> 00:06:11,440
最后可能3.4,3.5都是去拼接我们整个网络模型的。

74
00:06:11,520 --> 00:06:16,720
第五节的内容就去讲讲Transformer的这个结构具体是怎么去训练,

75
00:06:16,720 --> 00:06:21,280
到了第六节的内容就开始真正的实验的环节部分,

76
00:06:21,280 --> 00:06:24,400
针对不同的层,不同的结构,不同的入餐,

77
00:06:24,400 --> 00:06:28,080
作者都做了大量的应用实践的对比。

78
00:06:28,080 --> 00:06:30,640
因为文章是在2017年,

79
00:06:30,640 --> 00:06:34,720
那个时候NLP下游用户不是说非常的丰富,

80
00:06:34,720 --> 00:06:40,880
所以作者就用了两个简单的评价指标去评估Transformer的网络模型到底好还是不好。

81
00:06:40,960 --> 00:06:45,360
现在回到我们的slide里面,刚才那个图我已经简单的讲了一讲,

82
00:06:45,360 --> 00:06:50,960
现在把它打红的去看,Transformer的结构其实最重要的就是我们Attention机制,

83
00:06:50,960 --> 00:06:54,640
就是这个Attention机制,所有东西都离不开Attention。

84
00:06:54,640 --> 00:07:00,320
Fit Forward这个更像于FF层,我们简单的称它为潜贵神经网络就可以了,

85
00:07:00,320 --> 00:07:03,360
像这种先信的SoftMesh都是原有的,

86
00:07:03,360 --> 00:07:05,760
而最重要的就是我们Attention。

87
00:07:06,240 --> 00:07:13,440
实际上Transformer这个网络模型刚才只是其中一个Incoder或者Decoder一个具体的展开形态,

88
00:07:13,440 --> 00:07:18,880
那它的网络模型里面可能会有7层Incoder,然后再加7层Decoder,

89
00:07:18,880 --> 00:07:23,280
最后输入可能是中文,输出可能是英文的翻译,

90
00:07:23,280 --> 00:07:26,880
这么一种方式去组成我们Transformer的结构。

91
00:07:26,880 --> 00:07:29,040
在17年提出Transformer的时候,

92
00:07:29,040 --> 00:07:36,960
它的目的是取代INN和LSDM去解决梯度爆炸、梯度消失还长序列的问题,

93
00:07:36,960 --> 00:07:39,920
那个时候并没有出现预训练模型,

94
00:07:39,920 --> 00:07:46,320
所以网络模型的输入和输出训练的数据仍然还是使用自监督学习的方式,

95
00:07:46,320 --> 00:07:50,800
这就是我的输入和输出都是人工的进行校准对比标注过的。

96
00:07:50,800 --> 00:07:55,920
回到论文的图里面,左边的这个我们叫做Incoder,也就是编码的,

97
00:07:55,920 --> 00:07:58,400
右边我们叫做Decoder,反编码的,

98
00:07:58,400 --> 00:08:04,640
左边的这个假设输入的是中文,我们把它经过层层的Incoder之后得到一个项量,

99
00:08:04,640 --> 00:08:07,840
那这个项量又经过层层的Decoder之后,

100
00:08:07,840 --> 00:08:14,800
反解析成为英文,我们学习中间的参数使得我们的输入是中文,输出可以是英文。

101
00:08:14,800 --> 00:08:18,640
Attention模块最重要的就是QKV三个矩阵,

102
00:08:18,640 --> 00:08:21,840
假设我们现在的任务是查询项量Q,

103
00:08:21,840 --> 00:08:26,960
就是我们的这个Q,然后我们去计算Q跟各个K之间的相似度,

104
00:08:26,960 --> 00:08:33,120
就是我会计算Q1跟K1、K2、K3、K4之间的相似度,

105
00:08:33,120 --> 00:08:37,680
而每个相似度都有一个值,我们叫做V,就是value,

106
00:08:37,680 --> 00:08:45,840
所以这里面有QKV,然后Q2我们会查询它跟K1、K2、K3、K4之间的相似度,

107
00:08:46,240 --> 00:08:54,880
得到每个Key对应value的权重系数,然后对value进行加全求和,得到最终Attention的值,

108
00:08:54,880 --> 00:08:58,640
所以我们就会有QKV三个矩阵三个权重项量。

109
00:08:58,640 --> 00:09:01,680
那下面我们来看看具体的一个形式,

110
00:09:01,680 --> 00:09:06,080
我们首先去计算Q跟K的一个相似度,

111
00:09:06,080 --> 00:09:12,960
然后去计算Q1跟K2的一个相似度A12,然后再计算成A13、A14,

112
00:09:13,600 --> 00:09:16,880
最后对我们的所有数据做一个softmax的处理,

113
00:09:16,880 --> 00:09:20,400
也就是在这一层里面进行一个softmax的计算,

114
00:09:20,400 --> 00:09:25,920
通过反向T度传播去学习QKV之间的一个映射的关系,

115
00:09:25,920 --> 00:09:29,600
那我们刚才聊的只是一个Attention的机制,

116
00:09:29,600 --> 00:09:34,560
实际上Transformer里面叫做MultiHeadAttention,就是多头注意力机制,

117
00:09:34,560 --> 00:09:37,600
假设我们现在的多头有两个Head,

118
00:09:37,600 --> 00:09:42,640
这个Head的数量是我们通过前面输进去网络模型之前的进行配置的,

119
00:09:42,640 --> 00:09:45,840
假设我两个Head,我可以把这两个做一个并列,

120
00:09:45,840 --> 00:09:48,480
实际上在系统里面为了加速我们的运算,

121
00:09:48,480 --> 00:09:53,200
可能我会把第一个Q分成两个Q,把K分成两个K,

122
00:09:53,200 --> 00:09:57,680
然后计算的公式和计算的逻辑都跟刚才所描述的一样,

123
00:09:57,680 --> 00:10:00,560
那我们就得到了两个Head的计算方式,

124
00:10:00,560 --> 00:10:06,560
通过这种方式我们可以更好的对Transformer这个算子进行并行的操作和并行的计算,

125
00:10:06,560 --> 00:10:09,200
使得我们执行的时候跑得更快,

126
00:10:09,520 --> 00:10:13,280
Transformer的出现其实是为了解决Sequence to Sequence的问题,

127
00:10:13,280 --> 00:10:18,400
然后用Attention就是注意力的结构去代替我们的LSTM,

128
00:10:18,400 --> 00:10:22,960
那使用这种网络模型的结构具体对我们带来有什么好处和结果呢?

129
00:10:22,960 --> 00:10:27,360
可以看到实际上刚才我们看到的只是一层Transformer的结构,

130
00:10:27,360 --> 00:10:29,520
每一层还有很多小算子,

131
00:10:29,520 --> 00:10:34,640
于是Transformer的结构就使得我们每一层的计算复杂度就变得更优,

132
00:10:35,040 --> 00:10:39,120
第二个就是我们不需要像LSTM一样有很多的Gate,

133
00:10:39,120 --> 00:10:41,760
有我们的输入门、移往门、输出门,

134
00:10:41,760 --> 00:10:44,560
直接用点层的结果去进行计算,

135
00:10:44,560 --> 00:10:48,240
就是我们的QKV可以通过矩阵层去进行计算,

136
00:10:48,240 --> 00:10:51,840
那第三个就是模型更具有解释性,

137
00:10:51,840 --> 00:10:55,680
因为在LSTM里面我们对长序列进行处理的时候,

138
00:10:55,680 --> 00:10:57,920
对网络模型的解析是很难的,

139
00:10:57,920 --> 00:11:02,240
网络模型在序列的传播当中丢失了非常多的信息,

140
00:11:02,480 --> 00:11:07,520
第四个就是解决了序列很长的时候所引发的一系列的问题,

141
00:11:07,520 --> 00:11:11,600
但是带来的就是我们的网络模型急剧的膨胀,

142
00:11:11,600 --> 00:11:17,520
当时候在17年的时候我们觉得网络模型的数量膨胀是个很严重的问题,

143
00:11:17,520 --> 00:11:19,600
就像出现了Westner50之后呢,

144
00:11:19,600 --> 00:11:22,240
人们希望出现一种Mobinet,

145
00:11:22,240 --> 00:11:25,920
就像出现了Westner50、Westner101这种网络模型,

146
00:11:25,920 --> 00:11:27,840
这种精度已经很好的网络模型,

147
00:11:27,840 --> 00:11:29,120
但是人们还不够,

148
00:11:29,120 --> 00:11:32,160
希望网络模型的参数量越小越好,

149
00:11:32,160 --> 00:11:34,720
于是谷歌就研究了Mobinet一样,

150
00:11:34,720 --> 00:11:37,280
当时候觉得网络模型的参数量大,

151
00:11:37,280 --> 00:11:38,320
不是个好事,

152
00:11:38,320 --> 00:11:40,240
大的话我怎么做推理啊?

153
00:11:40,240 --> 00:11:44,960
在大模型真正出现之前其实有两个电机的工作的,

154
00:11:44,960 --> 00:11:47,280
第一个就是我们刚才聊到的Transom,

155
00:11:47,280 --> 00:11:52,240
第二个就是2017年同年稍微晚一点的MOE,

156
00:11:52,240 --> 00:11:54,960
同年稍微晚一点的MOE,

157
00:11:54,960 --> 00:11:59,440
实际上大模型很多工作都是谷歌去发起的,

158
00:11:59,440 --> 00:12:02,080
虽然现在Tensor4很少人去用了,

159
00:12:02,240 --> 00:12:04,640
但是大模型很多相关的工作,

160
00:12:04,640 --> 00:12:06,160
谷歌都落在Tensor4了,

161
00:12:06,160 --> 00:12:10,800
所以很多时候我们可能可以去看看Tensor4这个框架是怎么设计的,

162
00:12:10,800 --> 00:12:13,680
并行的一些系统或者一些文章概念,

163
00:12:13,680 --> 00:12:15,920
还是非常有帮助于我们去理解,

164
00:12:15,920 --> 00:12:17,120
分布是并行的。

165
00:12:17,120 --> 00:12:23,840
那MOE这个网络模型结构其实是基于1990年Hitman提出的Mistral of Export,

166
00:12:23,840 --> 00:12:27,120
前面就加了一个定语叫做西书的门控,

167
00:12:27,120 --> 00:12:31,600
这篇文章实际上描述的一个西书门控的混合专家模型,

168
00:12:31,600 --> 00:12:34,160
但是我们大部分都会叫做MOE,

169
00:12:34,160 --> 00:12:37,280
就把西书Git这个定语先把它去掉。

170
00:12:37,280 --> 00:12:40,800
这篇文章一开始去介绍一些以前人的工作,

171
00:12:40,800 --> 00:12:45,360
然后再去看看我们的MOE的网络模型的结构具体长什么样子的,

172
00:12:45,360 --> 00:12:47,600
可以看到里面有非常多的ASPR,

173
00:12:47,600 --> 00:12:49,840
然后这里面有一个Git门控,

174
00:12:49,840 --> 00:12:54,640
去控制在什么应用场景,什么残盒,什么权重的情况下,

175
00:12:54,640 --> 00:12:57,040
我们去激活对应的专家。

176
00:12:57,040 --> 00:13:00,480
在2.1里面就具体的描述了我的输出,

177
00:13:00,960 --> 00:13:02,960
为了保证西书性和均衡性,

178
00:13:02,960 --> 00:13:05,680
这篇文章就对SoftMesh进行了一些改造,

179
00:13:05,680 --> 00:13:08,560
首先第一个改造就是NoiseTopKGating,

180
00:13:08,560 --> 00:13:13,920
Noise这个工作就是Standalize和WNoise加入了一个权重的噪声,

181
00:13:13,920 --> 00:13:16,960
使得我们的网络模型训练的时候更加均衡,

182
00:13:16,960 --> 00:13:19,680
而不是某个ASPR的权重特别大。

183
00:13:19,680 --> 00:13:21,920
某个专家真的是专家,

184
00:13:21,920 --> 00:13:24,480
现在很多时候我们在网上经常去吐槽,

185
00:13:24,480 --> 00:13:27,120
专家呀专家,求你不要再建议了,

186
00:13:27,120 --> 00:13:28,480
就是这个概念,

187
00:13:28,560 --> 00:13:31,520
我们要让专家跟专家之间更加均衡,

188
00:13:31,520 --> 00:13:35,040
而不是某个专家一直在发表一些错误的言论。

189
00:13:39,360 --> 00:13:40,880
实际上我们的专家非常多,

190
00:13:40,880 --> 00:13:44,080
我们不可能听所有专家的建议。

191
00:13:44,080 --> 00:13:46,800
第二个内容就是提出了TopKGating,

192
00:13:46,800 --> 00:13:49,440
就是我们这里面的函数KeepTopKate,

193
00:13:49,440 --> 00:13:52,880
我们保证网络模型学习一定量专家的建议,

194
00:13:52,880 --> 00:13:55,280
其他专家的建议在某种情况下,

195
00:13:55,280 --> 00:13:56,960
我们可以把它自为负无从,

196
00:13:57,040 --> 00:13:58,880
让他不要再去学习了。

197
00:13:58,880 --> 00:14:02,480
在网上就是一些相关的要注意的事项,

198
00:14:02,480 --> 00:14:05,360
特别是分布式并行和我们的BatchSize的试字。

199
00:14:05,360 --> 00:14:08,400
第四点就是回到我们刚才讲到的,

200
00:14:08,400 --> 00:14:10,880
去平衡各个专家之间的一个作用。

201
00:14:10,880 --> 00:14:13,680
到了第五节的内容就是具体的实验,

202
00:14:13,680 --> 00:14:16,960
可以看到M1Z里面做了非常大量的实验,

203
00:14:16,960 --> 00:14:19,680
比TransMod网络模型的实验会更多,

204
00:14:19,680 --> 00:14:22,160
而且对比了以前非常多的不同的

205
00:14:22,160 --> 00:14:24,880
关于吸收性或者专家性的相关的工作。

206
00:14:25,200 --> 00:14:28,000
这里面比较有意思的就是M1Z网络模型

207
00:14:28,000 --> 00:14:29,600
虽然看上去很简单,

208
00:14:29,600 --> 00:14:32,080
但是这篇文章里面又附带了很多

209
00:14:32,080 --> 00:14:34,320
附录去介绍具体的计算公式,

210
00:14:34,320 --> 00:14:36,160
具体的计算逻辑,

211
00:14:36,160 --> 00:14:38,880
并且附上了大量的消融实验。

212
00:14:38,880 --> 00:14:40,400
回到我们的PPT里面,

213
00:14:40,400 --> 00:14:43,760
实际上M1Z它叫做吸收门控专家混合模型,

214
00:14:43,760 --> 00:14:44,800
就是我们刚才讲的,

215
00:14:44,800 --> 00:14:47,600
我们去控制哪个专家在某种情况下

216
00:14:47,600 --> 00:14:48,960
应该怎么去计算。

217
00:14:48,960 --> 00:14:51,360
为了保证我们的吸收性和均衡性,

218
00:14:51,360 --> 00:14:53,920
对SoftMesh进行了一些处理,

219
00:14:53,920 --> 00:14:55,760
加入了TopK,加入了Noised,

220
00:14:55,760 --> 00:14:59,600
我们可以看到原来的网络模型是通过一个SoftMesh的,

221
00:14:59,600 --> 00:15:02,400
但是实际上我们是通过Gdelta,

222
00:15:02,400 --> 00:15:05,200
就Gdelta实际上就是下面这条公式,

223
00:15:05,200 --> 00:15:07,280
我们把KeytopK加进去了,

224
00:15:07,280 --> 00:15:09,520
而且还加入了我们的WNoised。

225
00:15:09,520 --> 00:15:12,240
MOE这个E算叫做专家,

226
00:15:12,240 --> 00:15:14,560
实际上我们可以把它理解为把大模型

227
00:15:14,560 --> 00:15:16,400
才分为多个小模型,

228
00:15:16,400 --> 00:15:18,960
每个小模型就是一个专家。

229
00:15:18,960 --> 00:15:21,200
对数位一个样本的数量来说,

230
00:15:21,280 --> 00:15:24,320
我们并不需要所有的专家都去计算,

231
00:15:24,320 --> 00:15:27,040
而是激活一部分的专家就可以了,

232
00:15:27,040 --> 00:15:29,760
这样就可以节赏我们大量的计算资源。

233
00:15:29,760 --> 00:15:31,920
而且我们有多个专家,

234
00:15:31,920 --> 00:15:34,960
不同的专家可以处理不同的效用任务,

235
00:15:34,960 --> 00:15:36,880
实现网络模型的增长,

236
00:15:36,880 --> 00:15:39,520
可以看到在MOE的网络模型实验里面,

237
00:15:39,520 --> 00:15:42,320
特别的去强调了我们的网络模型的增长,

238
00:15:42,320 --> 00:15:45,360
我们的网络模型的参数量不断的增加的情况下,

239
00:15:45,360 --> 00:15:48,400
实际上我们的GPU的计算量并不会很大,

240
00:15:48,400 --> 00:15:51,120
因为我们是通过小模型专家去进行计算的,

241
00:15:51,120 --> 00:15:53,600
而不是所有的专家都进行计算,

242
00:15:53,600 --> 00:15:56,400
这里面就是KeytopK所带来的好处。

