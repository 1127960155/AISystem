1
00:00:00,000 --> 00:00:06,800
哈喽大家好,我是钟鸣

2
00:00:06,800 --> 00:00:10,600
欢迎来到大模型与分布式训练这一个系列

3
00:00:10,600 --> 00:00:15,000
在这一系列里面我们现在来到了分布式训练系统

4
00:00:15,000 --> 00:00:18,120
在这一节里面我们将会讲两个东西

5
00:00:18,120 --> 00:00:22,400
一个就是并行处理硬件的一个架构

6
00:00:22,400 --> 00:00:27,080
第二个就是AI框架里面的分布式训练的功能

7
00:00:28,080 --> 00:00:29,480
今天晚上12点

8
00:00:29,640 --> 00:00:31,600
刚开完会今天晚上12点

9
00:00:31,600 --> 00:00:33,800
我终于又有时间过来更新了

10
00:00:34,680 --> 00:00:38,360
这里面第一个内容就是并行处理的硬件架构

11
00:00:38,960 --> 00:00:40,880
可以看到其实硬件架构

12
00:00:40,880 --> 00:00:43,240
我们从一开始的串型到并行

13
00:00:43,240 --> 00:00:45,400
再到大规模分布式并行

14
00:00:45,400 --> 00:00:47,480
中间是走了很多弯路

15
00:00:47,480 --> 00:00:50,560
也发展了很多并行处理的架构体系

16
00:00:50,560 --> 00:00:52,880
首先第一个就是SISD

17
00:00:52,880 --> 00:00:55,600
就是单子令单数据流

18
00:00:56,000 --> 00:00:57,800
S就是Single

19
00:00:57,800 --> 00:01:00,160
I就是Instruction

20
00:01:00,160 --> 00:01:01,880
X还是Single

21
00:01:01,880 --> 00:01:06,000
右边的图我们可以看到横坐标就是Data

22
00:01:06,000 --> 00:01:08,680
中坐标就是Instruction指令

23
00:01:08,680 --> 00:01:14,520
下面我们从前面单子令流单数据流了是SISD

24
00:01:14,520 --> 00:01:17,560
下面都是并行的一些硬件架构

25
00:01:17,560 --> 00:01:20,320
包括SIMD MISD MIMD

26
00:01:20,320 --> 00:01:23,440
下面我们逐个来打开看看它到底有什么不一样

27
00:01:24,440 --> 00:01:26,920
第一个是SISD系统

28
00:01:26,920 --> 00:01:28,280
简单的我们可以理解

29
00:01:28,280 --> 00:01:30,360
假设我们要做一个A1加B

30
00:01:30,360 --> 00:01:32,480
就是A加B的简单操作

31
00:01:32,480 --> 00:01:34,560
我的子令就是A加B

32
00:01:34,560 --> 00:01:36,560
我的数据就是A和B

33
00:01:36,560 --> 00:01:38,560
我们PU就是Processed Unit

34
00:01:38,560 --> 00:01:40,320
就是我们的处理单元

35
00:01:40,320 --> 00:01:42,680
把数据输给我们的处理单元

36
00:01:42,680 --> 00:01:44,200
把子令输给他

37
00:01:44,200 --> 00:01:46,520
处理单元就进行计算

38
00:01:46,520 --> 00:01:48,040
最后输出

39
00:01:48,040 --> 00:01:50,640
这种方式在每个时钟周期

40
00:01:50,680 --> 00:01:53,560
CPU只能处理一个数据流

41
00:01:53,560 --> 00:01:56,280
这种是最早期的CPU的一种方式

42
00:01:57,200 --> 00:01:59,600
随着我们处理的数据越来越多

43
00:01:59,600 --> 00:02:01,920
所以我们后面就发展了SIMD

44
00:02:01,920 --> 00:02:04,560
也就是单子令多数据流

45
00:02:04,560 --> 00:02:07,280
一个控制器控制多个处理器

46
00:02:07,280 --> 00:02:10,480
灰色的框就是我们的控制器

47
00:02:10,480 --> 00:02:12,520
中间的PU就是处理器

48
00:02:12,520 --> 00:02:16,280
可以看到这个控制器里面有三个处理器

49
00:02:16,280 --> 00:02:19,600
而每个处理器都处理不同的任务

50
00:02:19,640 --> 00:02:22,160
但是他们的子令都是相同的

51
00:02:22,160 --> 00:02:24,320
从相同的子令传下来

52
00:02:24,320 --> 00:02:25,720
处理不同的数据

53
00:02:25,720 --> 00:02:27,280
最后输出

54
00:02:27,280 --> 00:02:31,360
SIMD最主要是指令一些向量矩阵的运算

55
00:02:31,360 --> 00:02:35,800
所以说它非常适用于科学计算的场景

56
00:02:35,800 --> 00:02:38,760
处理的单元PU也可以不断的去增加

57
00:02:38,760 --> 00:02:43,400
但是处理的速度就受限于计算机的通讯带宽的约束

58
00:02:43,400 --> 00:02:46,440
也就是我的数据的传输的数率

59
00:02:46,440 --> 00:02:49,400
决定我整个控制器的吞吐力

60
00:02:49,400 --> 00:02:54,520
另外一种就是MISD多子令单数据流的这种硬件子令

61
00:02:54,520 --> 00:02:58,440
就是我可以通过多个子令去处理相同的数据

62
00:02:58,440 --> 00:03:01,640
这里面很有意思的就是数据还是A和B

63
00:03:01,640 --> 00:03:05,040
但是处理的流程就变成一个加一个减一个乘

64
00:03:05,040 --> 00:03:07,680
这种模型我之前是没有见过的

65
00:03:07,680 --> 00:03:14,080
而且我也没有想到过有任何一种计算方式是适用于这种的

66
00:03:14,080 --> 00:03:15,680
同学们能够想得到的

67
00:03:15,680 --> 00:03:19,240
非常欢迎你们在弹幕或者评论里面去留言

68
00:03:19,240 --> 00:03:21,120
反正我是没有见过的

69
00:03:21,120 --> 00:03:23,520
MISD作为理论模型出现

70
00:03:23,520 --> 00:03:26,120
并没有突兀到实际的应用当中

71
00:03:28,440 --> 00:03:30,600
最后一个就是MIMD

72
00:03:30,600 --> 00:03:32,600
MIMD这种方式我们经常见

73
00:03:32,600 --> 00:03:34,600
而且经常去使用

74
00:03:34,600 --> 00:03:40,160
也就是在多个数据集上面去执行多个子令流的一个处理器

75
00:03:40,160 --> 00:03:42,840
这种也是并行方式里面用的最多的

76
00:03:42,840 --> 00:03:44,680
我们以一个处理单元为例子

77
00:03:44,680 --> 00:03:46,920
里面有非常多的处理器

78
00:03:46,920 --> 00:03:48,120
有三个子令流

79
00:03:48,120 --> 00:03:50,000
每个处理器对应一个子令流

80
00:03:50,000 --> 00:03:52,000
然后都有很多的数据

81
00:03:52,000 --> 00:03:53,800
不同的数据传给他

82
00:03:53,800 --> 00:03:55,160
最后输出来

83
00:03:55,160 --> 00:03:57,200
这个就是MIMD的方式

84
00:03:57,200 --> 00:04:02,920
那MIMD又分为共享内存的MIMD和分布式内存的MIMD

85
00:04:02,920 --> 00:04:05,520
下面来看看两者有什么区别

86
00:04:05,520 --> 00:04:08,400
共享内存的MIMD更容易理解

87
00:04:08,400 --> 00:04:11,600
就是我们现在一个处理器上面有很多个核

88
00:04:11,600 --> 00:04:15,800
包括我们的英特尔的IC处理器里面可能有八个核

89
00:04:15,840 --> 00:04:19,200
中间通过内存总线进行数据的传输

90
00:04:19,200 --> 00:04:21,800
那另外一种就是分布式内存

91
00:04:21,800 --> 00:04:23,920
另外一个就是分布式内存

92
00:04:23,920 --> 00:04:26,040
假设这是我其中一个机器

93
00:04:26,040 --> 00:04:28,080
或者其中一个CPU

94
00:04:28,080 --> 00:04:30,680
那里面其中一个核CPU

95
00:04:30,680 --> 00:04:33,960
我可能每一个CPU都有自己的内存

96
00:04:33,960 --> 00:04:38,960
但是CPU跟CPU之间是通过进程通道进行通讯的

97
00:04:38,960 --> 00:04:41,360
这种叫做分布式内存

98
00:04:41,360 --> 00:04:45,240
应该在90年代末出现一种比较特殊的硬件

99
00:04:45,280 --> 00:04:48,560
提出了一种新的架构叫做SIMT

100
00:04:48,560 --> 00:04:51,320
那T就是Flat就是我们的线程

101
00:04:51,320 --> 00:04:55,520
这种方式是属于SIMD的另外一种扩展

102
00:04:55,520 --> 00:04:59,120
这个硬件最特殊的代表就是GPU

103
00:04:59,120 --> 00:05:02,680
那GPU就是通过单指令多线程

104
00:05:02,680 --> 00:05:06,120
有效的管理核执行多个单线程

105
00:05:06,120 --> 00:05:09,600
每一条指令的数据都是分开循指的

106
00:05:09,600 --> 00:05:12,280
而且用户可以控制每一个线程

107
00:05:12,280 --> 00:05:14,520
每个线程都有自己的逻辑

108
00:05:15,520 --> 00:05:17,840
这样非常方便用户去编程

109
00:05:17,840 --> 00:05:23,560
这种新的加固出现就衍生了我们现在深度学习

110
00:05:23,560 --> 00:05:27,600
经常会用到的GPU或者NPU的编程方式

111
00:05:27,600 --> 00:05:29,240
或者硬件体系结构

112
00:05:35,560 --> 00:05:38,520
我们硬件结构从串型发展到并行

113
00:05:38,520 --> 00:05:39,960
最后出现GPU

114
00:05:40,000 --> 00:05:43,880
还是为了让同一时间能够处理更多的数据

115
00:05:43,880 --> 00:05:47,680
那软件上我们又有了分布式训练系统

116
00:05:47,680 --> 00:05:50,720
这个分布式训练系统最主要的意义

117
00:05:51,800 --> 00:05:54,520
这个分布式训练系统最主要的意义就是

118
00:05:54,520 --> 00:05:57,920
为我们提供非常容易方便高效的

119
00:05:57,920 --> 00:06:00,680
对我们的大模型进行训练

120
00:06:00,680 --> 00:06:04,240
那我们可以看到分布式系统其实是分开三层的

121
00:06:04,240 --> 00:06:07,360
最上层面向用户的就是提供一些接口

122
00:06:07,400 --> 00:06:08,480
提供一些接口

123
00:06:08,480 --> 00:06:10,880
非常方便我们去写代码

124
00:06:10,880 --> 00:06:14,000
那中间就是单节点的一个训练

125
00:06:14,000 --> 00:06:16,600
单节点训练就是我们刚才所说到的

126
00:06:16,600 --> 00:06:18,240
针对每一个处理单元

127
00:06:18,240 --> 00:06:21,240
每个控制器如何执行训练

128
00:06:21,240 --> 00:06:24,200
那最后我们所有的整个分布式训练系统

129
00:06:24,200 --> 00:06:27,320
是通过通信的方式进行协调的

130
00:06:27,320 --> 00:06:28,120
每个节点

131
00:06:28,120 --> 00:06:30,520
每个控制器之间需要通讯

132
00:06:30,520 --> 00:06:31,800
告诉另外一个机器

133
00:06:31,800 --> 00:06:32,920
我算到哪了

134
00:06:32,920 --> 00:06:34,080
你算到哪了

135
00:06:34,080 --> 00:06:36,000
我们的数据应该怎么同步

136
00:06:36,000 --> 00:06:37,080
应该怎么协调

137
00:06:37,080 --> 00:06:38,760
通讯协调的内容

138
00:06:38,760 --> 00:06:41,520
我们将会在后面的章节去展开

139
00:06:42,920 --> 00:06:47,120
首先看看分布式训练系统有两种具体的方式

140
00:06:47,120 --> 00:06:50,400
那一种就是框架内里面去实现的

141
00:06:50,400 --> 00:06:54,720
另外一种是跨框架的一个通用的分布式训练系统

142
00:06:54,720 --> 00:06:59,400
具体我们用几个现在比较出名的AI框架去举例子

143
00:06:59,400 --> 00:07:02,200
第一个就是框架内嵌的分布式系统

144
00:07:02,240 --> 00:07:06,680
这些AI框架内部已经自带了分布式训练系统

145
00:07:06,680 --> 00:07:08,000
这些能力

146
00:07:08,000 --> 00:07:09,120
那另外第二个

147
00:07:09,120 --> 00:07:10,400
类似于Pytosh

148
00:07:10,400 --> 00:07:13,000
这种也是框架内嵌的

149
00:07:13,000 --> 00:07:17,000
Pytosh就提供了非常开放性的功能给用户去使用的

150
00:07:17,000 --> 00:07:20,000
那第三种类似于Hover或者DeepSpeed这种

151
00:07:20,000 --> 00:07:23,280
就是跨框架的一种通用的分布式训练系统

152
00:07:23,280 --> 00:07:26,480
就是它不一定完全依赖于某种框架

153
00:07:26,480 --> 00:07:29,320
它可以居于它自己去实现很多功能

154
00:07:29,360 --> 00:07:33,800
这种一般来说它是聚焦于某种并行的策略

155
00:07:33,800 --> 00:07:36,640
下面我们组个框架来打开看看

156
00:07:36,640 --> 00:07:39,760
具体在分布式训练系统里面有什么不一样

157
00:07:41,640 --> 00:07:45,040
可以看到Tensor4它有非常多的API

158
00:07:45,040 --> 00:07:46,320
首先它有Kias

159
00:07:46,320 --> 00:07:47,640
然后有自己的API

160
00:07:47,640 --> 00:07:50,160
还有Estimator的API

161
00:07:50,160 --> 00:07:54,320
而每一种API都有不同的分布式的一些策略

162
00:07:54,320 --> 00:07:56,360
或者分布式的接口

163
00:07:56,360 --> 00:07:58,440
几年前我去学Tensor4的时候

164
00:07:58,440 --> 00:08:01,920
就觉得Tensor4的API个列的实在太严重了

165
00:08:01,920 --> 00:08:04,680
而且学习成本非常高

166
00:08:04,680 --> 00:08:06,880
Tensor4在早期的版本里面

167
00:08:06,880 --> 00:08:09,240
其实已经加入了参数服务器

168
00:08:09,240 --> 00:08:11,960
Primeter Surface这种功能

169
00:08:11,960 --> 00:08:16,200
最主要的思路就是通过多个Walker独立的去执行

170
00:08:16,200 --> 00:08:19,720
然后分布式的共享我们的网络模型的参数

171
00:08:19,720 --> 00:08:21,960
下面我们来看看具体的代码

172
00:08:21,960 --> 00:08:24,760
Tensor4参数服务器的一个具体的使用

173
00:08:24,760 --> 00:08:27,640
首先我们要定义我们的Walker和PS

174
00:08:27,680 --> 00:08:29,480
就是指定节点的信息

175
00:08:29,480 --> 00:08:32,720
那Walker就包括我们的网络模型的信息

176
00:08:32,720 --> 00:08:35,320
Walker主要是包括我们的网络模型

177
00:08:35,320 --> 00:08:39,040
而PS主要是对我们的参数进行同步的

178
00:08:39,040 --> 00:08:40,320
所以我们执行的时候

179
00:08:40,320 --> 00:08:41,960
有两个角色需要定义的

180
00:08:41,960 --> 00:08:42,920
一个就是PS

181
00:08:42,920 --> 00:08:43,920
一个就是Walker

182
00:08:44,880 --> 00:08:46,880
不管是Tensor4还是MathBall

183
00:08:46,880 --> 00:08:50,080
它最底层的实现都是基于计算图的

184
00:08:50,080 --> 00:08:51,520
分布式进行签分

185
00:08:51,520 --> 00:08:54,120
而计算图我们在之前的系列里面

186
00:08:54,120 --> 00:08:56,840
已经详细的去讲清楚了

187
00:08:56,840 --> 00:08:58,640
我也希望能够半小时

188
00:08:58,640 --> 00:09:01,760
让大家能够搞明白什么是计算图

189
00:09:01,760 --> 00:09:03,840
所以做了一个统一的方面

190
00:09:03,840 --> 00:09:06,040
让大家方便去搜索或者学习

191
00:09:06,040 --> 00:09:09,040
还有跟我一起讨论AI系统的知识

192
00:09:10,280 --> 00:09:11,960
接着我们看两个图

193
00:09:11,960 --> 00:09:14,200
首先我们现在有两个服务器

194
00:09:14,200 --> 00:09:15,240
一个是SurfaceLink

195
00:09:15,240 --> 00:09:16,640
一个是Surface1

196
00:09:16,640 --> 00:09:19,480
现在在SurfaceLink上面有一个网络模型

197
00:09:19,480 --> 00:09:22,120
接着我对这个网络模型进行签分

198
00:09:22,120 --> 00:09:25,640
那签分之后就可能某一部分留在SurfaceLink

199
00:09:25,680 --> 00:09:27,880
另外一部分留在Surface1

200
00:09:27,880 --> 00:09:31,400
然后中间嵌入一个Send With Safe算子

201
00:09:31,400 --> 00:09:35,760
中间就通过Send With Safe算子进行通信

202
00:09:35,760 --> 00:09:38,880
刚才那种模式我们叫做模型并行

203
00:09:38,880 --> 00:09:40,880
我们还有另外一种并行模式

204
00:09:40,880 --> 00:09:42,280
叫做数据并行

205
00:09:42,280 --> 00:09:45,800
数据并行不仅仅是指我们的训练数据

206
00:09:45,800 --> 00:09:47,080
T度的数据

207
00:09:47,080 --> 00:09:50,160
这些都属于我们数据并行的范围

208
00:09:50,160 --> 00:09:51,720
所以我们就会对规点

209
00:09:51,720 --> 00:09:54,680
还有我们的权重进行并行

210
00:09:54,720 --> 00:09:57,040
训练当中的每一个mini-batch

211
00:09:57,040 --> 00:09:58,880
都会进行一次通讯的

212
00:09:58,880 --> 00:10:00,320
也就是每一条虚线

213
00:10:00,320 --> 00:10:02,920
我们都可能会进行一次的通讯

214
00:10:02,920 --> 00:10:04,720
通讯量就会非常大了

215
00:10:05,720 --> 00:10:08,240
下面我们来看看另外一个AI框架

216
00:10:08,240 --> 00:10:09,280
PyTorch

217
00:10:09,280 --> 00:10:11,400
PyTorch比较灵活自由

218
00:10:11,400 --> 00:10:13,160
它包含了两种通讯方式

219
00:10:13,160 --> 00:10:14,200
一种是点对点

220
00:10:14,200 --> 00:10:15,600
一种是集中式

221
00:10:15,600 --> 00:10:16,800
也叫集合式

222
00:10:16,800 --> 00:10:21,120
我们将会在后面的章节详细的展开集合式通讯

223
00:10:21,800 --> 00:10:24,120
点对点通讯比较好理解

224
00:10:24,120 --> 00:10:27,560
我们零服务器跟三服务器进行通讯

225
00:10:27,560 --> 00:10:30,920
可能就会通过网络或者总线特殊的事倍

226
00:10:30,920 --> 00:10:32,120
进行一个传输

227
00:10:32,120 --> 00:10:34,240
我们上面从零传到三

228
00:10:34,240 --> 00:10:35,600
这里面是同一层

229
00:10:35,600 --> 00:10:37,280
我们从零传到三

230
00:10:37,280 --> 00:10:39,440
这种就是点对点通讯

231
00:10:39,440 --> 00:10:43,000
点对点通讯里面又分同步和异步

232
00:10:43,000 --> 00:10:45,480
右边的这个代码就是同步的方式

233
00:10:45,480 --> 00:10:46,400
我一个是Sign

234
00:10:46,400 --> 00:10:47,680
一个是VC

235
00:10:47,680 --> 00:10:48,800
通过这种方式

236
00:10:48,800 --> 00:10:52,040
我去指定Wank0去发送数据给Wank1

237
00:10:53,000 --> 00:10:55,760
另外一种我们还有异步通讯

238
00:10:55,760 --> 00:10:59,680
异步通讯就可能通讯的时间和节奏不一样了

239
00:10:59,680 --> 00:11:02,320
这里面就可以实现用户指定的异步

240
00:11:02,320 --> 00:11:04,720
我们可以看到这里面有一个请求

241
00:11:04,720 --> 00:11:06,040
然后也有一个等待

242
00:11:07,320 --> 00:11:09,680
最后就是集合式通讯

243
00:11:09,680 --> 00:11:12,560
那Pytorch的集合式通讯是做得非常灵活

244
00:11:12,560 --> 00:11:13,360
非常好的

245
00:11:13,360 --> 00:11:14,800
它可以支持Gether

246
00:11:14,800 --> 00:11:17,160
它可以支持Gether,Boccus,Sketter

247
00:11:17,160 --> 00:11:18,560
就是一对多,多对一

248
00:11:18,560 --> 00:11:21,120
还有多对多的不同的通讯方式

249
00:11:21,160 --> 00:11:22,840
组合起来非常灵活

250
00:11:23,840 --> 00:11:26,040
这里面我们以AuidDeals为例子

251
00:11:26,040 --> 00:11:29,680
可以看到这里面我们去对Tensor进行AuidDeals

252
00:11:29,680 --> 00:11:32,240
把数据进行多对多的一个通讯方式

253
00:11:33,560 --> 00:11:35,840
下面是一个Minist网络模型

254
00:11:35,840 --> 00:11:37,840
然后进行多对多的通讯

255
00:11:37,840 --> 00:11:39,920
可以看到在训练的过程当中

256
00:11:39,920 --> 00:11:41,480
我们做了一个平均规点

257
00:11:41,480 --> 00:11:43,800
平均规点的方式就是在这里面

258
00:11:43,800 --> 00:11:45,720
我们在多个机器里面训练

259
00:11:45,720 --> 00:11:48,520
然后多个机器进行训练

260
00:11:48,520 --> 00:11:49,960
最后通过AuidDeals

261
00:11:49,960 --> 00:11:52,640
把所有训练的参数聚集起来

262
00:11:52,640 --> 00:11:54,160
然后再起一个均值

263
00:11:54,160 --> 00:11:55,680
最后再回传回来

264
00:11:56,800 --> 00:11:58,920
MindSpot的主要处理方式

265
00:11:58,920 --> 00:12:02,720
跟Tensor 4差不多都是基于计算图进行处理的

266
00:12:02,720 --> 00:12:04,240
但是MindSpot比较特别的

267
00:12:04,240 --> 00:12:07,240
就是我们对计算图进行切分之后

268
00:12:07,240 --> 00:12:09,680
不再是一个SendAuidSafe的算子

269
00:12:09,680 --> 00:12:12,240
而是替换成为一个OPS

270
00:12:12,240 --> 00:12:13,760
就是我们的通讯算子

271
00:12:13,760 --> 00:12:14,720
这个通讯算子

272
00:12:14,720 --> 00:12:17,600
我们就可以用类似于Pytorch的灵活方式

273
00:12:17,600 --> 00:12:19,080
去声明我这个通讯

274
00:12:19,080 --> 00:12:20,720
到底是用AuidDeals还是Augad

275
00:12:20,720 --> 00:12:25,240
是用一对多还是多对一的通讯方式进行组合

276
00:12:25,240 --> 00:12:27,640
这就是MindSpot基于计算图之上

277
00:12:27,640 --> 00:12:30,720
但是结合Pytorch的灵活用性

278
00:12:30,720 --> 00:12:33,320
提出一种新的并行的方式

279
00:12:33,320 --> 00:12:38,000
下面我们来看看MindSpot里面的自动并行的原理

280
00:12:38,000 --> 00:12:39,200
右边的这个图

281
00:12:39,200 --> 00:12:42,000
就是MindSpot自动并行的一个架构图

282
00:12:42,000 --> 00:12:43,080
我们可以看到

283
00:12:43,080 --> 00:12:44,960
输入是一个ANF的Graph

284
00:12:44,960 --> 00:12:47,000
我们可以理解为一个计算图

285
00:12:47,040 --> 00:12:49,280
输出也是一个计算图

286
00:12:49,280 --> 00:12:51,760
真正在自动并行里面

287
00:12:51,760 --> 00:12:54,320
我们对正向的计算图进行编例

288
00:12:54,320 --> 00:12:55,640
以分布式算子

289
00:12:55,640 --> 00:12:58,040
就是Distribution Operator为单位

290
00:12:58,040 --> 00:13:00,080
对张量进行一个切分建模

291
00:13:00,080 --> 00:13:02,160
这是一个建模的过程

292
00:13:02,160 --> 00:13:04,280
表示一个算子的输入输出是张量

293
00:13:04,280 --> 00:13:07,080
怎么样分布在集群的不同的卡上面

294
00:13:07,080 --> 00:13:10,120
这个工作我们叫做TensorLayout

295
00:13:10,120 --> 00:13:12,160
为了得到张量的排布模型

296
00:13:12,160 --> 00:13:13,680
也就是TensorLayout

297
00:13:13,680 --> 00:13:16,600
我们每个算子都有自己的一个切分策略

298
00:13:16,600 --> 00:13:18,680
也就是Strade Strategy

299
00:13:18,680 --> 00:13:19,920
分布式算子

300
00:13:19,920 --> 00:13:21,720
会根据张量排布模型

301
00:13:21,720 --> 00:13:23,200
判断是否在图中

302
00:13:23,200 --> 00:13:25,600
我们要插入额外的计算通讯

303
00:13:25,600 --> 00:13:29,600
以保证我们整个算子的逻辑运算是正确的

304
00:13:29,600 --> 00:13:32,120
但是当一个算子的输出张量模型

305
00:13:32,120 --> 00:13:35,400
跟后一个算子的输出张量模型不一致的时候

306
00:13:35,400 --> 00:13:37,720
就需要引入计算通讯操作

307
00:13:37,720 --> 00:13:40,360
这些实现张量排布的变化

308
00:13:40,360 --> 00:13:43,360
也就是第三步张量排布的变化

309
00:13:43,360 --> 00:13:46,360
我们叫做TensorWidthDistribution

310
00:13:46,400 --> 00:13:51,400
通过这个算法去推导得到任意排布的张量之间的通讯转换模式

311
00:13:52,640 --> 00:13:55,600
通数的来讲就是上一个张量跟下一个张量

312
00:13:55,600 --> 00:13:57,680
之间的排布变化了之后

313
00:13:57,680 --> 00:14:00,120
他们没法正确的对接起来了

314
00:14:00,120 --> 00:14:02,920
这个时候我们就需要根据算法

315
00:14:02,920 --> 00:14:05,080
去对我们的张量进行变换

316
00:14:05,080 --> 00:14:09,760
使得上一个算子跟下一个算子能够正常的对接起来

317
00:14:10,600 --> 00:14:12,280
为了进一步降低用户

318
00:14:12,280 --> 00:14:14,880
需要对分布式算子进行一切分的工作

319
00:14:14,880 --> 00:14:18,080
这里面有一个代价函数叫做CostModel

320
00:14:18,080 --> 00:14:21,160
通过CostModel可以计算出一定数量

321
00:14:21,160 --> 00:14:24,280
可以根据计算开销内存开销通讯开销

322
00:14:24,280 --> 00:14:26,560
通过动态规划的方法

323
00:14:26,560 --> 00:14:29,560
去高效的找到一个最好的切分策略

324
00:14:29,560 --> 00:14:30,880
找到这个切分策略

325
00:14:30,880 --> 00:14:33,920
最后就给到我们的真正的图切分的模块

326
00:14:33,920 --> 00:14:36,960
然后把我们的网络模型切分出来之后

327
00:14:37,080 --> 00:14:38,360
这个就是Subgraph

328
00:14:38,360 --> 00:14:40,080
这个就是子计算图

329
00:14:40,080 --> 00:14:44,000
把子计算图分布在不同的硬件上面去执行

330
00:14:45,880 --> 00:14:49,160
在AI框架分布式并行系统里面

331
00:14:49,160 --> 00:14:51,360
其实有几个内容我们是忽略的

332
00:14:51,360 --> 00:14:53,560
一个是数据的读取和预触力

333
00:14:53,560 --> 00:14:56,280
也就是我们框架在进行计算的时候

334
00:14:56,280 --> 00:15:00,280
实际上我们的数据已经可以开始分片的搬运了

335
00:15:00,280 --> 00:15:02,040
另外一个就是容错和固

336
00:15:02,040 --> 00:15:03,840
机器学习里面有一个重要的特点

337
00:15:03,840 --> 00:15:05,840
就是训练的数据非常

338
00:15:05,840 --> 00:15:08,800
可能对部分数据的丢失不是很敏感

339
00:15:08,800 --> 00:15:12,480
但是我们对这些数据如何保存和恢复

340
00:15:12,480 --> 00:15:14,600
这也是一个很重要的工作

341
00:15:14,640 --> 00:15:17,080
也对框架提出了非常多的挑战

342
00:15:17,440 --> 00:15:20,200
这一节内容里面我们主要学习了三个内容

343
00:15:20,200 --> 00:15:23,000
我们单服务器从串行到并行

344
00:15:23,000 --> 00:15:26,160
硬件架构从SISD到MMD的发展

345
00:15:26,160 --> 00:15:30,800
实际上目前使用的最广的是支持SIMT编程的

346
00:15:30,800 --> 00:15:32,400
GPU的一个硬件架构

347
00:15:32,760 --> 00:15:36,840
GPU的硬件架构其实我非常值得大家一起去学习

348
00:15:36,840 --> 00:15:38,720
然后SIMT的编程方式

349
00:15:38,720 --> 00:15:41,400
也非常值得大家一起去使用一下

350
00:15:42,120 --> 00:15:45,080
第二个就是内嵌分布式策略的一个训练系统

351
00:15:45,080 --> 00:15:48,200
最著名的代表就是Tensor4和MindSpot

352
00:15:48,200 --> 00:15:50,920
主要是基于计算图进行一个切分的

353
00:15:50,920 --> 00:15:54,280
另外一种就是提供通讯原语的分布式训练系统

354
00:15:54,280 --> 00:15:56,960
这种最主要的代表就是Pytorch

355
00:15:56,960 --> 00:16:00,080
提供非常灵活应用的分布式策略

356
00:16:00,080 --> 00:16:01,600
或者分布式通讯原语

357
00:16:04,280 --> 00:16:05,880
在这么内卷的环境下

358
00:16:05,880 --> 00:16:08,240
做到经常更新时时不易

359
00:16:08,240 --> 00:16:10,600
非常欢迎大家对我一键三连

360
00:16:10,920 --> 00:16:11,680
谢谢各位

361
00:16:11,680 --> 00:16:12,600
Bye Bye

