1
00:00:00,111 --> 00:00:04,550
【字幕生成：奔崩  字幕校对：管一鸣】

2
00:00:04,550 --> 00:00:06,800
哈喽大家好,我是ZOMI

3
00:00:06,800 --> 00:00:10,600
欢迎来到大模型与分布式训练这一个系列

4
00:00:10,600 --> 00:00:15,000
在这一系列里面我们现在来到了分布式训练系统

5
00:00:15,000 --> 00:00:18,120
在这一节里面我们将会讲两个东西

6
00:00:18,120 --> 00:00:22,400
一个就是并行处理硬件的一个架构

7
00:00:22,400 --> 00:00:27,080
第二个就是AI框架里面的分布式训练的功能

8
00:00:28,080 --> 00:00:29,480
今天晚上12点

9
00:00:29,640 --> 00:00:31,600
刚开完会今天晚上12点

10
00:00:31,600 --> 00:00:33,800
我终于又有时间过来更新了

11
00:00:34,680 --> 00:00:38,360
这里面第一个内容就是并行处理的硬件架构

12
00:00:38,960 --> 00:00:40,880
可以看到其实硬件架构

13
00:00:40,880 --> 00:00:43,240
我们从一开始的串行到并行

14
00:00:43,240 --> 00:00:45,400
再到大规模分布式并行

15
00:00:45,400 --> 00:00:47,480
中间是走了很多弯路

16
00:00:47,480 --> 00:00:50,560
也发展了很多并行处理的架构体系

17
00:00:50,560 --> 00:00:52,880
首先第一个就是SISD

18
00:00:52,880 --> 00:00:55,600
就是单指令单数据流

19
00:00:56,000 --> 00:00:57,800
S就是Single

20
00:00:57,800 --> 00:01:00,160
I就是Instruction

21
00:01:00,160 --> 00:01:01,880
S还是Single

22
00:01:01,880 --> 00:01:06,000
右边的图我们可以看到横坐标就是Data

23
00:01:06,000 --> 00:01:08,680
中坐标就是Instruction指令

24
00:01:08,680 --> 00:01:14,520
下面我们从前面单指令流单数据流是SISD

25
00:01:14,520 --> 00:01:17,560
下面都是并行的一些硬件架构

26
00:01:17,560 --> 00:01:20,320
包括SIMD MISD MIMD

27
00:01:20,320 --> 00:01:23,440
下面我们逐个来打开看看它到底有什么不一样

28
00:01:24,440 --> 00:01:26,920
第一个是SISD系统

29
00:01:26,920 --> 00:01:28,280
简单的我们可以理解

30
00:01:28,280 --> 00:01:30,360
假设我们要做一个A1加B

31
00:01:30,360 --> 00:01:32,480
就是A加B的简单操作

32
00:01:32,480 --> 00:01:34,560
我的指令就是A加B

33
00:01:34,560 --> 00:01:36,560
我的数据就是A和B

34
00:01:36,560 --> 00:01:38,560
我们PU就是Processed Unit

35
00:01:38,560 --> 00:01:40,320
就是我们的处理单元

36
00:01:40,320 --> 00:01:42,680
把数据输给我们的处理单元

37
00:01:42,680 --> 00:01:44,200
把指令输给他

38
00:01:44,200 --> 00:01:46,520
处理单元就进行计算

39
00:01:46,520 --> 00:01:48,040
最后输出

40
00:01:48,040 --> 00:01:50,640
这种方式在每个时钟周期

41
00:01:50,680 --> 00:01:53,560
CPU只能处理一个数据流

42
00:01:53,560 --> 00:01:56,280
这种是最早期的CPU的一种方式

43
00:01:57,200 --> 00:01:59,600
随着我们处理的数据越来越多

44
00:01:59,600 --> 00:02:01,920
所以我们后面就发展了SIMD

45
00:02:01,920 --> 00:02:04,560
也就是单指令多数据流

46
00:02:04,560 --> 00:02:07,280
一个控制器控制多个处理器

47
00:02:07,280 --> 00:02:10,480
灰色的框就是我们的控制器

48
00:02:10,480 --> 00:02:12,520
中间的PU就是处理器

49
00:02:12,520 --> 00:02:16,280
可以看到这个控制器里面有三个处理器

50
00:02:16,280 --> 00:02:19,600
而每个处理器都处理不同的任务

51
00:02:19,640 --> 00:02:22,160
但是他们的指令都是相同的

52
00:02:22,160 --> 00:02:24,320
从相同的指令传下来

53
00:02:24,320 --> 00:02:25,720
处理不同的数据

54
00:02:25,720 --> 00:02:27,280
最后输出

55
00:02:27,280 --> 00:02:31,360
SIMD最主要是指令一些向量矩阵的运算

56
00:02:31,360 --> 00:02:35,800
所以说它非常适用于科学计算的场景

57
00:02:35,800 --> 00:02:38,760
处理的单元PU也可以不断的去增加

58
00:02:38,760 --> 00:02:43,400
但是处理的速度就受限于计算机的通讯带宽的约束

59
00:02:43,400 --> 00:02:46,440
也就是我的数据的传输的速率

60
00:02:46,440 --> 00:02:49,400
决定我整个控制器的吞吐力

61
00:02:49,400 --> 00:02:54,520
另外一种就是MISD多指令单数据流的这种硬件子令

62
00:02:54,520 --> 00:02:58,440
就是我可以通过多个子令去处理相同的数据

63
00:02:58,440 --> 00:03:01,640
这里面很有意思的就是数据还是A和B

64
00:03:01,640 --> 00:03:05,040
但是处理的流程就变成一个加一个减一个乘

65
00:03:05,040 --> 00:03:07,680
这种模型我之前是没有见过的

66
00:03:07,680 --> 00:03:14,080
而且我也没有想到过有任何一种计算方式是适用于这种的

67
00:03:14,080 --> 00:03:15,680
同学们能够想得到的

68
00:03:15,680 --> 00:03:19,240
非常欢迎你们在弹幕或者评论里面去留言

69
00:03:19,240 --> 00:03:21,120
反正我是没有见过的

70
00:03:21,120 --> 00:03:23,520
MISD作为理论模型出现

71
00:03:23,520 --> 00:03:26,120
并没有投入到实际的应用当中

72
00:03:28,440 --> 00:03:30,600
最后一个就是MIMD

73
00:03:30,600 --> 00:03:32,600
MIMD这种方式我们经常见

74
00:03:32,600 --> 00:03:34,600
而且经常去使用

75
00:03:34,600 --> 00:03:40,160
也就是在多个数据集上面去执行多个指令流的一个处理器

76
00:03:40,160 --> 00:03:42,840
这种也是并行方式里面用的最多的

77
00:03:42,840 --> 00:03:44,680
我们以一个处理单元为例子

78
00:03:44,680 --> 00:03:46,920
里面有非常多的处理器

79
00:03:46,920 --> 00:03:48,120
有三个指令流

80
00:03:48,120 --> 00:03:50,000
每个处理器对应一个指令流

81
00:03:50,000 --> 00:03:52,000
然后都有很多的数据

82
00:03:52,000 --> 00:03:53,800
不同的数据传给他

83
00:03:53,800 --> 00:03:55,160
最后输出来

84
00:03:55,160 --> 00:03:57,200
这个就是MIMD的方式

85
00:03:57,200 --> 00:04:02,920
那MIMD又分为共享内存的MIMD和分布式内存的MIMD

86
00:04:02,920 --> 00:04:05,520
下面来看看两者有什么区别

87
00:04:05,520 --> 00:04:08,400
共享内存的MIMD更容易理解

88
00:04:08,400 --> 00:04:11,600
就是我们现在一个处理器上面有很多个核

89
00:04:11,600 --> 00:04:15,800
包括我们的英特尔的IC处理器里面可能有八个核

90
00:04:15,840 --> 00:04:19,200
中间通过内存总线进行数据的传输

91
00:04:19,200 --> 00:04:21,800
那另外一种就是分布式内存

92
00:04:21,800 --> 00:04:23,920
另外一个就是分布式内存

93
00:04:23,920 --> 00:04:26,040
假设这是我其中一个机器

94
00:04:26,040 --> 00:04:28,080
或者其中一个CPU

95
00:04:28,080 --> 00:04:30,680
那里面其中一个核CPU

96
00:04:30,680 --> 00:04:33,960
我可能每一个CPU都有自己的内存

97
00:04:33,960 --> 00:04:38,960
但是CPU跟CPU之间是通过进程通道进行通讯的

98
00:04:38,960 --> 00:04:41,360
这种叫做分布式内存

99
00:04:41,360 --> 00:04:45,240
应该在90年代末出现一种比较特殊的硬件

100
00:04:45,280 --> 00:04:48,560
提出了一种新的架构叫做SIMT

101
00:04:48,560 --> 00:04:51,320
那T就是thread就是我们的线程

102
00:04:51,320 --> 00:04:55,520
这种方式是属于SIMD的另外一种扩展

103
00:04:55,520 --> 00:04:59,120
这个硬件最特殊的代表就是GPU

104
00:04:59,120 --> 00:05:02,680
那GPU就是通过单指令多线程

105
00:05:02,680 --> 00:05:06,120
有效的管理核执行多个单线程

106
00:05:06,120 --> 00:05:09,600
每一条指令的数据都是分开循址的

107
00:05:09,600 --> 00:05:12,280
而且用户可以控制每一个线程

108
00:05:12,280 --> 00:05:14,520
每个线程都有自己的逻辑

109
00:05:15,520 --> 00:05:17,840
这样非常方便用户去编程

110
00:05:17,840 --> 00:05:23,560
这种新的架构出现就衍生了我们现在深度学习

111
00:05:23,560 --> 00:05:27,600
经常会用到的GPU或者NPU的编程方式

112
00:05:27,600 --> 00:05:29,240
或者硬件体系结构

113
00:05:35,560 --> 00:05:38,520
我们硬件结构从串行发展到并行

114
00:05:38,520 --> 00:05:39,960
最后出现GPU

115
00:05:40,000 --> 00:05:43,880
还是为了让同一时间能够处理更多的数据

116
00:05:43,880 --> 00:05:47,680
那软件上我们又有了分布式训练系统

117
00:05:47,680 --> 00:05:50,720
这个分布式训练系统最主要的意义

118
00:05:51,800 --> 00:05:54,520
这个分布式训练系统最主要的意义就是

119
00:05:54,520 --> 00:05:57,920
为我们提供非常容易方便高效的

120
00:05:57,920 --> 00:06:00,680
对我们的大模型进行训练

121
00:06:00,680 --> 00:06:04,240
那我们可以看到分布式系统其实是分开三层的

122
00:06:04,240 --> 00:06:07,360
最上层面向用户的就是提供一些接口

123
00:06:07,400 --> 00:06:08,480
提供一些接口

124
00:06:08,480 --> 00:06:10,880
非常方便我们去写代码

125
00:06:10,880 --> 00:06:14,000
那中间就是单节点的一个训练

126
00:06:14,000 --> 00:06:16,600
单节点训练就是我们刚才所说到的

127
00:06:16,600 --> 00:06:18,240
针对每一个处理单元

128
00:06:18,240 --> 00:06:21,240
每个控制器如何执行训练

129
00:06:21,240 --> 00:06:24,200
那最后我们所有的整个分布式训练系统

130
00:06:24,200 --> 00:06:27,320
是通过通信的方式进行协调的

131
00:06:27,320 --> 00:06:28,120
每个节点

132
00:06:28,120 --> 00:06:30,520
每个控制器之间需要通讯

133
00:06:30,520 --> 00:06:31,800
告诉另外一个机器

134
00:06:31,800 --> 00:06:32,920
我算到哪了

135
00:06:32,920 --> 00:06:34,080
你算到哪了

136
00:06:34,080 --> 00:06:36,000
我们的数据应该怎么同步

137
00:06:36,000 --> 00:06:37,080
应该怎么协调

138
00:06:37,080 --> 00:06:38,760
通讯协调的内容

139
00:06:38,760 --> 00:06:41,520
我们将会在后面的章节去展开

140
00:06:42,920 --> 00:06:47,120
首先看看分布式训练系统有两种具体的方式

141
00:06:47,120 --> 00:06:50,400
那一种就是框架内里面去实现的

142
00:06:50,400 --> 00:06:54,720
另外一种是跨框架的一个通用的分布式训练系统

143
00:06:54,720 --> 00:06:59,400
具体我们用几个现在比较出名的AI框架去举例子

144
00:06:59,400 --> 00:07:02,200
第一个就是框架内嵌的分布式系统

145
00:07:02,240 --> 00:07:06,680
这些AI框架内部已经自带了分布式训练系统

146
00:07:06,680 --> 00:07:08,000
这些能力

147
00:07:08,000 --> 00:07:09,120
那另外第二个

148
00:07:09,120 --> 00:07:10,400
类似于Pytorch

149
00:07:10,400 --> 00:07:13,000
这种也是框架内嵌的

150
00:07:13,000 --> 00:07:17,000
Pytorch就提供了非常开放性的功能给用户去使用的

151
00:07:17,000 --> 00:07:20,000
那第三种类似于Horovod或者DeepSpeed这种

152
00:07:20,000 --> 00:07:23,280
就是跨框架的一种通用的分布式训练系统

153
00:07:23,280 --> 00:07:26,480
就是它不一定完全依赖于某种框架

154
00:07:26,480 --> 00:07:29,320
它可以居于它自己去实现很多功能

155
00:07:29,360 --> 00:07:33,800
这种一般来说它是聚焦于某种并行的策略

156
00:07:33,800 --> 00:07:36,640
下面我们逐个框架来打开看看

157
00:07:36,640 --> 00:07:39,760
具体在分布式训练系统里面有什么不一样

158
00:07:41,640 --> 00:07:45,040
可以看到TensorFlow它有非常多的API

159
00:07:45,040 --> 00:07:46,320
首先它有Keras

160
00:07:46,320 --> 00:07:47,640
然后有自己的API

161
00:07:47,640 --> 00:07:50,160
还有Estimator的API

162
00:07:50,160 --> 00:07:54,320
而每一种API都有不同的分布式的一些策略

163
00:07:54,320 --> 00:07:56,360
或者分布式的接口

164
00:07:56,360 --> 00:07:58,440
几年前我去学TensorFlow的时候

165
00:07:58,440 --> 00:08:01,920
就觉得TensorFlow的API割裂的实在太严重了

166
00:08:01,920 --> 00:08:04,680
而且学习成本非常高

167
00:08:04,680 --> 00:08:06,880
TensorFlow在早期的版本里面

168
00:08:06,880 --> 00:08:09,240
其实已经加入了参数服务器

169
00:08:09,240 --> 00:08:11,960
Parameter Server这种功能

170
00:08:11,960 --> 00:08:16,200
最主要的思路就是通过多个Worker独立的去执行

171
00:08:16,200 --> 00:08:19,720
然后分布式的共享我们的网络模型的参数

172
00:08:19,720 --> 00:08:21,960
下面我们来看看具体的代码

173
00:08:21,960 --> 00:08:24,760
TensorFlow参数服务器的一个具体的使用

174
00:08:24,760 --> 00:08:27,640
首先我们要定义我们的Worker和PS

175
00:08:27,680 --> 00:08:29,480
就是指定节点的信息

176
00:08:29,480 --> 00:08:32,720
那Worker就包括我们的网络模型的信息

177
00:08:32,720 --> 00:08:35,320
Worker主要是包括我们的网络模型

178
00:08:35,320 --> 00:08:39,040
而PS主要是对我们的参数进行同步的

179
00:08:39,040 --> 00:08:40,320
所以我们执行的时候

180
00:08:40,320 --> 00:08:41,960
有两个角色需要定义的

181
00:08:41,960 --> 00:08:42,920
一个就是PS

182
00:08:42,920 --> 00:08:43,920
一个就是Walker

183
00:08:44,880 --> 00:08:46,880
不管是TensorFlow还是MindSpore

184
00:08:46,880 --> 00:08:50,080
它最底层的实现都是基于计算图的

185
00:08:50,080 --> 00:08:51,520
分布式进行签分

186
00:08:51,520 --> 00:08:54,120
而计算图我们在之前的系列里面

187
00:08:54,120 --> 00:08:56,840
已经详细的去讲清楚了

188
00:08:56,840 --> 00:08:58,640
我也希望能够半小时

189
00:08:58,640 --> 00:09:01,760
让大家能够搞明白什么是计算图

190
00:09:01,760 --> 00:09:03,840
所以做了一个统一的封面

191
00:09:03,840 --> 00:09:06,040
让大家方便去搜索或者学习

192
00:09:06,040 --> 00:09:09,040
还有跟我一起讨论AI系统的知识

193
00:09:10,280 --> 00:09:11,960
接着我们看两个图

194
00:09:11,960 --> 00:09:14,200
首先我们现在有两个服务器

195
00:09:14,200 --> 00:09:15,240
一个是Server0

196
00:09:15,240 --> 00:09:16,640
一个是Server1

197
00:09:16,640 --> 00:09:19,480
现在在Server0上面有一个网络模型

198
00:09:19,480 --> 00:09:22,120
接着我对这个网络模型进行切分

199
00:09:22,120 --> 00:09:25,640
那签分之后就可能某一部分留在Server0

200
00:09:25,680 --> 00:09:27,880
另外一部分留在Server1

201
00:09:27,880 --> 00:09:31,400
然后中间嵌入一个Send Recv算子

202
00:09:31,400 --> 00:09:35,760
中间就通过Send Recv算子进行通信

203
00:09:35,760 --> 00:09:38,880
刚才那种模式我们叫做模型并行

204
00:09:38,880 --> 00:09:40,880
我们还有另外一种并行模式

205
00:09:40,880 --> 00:09:42,280
叫做数据并行

206
00:09:42,280 --> 00:09:45,800
数据并行不仅仅是指我们的训练数据

207
00:09:45,800 --> 00:09:47,080
梯度的数据

208
00:09:47,080 --> 00:09:50,160
这些都属于我们数据并行的范围

209
00:09:50,160 --> 00:09:51,720
所以我们就会对gradient

210
00:09:51,720 --> 00:09:54,680
还有我们的权重进行并行

211
00:09:54,720 --> 00:09:57,040
训练当中的每一个mini-batch

212
00:09:57,040 --> 00:09:58,880
都会进行一次通讯的

213
00:09:58,880 --> 00:10:00,320
也就是每一条虚线

214
00:10:00,320 --> 00:10:02,920
我们都可能会进行一次的通讯

215
00:10:02,920 --> 00:10:04,720
通讯量就会非常大了

216
00:10:05,720 --> 00:10:08,240
下面我们来看看另外一个AI框架

217
00:10:08,240 --> 00:10:09,280
PyTorch

218
00:10:09,280 --> 00:10:11,400
PyTorch比较灵活自由

219
00:10:11,400 --> 00:10:13,160
它包含了两种通讯方式

220
00:10:13,160 --> 00:10:14,200
一种是点对点

221
00:10:14,200 --> 00:10:15,600
一种是集中式

222
00:10:15,600 --> 00:10:16,800
也叫集合式

223
00:10:16,800 --> 00:10:21,120
我们将会在后面的章节详细的展开集合式通讯

224
00:10:21,800 --> 00:10:24,120
点对点通讯比较好理解

225
00:10:24,120 --> 00:10:27,560
我们零服务器跟三服务器进行通讯

226
00:10:27,560 --> 00:10:30,920
可能就会通过网络或者总线特殊的设备

227
00:10:30,920 --> 00:10:32,120
进行一个传输

228
00:10:32,120 --> 00:10:34,240
我们上面从零传到三

229
00:10:34,240 --> 00:10:35,600
这里面是同一层

230
00:10:35,600 --> 00:10:37,280
我们从零传到三

231
00:10:37,280 --> 00:10:39,440
这种就是点对点通讯

232
00:10:39,440 --> 00:10:43,000
点对点通讯里面又分同步和异步

233
00:10:43,000 --> 00:10:45,480
右边的这个代码就是同步的方式

234
00:10:45,480 --> 00:10:46,400
我一个是Send

235
00:10:46,400 --> 00:10:47,680
一个是Recv

236
00:10:47,680 --> 00:10:48,800
通过这种方式

237
00:10:48,800 --> 00:10:52,040
我去指定rank0去发送数据给rank1

238
00:10:53,000 --> 00:10:55,760
另外一种我们还有异步通讯

239
00:10:55,760 --> 00:10:59,680
异步通讯就可能通讯的时间和节奏不一样了

240
00:10:59,680 --> 00:11:02,320
这里面就可以实现用户指定的异步

241
00:11:02,320 --> 00:11:04,720
我们可以看到这里面有一个请求

242
00:11:04,720 --> 00:11:06,040
然后也有一个等待

243
00:11:07,320 --> 00:11:09,680
最后就是集合式通讯

244
00:11:09,680 --> 00:11:12,560
那Pytorch的集合式通讯是做得非常灵活

245
00:11:12,560 --> 00:11:13,360
非常好的

246
00:11:13,360 --> 00:11:14,800
它可以支持Gather

247
00:11:14,800 --> 00:11:17,160
它可以支持Gather,Broadcast,Scatter

248
00:11:17,160 --> 00:11:18,560
就是一对多,多对一

249
00:11:18,560 --> 00:11:21,120
还有多对多的不同的通讯方式

250
00:11:21,160 --> 00:11:22,840
组合起来非常灵活

251
00:11:23,840 --> 00:11:26,040
这里面我们以All Reduce为例子

252
00:11:26,040 --> 00:11:29,680
可以看到这里面我们去对Tensor进行All Reduce

253
00:11:29,680 --> 00:11:32,240
把数据进行多对多的一个通讯方式

254
00:11:33,560 --> 00:11:35,840
下面是一个MNIST网络模型

255
00:11:35,840 --> 00:11:37,840
然后进行多对多的通讯

256
00:11:37,840 --> 00:11:39,920
可以看到在训练的过程当中

257
00:11:39,920 --> 00:11:41,480
我们做了一个平均gradient

258
00:11:41,480 --> 00:11:43,800
平均gradient的方式就是在这里面

259
00:11:43,800 --> 00:11:45,720
我们在多个机器里面训练

260
00:11:45,720 --> 00:11:48,520
然后多个机器进行训练

261
00:11:48,520 --> 00:11:49,960
最后通过All Reduce

262
00:11:49,960 --> 00:11:52,640
把所有训练的参数聚集起来

263
00:11:52,640 --> 00:11:54,160
然后再求一个均值

264
00:11:54,160 --> 00:11:55,680
最后再回传回来

265
00:11:56,800 --> 00:11:58,920
MindSpore的主要处理方式

266
00:11:58,920 --> 00:12:02,720
跟TensorFlow差不多都是基于计算图进行处理的

267
00:12:02,720 --> 00:12:04,240
但是MindSpore比较特别的

268
00:12:04,240 --> 00:12:07,240
就是我们对计算图进行切分之后

269
00:12:07,240 --> 00:12:09,680
不再是一个Send Recv的算子

270
00:12:09,680 --> 00:12:12,240
而是替换成为一个OPS

271
00:12:12,240 --> 00:12:13,760
就是我们的通讯算子

272
00:12:13,760 --> 00:12:14,720
这个通讯算子

273
00:12:14,720 --> 00:12:17,600
我们就可以用类似于Pytorch的灵活方式

274
00:12:17,600 --> 00:12:19,080
去声明我这个通讯

275
00:12:19,080 --> 00:12:20,720
到底是用All Reduce还是All Gather

276
00:12:20,720 --> 00:12:25,240
是用一对多还是多对一的通讯方式进行组合

277
00:12:25,240 --> 00:12:27,640
这就是MindSpore基于计算图之上

278
00:12:27,640 --> 00:12:30,720
但是结合Pytorch的灵活用性

279
00:12:30,720 --> 00:12:33,320
提出一种新的并行的方式

280
00:12:33,320 --> 00:12:38,000
下面我们来看看MindSpore里面的自动并行的原理

281
00:12:38,000 --> 00:12:39,200
右边的这个图

282
00:12:39,200 --> 00:12:42,000
就是MindSpore自动并行的一个架构图

283
00:12:42,000 --> 00:12:43,080
我们可以看到

284
00:12:43,080 --> 00:12:44,960
输入是一个ANF的Graph

285
00:12:44,960 --> 00:12:47,000
我们可以理解为一个计算图

286
00:12:47,040 --> 00:12:49,280
输出也是一个计算图

287
00:12:49,280 --> 00:12:51,735
真正在自动并行里面

288
00:12:51,735 --> 00:12:51,760
我们对正向的计算图进行遍历

289
00:12:51,760 --> 00:12:54,295
我们对正向的计算图进行遍历

290
00:12:54,320 --> 00:12:55,640
以分布式算子

291
00:12:55,640 --> 00:12:58,040
就是Distribution Operator为单位

292
00:12:58,040 --> 00:13:00,080
对张量进行一个切分建模

293
00:13:00,080 --> 00:13:02,160
这是一个建模的过程

294
00:13:02,160 --> 00:13:04,280
表示一个算子的输入输出是张量

295
00:13:04,280 --> 00:13:07,080
怎么样分布在集群的不同的卡上面

296
00:13:07,080 --> 00:13:10,120
这个工作我们叫做TensorLayout

297
00:13:10,120 --> 00:13:12,160
为了得到张量的排布模型

298
00:13:12,160 --> 00:13:13,680
也就是TensorLayout

299
00:13:13,680 --> 00:13:16,600
我们每个算子都有自己的一个切分策略

300
00:13:16,600 --> 00:13:18,680
也就是Strade Strategy

301
00:13:18,680 --> 00:13:19,920
分布式算子

302
00:13:19,920 --> 00:13:21,720
会根据张量排布模型

303
00:13:21,720 --> 00:13:23,200
判断是否在图中

304
00:13:23,200 --> 00:13:25,600
我们要插入额外的计算通讯

305
00:13:25,600 --> 00:13:29,600
以保证我们整个算子的逻辑运算是正确的

306
00:13:29,600 --> 00:13:32,120
但是当一个算子的输出张量模型

307
00:13:32,120 --> 00:13:35,400
跟后一个算子的输出张量模型不一致的时候

308
00:13:35,400 --> 00:13:37,720
就需要引入计算通讯操作

309
00:13:37,720 --> 00:13:40,360
这些实现张量排布的变化

310
00:13:40,360 --> 00:13:43,360
也就是第三步张量排布的变化

311
00:13:43,360 --> 00:13:46,360
我们叫做Tensor Redistribution

312
00:13:46,400 --> 00:13:51,400
通过这个算法去推导得到任意排布的张量之间的通讯转换模式

313
00:13:52,640 --> 00:13:55,600
通数的来讲就是上一个张量跟下一个张量

314
00:13:55,600 --> 00:13:57,680
之间的排布变化了之后

315
00:13:57,680 --> 00:14:00,120
他们没法正确的对接起来了

316
00:14:00,120 --> 00:14:02,920
这个时候我们就需要根据算法

317
00:14:02,920 --> 00:14:05,080
去对我们的张量进行变换

318
00:14:05,080 --> 00:14:09,760
使得上一个算子跟下一个算子能够正常的对接起来

319
00:14:10,600 --> 00:14:12,280
为了进一步降低用户

320
00:14:12,280 --> 00:14:14,880
需要对分布式算子进行一切分的工作

321
00:14:14,880 --> 00:14:18,080
这里面有一个代价函数叫做CostModel

322
00:14:18,080 --> 00:14:21,160
通过CostModel可以计算出一定数量

323
00:14:21,160 --> 00:14:24,280
可以根据计算开销内存开销通讯开销

324
00:14:24,280 --> 00:14:26,560
通过动态规划的方法

325
00:14:26,560 --> 00:14:29,560
去高效的找到一个最好的切分策略

326
00:14:29,560 --> 00:14:30,880
找到这个切分策略

327
00:14:30,880 --> 00:14:33,920
最后就给到我们的真正的图切分的模块

328
00:14:33,920 --> 00:14:36,960
然后把我们的网络模型切分出来之后

329
00:14:37,080 --> 00:14:38,360
这个就是Subgraph

330
00:14:38,360 --> 00:14:40,080
这个就是子计算图

331
00:14:40,080 --> 00:14:44,000
把子计算图分布在不同的硬件上面去执行

332
00:14:45,880 --> 00:14:49,160
在AI框架分布式并行系统里面

333
00:14:49,160 --> 00:14:51,360
其实有几个内容我们是忽略的

334
00:14:51,360 --> 00:14:53,560
一个是数据的读取和预处理

335
00:14:53,560 --> 00:14:56,280
也就是我们框架在进行计算的时候

336
00:14:56,280 --> 00:15:00,280
实际上我们的数据已经可以开始分片的搬运了

337
00:15:00,280 --> 00:15:02,040
另外一个就是容错和故障恢复

338
00:15:02,040 --> 00:15:03,840
机器学习里面有一个重要的特点

339
00:15:03,840 --> 00:15:05,840
就是训练的数据非常

340
00:15:05,840 --> 00:15:08,800
可能对部分数据的丢失不是很敏感

341
00:15:08,800 --> 00:15:12,480
但是我们对这些数据如何保存和恢复

342
00:15:12,480 --> 00:15:14,600
这也是一个很重要的工作

343
00:15:14,640 --> 00:15:17,080
也对框架提出了非常多的挑战

344
00:15:17,440 --> 00:15:20,200
这一节内容里面我们主要学习了三个内容

345
00:15:20,200 --> 00:15:23,000
我们单服务器从串行到并行

346
00:15:23,000 --> 00:15:26,160
硬件架构从SISD到MIMD的发展

347
00:15:26,160 --> 00:15:30,800
实际上目前使用的最广的是支持SIMT编程的

348
00:15:30,800 --> 00:15:32,400
GPU的一个硬件架构

349
00:15:32,760 --> 00:15:36,840
GPU的硬件架构其实我非常值得大家一起去学习

350
00:15:36,840 --> 00:15:38,720
然后SIMT的编程方式

351
00:15:38,720 --> 00:15:41,400
也非常值得大家一起去使用一下

352
00:15:42,120 --> 00:15:45,080
第二个就是内嵌分布式策略的一个训练系统

353
00:15:45,080 --> 00:15:48,200
最著名的代表就是TensorFlow和MindSpore

354
00:15:48,200 --> 00:15:50,920
主要是基于计算图进行一个切分的

355
00:15:50,920 --> 00:15:54,280
另外一种就是提供通讯原语的分布式训练系统

356
00:15:54,280 --> 00:15:56,960
这种最主要的代表就是Pytorch

357
00:15:56,960 --> 00:16:00,080
提供非常灵活应用的分布式策略

358
00:16:00,080 --> 00:16:01,600
或者分布式通讯原语

359
00:16:04,280 --> 00:16:05,880
在这么内卷的环境下

360
00:16:05,880 --> 00:16:08,240
做到经常更新实属不易

361
00:16:08,240 --> 00:16:10,600
非常欢迎大家对我一键三连

362
00:16:10,920 --> 00:16:11,680
谢谢各位

363
00:16:11,680 --> 00:16:12,600
拜了个拜

