1
00:00:00,000 --> 00:00:07,760
Hello 大家好,我是ZOMI

2
00:00:07,760 --> 00:00:11,040
今天来到了分步式算法系列

3
00:00:11,040 --> 00:00:12,560
那聊到分步式算法

4
00:00:12,560 --> 00:00:17,760
肯定现在只会说大模型的一个分步式算法

5
00:00:17,760 --> 00:00:21,320
在今天内容聚焦于在这一块

6
00:00:21,320 --> 00:00:24,400
之前的内容讲了分步式加AI集群

7
00:00:24,400 --> 00:00:28,560
今天看看大模型和训练遇到的一些挑战

8
00:00:28,680 --> 00:00:31,120
首先要回答一个什么是大模型

9
00:00:31,120 --> 00:00:32,560
大模型有什么用

10
00:00:32,560 --> 00:00:36,560
第二个来给大家汇报一下大模型训练的四个挑战

11
00:00:36,560 --> 00:00:40,640
也是现在去训练大模型所遇到的一些真正的挑战

12
00:00:41,960 --> 00:00:43,760
在视频开始介绍之前

13
00:00:43,760 --> 00:00:45,720
ZOMI想抛出几个问题

14
00:00:45,720 --> 00:00:48,680
希望能够引起大家的一个思考

15
00:00:48,680 --> 00:00:52,720
第一个是为什么预训练网络模型变得越来越重要

16
00:00:52,720 --> 00:00:55,960
第二个是预训练的大模型未来的发展趋势

17
00:00:56,960 --> 00:01:01,560
第三个就是如何训练一个百亿规模的大模型

18
00:01:01,560 --> 00:01:06,000
2021年8月份李飞飞和100多位学子

19
00:01:06,000 --> 00:01:09,040
联名发布了一份200多页的研究报告

20
00:01:09,040 --> 00:01:15,360
这份研究报告里面深度的中述了当前大规模域训练模型面临的机遇和挑战

21
00:01:15,360 --> 00:01:20,680
文章当中AI专家将大模型统一命名为Foundation Model

22
00:01:20,680 --> 00:01:24,040
可以翻译为基础模型或者是机式模型

23
00:01:24,280 --> 00:01:29,640
论文里面肯定了Foundation Model对智能体基本认知能力的推动作用

24
00:01:29,640 --> 00:01:34,080
同时也指出了大模型呈现出了涌现和同质化的一个特性

25
00:01:35,160 --> 00:01:36,760
回到大模型里面

26
00:01:36,760 --> 00:01:39,480
2017年Transformer结构的首次提出

27
00:01:39,480 --> 00:01:43,240
使得深度学习模型参数量突破了一个亿

28
00:01:43,240 --> 00:01:48,240
下面这个图就是从一开始的LeNet、AlexNet到ResNet50

29
00:01:48,240 --> 00:01:50,040
模型参数一个比一个大

30
00:01:50,040 --> 00:01:52,200
到了Bert网络模型的提出

31
00:01:52,240 --> 00:01:55,520
使得参数量首次超过了3亿规模

32
00:01:55,520 --> 00:01:59,360
那GTP-3的模型规模就超过了百亿了

33
00:01:59,360 --> 00:02:02,560
向鹏城盘古实现了千亿稠密的规模

34
00:02:02,560 --> 00:02:07,400
而Google的Switch Transformer的问世还一举突破了万亿规模

35
00:02:08,840 --> 00:02:12,760
大模型在产学各界掀起兼顾了一阵阵巨浪

36
00:02:12,760 --> 00:02:17,480
背后彰显的除了分布式并行和对AI算法的掌控能力之外

37
00:02:17,480 --> 00:02:20,400
还是一次大公司通过AI工程的创举

38
00:02:20,440 --> 00:02:24,040
利用大规模AI集群来进行扳手腕的一个故事

39
00:02:25,040 --> 00:02:27,080
随着网络模型越来越大

40
00:02:27,080 --> 00:02:31,280
单机单卡、一机多卡甚至多机多卡的小规模集群

41
00:02:31,280 --> 00:02:35,840
只要网络模型参数量一旦超过了10个亿以上的规模

42
00:02:35,840 --> 00:02:38,480
就很难用现有的资源去训练了

43
00:02:38,480 --> 00:02:41,080
于是有的研究者就会提出质疑

44
00:02:41,080 --> 00:02:42,720
一味的让模型变大

45
00:02:42,720 --> 00:02:44,760
让参数量爆炸式的增长

46
00:02:44,760 --> 00:02:48,040
真的能让AI模型学习变得更好吗

47
00:02:48,080 --> 00:02:50,200
真的能带来真正的智能吗

48
00:02:51,200 --> 00:02:53,600
虽然大模型刚提出的时候

49
00:02:53,600 --> 00:02:55,280
质疑的声音会有

50
00:02:55,280 --> 00:02:56,800
但不可否认的是

51
00:02:56,800 --> 00:03:01,080
大模型做到了早期预训练模型做不到、做不好的事情

52
00:03:01,080 --> 00:03:06,480
就好像自然语言处理当中的文字生成、文本理解、自动问答等效果的任务

53
00:03:06,480 --> 00:03:08,800
不仅生成的文本更加流畅了

54
00:03:08,800 --> 00:03:12,560
甚至内容的素实性又有了非常显著的改进

55
00:03:12,560 --> 00:03:13,520
当然了

56
00:03:13,520 --> 00:03:16,040
大模型最终能否走向通用人工智能

57
00:03:16,040 --> 00:03:17,200
仍然是一个未知数

58
00:03:19,000 --> 00:03:20,720
有了大模型的基本介绍

59
00:03:20,720 --> 00:03:22,800
来看看大模型的具体作用

60
00:03:22,800 --> 00:03:25,320
首先第一个就是模型碎片化

61
00:03:25,320 --> 00:03:27,840
大模型这时候提供了预训练方案

62
00:03:27,840 --> 00:03:31,440
现在AI面向的行业和业务场景非常多

63
00:03:31,440 --> 00:03:34,320
从开发、调参、优化、迭代到应用

64
00:03:34,320 --> 00:03:36,200
AI模型的研发成本很高

65
00:03:36,200 --> 00:03:39,080
而且很难满足市场的定制化需求

66
00:03:39,080 --> 00:03:41,200
所以这个时候很多人就会说

67
00:03:41,200 --> 00:03:45,120
现阶段的AI模型处于一个手工作坊式

68
00:03:45,120 --> 00:03:46,920
而为了解决这个问题

69
00:03:46,960 --> 00:03:48,640
大模型提供了另外一种

70
00:03:48,640 --> 00:03:51,720
预训练大模型加下游任务微调的方式

71
00:03:51,720 --> 00:03:54,760
大模型预训练可以有效地从大量标记

72
00:03:54,760 --> 00:03:57,520
和没有标记的数据当中获取知识

73
00:03:57,520 --> 00:04:00,240
通过将知识存储在大量的参数当中

74
00:04:00,240 --> 00:04:02,320
并对特定项目进行微调

75
00:04:02,320 --> 00:04:05,280
非常好地扩展了模型的泛化能力

76
00:04:05,280 --> 00:04:08,640
第二点就是大模型具备自监督学习的功能

77
00:04:08,640 --> 00:04:11,160
降低训练研发成本

78
00:04:11,160 --> 00:04:13,600
大模型的自监督学习方法

79
00:04:13,600 --> 00:04:15,960
可以减少了数据标注的成本

80
00:04:15,960 --> 00:04:19,960
使得小样本的学习也能够达到比以前更好的能力

81
00:04:19,960 --> 00:04:23,320
并且模型参数规模越大,优势越明显

82
00:04:23,320 --> 00:04:26,120
避免开发人员进行大规模的训练

83
00:04:26,120 --> 00:04:29,680
使用小样本就可以训练自己所需要的模型了

84
00:04:29,680 --> 00:04:31,760
极大地降低了开发的成本

85
00:04:31,760 --> 00:04:36,440
其实在一年前国家同时也出台了一个政策

86
00:04:36,440 --> 00:04:39,360
重新定义了什么是新一代农民工

87
00:04:39,360 --> 00:04:41,440
像这种搞数据标注的人

88
00:04:41,440 --> 00:04:43,960
就是新一代农民工了

89
00:04:43,960 --> 00:04:45,840
希望大家都不要惊吭

90
00:04:45,840 --> 00:04:47,920
不要成为新一代的农民工

91
00:04:47,920 --> 00:04:50,960
而是应该成为新一代的技术专家

92
00:04:54,680 --> 00:04:57,360
第三点就是大模型有望进一步

93
00:04:57,360 --> 00:05:00,240
突破现有模型结构的精度局限

94
00:05:00,240 --> 00:05:03,480
从深度学习的发展前十年的历程来看

95
00:05:03,480 --> 00:05:05,040
模型精度的提升

96
00:05:05,040 --> 00:05:07,840
主要是依赖于网络模型结构上的变革

97
00:05:07,840 --> 00:05:10,080
例如从AlexNet到ResNet50

98
00:05:10,080 --> 00:05:12,480
再到NAS搜索出来的EfficientNet

99
00:05:12,520 --> 00:05:14,400
ImageNet比赛的精度

100
00:05:14,400 --> 00:05:18,200
Top1的精度从58%提升到84%

101
00:05:18,200 --> 00:05:21,240
但是随着神经网络结构事迹的技术

102
00:05:21,240 --> 00:05:23,600
逐渐地走向成熟并收敛

103
00:05:23,600 --> 00:05:26,000
想要通过优化神经网络的结构

104
00:05:26,000 --> 00:05:27,840
来打破精度的局限

105
00:05:27,840 --> 00:05:30,160
现在来说是非常难的

106
00:05:30,160 --> 00:05:31,000
而近几年

107
00:05:31,000 --> 00:05:34,360
随着数据规模和模型的规模不断地增大

108
00:05:34,360 --> 00:05:37,400
模型的精度也进一步地得到了提升

109
00:05:37,400 --> 00:05:38,760
研究实验表明

110
00:05:38,760 --> 00:05:40,720
模型和数据规模的增大

111
00:05:40,760 --> 00:05:43,360
确实能够突破现有精度的一个局限

112
00:05:45,640 --> 00:05:49,000
由于大模型训练会比分布式训练更加复杂

113
00:05:49,000 --> 00:05:51,080
在深入了解大模型训练之前

114
00:05:51,080 --> 00:05:54,320
先来看看大模型训练会遇到哪些挑战

115
00:05:54,320 --> 00:05:57,520
理论上集群中的AI芯片数量越多

116
00:05:57,520 --> 00:05:59,360
模型训练的越快

117
00:05:59,360 --> 00:06:02,560
但是训练资源扩大到一定的规模的时候

118
00:06:02,560 --> 00:06:05,040
分布式并行就会出现局限性

119
00:06:05,040 --> 00:06:08,240
因为通讯的瓶颈会限制整个系统的性能

120
00:06:08,280 --> 00:06:11,520
增加计算资源反而会降低整体的加速比

121
00:06:13,440 --> 00:06:14,120
另外

122
00:06:14,120 --> 00:06:15,760
随着网络模型参数量

123
00:06:15,760 --> 00:06:21,040
从AlexNet的6000万到现在鹏城盘谷大模型的两千亿参数

124
00:06:21,040 --> 00:06:24,560
模型参数量的增大会使得内存急剧膨胀

125
00:06:24,560 --> 00:06:28,760
算子的增加也会使得就算使用模型或者流水线并行

126
00:06:28,760 --> 00:06:32,880
也很难在一个合理的时间内完成一个Step的训练

127
00:06:32,880 --> 00:06:37,600
最终会导致分布式训练不再适用于大模型训练任务

128
00:06:37,640 --> 00:06:40,960
上面就是大模型出现对AI系统的挑战

129
00:06:40,960 --> 00:06:43,760
面对大模型甚至分布式训练的技术上

130
00:06:43,760 --> 00:06:48,200
需要引入大规模训练的技术解决内存不够用的同时

131
00:06:48,200 --> 00:06:50,600
也不会被计算资源给限制住

132
00:06:51,600 --> 00:06:53,400
相比普通的分布式训练

133
00:06:53,400 --> 00:06:57,960
大模型训练在技术上需要考虑的问题就更加复杂了

134
00:06:57,960 --> 00:07:01,640
下面将大模型训练技术面临的挑战分为

135
00:07:01,640 --> 00:07:06,160
内存、通讯、计算和调由四个部分分别进行介绍

136
00:07:07,000 --> 00:07:11,000
首先,模型训练无可避免的问题就是内存墙

137
00:07:11,000 --> 00:07:13,360
以鹏城盘古大模型为例子

138
00:07:13,360 --> 00:07:18,000
2000亿的参数内存,占用就消耗了754GB

139
00:07:18,000 --> 00:07:21,960
训练的过程因为会有权重、激活、优化器状态

140
00:07:21,960 --> 00:07:25,360
再加上自动微分所产生的临时变量

141
00:07:25,360 --> 00:07:27,960
需要3500GB的内存

142
00:07:27,960 --> 00:07:30,720
一个大模型训练就需要100多块

143
00:07:30,720 --> 00:07:33,440
具有32个G内存的AI芯片

144
00:07:34,240 --> 00:07:36,800
以ResNet50的一轮迭代为例子

145
00:07:36,800 --> 00:07:38,760
看看内存的占用变化

146
00:07:38,760 --> 00:07:41,680
首先是网络模型开始计算的时候

147
00:07:41,680 --> 00:07:43,400
内存会不断的增加

148
00:07:43,400 --> 00:07:47,240
直到达到峰值1.2GB

149
00:07:47,240 --> 00:07:50,120
峰值过后内存会开始逐渐的释放

150
00:07:50,120 --> 00:07:53,640
内存占用慢慢降回到324Mbps

151
00:07:53,640 --> 00:07:55,120
一个step计算完之后

152
00:07:55,120 --> 00:07:56,920
仍然有一部分内存注流

153
00:07:56,920 --> 00:08:00,320
内存保持在324Mbps里面

154
00:08:00,320 --> 00:08:02,320
从上面的例子可以看出

155
00:08:02,360 --> 00:08:04,280
模型训练对内存的占用

156
00:08:04,280 --> 00:08:06,040
由常驻的静态内存

157
00:08:06,040 --> 00:08:09,400
和动态分配的动态内存两个部分组成

158
00:08:09,400 --> 00:08:10,760
在计算过程当中

159
00:08:10,760 --> 00:08:12,600
是否会遇到内存墙

160
00:08:12,600 --> 00:08:15,840
主要是由动态内存来决定的

161
00:08:15,840 --> 00:08:19,000
下面进一步打开静态和动态内存

162
00:08:19,000 --> 00:08:21,600
静态内存主要是由模型参数

163
00:08:21,600 --> 00:08:23,960
优化器状态信息组成

164
00:08:23,960 --> 00:08:26,320
在一个神经网络的计算过程当中

165
00:08:26,320 --> 00:08:28,320
卷积或者全连接层

166
00:08:28,320 --> 00:08:31,520
都会把权重WM长期保存下来

167
00:08:31,520 --> 00:08:34,440
用作网络的权重参数更新

168
00:08:34,440 --> 00:08:36,520
另外针对诸如Adam的优化器

169
00:08:36,520 --> 00:08:39,200
会存储优化器的动量等信息

170
00:08:40,200 --> 00:08:41,880
静态内存比较好理解

171
00:08:41,880 --> 00:08:44,400
下面主要是看看动态内存

172
00:08:44,400 --> 00:08:46,720
神经网络在计算的过程当中

173
00:08:46,720 --> 00:08:49,040
每一层的向前输出Fo

174
00:08:49,040 --> 00:08:50,320
都需要保存下来

175
00:08:50,320 --> 00:08:52,880
给反向传播的时候所使用

176
00:08:52,880 --> 00:08:55,600
而在神经网络的反向传播过程当中

177
00:08:55,600 --> 00:08:56,400
需要记录下权重的梯度WG

178
00:08:56,400 --> 00:08:58,280
在后续的Step更新当中

179
00:08:58,280 --> 00:08:59,600
给优化器使用

180
00:09:01,480 --> 00:09:02,960
梯度的信息

181
00:09:02,960 --> 00:09:04,960
另外还有梯度的输出OG

182
00:09:04,960 --> 00:09:07,000
同样在反向传播过程当中

183
00:09:07,000 --> 00:09:09,480
作为上一层网络模型的输入

184
00:09:09,480 --> 00:09:11,080
最后还有算子计算的时候的

185
00:09:11,080 --> 00:09:13,080
临时变量OPT等

186
00:09:13,080 --> 00:09:14,440
刚刚提到的OF

187
00:09:14,440 --> 00:09:15,000
OG

188
00:09:15,000 --> 00:09:15,720
WG

189
00:09:15,720 --> 00:09:16,240
OP

190
00:09:16,240 --> 00:09:17,120
积累下来

191
00:09:17,120 --> 00:09:19,280
会引起巨大的内存开销

192
00:09:19,280 --> 00:09:20,760
从RestNet50的例子

193
00:09:20,760 --> 00:09:21,960
可以看到

194
00:09:21,960 --> 00:09:24,480
动态内存是静态内存的三倍

195
00:09:25,360 --> 00:09:26,960
静态内存和动态内存

196
00:09:26,960 --> 00:09:29,160
都可能造成内存墙的问题

197
00:09:29,160 --> 00:09:29,920
相互独立

198
00:09:29,920 --> 00:09:31,320
但是由相互制约

199
00:09:31,320 --> 00:09:33,240
所以必须同时优化

200
00:09:34,840 --> 00:09:36,720
大模型通过模型并行

201
00:09:36,720 --> 00:09:39,640
由水线并行切分到AI集群之后

202
00:09:39,640 --> 00:09:42,160
通讯就会成为主要的性能瓶颈

203
00:09:42,840 --> 00:09:45,040
从模型切分的角度来考虑

204
00:09:45,040 --> 00:09:46,400
大模型再怎么切分

205
00:09:46,400 --> 00:09:48,240
其实还是一个模型

206
00:09:48,240 --> 00:09:50,840
因此模型切分到不同的机器后

207
00:09:50,840 --> 00:09:55,640
仍然需要通讯来对分布在不同的机器的参数进行聚合

208
00:09:56,200 --> 00:09:58,000
从通讯的角度来看

209
00:09:58,000 --> 00:09:59,160
参数聚合

210
00:09:59,200 --> 00:10:01,720
就会对通讯提出了很多要求

211
00:10:01,720 --> 00:10:03,560
例如使用同步的更新策略

212
00:10:03,560 --> 00:10:05,520
还是使用异步的更新策略

213
00:10:05,520 --> 00:10:08,840
或者还是对模型局部的变量进行同步更新呢

214
00:10:09,720 --> 00:10:11,840
另外在网络通讯的角度

215
00:10:11,840 --> 00:10:13,680
由于专用的AI芯片

216
00:10:13,680 --> 00:10:17,120
内存和计算单元ALU之间非常的接近

217
00:10:17,120 --> 00:10:20,160
使得片内的带宽可以非常非常大

218
00:10:20,160 --> 00:10:22,560
而计算速度也可以很快

219
00:10:22,560 --> 00:10:25,080
但是在集讯的网络传输速度

220
00:10:25,080 --> 00:10:28,840
远远不能够匹配专用的AI加速芯片的运算速度

221
00:10:30,120 --> 00:10:31,520
有人就会问了

222
00:10:31,520 --> 00:10:35,080
直接使用更大的带宽不就能解决这些问题吗

223
00:10:35,080 --> 00:10:39,400
随着网络带宽从1Gbps到100Gbps

224
00:10:39,400 --> 00:10:44,160
实际利用率将会从接近100%下降到40%左右

225
00:10:44,160 --> 00:10:45,880
网络带宽的增加

226
00:10:45,880 --> 00:10:48,320
带宽的利用率就会降得越来越低

227
00:10:48,320 --> 00:10:51,240
高带宽所带来的收益就会遇到瓶颈

228
00:10:51,240 --> 00:10:52,880
这个结论告诉

229
00:10:52,880 --> 00:10:54,760
不能随便增加带宽哦

230
00:10:54,760 --> 00:10:56,040
总体而言

231
00:10:56,040 --> 00:10:57,240
通讯过程当中

232
00:10:57,240 --> 00:10:59,480
需要综合考虑到数据参数量

233
00:10:59,480 --> 00:11:00,280
计算量

234
00:11:00,280 --> 00:11:01,200
计算类型

235
00:11:01,200 --> 00:11:02,400
数据样本量

236
00:11:02,400 --> 00:11:04,160
还有集讯的带宽TOP

237
00:11:04,160 --> 00:11:06,360
和通讯策略等不同的因素

238
00:11:06,360 --> 00:11:09,480
才能够设计出比较优的切分策略

239
00:11:09,480 --> 00:11:11,200
最大化的利用通讯效率

240
00:11:11,200 --> 00:11:12,840
提高通讯比

241
00:11:13,880 --> 00:11:15,640
第三点就是性能墙

242
00:11:15,640 --> 00:11:17,800
主要是指计算资源的利用率

243
00:11:18,520 --> 00:11:19,840
随着大模型的提出

244
00:11:19,840 --> 00:11:21,920
对算力的需求更加迫切

245
00:11:21,920 --> 00:11:23,800
理论上在4K的集讯上面

246
00:11:23,800 --> 00:11:26,000
每块卡要是快了一分钟

247
00:11:26,000 --> 00:11:27,600
那可以总体就快了

248
00:11:27,600 --> 00:11:29,480
接近68个小时

249
00:11:29,480 --> 00:11:32,080
大模型会增加对算力的需求

250
00:11:32,080 --> 00:11:33,560
但是随着大模型引入

251
00:11:33,560 --> 00:11:35,920
各项分布式并行技术的同时

252
00:11:35,920 --> 00:11:38,400
反过来又会降低AI芯片的

253
00:11:38,400 --> 00:11:39,480
计算吞吐量

254
00:11:40,520 --> 00:11:43,000
大集群大模型的分布式训练

255
00:11:43,000 --> 00:11:45,800
混合了其实很多种并行的策略

256
00:11:45,800 --> 00:11:47,640
从算子级别上面

257
00:11:47,640 --> 00:11:49,120
如何利用编译技术

258
00:11:49,120 --> 00:11:50,880
对小算子进行融合

259
00:11:50,880 --> 00:11:53,520
如何找到更多的多线程计算策略

260
00:11:53,520 --> 00:11:55,840
是个非常值得探讨的话题

261
00:11:55,880 --> 00:11:57,640
那在计算图上面

262
00:11:57,640 --> 00:11:59,160
如何对整图搜索出

263
00:11:59,160 --> 00:12:02,240
更适合在AI集群的切分子图策略

264
00:12:02,240 --> 00:12:03,960
是个凸优化的问题

265
00:12:04,960 --> 00:12:06,640
到了集群任务上

266
00:12:06,640 --> 00:12:09,600
性能的瓶颈从计算转移到通讯

267
00:12:09,600 --> 00:12:12,600
这个时候需要尽可能的把通讯时延

268
00:12:12,600 --> 00:12:14,160
隐藏在计算里面

269
00:12:14,160 --> 00:12:17,200
是大规模训练的一个核心关注点

270
00:12:17,200 --> 00:12:18,160
因此

271
00:12:18,160 --> 00:12:20,560
性能强不仅仅要求AI芯片的

272
00:12:20,560 --> 00:12:22,080
计算性能足够的彪悍

273
00:12:23,240 --> 00:12:25,040
同时也依赖于AI框架的

274
00:12:25,040 --> 00:12:28,680
大规模分布式训练的运行和调度策略

275
00:12:28,680 --> 00:12:32,560
以及分布式并行等各种优化手段的一个权衡

276
00:12:33,880 --> 00:12:36,840
在数千节点的集群上面进行模型开发

277
00:12:36,840 --> 00:12:38,960
听到就头皮发麻

278
00:12:38,960 --> 00:12:41,640
我平时把Efficient的网络模型魔改

279
00:12:41,640 --> 00:12:43,360
还不一定能够收敛

280
00:12:43,360 --> 00:12:45,520
调一次参数再训练一次

281
00:12:45,520 --> 00:12:48,200
单机多卡一个星期就过去了

282
00:12:48,200 --> 00:12:50,080
要是再大模型训练一个月

283
00:12:50,080 --> 00:12:51,240
遇到Lost跑飞呢

284
00:12:51,240 --> 00:12:52,160
那怎么办呢

285
00:12:53,160 --> 00:12:55,640
所以在数千节点的集群上

286
00:12:55,640 --> 00:12:57,960
需要考虑到提升算法工程师

287
00:12:57,960 --> 00:12:59,960
分布式调试调优的效率

288
00:12:59,960 --> 00:13:01,880
另外还要考虑降低工程师

289
00:13:01,880 --> 00:13:04,880
对大模型进行并行切分的一个难度

290
00:13:05,880 --> 00:13:07,120
除了对人的考虑

291
00:13:07,120 --> 00:13:09,120
还需要对硬件集群的管理

292
00:13:09,120 --> 00:13:10,920
需要保证计算的正确性

293
00:13:10,920 --> 00:13:12,560
性能还有可用性

294
00:13:12,560 --> 00:13:14,600
例如要是有一台机器坏了

295
00:13:14,600 --> 00:13:17,320
如何快速的恢复训练的参数

296
00:13:18,320 --> 00:13:19,960
总结一句话

297
00:13:20,960 --> 00:13:25,240
大模型训练考验的是算法，数据，框架 

298
00:13:25,240 --> 00:13:28,920
资源调度等全栈全流程的综合能力

299
00:13:28,920 --> 00:13:31,240
最后希望MindSpore能够在大模型

300
00:13:31,240 --> 00:13:33,160
继续引领业界

301
00:13:33,160 --> 00:13:33,760
好了

302
00:13:33,760 --> 00:13:34,560
谢谢各位

303
00:13:34,560 --> 00:13:35,760
拜了个拜

