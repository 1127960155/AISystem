1
00:00:00,000 --> 00:00:04,600
Morning

2
00:00:04,640 --> 00:00:06,360
Hello 大家好

3
00:00:06,360 --> 00:00:09,680
现在已经晚上凌晨1点多了

4
00:00:09,720 --> 00:00:13,960
今天我们还是继续我们分布式集群系列里面的

5
00:00:13,960 --> 00:00:16,280
AI集群架构

6
00:00:16,280 --> 00:00:20,520
这个AI集群架构其实说白了就是我们的PS架构

7
00:00:20,680 --> 00:00:22,200
在上一节内容里面

8
00:00:22,200 --> 00:00:24,600
我们简单的提了一下AI集群

9
00:00:24,600 --> 00:00:25,160
大模型

10
00:00:25,200 --> 00:00:26,720
还有分布式训练系统

11
00:00:26,920 --> 00:00:29,520
其实在上一节课的内容里面

12
00:00:29,760 --> 00:00:32,440
纯熟吹牛逼的作用

13
00:00:32,440 --> 00:00:34,480
它没有实际性的影响

14
00:00:38,160 --> 00:00:39,760
在参数服务器里面

15
00:00:39,760 --> 00:00:42,640
我将会给大家一起去分享几个知识

16
00:00:42,640 --> 00:00:45,720
这几个知识点还是有点小故事在里面的

17
00:00:45,760 --> 00:00:48,080
第一个就是参数服务器的一个模式

18
00:00:48,080 --> 00:00:49,520
就是我们的PS架构

19
00:00:49,560 --> 00:00:52,120
有了网络拓哺的架构之后

20
00:00:52,320 --> 00:00:54,640
我们很重要的就是对我们的数据

21
00:00:54,640 --> 00:00:58,400
就是我们的深度学习的权重模型参数

22
00:00:58,520 --> 00:01:01,320
进行同步或者异步的一个并行更新

23
00:01:01,680 --> 00:01:05,600
最后就提出了一个环同步win-overduce的算法

24
00:01:06,080 --> 00:01:09,080
这个算法是我们的友商百度去发明的

25
00:01:09,120 --> 00:01:10,040
虽然是友商

26
00:01:10,040 --> 00:01:11,840
但是这个算法已经同步的

27
00:01:11,840 --> 00:01:13,280
落入到了Tensor 4

28
00:01:13,280 --> 00:01:14,800
Python全有Mansport里面

29
00:01:14,840 --> 00:01:18,480
所以这个还是很佩服我们的友商所做的一些贡献

30
00:01:24,640 --> 00:01:27,640
下面我们来重新的去看一下我们的分布式系统

31
00:01:27,640 --> 00:01:29,600
就是整个分布式的整体架构

32
00:01:29,960 --> 00:01:32,960
从上往下就是我们有一些大模型的算法

33
00:01:33,200 --> 00:01:35,320
大模型的算法最近也是特别的多

34
00:01:35,320 --> 00:01:36,600
我就不在这里累数了

35
00:01:36,600 --> 00:01:38,840
从Transformer到现在的MOE

36
00:01:38,880 --> 00:01:41,880
从百亿十亿千亿到万亿级别的

37
00:01:41,880 --> 00:01:44,040
下面我们今天主要是围绕着

38
00:01:44,040 --> 00:01:46,360
我们分布式训练通讯与协调

39
00:01:46,360 --> 00:01:47,960
下面这个功能

40
00:01:47,960 --> 00:01:50,240
最主要的是围绕着我们的通讯

41
00:01:50,240 --> 00:01:51,760
我们的通讯的top

42
00:01:51,760 --> 00:01:53,120
今天最主要的内容

43
00:01:53,200 --> 00:01:55,800
就是提升计算数率里面的4倍数

44
00:01:56,120 --> 00:01:57,560
4倍数的越多

45
00:01:57,560 --> 00:01:59,720
不代表我们的计算数率越高

46
00:02:00,560 --> 00:02:02,360
4倍数多到一定的程度

47
00:02:02,360 --> 00:02:04,200
可能我们的计算数率就上不去了

48
00:02:04,200 --> 00:02:06,560
所以我们需要通过优化我们的网络

49
00:02:06,560 --> 00:02:08,400
或者服务器的一个架构模式

50
00:02:08,400 --> 00:02:10,560
来去提升我们的计算数率

51
00:02:11,800 --> 00:02:14,360
下面我们来理理一个概念

52
00:02:14,520 --> 00:02:17,480
就是分布式架构的一个参数服务器

53
00:02:18,000 --> 00:02:19,280
谈到参数服务器

54
00:02:19,640 --> 00:02:22,480
最开始或者最始终拥指的一篇文章

55
00:02:22,600 --> 00:02:23,440
就是李木

56
00:02:24,360 --> 00:02:26,960
亚马逊首席科学家的这篇文章

57
00:02:26,960 --> 00:02:30,560
Skylnk distribute machine learning with the parameter surface

58
00:02:31,160 --> 00:02:33,800
parameter surface就是我们的参数服务器

59
00:02:33,800 --> 00:02:37,680
这里面就很好的去定义了什么为字参数服务器

60
00:02:38,040 --> 00:02:42,880
其实去年我还是停留在一个很初始的理解阶段

61
00:02:43,280 --> 00:02:45,600
我一开始以为参数服务器这种模式

62
00:02:45,800 --> 00:02:47,480
是跟集合通讯模式

63
00:02:47,480 --> 00:02:49,760
就是我们的网络top的架构有两种

64
00:02:49,760 --> 00:02:51,000
一种是参数服务器

65
00:02:51,000 --> 00:02:53,120
另外一种是集合通讯

66
00:02:53,840 --> 00:02:56,040
而我们公司的很多的架构师

67
00:02:56,040 --> 00:02:58,120
都是以这种概念去理解的

68
00:02:58,320 --> 00:03:02,120
但是我最近在重新回看李木的这篇文章的时候

69
00:03:02,120 --> 00:03:05,320
又重新的去看了一下李木写的一些博客

70
00:03:05,600 --> 00:03:08,880
于是我对参数服务器又有了新的理解

71
00:03:09,160 --> 00:03:11,280
首先我们看一下左边的图

72
00:03:11,280 --> 00:03:13,320
CPU作为参数服务器

73
00:03:13,320 --> 00:03:15,800
也就是我们PU作为参数服务器

74
00:03:15,800 --> 00:03:18,440
首先PU把一些计算下发下来

75
00:03:18,440 --> 00:03:22,080
就把我们的网络模型给到每一个卡去执行运算

76
00:03:22,240 --> 00:03:22,640
接着

77
00:03:22,960 --> 00:03:25,440
从第二个蓝色的做T度聚合

78
00:03:25,440 --> 00:03:28,440
最后CPU做完T度聚合之后

79
00:03:28,440 --> 00:03:31,360
把所有的参数给到每一个机器

80
00:03:31,360 --> 00:03:32,840
就是绿色的第三步

81
00:03:33,200 --> 00:03:37,760
其实我们还有另外一种就是GPU0作为参数服务器

82
00:03:37,760 --> 00:03:42,480
这一台GPU就这一片GPU作为参数服务器

83
00:03:43,040 --> 00:03:48,120
CPU去下发指令给每一个GPU去计算T度和损失函数

84
00:03:48,600 --> 00:03:50,720
接着我们把所有的数据

85
00:03:50,760 --> 00:03:54,240
都汇集在第一台GPU上面去做一个T度聚合

86
00:03:54,520 --> 00:03:58,640
最后就把我们的参数广播给其他的GPU

87
00:03:59,240 --> 00:04:03,240
第三种就是参数服务器分布在所有的GPU上面

88
00:04:03,400 --> 00:04:04,920
也就是所谓这一台GPU

89
00:04:04,920 --> 00:04:05,640
这一台GPU

90
00:04:05,640 --> 00:04:06,480
这一台GPU

91
00:04:06,680 --> 00:04:07,880
还有第四台GPU

92
00:04:08,080 --> 00:04:10,080
全都是参数服务器

93
00:04:10,200 --> 00:04:12,160
我们可以把参数服务器理解为

94
00:04:12,280 --> 00:04:13,960
它不仅仅是CPU的

95
00:04:14,080 --> 00:04:16,360
而且它可以每一台GPU去做的

96
00:04:16,520 --> 00:04:18,160
于是我们看看具体的计算

97
00:04:18,400 --> 00:04:23,520
CPU下发指令给每一款GPU去做一个前向和反向的运算

98
00:04:24,120 --> 00:04:27,960
接着通过蓝色的模块对它进行T度聚合

99
00:04:28,080 --> 00:04:30,360
最后更新参数并广播

100
00:04:30,800 --> 00:04:33,240
这种方式就是我们分布式并行

101
00:04:33,240 --> 00:04:36,440
还有大模型所采取最常用的一种方式

102
00:04:36,680 --> 00:04:40,760
所以说我们分布式的架构主要是指参数服务器

103
00:04:41,600 --> 00:04:44,080
并没有所谓集合通讯的模式

104
00:04:44,320 --> 00:04:47,800
深度学习分布式服务器里面最流行的一种模式

105
00:04:47,800 --> 00:04:50,720
就是参数服务器模式PS架构

106
00:04:50,880 --> 00:04:54,760
而相对应的应该是CS BS不同的架构

107
00:04:55,320 --> 00:04:59,800
而集合通讯的这种模式是指它们之间的一个通讯的模式

108
00:04:59,920 --> 00:05:04,120
通讯模式是指点对点通讯和集合通讯两种

109
00:05:04,240 --> 00:05:08,680
这是我们上一节讲AI框架分布式功能的时候去讲到的

110
00:05:08,840 --> 00:05:11,600
我们也会在下一节当中详细的去展开

111
00:05:11,760 --> 00:05:14,360
点对点通讯和集合通讯的一个差别

112
00:05:14,480 --> 00:05:19,200
所以这里面参数服务器PS架构是我们很经典的一种模式

113
00:05:19,560 --> 00:05:22,920
包括现在使用到的WinAlwaysDeals大模型分布式训练

114
00:05:22,920 --> 00:05:24,680
都是采用这种模式架构

115
00:05:26,120 --> 00:05:28,120
定义完我们分布式模式架构之后

116
00:05:28,240 --> 00:05:31,120
我们来看看具体的怎么进行同步的

117
00:05:31,480 --> 00:05:35,960
假设我们现在有四款设备Devices1到4

118
00:05:36,080 --> 00:05:39,000
每一款蓝色就代表具体的计算

119
00:05:39,280 --> 00:05:42,280
而中间的白色就代表空载的时间

120
00:05:42,320 --> 00:05:43,640
这个设置里面是空载的

121
00:05:43,640 --> 00:05:44,520
这是空载的

122
00:05:44,520 --> 00:05:45,720
这是空载的时间

123
00:05:46,160 --> 00:05:50,640
我们会有个Communication的时间窗口去对数据进行同步

124
00:05:50,920 --> 00:05:53,280
这里面Devices1计算完之后

125
00:05:53,280 --> 00:05:55,480
它不能马上进行数据同步

126
00:05:55,640 --> 00:05:58,400
它还要等Devices2算完之后

127
00:05:58,400 --> 00:06:01,280
这里面我们会等到所有的设备都算完

128
00:06:01,440 --> 00:06:04,400
然后用最慢的设备作为时间窗

129
00:06:04,400 --> 00:06:06,360
然后对数据进行同步

130
00:06:06,680 --> 00:06:11,480
这种方式优点很明显保证我们的模型的参数都是同步的

131
00:06:11,640 --> 00:06:13,160
但是缺点也非常明显

132
00:06:13,160 --> 00:06:15,080
会造成我们的计算资源

133
00:06:15,240 --> 00:06:17,840
中间这些空的白色的我们叫做bubble

134
00:06:17,840 --> 00:06:18,760
bubble越多

135
00:06:18,760 --> 00:06:21,280
代表我们浪费的计算资源越大

136
00:06:22,000 --> 00:06:24,120
另外一种就是一步并行

137
00:06:24,280 --> 00:06:26,200
一步并行这种方式可以看到

138
00:06:26,200 --> 00:06:28,320
基本上bubble的时间很少

139
00:06:28,320 --> 00:06:30,120
大部分的时间都塞满了

140
00:06:30,400 --> 00:06:32,840
我Devices1做完前向计算之后

141
00:06:33,040 --> 00:06:34,480
我再做反向计算

142
00:06:34,640 --> 00:06:35,760
然后反向计算之后

143
00:06:35,880 --> 00:06:39,000
我就把所有的一些参数同步给另外一个机器

144
00:06:39,120 --> 00:06:41,240
但是这个机器还没有开始运输

145
00:06:41,240 --> 00:06:42,400
还没有开始结束

146
00:06:42,800 --> 00:06:44,440
我会同步给Devices2

147
00:06:44,560 --> 00:06:47,760
但是Devices2还没有计算完第一个反向

148
00:06:47,840 --> 00:06:50,440
所以它就会执行第一个正向的运算了

149
00:06:50,600 --> 00:06:52,800
这个时候就会引起很大的一个问题

150
00:06:52,800 --> 00:06:54,880
就是我的网络模型不收敛

151
00:06:54,880 --> 00:06:56,800
我相同的数据讯完之后

152
00:06:56,800 --> 00:06:59,200
又被后面来的重新冲突了

153
00:06:59,560 --> 00:07:03,480
所以一步并行没办法在工业化场景去使用

154
00:07:03,480 --> 00:07:05,480
这也是一种不成熟的方案

155
00:07:06,000 --> 00:07:09,160
于是就出现了另外一种半同步的方式

156
00:07:09,200 --> 00:07:13,560
这种方式就是动态去调整我们等待的时间窗口

157
00:07:13,720 --> 00:07:15,840
这个就是我们等待的时间窗口

158
00:07:16,000 --> 00:07:18,720
如果Devices3执行的太慢了

159
00:07:18,720 --> 00:07:20,280
那我们就不等Devices3了

160
00:07:20,280 --> 00:07:21,680
直接把它覆盖过去

161
00:07:21,680 --> 00:07:24,600
以大多数Workers能够接受的时间窗口

162
00:07:25,120 --> 00:07:26,040
进行更新

163
00:07:26,320 --> 00:07:29,240
刚才讲的并行的同步和异步

164
00:07:29,520 --> 00:07:31,040
其实是一种概念

165
00:07:31,040 --> 00:07:33,320
实际上我们在网络TOP里面

166
00:07:33,480 --> 00:07:35,480
还是要去解决很多的问题

167
00:07:35,960 --> 00:07:39,040
如何在物理上对各个机器进行同步

168
00:07:39,120 --> 00:07:41,280
这里面我们有一个环同步

169
00:07:41,520 --> 00:07:45,760
GPU跟GPU之间是通过NVLink来进行连接的

170
00:07:45,920 --> 00:07:49,920
而GPU跟GPU之间从第一款GPU到最后一款GPU

171
00:07:49,920 --> 00:07:51,840
它们中间是有个环的

172
00:07:52,120 --> 00:07:52,440
Cut

173
00:07:52,800 --> 00:07:54,720
这里面有八款GPU

174
00:07:54,720 --> 00:07:57,480
从GPU0到GPU7就是八个GPU

175
00:07:57,640 --> 00:07:59,680
我可以通过任何一种方式

176
00:07:59,800 --> 00:08:02,280
进行对任何一个GPU进行连接

177
00:08:02,280 --> 00:08:05,080
这里面我们以GPU0作为例子

178
00:08:05,080 --> 00:08:06,920
它里面有六个NVLink

179
00:08:06,960 --> 00:08:08,560
每个NVLink都有两路

180
00:08:08,560 --> 00:08:10,680
就是可以接收数据的输出

181
00:08:10,680 --> 00:08:12,600
还有数据的输入两者

182
00:08:12,600 --> 00:08:16,400
我们可以看到1、2、3、4、5、6

183
00:08:16,760 --> 00:08:18,440
通过六路的数据

184
00:08:18,840 --> 00:08:20,000
通过六路的数据

185
00:08:20,000 --> 00:08:22,800
我可以跟任何一款GPU进行通讯

186
00:08:23,200 --> 00:08:26,960
我们现在的目的是希望通过最小的链路

187
00:08:26,960 --> 00:08:29,200
然后可以编译了六个GPU

188
00:08:29,320 --> 00:08:31,880
把我们的数据进行一次的同步

189
00:08:32,640 --> 00:08:36,040
这个就是真正的在物理上的环同步

190
00:08:36,120 --> 00:08:38,040
我们希望通过环同步

191
00:08:38,040 --> 00:08:40,800
使得我们所有的GPU变异的更快

192
00:08:41,280 --> 00:08:42,480
这里面举个例子

193
00:08:42,920 --> 00:08:45,120
假设我们现在有八块GPU

194
00:08:45,120 --> 00:08:46,880
如果我们都是双向通讯

195
00:08:46,880 --> 00:08:49,400
它们之间的通讯效率是很慢的

196
00:08:49,640 --> 00:08:51,880
上面就是第一种环的方式

197
00:08:51,880 --> 00:08:53,760
我从第一变异到第二

198
00:08:54,080 --> 00:08:57,360
我从1、2、3、4、5、6、7、8

199
00:08:57,360 --> 00:08:59,080
这么一次环的变异

200
00:08:59,360 --> 00:09:00,560
这是一种方式

201
00:09:00,880 --> 00:09:01,960
第二种方式

202
00:09:01,960 --> 00:09:04,320
现在我就是1、4

203
00:09:05,200 --> 00:09:10,840
1、4、6、3、5、8、2、7、1

204
00:09:11,120 --> 00:09:13,120
这种环同步方式也是可以的

205
00:09:13,120 --> 00:09:14,120
大家可以看到

206
00:09:14,320 --> 00:09:16,320
像刚才第二种环同步方式

207
00:09:16,680 --> 00:09:19,680
基本上我的数据的方向是很明确的

208
00:09:19,680 --> 00:09:21,160
我没有一个反向

209
00:09:21,160 --> 00:09:23,720
我直接从1然后往开始数

210
00:09:23,720 --> 00:09:24,800
就直接到8

211
00:09:24,800 --> 00:09:26,120
然后再重回到1

212
00:09:27,400 --> 00:09:28,560
像这种环同步

213
00:09:28,680 --> 00:09:30,840
我们下面又有了百度的一种

214
00:09:30,880 --> 00:09:32,800
非常著名的环同步的方式

215
00:09:32,800 --> 00:09:34,320
叫做Win or Reduced

216
00:09:34,520 --> 00:09:36,480
我们现在以下面物理Top

217
00:09:36,480 --> 00:09:38,240
来做一个具体的例子

218
00:09:38,240 --> 00:09:41,560
首先每款GPU里面都有两个MV Link

219
00:09:41,560 --> 00:09:43,640
MV Link只是一个单路的

220
00:09:43,640 --> 00:09:45,200
我可以输出数据

221
00:09:45,200 --> 00:09:46,640
另外一个MV Link接口

222
00:09:46,800 --> 00:09:48,400
我只是接收数据

223
00:09:48,640 --> 00:09:49,680
这样就很清楚

224
00:09:49,680 --> 00:09:51,040
每个GPU的一个接口

225
00:09:51,200 --> 00:09:52,400
负责输出数据

226
00:09:52,400 --> 00:09:53,120
另外一个接口

227
00:09:53,320 --> 00:09:55,400
负责输入数据的处理

228
00:09:55,400 --> 00:09:57,280
这样就构成一个环了

229
00:09:57,280 --> 00:10:00,240
在我们部署的国家AI计算中心的服务器

230
00:10:00,240 --> 00:10:03,320
也是以这种环的方式去部署我们的物理机器的

231
00:10:03,920 --> 00:10:07,200
下面我们来讲讲Win or Reduced的算法

232
00:10:07,200 --> 00:10:09,800
Win or Reduced这个算法分成两步

233
00:10:09,800 --> 00:10:11,440
第一步是Scatter Reduced

234
00:10:11,440 --> 00:10:12,840
第二步是Outget

235
00:10:12,840 --> 00:10:15,680
我们现在先来看看第一步Scatter Reduced

236
00:10:16,040 --> 00:10:18,000
首先我们还是跟上面一样

237
00:10:18,000 --> 00:10:19,800
我们现在有5个机器

238
00:10:19,800 --> 00:10:21,960
从0123455个机器

239
00:10:21,960 --> 00:10:24,440
每台机器都有自己的一些数据

240
00:10:24,440 --> 00:10:26,920
那A0 B0 C0 D0 E0

241
00:10:26,920 --> 00:10:29,240
是我们GPU0的一些数据

242
00:10:29,360 --> 00:10:32,120
现在我们需要对数据进行同步

243
00:10:32,120 --> 00:10:34,240
把所有的数据都汇集起来

244
00:10:34,240 --> 00:10:36,960
首先我们会把A0的数据给A1

245
00:10:36,960 --> 00:10:38,720
所以现在A1加A2

246
00:10:38,720 --> 00:10:41,360
然后不断的去轮回叠带

247
00:10:41,720 --> 00:10:44,800
接着我把A1到A2的数据给GPU2

248
00:10:44,800 --> 00:10:46,640
可以看到GPU2已经多了

249
00:10:46,640 --> 00:10:50,200
接下来我还是以一个环的方式不断的去往下叠带

250
00:10:50,200 --> 00:10:51,360
因为这是一个环

251
00:10:51,360 --> 00:10:52,920
数据不断的回流

252
00:10:52,920 --> 00:10:55,960
然后最后我遍逆完一个环之后

253
00:10:56,080 --> 00:11:01,280
可以看到每一块GPU都有一个全数据的一个备份

254
00:11:01,280 --> 00:11:03,960
GPU0就有所有的B的备份

255
00:11:03,960 --> 00:11:06,920
GPU1就有所有C数据的备份

256
00:11:06,920 --> 00:11:07,880
以此类推

257
00:11:07,880 --> 00:11:10,600
GPU4就有所有A的数据的备份

258
00:11:10,600 --> 00:11:14,280
接着我希望把所有的数据都广播

259
00:11:14,280 --> 00:11:15,240
其他GPU

260
00:11:15,240 --> 00:11:16,840
其他设备

261
00:11:16,840 --> 00:11:18,960
接着我可能执行第二步

262
00:11:19,760 --> 00:11:20,800
就是Orgator

263
00:11:20,800 --> 00:11:22,360
Orgator比较简单

264
00:11:22,360 --> 00:11:23,960
还是根据我们的环

265
00:11:23,960 --> 00:11:25,240
环执行一遍

266
00:11:25,320 --> 00:11:27,400
我把数据重新给亏他

267
00:11:27,400 --> 00:11:28,800
做一个广播

268
00:11:28,800 --> 00:11:30,160
可以看到第一次广播

269
00:11:30,160 --> 00:11:32,960
我已经有了两个全列的数据

270
00:11:32,960 --> 00:11:33,920
第三次广播

271
00:11:33,920 --> 00:11:35,880
我已经有了更多的数据

272
00:11:35,880 --> 00:11:38,440
同样的执行完一个环之后

273
00:11:38,440 --> 00:11:42,240
我所有的GPU已经拥有了所有数据的备份

274
00:11:42,240 --> 00:11:46,280
这种WinAllReduce的方式只需要遍逆两次环

275
00:11:46,280 --> 00:11:50,440
所有的GPU之间就已经完成了所有的通信了

276
00:11:50,440 --> 00:11:51,360
像这种方式

277
00:11:51,360 --> 00:11:54,520
下面这个图就是WinAllReduce这个论文里面

278
00:11:54,560 --> 00:11:55,800
做的一个实验

279
00:11:55,800 --> 00:11:59,600
他用一个3E规模的NLP模型

280
00:11:59,600 --> 00:12:01,800
然后去做一个检测

281
00:12:01,800 --> 00:12:03,520
随着GPU数量的增加

282
00:12:03,520 --> 00:12:05,560
我们每秒处理样本的数量

283
00:12:05,560 --> 00:12:07,480
其实是呈线性的关系的

284
00:12:07,480 --> 00:12:08,600
那呈线性的关系

285
00:12:08,600 --> 00:12:11,840
就是我们的加速比非常的接近线性

286
00:12:11,840 --> 00:12:13,440
这个现象是非常好的

287
00:12:13,440 --> 00:12:16,720
因为很有可能随着我们的GPU的数量的增加

288
00:12:16,720 --> 00:12:18,800
我们的加速比并没有增加

289
00:12:18,800 --> 00:12:20,440
反而是下降的

290
00:12:20,440 --> 00:12:21,840
这个趋势是很明显的

291
00:12:23,240 --> 00:12:24,040
可以看到了

292
00:12:24,040 --> 00:12:26,960
刚才我们讲完WinAllReduce这个算法

293
00:12:26,960 --> 00:12:32,520
参数服务器实际上是分布在每一块GPU上面的

294
00:12:32,520 --> 00:12:35,480
所以我们说参数服务器是深度学习里面

295
00:12:35,480 --> 00:12:39,120
最重要或者现在最常用的一种服务器加构模式

296
00:12:39,880 --> 00:12:41,760
我们今天的课程比较简单

297
00:12:41,760 --> 00:12:42,680
我们来总结一下

298
00:12:42,680 --> 00:12:44,440
就是大规模分布式训练里面

299
00:12:44,440 --> 00:12:46,640
就会用到一个PS的架构

300
00:12:47,000 --> 00:12:49,120
参数服务器分布在多个GPU

301
00:12:49,120 --> 00:12:52,600
是PS的一种特殊的形态或者特殊的架构

302
00:12:52,760 --> 00:12:53,960
在PS架构下面

303
00:12:53,960 --> 00:12:56,760
我们通过集合通讯来实现缓同步

304
00:12:56,760 --> 00:12:59,400
从而同步所有GPU里面的参数

305
00:12:59,400 --> 00:13:03,680
而WinAllReduce又是非常经典的一种同步的算法

306
00:13:04,920 --> 00:13:06,520
在这么累赘的环境下

307
00:13:06,520 --> 00:13:08,920
做到经常更新实属不易

308
00:13:08,920 --> 00:13:11,240
非常欢迎大家对我一键三连

309
00:13:11,600 --> 00:13:12,360
谢谢各位

310
00:13:12,360 --> 00:13:13,320
白了个掰

