1
00:00:00,000 --> 00:00:07,775
嗨,大家好,我是ZOMI

2
00:00:07,775 --> 00:00:10,300
今天我们来到分布式训练系列里面的

3
00:00:10,300 --> 00:00:11,520
大模型算法结构

4
00:00:11,520 --> 00:00:14,150
那聊到大模型的算法结构

5
00:00:14,150 --> 00:00:15,850
我们主要是去看看

6
00:00:15,850 --> 00:00:18,400
大模型算法的一个整体的发展

7
00:00:18,400 --> 00:00:20,975
从没有到有,从小到大

8
00:00:20,975 --> 00:00:23,575
然后呢,我想到那个耶来

9
00:00:23,575 --> 00:00:26,375
接着我们来看看大模型算法的 

10
00:00:26,375 --> 00:00:29,200
最重要的或者最著名的两个结构。

11
00:00:29,200 --> 00:00:31,275
第一个呢,就是我们现在基本上

12
00:00:31,275 --> 00:00:34,240
经常会用到的Transformer这个结构

13
00:00:34,240 --> 00:00:35,375
那第二个呢

14
00:00:35,375 --> 00:00:37,700
我们去看看能够让我们的网络模型 

15
00:00:37,700 --> 00:00:39,520
上万亿规模的MOE结构

16
00:00:40,160 --> 00:00:42,625
可以看到网络模型的规模越大

17
00:00:42,625 --> 00:00:44,000
一台机器是放不下。

18
00:00:44,000 --> 00:00:45,825
所以面向大模型呢,

19
00:00:45,825 --> 00:00:48,240
我们要进行大规模分布式训练。

20
00:00:48,240 --> 00:00:52,080
但是呢,今天我们主要是聊聊大模型的一些结构。

21
00:00:52,160 --> 00:00:54,300
而在后面的分享里面呢,

22
00:00:54,300 --> 00:00:57,775
我们才去看看怎么把这些模型并行的切分到 

23
00:00:57,775 --> 00:01:00,720
不同的机器上面去做一个训练的加速

24
00:01:00,720 --> 00:01:03,275
从2011年到2022年

25
00:01:03,275 --> 00:01:06,720
我们的模型的参数量其实是不断的增加的

26
00:01:06,720 --> 00:01:09,400
而在2016年到2017年的时候

27
00:01:09,400 --> 00:01:11,875
Transformer的出现呢

28
00:01:11,875 --> 00:01:13,920
就形成了大模型红色的这条线

29
00:01:13,920 --> 00:01:17,520
相当于这大模型里面我们已经有了一个断层

30
00:01:17,520 --> 00:01:19,760
那大模型对我们来说意味着什么

31
00:01:19,920 --> 00:01:23,440
大模型又能帮助我们解决哪些问题呢

32
00:01:23,440 --> 00:01:25,440
我们看看左边的三个

33
00:01:25,440 --> 00:01:28,480
首先呢,大模型需要大量的数据

34
00:01:28,480 --> 00:01:30,250
既然数据量非常大

35
00:01:30,250 --> 00:01:32,800
就不可能每个数据都进行标注

36
00:01:32,800 --> 00:01:36,225
所以呢,大模型就有了一个很重要的工作

37
00:01:36,225 --> 00:01:39,920
就是引入了自监督学习或者无监督学习的方法

38
00:01:39,920 --> 00:01:41,850
第二个优点就是大模型

39
00:01:41,850 --> 00:01:44,160
我们可以看到为什么要叫大

40
00:01:44,160 --> 00:01:46,675
因为模型的参数量非常多

41
00:01:46,675 --> 00:01:47,680
参数量多

42
00:01:47,840 --> 00:01:51,840
网络模型的精度就有了进一步的突破

43
00:01:51,840 --> 00:01:56,400
第三个就是大模型可以解决很多下游任务的问题

44
00:01:56,400 --> 00:01:59,750
在Bert和Transformer出现之后

45
00:01:59,750 --> 00:02:03,525
大模型就提供了预训练或者Zero-Shot的这种方式

46
00:02:03,525 --> 00:02:05,120
解决了模型碎片化

47
00:02:05,120 --> 00:02:07,400
我解决十几个NLP的任务

48
00:02:07,400 --> 00:02:10,960
可能只需要用一个大模型就可以解决了

49
00:02:10,960 --> 00:02:13,000
不需要开发十个大模型

50
00:02:13,000 --> 00:02:15,120
每个大模型对应一个任务

51
00:02:16,080 --> 00:02:17,550
下面我们看看

52
00:02:17,550 --> 00:02:19,225
在分布式训练里面

53
00:02:19,225 --> 00:02:21,520
我们要解决训练耗时的问题

54
00:02:21,520 --> 00:02:25,600
可能会跟训练的规模还有单步的计算量相关 

55
00:02:25,600 --> 00:02:29,200
单步的计算量又跟我们的网络模型相关

56
00:02:29,200 --> 00:02:32,240
这几天我看到一些非常派言听闻的标题

57
00:02:32,240 --> 00:02:36,880
更具将于网络模型和设计和领域相关的网络模型

58
00:02:36,880 --> 00:02:38,425
2022年10月24号 

59
00:02:38,425 --> 00:02:40,775
2022年10月24号的这些标题

60
00:02:40,775 --> 00:02:42,240
我给大家去念一念

61
00:02:42,720 --> 00:02:47,875
第一个就是谷歌Flan-T5诞生了1800种语言

62
00:02:47,875 --> 00:02:50,880
 超大规模微调1800种语言

63
00:02:50,880 --> 00:02:54,150
第二个就是多语言图像描述

64
00:02:54,150 --> 00:02:58,000
最强评估基准,XM3600来了

65
00:02:58,000 --> 00:03:01,680
涵盖36种语言,36种语言

66
00:03:02,160 --> 00:03:07,440
第三个就是小扎亲自演示首个闽南语翻译系统

67
00:03:07,440 --> 00:03:10,425
主攻3000多种无文字语言

68
00:03:10,425 --> 00:03:12,800
3000多种无文字语言

69
00:03:12,800 --> 00:03:15,700
现在的大模型都已经这么牛逼了吗

70
00:03:15,700 --> 00:03:18,400
一下子就解决了这么多下游任务

71
00:03:21,360 --> 00:03:23,175
于是在这一节分享里面

72
00:03:23,175 --> 00:03:27,120
我想给大家去聊一聊大模型的整个结构的演进

73
00:03:27,120 --> 00:03:28,800
是相关结构的演进

74
00:03:28,800 --> 00:03:32,750
我们从一开始的Transformer取代了RNN

75
00:03:32,750 --> 00:03:35,200
让我们迈进了整个大模型的时代

76
00:03:35,200 --> 00:03:38,275
其实有了Transformer之后

77
00:03:38,275 --> 00:03:40,850
我们的模型的规模参数可能还停留在

78
00:03:40,850 --> 00:03:42,880
一个亿或者千万的级别上面

79
00:03:42,880 --> 00:03:45,125
在2017年的时候

80
00:03:45,125 --> 00:03:48,400
谷歌针对Hitton的MOE模型

81
00:03:48,400 --> 00:03:51,200
又提出了稀疏混合专家的结构

82
00:03:51,200 --> 00:03:54,400
让我们的模型量可以进一步的突破了百亿

83
00:03:54,560 --> 00:03:56,975
那像Bert网络模型就是

84
00:03:56,975 --> 00:04:00,480
首个稠密的突破十亿规模的NLP大模型

85
00:04:00,480 --> 00:04:03,975
所以它对大模型的贡献是非常大的

86
00:04:03,975 --> 00:04:06,250
直到GPT-3的出现

87
00:04:06,250 --> 00:04:09,440
一下子刷新了人们对大模型的认知

88
00:04:09,440 --> 00:04:12,650
原来大模型还可以去到千亿规模

89
00:04:12,650 --> 00:04:15,520
千亿规模的参数量是非常大的,

90
00:04:15,520 --> 00:04:18,825
可能一个GDP-3的网络模型的权重

91
00:04:18,825 --> 00:04:20,960
就已经快接近一个G了

92
00:04:21,360 --> 00:04:22,650
在2021年的时候

93
00:04:22,650 --> 00:04:26,000
谷歌又发明了Switch Transformer,

94
00:04:26,000 --> 00:04:30,640
这个Switch Transformer已经突破了首个万亿的大模型 

95
00:04:30,640 --> 00:04:34,175
刚才我们聊到了GTP-3还是千亿

96
00:04:34,175 --> 00:04:37,440
千亿之后又来到了万亿的大模型

97
00:04:37,440 --> 00:04:39,325
这个还是很牛逼的

98
00:04:39,325 --> 00:04:41,360
后来又有了GLaM

99
00:04:42,240 --> 00:04:45,600
在保持相同万亿规模的参数量的时候,

100
00:04:45,600 --> 00:04:48,640
谷歌推出了GLaM网络模型

101
00:04:49,040 --> 00:04:52,800
让大规模语言模型进一步提升它的精度

102
00:04:52,800 --> 00:04:56,480
下面我们逐个来展开一下我们的大模型的结构

103
00:04:56,480 --> 00:05:00,640
大模型的参数量是怎么一步一步往上走的

104
00:05:05,920 --> 00:05:07,550
Transformer这篇文章

105
00:05:07,550 --> 00:05:10,400
原文叫做Attention is all you need

106
00:05:10,400 --> 00:05:13,760
就是你只需要注意力机制就行了

107
00:05:13,760 --> 00:05:18,560
这篇文章更多的是一个示例和公式讲的没那么详细

108
00:05:18,560 --> 00:05:22,480
后面我希望用一个简单的图去给大家讲示的

109
00:05:22,480 --> 00:05:25,600
我们现在来粗略的浏览一下这篇文章

110
00:05:25,600 --> 00:05:26,600
首先文章里面这个

111
00:05:26,600 --> 00:05:29,760
就代表我们Transformer的网络结构

112
00:05:29,760 --> 00:05:32,880
网络模型从一开始的Input Embedding

113
00:05:32,880 --> 00:05:34,960
然后再加一个Position Encoding

114
00:05:34,960 --> 00:05:37,040
然后输给我们的网络模型

115
00:05:37,040 --> 00:05:40,400
输进去的时候第一个会遇到Multi-head Attent

116
00:05:40,400 --> 00:05:40,800
ion,

117
00:05:40,800 --> 00:05:44,480
就是多头的注意力,接着有一个Normalize,

118
00:05:44,480 --> 00:05:46,240
然后再做一个FeedForward

119
00:05:46,240 --> 00:05:49,120
然后就传给我们的Encorder层

120
00:05:49,120 --> 00:05:51,440
我们可以理解为左边的是Encorder

121
00:05:51,440 --> 00:05:54,000
右边的是Decoder的这种模式

122
00:05:54,000 --> 00:05:58,720
3.2节开始就去讲讲Attention的机制

123
00:05:58,720 --> 00:06:02,880
这里面就通过QKV去实现我们Attention的机制

124
00:06:02,880 --> 00:06:06,560
3.3就去讲讲FeedForward到底是个什么东西

125
00:06:06,560 --> 00:06:11,440
最后可能3.4,3.5都是去拼接我们整个网络模型的

126
00:06:11,520 --> 00:06:14,875
第五节的内容就去讲讲Transformer的

127
00:06:14,875 --> 00:06:16,720
这个结构具体是怎么去训练,

128
00:06:16,720 --> 00:06:21,280
到了第六节的内容就开始真正的实验的环节部分

129
00:06:21,280 --> 00:06:24,400
针对不同的层,不同的结构,不同的入参,

130
00:06:24,400 --> 00:06:28,080
作者都做了大量的应用实践的对比

131
00:06:28,080 --> 00:06:30,640
因为文章是在2017年

132
00:06:30,640 --> 00:06:34,720
那个时候NLP下游用户不是说非常的丰富,

133
00:06:34,720 --> 00:06:37,000
所以作者就用了两个简单的评价指标

134
00:06:37,000 --> 00:06:40,880
去评估Transformer的网络模型到底好还是不好

135
00:06:40,960 --> 00:06:43,150
现在回到我们的slide里面

136
00:06:43,150 --> 00:06:45,360
刚才那个图我已经简单的讲了一讲

137
00:06:45,360 --> 00:06:46,475
现在把它横的去看

138
00:06:46,475 --> 00:06:48,900
Transformer的结构其实最重要的

139
00:06:48,900 --> 00:06:50,960
 就是我们Attention机制

140
00:06:50,960 --> 00:06:52,750
就是这个Attention机制

141
00:06:52,750 --> 00:06:54,640
所有东西都离不开Attention 

142
00:06:54,640 --> 00:06:57,375
Feed Forward这个更像于FF层

143
00:06:57,375 --> 00:07:00,320
我们简单的称它为前馈神经网络就可以了

144
00:07:00,320 --> 00:07:03,360
像这种线性的Softmax都是原有的

145
00:07:03,360 --> 00:07:05,760
而最重要的就是我们Attention

146
00:07:06,240 --> 00:07:08,950
实际上Transformer这个网络模型

147
00:07:08,950 --> 00:07:11,830
刚才只是其中一个Encoder 

148
00:07:11,830 --> 00:07:13,440
或者Decoder一个具体的展开形态

149
00:07:13,440 --> 00:07:16,750
那它的网络模型里面可能会有7层Encoder

150
00:07:16,750 --> 00:07:18,880
 然后再加7层Decoder

151
00:07:18,880 --> 00:07:21,075
最后输入可能是中文

152
00:07:21,075 --> 00:07:23,280
输出可能是英文的翻译

153
00:07:23,280 --> 00:07:26,880
这么一种方式去组成我们Transformer的结构

154
00:07:26,880 --> 00:07:29,040
在17年提出Transformer的时候

155
00:07:29,040 --> 00:07:32,550
它的目的是取代RNN和LSTM

156
00:07:32,550 --> 00:07:36,960
去解决梯度爆炸、梯度消失和长序列的问题

157
00:07:36,960 --> 00:07:39,920
那个时候并没有出现预训练模型

158
00:07:39,920 --> 00:07:42,325
所以网络模型的输入和输出

159
00:07:42,325 --> 00:07:46,320
训练的数据仍然还是使用自监督学习的方式

160
00:07:46,320 --> 00:07:48,025
这就是我的输入和输出

161
00:07:48,025 --> 00:07:50,800
都是人工的进行校准对比标注过的

162
00:07:50,800 --> 00:07:53,075
回到论文的图里面

163
00:07:53,075 --> 00:07:55,025
左边的个我们叫做Encoder,

164
00:07:55,025 --> 00:07:55,920
也就是编码的

165
00:07:55,920 --> 00:07:57,625
右边我们叫做Decoder

166
00:07:57,625 --> 00:07:58,400
反编码的

167
00:07:58,400 --> 00:08:00,700
左边的这个假设输入的是中文

168
00:08:00,700 --> 00:08:04,640
 我们把它经过层层的Encoder之后得到一个向量

169
00:08:04,640 --> 00:08:07,840
那这个向量又经过层层的Decoder之后

170
00:08:07,840 --> 00:08:09,550
反解析成为英文

171
00:08:09,550 --> 00:08:11,275
我们学习中间的参数

172
00:08:11,275 --> 00:08:13,400
使得我们的输入是中文

173
00:08:13,400 --> 00:08:14,800
输出可以是英文

174
00:08:14,800 --> 00:08:18,640
Attention模块最重要的就是QKV三个矩阵

175
00:08:18,640 --> 00:08:21,840
假设我们现在的任务是查询向量Q

176
00:08:21,840 --> 00:08:23,550
就是我们的这个Q

177
00:08:23,550 --> 00:08:26,960
然后我们去计算Q跟各个K之间的相似度

178
00:08:26,960 --> 00:08:28,190
就是我会计算

179
00:08:28,190 --> 00:08:33,120
Q1跟K1、K2、K3、K4之间的相似度

180
00:08:33,120 --> 00:08:35,050
而每个相似度都有一个值

181
00:08:35,050 --> 00:08:37,680
我们叫做V,就是value

182
00:08:37,680 --> 00:08:39,750
所以这里面有QKV

183
00:08:39,750 --> 00:08:41,800
然后Q2我们会查询

184
00:08:41,800 --> 00:08:45,840
它跟K1、K2、K3、K4之间的相似度

185
00:08:46,240 --> 00:08:49,475
得到每个Key对应value的权重系数

186
00:08:49,475 --> 00:08:52,250
然后对value进行加权求和

187
00:08:52,250 --> 00:08:54,880
得到最终Attention的值

188
00:08:54,880 --> 00:08:56,775
所以我们就会有QKV

189
00:08:56,775 --> 00:08:58,640
三个矩阵三个权重向量

190
00:08:58,640 --> 00:09:01,680
那下面我们来看看具体的一个形式

191
00:09:01,680 --> 00:09:06,080
我们首先去计算Q跟K的一个相似度

192
00:09:06,805 --> 00:09:10,150
然后去计算Q1跟K2的一个相似度A12

193
00:09:10,475 --> 00:09:12,960
然后再计算成A13、A14

194
00:09:13,600 --> 00:09:16,880
最后对我们的所有数据做一个softmax的处理

195
00:09:16,880 --> 00:09:18,400
也就是在这一层里面

196
00:09:18,400 --> 00:09:20,400
进行一个softmax的计算

197
00:09:20,400 --> 00:09:22,825
通过反向梯度传播去学习

198
00:09:22,825 --> 00:09:25,920
QKV之间的一个映射的关系

199
00:09:25,920 --> 00:09:29,600
那我们刚才聊的只是一个Attention的机制

200
00:09:29,600 --> 00:09:33,400
实际上Transformer里面叫做MultiHeadAttention

201
00:09:33,400 --> 00:09:34,560
就是多头注意力机制

202
00:09:34,560 --> 00:09:37,600
假设我们现在的多头有两个Head

203
00:09:37,600 --> 00:09:39,275
这个Head的数量是

204
00:09:39,275 --> 00:09:42,640
我们通过前面输进去网络模型之前的进行配置的

205
00:09:42,640 --> 00:09:44,150
假设我两个Head

206
00:09:44,150 --> 00:09:45,840
我可以把这两个做一个并列 

207
00:09:45,840 --> 00:09:48,480
实际上在系统里面为了加速我们的运算

208
00:09:48,480 --> 00:09:51,400
可能我会把第一个Q分成两个Q

209
00:09:51,400 --> 00:09:53,200
把K分成两个K

210
00:09:53,200 --> 00:09:55,475
然后计算的公式和计算的逻辑

211
00:09:55,475 --> 00:09:57,680
都跟刚才所描述的一样

212
00:09:57,680 --> 00:10:00,560
那我们就得到了两个Head的计算方式

213
00:10:00,560 --> 00:10:04,125
通过这种方式我们可以更好的对Transformer

214
00:10:04,125 --> 00:10:06,560
这个算子进行并行的操作和并行的计算,

215
00:10:06,560 --> 00:10:09,200
使得我们执行的时候跑得更快

216
00:10:09,520 --> 00:10:11,375
Transformer的出现其实是为了解决

217
00:10:11,375 --> 00:10:13,280
Sequence to Sequence的问题

218
00:10:13,280 --> 00:10:15,200
然后用Attention

219
00:10:15,200 --> 00:10:18,400
就是注意力的结构去代替我们的LSTM

220
00:10:18,400 --> 00:10:20,375
那使用这种网络模型的结构

221
00:10:20,375 --> 00:10:22,960
具体对我们带来有什么好处和结果呢

222
00:10:22,960 --> 00:10:24,775
可以看到实际上刚才我们看到的

223
00:10:24,775 --> 00:10:27,360
只是一层Transformer的结构,

224
00:10:27,360 --> 00:10:29,520
每一层还有很多小算子

225
00:10:29,520 --> 00:10:32,325
于是Transformer的结构就使得

226
00:10:32,325 --> 00:10:34,640
我们每一层的计算复杂度就变得更优

227
00:10:35,040 --> 00:10:39,120
第二个就是我们不需要像LSTM一样有很多的gate

228
00:10:39,120 --> 00:10:41,760
有我们的输入门、移往门、输出门,

229
00:10:41,760 --> 00:10:44,560
直接用点乘的结果去进行计算

230
00:10:44,560 --> 00:10:46,050
就是我们的QKV

231
00:10:46,050 --> 00:10:48,240
可以通过矩阵层去进行计算

232
00:10:48,240 --> 00:10:51,840
那第三个就是模型更具有解释性

233
00:10:51,840 --> 00:10:53,425
因为在LSTM里面

234
00:10:53,425 --> 00:10:55,680
我们对长序列进行处理的时候

235
00:10:55,680 --> 00:10:57,920
对网络模型的解析是很难的

236
00:10:57,920 --> 00:11:00,575
网络模型在序列的传播当中

237
00:11:00,575 --> 00:11:02,240
丢失了非常多的信息

238
00:11:02,480 --> 00:11:06,075
第四个就是解决了序列很长的时候所引发

239
00:11:06,075 --> 00:11:07,520
的一系列的问题,

240
00:11:07,520 --> 00:11:11,600
但是带来的就是我们的网络模型急剧的膨胀

241
00:11:11,600 --> 00:11:13,600
当时候在17年的时候

242
00:11:13,600 --> 00:11:17,520
我们觉得网络模型的数量膨胀是个很严重的问题

243
00:11:17,520 --> 00:11:19,600
就像出现了ResNet50之后呢

244
00:11:19,600 --> 00:11:22,240
人们希望出现一种Mobinet

245
00:11:22,240 --> 00:11:25,920
就像出现了ResNet50、ResNet101这种网络模型

246
00:11:25,920 --> 00:11:27,840
这种精度已经很好的网络模型

247
00:11:27,840 --> 00:11:29,120
但是人们还不够

248
00:11:29,120 --> 00:11:32,160
希望网络模型的参数量越小越好 

249
00:11:32,160 --> 00:11:34,720
于是谷歌就研究了Mobinet一样

250
00:11:34,720 --> 00:11:37,280
当时候觉得网络模型的参数量大

251
00:11:37,280 --> 00:11:38,320
不是个好事

252
00:11:38,320 --> 00:11:40,240
大的话我怎么做推理啊

253
00:11:40,240 --> 00:11:42,525
在大模型真正出现之前

254
00:11:42,525 --> 00:11:44,960
其实有两个奠基的工作的

255
00:11:44,960 --> 00:11:47,280
第一个就是我们刚才聊到的Transomer

256
00:11:47,280 --> 00:11:52,240
第二个就是2017年同年稍微晚一点的MOE

257
00:11:52,240 --> 00:11:54,960
同年稍微晚一点的MOE

258
00:11:54,960 --> 00:11:59,440
实际上大模型很多工作都是谷歌去发起的

259
00:11:59,440 --> 00:12:02,080
虽然现在TensorFlow很少人去用了

260
00:12:02,240 --> 00:12:04,640
但是大模型很多相关的工作

261
00:12:04,640 --> 00:12:06,160
谷歌都落在TensorFlow了

262
00:12:06,160 --> 00:12:08,150
所以很多时候我们可能可以去看看

263
00:12:08,150 --> 00:12:10,800
TensorFlow 这个框架是怎么设计的,

264
00:12:10,800 --> 00:12:13,680
并行的一些系统或者一些文章概念

265
00:12:13,680 --> 00:12:17,120
还是非常有帮助于我们去理解分布式并行的

266
00:12:17,120 --> 00:12:19,225
那MOE这个网络模型结构

267
00:12:19,225 --> 00:12:23,840
其实是基于1990年Hitton提出的Mixture of Exports

268
00:12:23,840 --> 00:12:27,120
前面就加了一个定语叫做稀疏的门控

269
00:12:27,120 --> 00:12:31,600
这篇文章实际上描述的一个稀疏门控的混合专家模型

270
00:12:31,600 --> 00:12:34,160
但是我们大部分都会叫做MOE

271
00:12:34,160 --> 00:12:37,280
就把稀疏gate这个定语先把它去掉

272
00:12:37,280 --> 00:12:40,800
这篇文章一开始去介绍一些以前人的工作

273
00:12:40,800 --> 00:12:44,025
然后再去看看我们的MOE的网络模型的结构

274
00:12:44,025 --> 00:12:45,360
具体长什么样子的,

275
00:12:45,360 --> 00:12:47,600
可以看到里面有非常多的exprot

276
00:12:47,600 --> 00:12:49,840
然后这里面有一个Gate门控

277
00:12:49,840 --> 00:12:51,900
去控制在什么应用场景

278
00:12:51,900 --> 00:12:54,640
什么场合,什么权重的情况下

279
00:12:54,640 --> 00:12:57,040
我们去激活对应的专家

280
00:12:57,040 --> 00:13:00,480
在2.1里面就具体的描述了我的输出

281
00:13:00,960 --> 00:13:02,960
为了保证稀疏性和均衡性

282
00:13:02,960 --> 00:13:05,680
这篇文章就对Softmax进行了一些改造

283
00:13:05,680 --> 00:13:08,560
首先第一个改造就是NoiseTopKGating

284
00:13:08,560 --> 00:13:11,139
Noise这个工作就是StanardNoramlize 

285
00:13:11,139 --> 00:13:13,920
和WNoise加入了一个权重的噪声

286
00:13:13,920 --> 00:13:16,960
使得我们的网络模型训练的时候更加均衡

287
00:13:16,960 --> 00:13:19,680
而不是某个export的权重特别大

288
00:13:19,680 --> 00:13:21,920
某个专家真的是专家

289
00:13:21,920 --> 00:13:24,480
现在很多时候我们在网上经常去吐槽

290
00:13:24,480 --> 00:13:27,120
专家呀专家,求你不要再建议了

291
00:13:27,120 --> 00:13:28,480
就是这个概念

292
00:13:28,560 --> 00:13:31,520
我们要让专家跟专家之间更加均衡

293
00:13:31,520 --> 00:13:35,040
而不是某个专家一直在发表一些错误的言论

294
00:13:39,360 --> 00:13:40,880
实际上我们的专家非常多

295
00:13:40,880 --> 00:13:44,080
我们不可能听所有专家的建议

296
00:13:44,080 --> 00:13:46,800
第二个内容就是提出了Top-K Gating

297
00:13:46,800 --> 00:13:49,440
就是我们这里面的函数KeepTopK

298
00:13:49,440 --> 00:13:52,880
我们保证网络模型学习一定量专家的建议

299
00:13:52,880 --> 00:13:55,280
其他专家的建议在某种情况下

300
00:13:55,280 --> 00:13:56,960
我们可以把它自为负无穷

301
00:13:57,040 --> 00:13:58,880
让他不要再去学习了

302
00:13:58,880 --> 00:14:02,480
在网上就是一些相关的要注意的事项

303
00:14:02,480 --> 00:14:03,600
特别是分布式并行和

304
00:14:03,600 --> 00:14:05,360
我们的BatchSize的设置

305
00:14:05,360 --> 00:14:08,400
第四点就是回到我们刚才讲到的

306
00:14:08,400 --> 00:14:10,880
去平衡各个专家之间的一个作用

307
00:14:10,880 --> 00:14:13,680
到了第五节的内容就是具体的实验

308
00:14:13,680 --> 00:14:16,960
可以看到MOE里面做了非常大量的实验

309
00:14:16,960 --> 00:14:19,680
比Transformer网络模型的实验会更多

310
00:14:19,680 --> 00:14:22,160
而且对比了以前非常多的不同的

311
00:14:22,160 --> 00:14:24,880
关于吸收性或者专家性的相关的工作

312
00:14:25,200 --> 00:14:28,000
这里面比较有意思的就是MOE网络模型

313
00:14:28,000 --> 00:14:29,600
虽然看上去很简单

314
00:14:29,600 --> 00:14:32,080
但是这篇文章里面又附带了很多

315
00:14:32,080 --> 00:14:34,320
附录去介绍具体的计算公式

316
00:14:34,320 --> 00:14:36,160
具体的计算逻辑

317
00:14:36,160 --> 00:14:38,880
并且附上了大量的消融实验

318
00:14:38,880 --> 00:14:40,400
回到我们的PPT里面

319
00:14:40,400 --> 00:14:43,760
实际上MOE它叫做稀疏门控专家混合模型

320
00:14:43,760 --> 00:14:44,800
就是我们刚才讲的

321
00:14:44,800 --> 00:14:47,600
我们去控制哪个专家在某种情况下 

322
00:14:47,600 --> 00:14:48,960
应该怎么去计算

323
00:14:48,960 --> 00:14:51,360
为了保证我们的稀疏性和均衡性

324
00:14:51,360 --> 00:14:53,920
对Softmax进行了一些处理

325
00:14:53,920 --> 00:14:54,675
加入了TopK

326
00:14:54,675 --> 00:14:55,760
 加入了Noised

327
00:14:55,760 --> 00:14:59,600
我们可以看到原来的网络模型是通过一个Softmax的

328
00:14:59,600 --> 00:15:02,400
但是实际上我们是通过Gdelta

329
00:15:02,400 --> 00:15:05,200
就Gdelta实际上就是下面这条公式,

330
00:15:05,200 --> 00:15:07,280
我们把KeytopK加进去了

331
00:15:07,280 --> 00:15:09,520
而且还加入了我们的WNoised

332
00:15:09,520 --> 00:15:12,240
MOE这个E算叫做专家

333
00:15:12,240 --> 00:15:14,560
实际上我们可以把它理解为把大模型

334
00:15:14,560 --> 00:15:16,400
拆分为多个小模型,

335
00:15:16,400 --> 00:15:18,960
每个小模型就是一个专家

336
00:15:18,960 --> 00:15:21,200
对数位一个样本的数量来说

337
00:15:21,280 --> 00:15:24,320
我们并不需要所有的专家都去计算

338
00:15:24,320 --> 00:15:27,040
而是激活一部分的专家就可以了

339
00:15:27,040 --> 00:15:29,760
这样就可以节省我们大量的计算资源

340
00:15:29,760 --> 00:15:31,920
而且我们有多个专家

341
00:15:31,920 --> 00:15:34,960
不同的专家可以处理不同的下游任务

342
00:15:34,960 --> 00:15:36,880
实现网络模型的增长

343
00:15:36,880 --> 00:15:39,520
可以看到在MOE的网络模型实验里面

344
00:15:39,520 --> 00:15:42,320
特别的去强调了我们的网络模型的增长

345
00:15:42,320 --> 00:15:45,360
我们的网络模型的参数量不断的增加的情况下

346
00:15:45,360 --> 00:15:48,400
实际上我们的GPU的计算量并不会很大

347
00:15:48,400 --> 00:15:51,120
因为我们是通过小模型专家去进行计算的

348
00:15:51,120 --> 00:15:53,600
而不是所有的专家都进行计算

349
00:15:53,600 --> 00:15:56,400
这里面就是Keep Top-K所带来的好处

