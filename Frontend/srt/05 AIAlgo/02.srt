1
00:00:00,000 --> 00:00:07,775
嗨,大家好,我是ZOMI
2
00:00:07,775 --> 00:00:11,520
今天来到分布式训练系列里面的大模型算法结构
3
00:00:11,520 --> 00:00:18,400
那聊到大模型的算法结构,主要是去看看大模型算法的一个整体的发展
4
00:00:18,400 --> 00:00:20,975
从没有到有,从小到大
5
00:00:20,975 --> 00:00:23,575
然后呢,我想到那个耶来
6
00:00:23,575 --> 00:00:29,200
接着来看看大模型算法的最重要的或者最著名的两个结构
7
00:00:29,200 --> 00:00:34,240
第一个呢,就是现在基本上经常会用到的Transformer这个结构
8
00:00:34,240 --> 00:00:39,520
那第二个呢,去看看能够让的网络模型上万亿规模的MOE结构
9
00:00:40,160 --> 00:00:44,000
可以看到网络模型的规模越大,一台机器是放不下
10
00:00:44,000 --> 00:00:48,240
所以面向大模型呢,要进行大规模分布式训练
11
00:00:48,240 --> 00:00:52,080
但是呢,今天主要是聊聊大模型的一些结构
12
00:00:52,160 --> 00:00:54,300
而在后面的分享里面呢
13
00:00:54,300 --> 00:01:00,720
才去看看怎么把这些模型并行的切分到 不同的机器上面去做一个训练的加速
14
00:01:00,720 --> 00:01:06,720
从2011年到2022年,的模型的参数量其实是不断的增加的
15
00:01:06,720 --> 00:01:11,875
而在2016年到2017年的时候,Transformer的出现呢
16
00:01:11,875 --> 00:01:13,920
就形成了大模型红色的这条线
17
00:01:13,920 --> 00:01:17,520
相当于这大模型里面已经有了一个断层
18
00:01:17,520 --> 00:01:19,760
那大模型对来说意味着什么?

19
00:01:19,920 --> 00:01:23,440
大模型又能帮助解决哪些问题呢?

20
00:01:23,440 --> 00:01:25,440
看看左边的三个
21
00:01:25,440 --> 00:01:28,480
首先呢,大模型需要大量的数据
22
00:01:28,480 --> 00:01:32,800
既然数据量非常大,就不可能每个数据都进行标注
23
00:01:32,800 --> 00:01:39,920
所以呢,大模型就有了一个很重要的工作,就是引入了自监督学习或者无监督学习的方法
24
00:01:39,920 --> 00:01:44,160
第二个优点就是大模型,可以看到为什么要叫大
25
00:01:44,160 --> 00:01:47,680
因为模型的参数量非常多,参数量多
26
00:01:47,840 --> 00:01:51,840
网络模型的精度就有了进一步的突破
27
00:01:51,840 --> 00:01:56,400
第三个就是大模型可以解决很多下游任务的问题
28
00:01:56,400 --> 00:01:59,750
在Bert和Transformer出现之后
29
00:01:59,750 --> 00:02:05,120
大模型就提供了预训练或者CoT的这种方式,解决了模型碎片化
30
00:02:05,120 --> 00:02:10,960
我解决十几个NLP的任务,可能只需要用一个大模型就可以解决了
31
00:02:10,960 --> 00:02:15,120
不需要开发十个大模型,每个大模型对应一个任务
32
00:02:16,080 --> 00:02:21,520
下面看看,在分布式训练里面,要解决训练耗时的问题
33
00:02:21,520 --> 00:02:25,600
可能会跟训练的规模还有单步的计算量相关
34
00:02:25,600 --> 00:02:29,200
单步的计算量又跟的网络模型相关
35
00:02:29,200 --> 00:02:32,240
这几天我看到一些非常派言听闻的标题
36
00:02:32,240 --> 00:02:36,880
更具将于网络模型和设计和领域相关的网络模型
37
00:02:36,880 --> 00:02:42,240
2022年10月24号,2022年10月24号的这些标题,我给大家去念一念
38
00:02:42,720 --> 00:02:50,880
第一个就是谷歌Flan-T5诞生了1800种语言, 超大规模微调1800种语言
39
00:02:50,880 --> 00:03:01,680
第二个就是多语言图像描述,最强评估基准,XM3600来了, 涵盖36种语言,36种语言
40
00:03:02,160 --> 00:03:07,440
第三个就是小扎亲自演示首个闽南语翻译系统
41
00:03:07,440 --> 00:03:12,800
主攻3000多种无文字语言,3000多种无文字语言
42
00:03:12,800 --> 00:03:18,400
现在的大模型都已经这么牛逼了吗?一下子就解决了这么多下游任务
43
00:03:21,360 --> 00:03:27,120
于是在这一节分享里面,我想给大家去聊一聊大模型的整个结构的演进
44
00:03:27,120 --> 00:03:28,800
是相关结构的演进
45
00:03:28,800 --> 00:03:35,200
从一开始的Transformer取代了RNN,让迈进了整个大模型的时代
46
00:03:35,200 --> 00:03:42,880
其实有了Transformer之后,的模型的规模参数可能还停留在一个亿或者千万的级别上面
47
00:03:42,880 --> 00:03:51,200
在2017年的时候,谷歌针对Hitton的MOE模型又提出了稀疏混合专家的结构
48
00:03:51,200 --> 00:03:54,400
让的模型量可以进一步的突破了百亿
49
00:03:54,560 --> 00:04:00,480
那像Bert网络模型就是首个稠密的突破十亿规模的NLP大模型
50
00:04:00,480 --> 00:04:09,440
所以它对大模型的贡献是非常大的,直到GPT-3的出现一下子刷新了人们对大模型的认知
51
00:04:09,440 --> 00:04:15,520
原来大模型还可以去到千亿规模,千亿规模的参数量是非常大的
52
00:04:15,520 --> 00:04:20,960
可能一个GDP-3的网络模型的权重就已经快接近一个G了
53
00:04:21,360 --> 00:04:26,000
在2021年的时候谷歌又发明了Switch Transformer
54
00:04:26,000 --> 00:04:30,640
这个Switch Transformer已经突破了首个万亿的大模型
55
00:04:30,640 --> 00:04:37,440
刚才聊到了GTP-3还是千亿,千亿之后又来到了万亿的大模型
56
00:04:37,440 --> 00:04:41,360
这个还是很牛逼的,后来又有了GLaM
57
00:04:42,240 --> 00:04:48,640
在保持相同万亿规模的参数量的时候,谷歌推出了GLaM网络模型, 

58
00:04:49,040 --> 00:04:52,800
让大规模语言模型进一步提升它的精度
59
00:04:52,800 --> 00:04:56,480
下面谷歌来展开一下的大模型的结构
60
00:04:56,480 --> 00:05:00,640
大模型的参数量是怎么一步一步往上走的
61
00:05:05,920 --> 00:05:10,400
Transformer这篇文章原文叫做Attention is all you need
62
00:05:10,400 --> 00:05:13,760
就是你只需要注意力机制就行了
63
00:05:13,760 --> 00:05:18,560
这篇文章更多的是一个示例和公式讲的没那么详细
64
00:05:18,560 --> 00:05:22,480
后面我希望用一个简单的图去给大家讲示的
65
00:05:22,480 --> 00:05:25,600
现在来粗略的浏览一下这篇文章
66
00:05:25,600 --> 00:05:29,760
首先文章里面这个就代表Transformer的网络结构
67
00:05:29,760 --> 00:05:32,880
网络模型从一开始的Input Embedding
68
00:05:32,880 --> 00:05:34,960
然后再加一个Position Encoding

69
00:05:34,960 --> 00:05:37,040
然后输给的网络模型
70
00:05:37,040 --> 00:05:40,800
输进去的时候第一个会遇到Multi-head Attention
71
00:05:40,800 --> 00:05:44,480
就是多头的注意力,接着有一个Normalize
72
00:05:44,480 --> 00:05:46,240
然后再做一个FeedForward
73
00:05:46,240 --> 00:05:49,120
然后就传给的Incorder成
74
00:05:49,120 --> 00:05:51,440
可以理解为左边的是Incorder
75
00:05:51,440 --> 00:05:54,000
右边的是Decoder的这种模式
76
00:05:54,000 --> 00:05:58,720
3.2节开始就去讲讲Attention的机制
77
00:05:58,720 --> 00:06:02,880
这里面就通过QKV去实现Attention的机制
78
00:06:02,880 --> 00:06:06,560
3.3就去讲讲FeedForward到底是个什么东西
79
00:06:06,560 --> 00:06:11,440
最后可能3.4,3.5都是去拼接整个网络模型的
80
00:06:11,520 --> 00:06:16,720
第五节的内容就去讲讲Transformer的这个结构具体是怎么去训练
81
00:06:16,720 --> 00:06:21,280
到了第六节的内容就开始真正的实验的环节部分
82
00:06:21,280 --> 00:06:24,400
针对不同的层,不同的结构,不同的入参
83
00:06:24,400 --> 00:06:28,080
作者都做了大量的应用实践的对比
84
00:06:28,080 --> 00:06:30,640
因为文章是在2017年
85
00:06:30,640 --> 00:06:34,720
那个时候NLP下游用户不是说非常的丰富
86
00:06:34,720 --> 00:06:40,880
所以作者就用了两个简单的评价指标去评估Transformer的网络模型到底好还是不好
87
00:06:40,960 --> 00:06:45,360
现在回到的slide里面,刚才那个图我已经简单的讲了一讲
88
00:06:45,360 --> 00:06:50,960
现在把它横的去看,Transformer的结构其实最重要的 就是Attention机制
89
00:06:50,960 --> 00:06:54,640
就是这个Attention机制,所有东西都离不开Attention
90
00:06:54,640 --> 00:07:00,320
Feed Forward这个更像于FF层,简单的称它为前馈神经网络就可以了
91
00:07:00,320 --> 00:07:03,360
像这种线性的Softmax都是原有的
92
00:07:03,360 --> 00:07:05,760
而最重要的就是Attention
93
00:07:06,240 --> 00:07:13,440
实际上Transformer这个网络模型刚才只是其中一个Encoder 或者Decoder一个具体的展开形态
94
00:07:13,440 --> 00:07:18,880
那它的网络模型里面可能会有7层Encoder, 然后再加7层Decoder
95
00:07:18,880 --> 00:07:23,280
最后输入可能是中文,输出可能是英文的翻译
96
00:07:23,280 --> 00:07:26,880
这么一种方式去组成Transformer的结构
97
00:07:26,880 --> 00:07:29,040
在17年提出Transformer的时候
98
00:07:29,040 --> 00:07:36,960
它的目的是取代RNN和LSTM去解决梯度爆炸、梯度消失和长序列的问题
99
00:07:36,960 --> 00:07:39,920
那个时候并没有出现预训练模型
100
00:07:39,920 --> 00:07:46,320
所以网络模型的输入和输出训练的数据仍然还是使用自监督学习的方式
101
00:07:46,320 --> 00:07:50,800
这就是我的输入和输出都是人工的进行校准对比标注过的
102
00:07:50,800 --> 00:07:55,920
回到论文的图里面,左边的这个叫做Encoder,也就是编码的
103
00:07:55,920 --> 00:07:58,400
右边叫做Decoder,反编码的
104
00:07:58,400 --> 00:08:04,640
左边的这个假设输入的是中文, 把它经过层层的Encoder之后得到一个向量
105
00:08:04,640 --> 00:08:07,840
那这个向量又经过层层的Decoder之后
106
00:08:07,840 --> 00:08:09,550
反解析成为英文
107
00:08:09,550 --> 00:08:14,800
学习中间的参数使得的输入是中文,输出可以是英文
108
00:08:14,800 --> 00:08:18,640
Attention模块最重要的就是QKV三个矩阵
109
00:08:18,640 --> 00:08:21,840
假设现在的任务是查询向量Q
110
00:08:21,840 --> 00:08:26,960
就是的这个Q,然后去计算Q跟各个K之间的相似度
111
00:08:26,960 --> 00:08:33,120
就是我会计算Q1跟K1、K2、K3、K4之间的相似度
112
00:08:33,120 --> 00:08:37,680
而每个相似度都有一个值,叫做V,就是value
113
00:08:37,680 --> 00:08:45,840
所以这里面有QKV,然后Q2会查询它跟K1、K2、K3、K4之间的相似度
114
00:08:46,240 --> 00:08:52,250
得到每个Key对应value的权重系数,然后对value进行加全求和
115
00:08:52,250 --> 00:08:54,880
得到最终Attention的值
116
00:08:54,880 --> 00:08:58,640
所以就会有QKV三个矩阵三个权重向量
117
00:08:58,640 --> 00:09:01,680
那下面来看看具体的一个形式
118
00:09:01,680 --> 00:09:06,080
首先去计算Q跟K的一个相似度
119
00:09:06,080 --> 00:09:12,960
然后去计算Q1跟K2的一个相似度A12,然后再计算成A13、A14
120
00:09:13,600 --> 00:09:16,880
最后对的所有数据做一个softmax的处理
121
00:09:16,880 --> 00:09:20,400
也就是在这一层里面进行一个softmax的计算
122
00:09:20,400 --> 00:09:25,920
通过反向梯度传播去学习QKV之间的一个映射的关系
123
00:09:25,920 --> 00:09:29,600
那刚才聊的只是一个Attention的机制
124
00:09:29,600 --> 00:09:34,560
实际上Transformer里面叫做MultiHeadAttention,就是多头注意力机制
125
00:09:34,560 --> 00:09:37,600
假设现在的多头有两个Head
126
00:09:37,600 --> 00:09:42,640
这个Head的数量是通过前面输进去网络模型之前的进行配置的
127
00:09:42,640 --> 00:09:45,840
假设我两个Head,我可以把这两个做一个并列
128
00:09:45,840 --> 00:09:48,480
实际上在系统里面为了加速的运算
129
00:09:48,480 --> 00:09:53,200
可能我会把第一个Q分成两个Q,把K分成两个K
130
00:09:53,200 --> 00:09:57,680
然后计算的公式和计算的逻辑都跟刚才所描述的一样
131
00:09:57,680 --> 00:10:00,560
那就得到了两个Head的计算方式
132
00:10:00,560 --> 00:10:06,560
通过这种方式可以更好的对Transformer这个算子进行并行的操作和并行的计算
133
00:10:06,560 --> 00:10:09,200
使得执行的时候跑得更快
134
00:10:09,520 --> 00:10:13,280
Transformer的出现其实是为了解决Sequence to Sequence的问题
135
00:10:13,280 --> 00:10:18,400
然后用Attention就是注意力的结构去代替的LSTM
136
00:10:18,400 --> 00:10:22,960
那使用这种网络模型的结构具体对带来有什么好处和结果呢?

137
00:10:22,960 --> 00:10:27,360
可以看到实际上刚才看到的只是一层Transformer的结构
138
00:10:27,360 --> 00:10:29,520
每一层还有很多小算子
139
00:10:29,520 --> 00:10:34,640
于是Transformer的结构就使得每一层的计算复杂度就变得更优
140
00:10:35,040 --> 00:10:39,120
第二个就是不需要像LSTM一样有很多的Gate
141
00:10:39,120 --> 00:10:41,760
有的输入门、移往门、输出门
142
00:10:41,760 --> 00:10:44,560
直接用点层的结果去进行计算
143
00:10:44,560 --> 00:10:48,240
就是的QKV可以通过矩阵层去进行计算
144
00:10:48,240 --> 00:10:51,840
那第三个就是模型更具有解释性
145
00:10:51,840 --> 00:10:55,680
因为在LSTM里面对长序列进行处理的时候
146
00:10:55,680 --> 00:10:57,920
对网络模型的解析是很难的
147
00:10:57,920 --> 00:11:02,240
网络模型在序列的传播当中丢失了非常多的信息
148
00:11:02,480 --> 00:11:07,520
第四个就是解决了序列很长的时候所引发的一系列的问题
149
00:11:07,520 --> 00:11:11,600
但是带来的就是的网络模型急剧的膨胀
150
00:11:11,600 --> 00:11:17,520
当时候在17年的时候觉得网络模型的数量膨胀是个很严重的问题
151
00:11:17,520 --> 00:11:19,600
就像出现了ResNet50之后呢
152
00:11:19,600 --> 00:11:22,240
人们希望出现一种Mobinet
153
00:11:22,240 --> 00:11:25,920
就像出现了ResNet50、ResNet101这种网络模型
154
00:11:25,920 --> 00:11:27,840
这种精度已经很好的网络模型
155
00:11:27,840 --> 00:11:29,120
但是人们还不够
156
00:11:29,120 --> 00:11:32,160
希望网络模型的参数量越小越好
157
00:11:32,160 --> 00:11:34,720
于是谷歌就研究了Mobinet一样
158
00:11:34,720 --> 00:11:37,280
当时候觉得网络模型的参数量大
159
00:11:37,280 --> 00:11:38,320
不是个好事
160
00:11:38,320 --> 00:11:40,240
大的话我怎么做推理啊?

161
00:11:40,240 --> 00:11:44,960
在大模型真正出现之前其实有两个奠基的工作的
162
00:11:44,960 --> 00:11:47,280
第一个就是刚才聊到的Transomer
163
00:11:47,280 --> 00:11:52,240
第二个就是2017年同年稍微晚一点的MOE
164
00:11:52,240 --> 00:11:54,960
同年稍微晚一点的MOE
165
00:11:54,960 --> 00:11:59,440
实际上大模型很多工作都是谷歌去发起的
166
00:11:59,440 --> 00:12:02,080
虽然现在TensorFlow很少人去用了
167
00:12:02,240 --> 00:12:04,640
但是大模型很多相关的工作
168
00:12:04,640 --> 00:12:06,160
谷歌都落在TensorFlow了
169
00:12:06,160 --> 00:12:10,800
所以很多时候可能可以去看看TensorFlow 这个框架是怎么设计的
170
00:12:10,800 --> 00:12:13,680
并行的一些系统或者一些文章概念
171
00:12:13,680 --> 00:12:17,120
还是非常有帮助于去理解分布式并行的。 

172
00:12:17,120 --> 00:12:23,840
那MOE这个网络模型结构其实是基于1990年Hitton提出的Mixture of Exports
173
00:12:23,840 --> 00:12:27,120
前面就加了一个定语叫做稀疏的门控
174
00:12:27,120 --> 00:12:31,600
这篇文章实际上描述的一个稀疏门控的混合专家模型
175
00:12:31,600 --> 00:12:34,160
但是大部分都会叫做MOE
176
00:12:34,160 --> 00:12:37,280
就把稀疏gate这个定语先把它去掉
177
00:12:37,280 --> 00:12:40,800
这篇文章一开始去介绍一些以前人的工作
178
00:12:40,800 --> 00:12:45,360
然后再去看看的MOE的网络模型的结构具体长什么样子的
179
00:12:45,360 --> 00:12:47,600
可以看到里面有非常多的exprot
180
00:12:47,600 --> 00:12:49,840
然后这里面有一个Gate门控
181
00:12:49,840 --> 00:12:54,640
去控制在什么应用场景,什么场合,什么权重的情况下
182
00:12:54,640 --> 00:12:57,040
去激活对应的专家
183
00:12:57,040 --> 00:13:00,480
在2.1里面就具体的描述了我的输出
184
00:13:00,960 --> 00:13:02,960
为了保证稀疏性和均衡性
185
00:13:02,960 --> 00:13:05,680
这篇文章就对Softmax进行了一些改造
186
00:13:05,680 --> 00:13:08,560
首先第一个改造就是NoiseTopKGating
187
00:13:08,560 --> 00:13:13,920
Noise这个工作就是StanardNoramlize 和WNoise加入了一个权重的噪声
188
00:13:13,920 --> 00:13:16,960
使得的网络模型训练的时候更加均衡
189
00:13:16,960 --> 00:13:19,680
而不是某个export的权重特别大
190
00:13:19,680 --> 00:13:21,920
某个专家真的是专家
191
00:13:21,920 --> 00:13:24,480
现在很多时候在网上经常去吐槽
192
00:13:24,480 --> 00:13:27,120
专家呀专家,求你不要再建议了
193
00:13:27,120 --> 00:13:28,480
就是这个概念
194
00:13:28,560 --> 00:13:31,520
要让专家跟专家之间更加均衡
195
00:13:31,520 --> 00:13:35,040
而不是某个专家一直在发表一些错误的言论
196
00:13:39,360 --> 00:13:40,880
实际上的专家非常多
197
00:13:40,880 --> 00:13:44,080
不可能听所有专家的建议
198
00:13:44,080 --> 00:13:46,800
第二个内容就是提出了TopKGating
199
00:13:46,800 --> 00:13:49,440
就是这里面的函数KeepTopKate
200
00:13:49,440 --> 00:13:52,880
保证网络模型学习一定量专家的建议
201
00:13:52,880 --> 00:13:55,280
其他专家的建议在某种情况下
202
00:13:55,280 --> 00:13:56,960
可以把它自为负无穷
203
00:13:57,040 --> 00:13:58,880
让他不要再去学习了
204
00:13:58,880 --> 00:14:02,480
在网上就是一些相关的要注意的事项
205
00:14:02,480 --> 00:14:05,360
特别是分布式并行和的BatchSize的设置
206
00:14:05,360 --> 00:14:08,400
第四点就是回到刚才讲到的
207
00:14:08,400 --> 00:14:10,880
去平衡各个专家之间的一个作用
208
00:14:10,880 --> 00:14:13,680
到了第五节的内容就是具体的实验
209
00:14:13,680 --> 00:14:16,960
可以看到MOE里面做了非常大量的实验
210
00:14:16,960 --> 00:14:19,680
比Transformer网络模型的实验会更多
211
00:14:19,680 --> 00:14:22,160
而且对比了以前非常多的不同的

212
00:14:22,160 --> 00:14:24,880
关于吸收性或者专家性的相关的工作
213
00:14:25,200 --> 00:14:28,000
这里面比较有意思的就是MOE网络模型

214
00:14:28,000 --> 00:14:29,600
虽然看上去很简单
215
00:14:29,600 --> 00:14:32,080
但是这篇文章里面又附带了很多

216
00:14:32,080 --> 00:14:34,320
附录去介绍具体的计算公式
217
00:14:34,320 --> 00:14:36,160
具体的计算逻辑
218
00:14:36,160 --> 00:14:38,880
并且附上了大量的消融实验
219
00:14:38,880 --> 00:14:40,400
回到的PPT里面
220
00:14:40,400 --> 00:14:43,760
实际上MOE它叫做稀疏门控专家混合模型
221
00:14:43,760 --> 00:14:44,800
就是刚才讲的
222
00:14:44,800 --> 00:14:47,600
去控制哪个专家在某种情况下

223
00:14:47,600 --> 00:14:48,960
应该怎么去计算
224
00:14:48,960 --> 00:14:51,360
为了保证的稀疏性和均衡性
225
00:14:51,360 --> 00:14:53,920
对Softmax进行了一些处理
226
00:14:53,920 --> 00:14:55,760
加入了TopK,加入了Noised
227
00:14:55,760 --> 00:14:59,600
可以看到原来的网络模型是通过一个Softmax的
228
00:14:59,600 --> 00:15:02,400
但是实际上是通过Gdelta
229
00:15:02,400 --> 00:15:05,200
就Gdelta实际上就是下面这条公式
230
00:15:05,200 --> 00:15:07,280
把KeytopK加进去了
231
00:15:07,280 --> 00:15:09,520
而且还加入了的WNoised
232
00:15:09,520 --> 00:15:12,240
MOE这个E算叫做专家
233
00:15:12,240 --> 00:15:14,560
实际上可以把它理解为把大模型

234
00:15:14,560 --> 00:15:16,400
才分为多个小模型
235
00:15:16,400 --> 00:15:18,960
每个小模型就是一个专家
236
00:15:18,960 --> 00:15:21,200
对数位一个样本的数量来说
237
00:15:21,280 --> 00:15:24,320
并不需要所有的专家都去计算
238
00:15:24,320 --> 00:15:27,040
而是激活一部分的专家就可以了
239
00:15:27,040 --> 00:15:29,760
这样就可以节赏大量的计算资源
240
00:15:29,760 --> 00:15:31,920
而且有多个专家
241
00:15:31,920 --> 00:15:34,960
不同的专家可以处理不同的下游任务
242
00:15:34,960 --> 00:15:36,880
实现网络模型的增长
243
00:15:36,880 --> 00:15:39,520
可以看到在MOE的网络模型实验里面
244
00:15:39,520 --> 00:15:42,320
特别的去强调了的网络模型的增长
245
00:15:42,320 --> 00:15:45,360
的网络模型的参数量不断的增加的情况下
246
00:15:45,360 --> 00:15:48,400
实际上的GPU的计算量并不会很大
247
00:15:48,400 --> 00:15:51,120
因为是通过小模型专家去进行计算的
248
00:15:51,120 --> 00:15:53,600
而不是所有的专家都进行计算
249
00:15:53,600 --> 00:15:56,400
这里面就是KeytopK所带来的好处
