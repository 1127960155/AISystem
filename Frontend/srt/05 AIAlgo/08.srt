1
00:00:04,680 --> 00:00:06,400
Hello 大家好

2
00:00:06,400 --> 00:00:08,760
我们又回来了

3
00:00:08,760 --> 00:00:09,880
然后我是ZMOI

4
00:00:09,880 --> 00:00:13,200
我们今天来聊一聊分布式算法这个系列

5
00:00:13,200 --> 00:00:14,200
那现在呢

6
00:00:14,200 --> 00:00:18,280
我们已经来到了分布式算法里面的最后一个小内容

7
00:00:18,280 --> 00:00:19,920
分布式的大模型算法

8
00:00:19,920 --> 00:00:22,200
我们不会在这里面讲太多

9
00:00:22,200 --> 00:00:26,440
而是重点讲讲几个比较有影响力的SOTA大模型

10
00:00:26,440 --> 00:00:27,840
那首先第一个就是BERT

11
00:00:27,840 --> 00:00:29,560
引起我们的预训练的

12
00:00:29,560 --> 00:00:32,160
第二个就是GTP3

13
00:00:32,160 --> 00:00:33,720
作为一个FewShot Learning

14
00:00:33,720 --> 00:00:36,080
非常之有名的网络模型

15
00:00:36,080 --> 00:00:39,520
我们的模型参数量也是上了千亿规模的

16
00:00:39,520 --> 00:00:42,040
最后来一个上万亿规模参数量的

17
00:00:42,040 --> 00:00:44,920
用了MOE结构的Switch Transformer

18
00:00:45,880 --> 00:00:46,880
Hi 大家好

19
00:00:46,880 --> 00:00:47,840
我是ZOMI

20
00:00:47,840 --> 00:00:53,040
我们又来到了大模型和分布式训练这个系列里面的分享内容

21
00:00:53,040 --> 00:00:57,080
今天我想给大家一起去分享一下大模型的一个算法结构

22
00:00:57,120 --> 00:01:00,280
去看看我们的模型的发展的情况

23
00:01:00,280 --> 00:01:01,920
从我们最熟悉的模型

24
00:01:01,920 --> 00:01:05,360
其实一开始并没有提模型的参数量有多大

25
00:01:05,360 --> 00:01:08,040
一般来说模型的进步有多好而已

26
00:01:08,040 --> 00:01:11,080
后来等我们引入了大模型之后

27
00:01:11,080 --> 00:01:15,040
就开始的去提我们的模型到底是百亿规模的

28
00:01:15,040 --> 00:01:16,280
还是千亿规模的

29
00:01:16,280 --> 00:01:17,960
还是万亿规模的

30
00:01:17,960 --> 00:01:20,400
可以看到网络模型的规模越大

31
00:01:20,400 --> 00:01:21,960
一台机器是放不下

32
00:01:21,960 --> 00:01:23,360
所以面向大模型

33
00:01:23,360 --> 00:01:26,160
我们要进行大规模分布式训练

34
00:01:26,200 --> 00:01:29,960
但是今天我们主要是聊聊大模型的一些结构

35
00:01:29,960 --> 00:01:31,440
而在后面的分享里面

36
00:01:31,440 --> 00:01:34,240
我们才去看看怎么把这些模型

37
00:01:34,240 --> 00:01:36,840
并行的切分到不同的机器上面

38
00:01:36,840 --> 00:01:38,800
去做一个训练的加速

39
00:01:38,800 --> 00:01:42,520
Transformer和MOE实际上是大模型的一个奠基

40
00:01:42,520 --> 00:01:43,800
而真正的大模型

41
00:01:43,800 --> 00:01:45,960
我们从BERT网络模型开始

42
00:01:45,960 --> 00:01:50,080
然后首个突破十亿规模的NLP大模型

43
00:01:50,080 --> 00:01:52,680
它采用的是一个双向编码的方式

44
00:01:52,680 --> 00:01:55,920
来改进我们的整个网络模型的架构

45
00:01:55,920 --> 00:01:59,280
蓝色的这个框实际上是Transformer的一个示例

46
00:01:59,280 --> 00:02:01,760
里面我们可以看到有非常多的连线

47
00:02:01,760 --> 00:02:03,880
每一条连线都向左向右

48
00:02:03,880 --> 00:02:06,080
这里面就是BERT提出的一个概念

49
00:02:06,080 --> 00:02:07,560
双向编码

50
00:02:07,560 --> 00:02:09,920
这里面Transformer带来一个很重要的概念

51
00:02:09,920 --> 00:02:12,400
就是预训练的网络模型

52
00:02:12,400 --> 00:02:16,400
然后进行一些简单的效用任务的微调就可以了

53
00:02:16,400 --> 00:02:19,520
现在我们来看看BERT网络模型这篇论文

54
00:02:19,520 --> 00:02:22,120
我们经常会在一些论文或者博客里面

55
00:02:22,120 --> 00:02:23,920
看到这么一个小玩具

56
00:02:23,920 --> 00:02:27,320
这个呆呆的样子实际上是芝麻街兄弟里面的一员

57
00:02:27,320 --> 00:02:29,040
他的名字叫做BERT

58
00:02:29,040 --> 00:02:30,520
我们看看论文

59
00:02:30,520 --> 00:02:32,840
在论文里面的BERT它其实是

60
00:02:32,840 --> 00:02:36,760
Deep Break Direction Transformer for Language Understanding

61
00:02:36,760 --> 00:02:40,360
而BERT取决于双向Encorder

62
00:02:40,360 --> 00:02:42,120
Representation Transformer

63
00:02:42,120 --> 00:02:45,000
这几个英文单词的首字母大写

64
00:02:45,000 --> 00:02:47,640
这篇论文同样我们简单的来翻一翻

65
00:02:47,640 --> 00:02:49,760
首先是我们的Introduction

66
00:02:49,760 --> 00:02:53,360
接着去理解一下什么叫做预训练网络模型

67
00:02:53,360 --> 00:02:54,920
或者Fine Tuning

68
00:02:54,920 --> 00:02:57,160
接着去讲讲一些相关的工作

69
00:02:57,160 --> 00:02:59,440
第一个就是无监督学习

70
00:02:59,440 --> 00:03:02,480
无监督学习也是BERT里面很重要的一个概念

71
00:03:02,480 --> 00:03:03,920
第二个就是Fine Tuning

72
00:03:03,920 --> 00:03:05,800
就是我们的微调任务

73
00:03:05,800 --> 00:03:07,160
可以看到Pretraining

74
00:03:07,160 --> 00:03:08,560
就是我们的预训练里面

75
00:03:08,560 --> 00:03:11,080
用的是一个无监督的学习的方法

76
00:03:11,080 --> 00:03:13,280
而后面针对不同的效用任务

77
00:03:13,280 --> 00:03:15,720
使用的是一个Find Tuning的工作

78
00:03:15,720 --> 00:03:18,920
在第三步就会详细的去介绍BUT网络模型的

79
00:03:18,920 --> 00:03:21,240
组织结构网络模型的方式

80
00:03:21,280 --> 00:03:24,760
3.1里面就讲了Pretraining BERT里面分为两个text

81
00:03:24,760 --> 00:03:26,760
第一个text就是maxlm

82
00:03:26,760 --> 00:03:29,000
第二个text就是next-sequent predict

83
00:03:29,000 --> 00:03:32,440
就是下一句话或者下一个单词的预测

84
00:03:32,440 --> 00:03:34,840
这个图我们将会在后面进行展开

85
00:03:34,840 --> 00:03:35,560
Experience

86
00:03:35,560 --> 00:03:38,320
Experience主要是基于GLUE这个benchmark

87
00:03:38,320 --> 00:03:41,440
就是GLUE这个下游任务去进行对比的

88
00:03:41,440 --> 00:03:43,840
而BERT网络模型在各个下游任务里

89
00:03:43,840 --> 00:03:46,480
都取得了一个非常SOTA的结果

90
00:03:46,480 --> 00:03:49,960
同样BERT网络模型后面又有非常多的附录

91
00:03:50,000 --> 00:03:53,720
而附录也是非常值得我们深入的去研究这个网络模型

92
00:03:53,720 --> 00:03:55,360
具体长什么样子的

93
00:03:55,360 --> 00:03:57,880
会提供了很多详细的信息

94
00:03:57,880 --> 00:03:59,360
从论文里面我们知道

95
00:03:59,360 --> 00:04:02,000
BERT网络模型是一个双向编码的网络模型

96
00:04:02,000 --> 00:04:05,880
但是这个双向编码实际上它只是一个概念

97
00:04:05,880 --> 00:04:09,320
在transformer的结构还是那个transformer的结构

98
00:04:09,320 --> 00:04:12,640
那我们看看它具体双向表现在哪里

99
00:04:12,640 --> 00:04:15,640
首先它网络模型分为一个Pretraining和Fine Tuning

100
00:04:15,640 --> 00:04:16,760
FineTuning我们先不管

101
00:04:16,760 --> 00:04:18,200
我们看看Pretraining

102
00:04:18,240 --> 00:04:22,960
数据的输出和输入就取决于我们的网络模型的训练的过程当中

103
00:04:22,960 --> 00:04:26,720
它到底是一个监督学习或者无监督学习的方式

104
00:04:26,720 --> 00:04:29,520
这里面有几个内容可能我们要稍微注意一下的

105
00:04:29,520 --> 00:04:31,360
这一个就是CLS

106
00:04:31,360 --> 00:04:33,040
第二个SEP

107
00:04:33,040 --> 00:04:34,840
第三个就是MASK

108
00:04:34,840 --> 00:04:37,400
SEP和CLS只是一个标志位

109
00:04:37,400 --> 00:04:39,400
标志我是同句子的开头

110
00:04:39,400 --> 00:04:40,560
句子的段距

111
00:04:40,560 --> 00:04:43,360
而MASK这个才是最重要的概念

112
00:04:43,360 --> 00:04:44,920
假设我们现在输一段话

113
00:04:44,920 --> 00:04:46,200
MyDot is cute

114
00:04:46,200 --> 00:04:48,760
那这个Cute我把它MASK就是遮住

115
00:04:48,760 --> 00:04:50,760
然后输进去我们的网络模型

116
00:04:50,760 --> 00:04:52,920
输出的时候我把MyDot is cute

117
00:04:52,920 --> 00:04:54,800
然后同样作为我的输出

118
00:04:54,800 --> 00:04:58,800
然后让我们的神经网络模型去预测这个Cute

119
00:04:58,800 --> 00:05:00,800
从而实现自监督的功能

120
00:05:00,800 --> 00:05:03,680
就我的数据已经不用人工的去标了

121
00:05:03,680 --> 00:05:08,120
让我们的神经网络模型自动的去从我们的数据之间发现它的规律

122
00:05:08,120 --> 00:05:09,960
而双向代表的什么意思呢

123
00:05:09,960 --> 00:05:12,960
假设我去预测是它的前后的关系

124
00:05:13,000 --> 00:05:14,720
前面是Dot还是Cute呢

125
00:05:14,720 --> 00:05:17,240
就是前后左右我都会进行一个预测

126
00:05:17,240 --> 00:05:19,600
假设是我们Transformer的Q

127
00:05:19,600 --> 00:05:21,000
我们的KE可能是Dot

128
00:05:21,000 --> 00:05:21,800
K20是Cute

129
00:05:21,800 --> 00:05:23,520
K30是HE

130
00:05:23,520 --> 00:05:26,720
因为Transformer网络模型QKV的特殊结构

131
00:05:26,720 --> 00:05:28,000
所以通过MASK之后

132
00:05:28,000 --> 00:05:30,960
它就变得有一种双向的概念

133
00:05:30,960 --> 00:05:33,440
所以双向是来自于这

134
00:05:33,440 --> 00:05:39,120
通过输进去BERT网络模型去预测单词到底是什么

135
00:05:39,120 --> 00:05:40,800
同样我们除了MASK单词

136
00:05:40,800 --> 00:05:42,640
我们还可以MASK下一个句子

137
00:05:43,480 --> 00:05:47,960
那第一个任务就是在句子里面随机的去遮住一部分单词

138
00:05:47,960 --> 00:05:51,640
然后去预测这部分单词到底是什么内容

139
00:05:51,640 --> 00:05:54,360
那第二任务就是Next Sequence Predict

140
00:05:54,360 --> 00:05:57,120
就是预测下一个句子是什么

141
00:05:57,960 --> 00:06:02,280
为什么我们说BERT网络模型对于大模型来说非常重要呢

142
00:06:02,840 --> 00:06:06,120
是因为BERT网络模型穿了非常多Transformer的Encoder

143
00:06:06,120 --> 00:06:08,800
它实际上通过超大规模的数据

144
00:06:08,800 --> 00:06:11,080
还有非常大的网络模型结构

145
00:06:11,080 --> 00:06:14,320
并且还用了谷歌的TPU去进行计算的

146
00:06:14,320 --> 00:06:17,920
在11个NLP应用里面都取得了非常好的结果

147
00:06:17,920 --> 00:06:19,760
这里面除了网络模型的大

148
00:06:19,760 --> 00:06:21,440
还用了超大的数据

149
00:06:21,440 --> 00:06:23,600
另外训练还分为了两个阶段

150
00:06:23,600 --> 00:06:26,320
第一个是预训练的阶段和FineTuning的阶段

151
00:06:26,320 --> 00:06:31,760
预训练阶段这个概念把我们的网络模型引入了一个更高规模的规格

152
00:06:33,680 --> 00:06:35,040
这些算法讲的好累

153
00:06:36,160 --> 00:06:37,520
虽然我很努力的在讲

154
00:06:38,800 --> 00:06:41,120
但是可能你仍然还是听不明白

155
00:06:41,120 --> 00:06:42,440
听不明白没关系

156
00:06:42,440 --> 00:06:44,880
其实我更希望大家去阅读原论文

157
00:06:44,880 --> 00:06:48,840
去仔细的去研究一下这个网络模型到底有什么用

158
00:06:48,840 --> 00:06:50,720
具体理念是怎么去实现的

159
00:06:50,720 --> 00:06:53,320
下面我们来到第四个重要的网络模型

160
00:06:53,320 --> 00:06:55,400
就是我们的GPT-3

161
00:06:55,400 --> 00:07:00,520
前面的GPT-1和GPT-2实际上并没有太多的Outstanding的工作

162
00:07:00,520 --> 00:07:06,520
但是当GPT-3的网络模型的参数量都已经上到了接近2100一规模的时候

163
00:07:06,560 --> 00:07:09,600
它带来的是一个全新的语言模型

164
00:07:09,600 --> 00:07:12,760
这个全新的语言模型跟我们刚才的BERT

165
00:07:12,760 --> 00:07:14,920
它需要经过微调不一样

166
00:07:14,920 --> 00:07:16,120
GTP-3

167
00:07:16,120 --> 00:07:18,960
居于刚才的BERT做了一个无监督之外

168
00:07:18,960 --> 00:07:20,760
它还实现了一个Zero Shot

169
00:07:20,760 --> 00:07:22,720
就是我不需要Fine Turning了

170
00:07:22,720 --> 00:07:24,680
我直接进行个zero-shot

171
00:07:24,680 --> 00:07:28,920
然后我的预训练模型直接可以对应到具体的下游任务

172
00:07:28,920 --> 00:07:30,400
这里面右边的这个图

173
00:07:30,400 --> 00:07:34,720
就是讲GPT-3的一个Few-Shot、One-Shot和Zero-Shot具体的效果

174
00:07:34,720 --> 00:07:38,040
可以看到它的Few-Shot的效果是非常好的

175
00:07:38,040 --> 00:07:40,920
至于Zero可能还是有点差别

176
00:07:40,920 --> 00:07:43,800
Few-Shot就代表我需要有一些引导的单词

177
00:07:43,800 --> 00:07:45,360
有一些引导的向下文

178
00:07:45,360 --> 00:07:47,120
让它可以预测的更好

179
00:07:47,120 --> 00:07:49,600
而Zero-Shot就我不需要任何引导

180
00:07:49,600 --> 00:07:51,760
我直接去对下游任务进行推理

181
00:07:51,760 --> 00:07:53,800
它的区别就在于这

182
00:07:53,800 --> 00:07:56,200
最近印度小哥真的非常火

183
00:07:56,200 --> 00:07:58,600
除了当上了印度首相之外

184
00:07:58,600 --> 00:08:01,800
他们在AI的分享知识和AI理论的创新方面

185
00:08:01,800 --> 00:08:03,760
还是有非常多的工作的

186
00:08:03,800 --> 00:08:08,240
下面我们来看看这个印度小哥的一个分享

187
00:08:08,240 --> 00:08:13,160
现在具体一起来看看GPT-3是怎么去实现的

188
00:08:13,160 --> 00:08:15,920
GPT-3的数现在变成了一个Promote

189
00:08:15,920 --> 00:08:17,480
是一个英文的句子

190
00:08:17,480 --> 00:08:19,880
输出也是一个英文的句子

191
00:08:19,880 --> 00:08:22,800
具体是通过一个Unsuperized Pre-training

192
00:08:22,800 --> 00:08:25,320
就是无监督的一个训练学习

193
00:08:25,320 --> 00:08:29,160
训练的数据集有300 Billion Token of Text

194
00:08:29,160 --> 00:08:32,920
而训练的目标就是简单的去预测下一个单词

195
00:08:32,920 --> 00:08:35,360
所以它是非常简单的模式

196
00:08:35,360 --> 00:08:37,480
而Robot must什么呢

197
00:08:37,480 --> 00:08:39,200
它就是预测一个单词

198
00:08:39,200 --> 00:08:41,200
然后给GPT-3进行训练

199
00:08:41,200 --> 00:08:45,240
然后最终输出我们GPT-3的一个网络模型

200
00:08:45,240 --> 00:08:48,520
现在我们来看看具体的Sentence有什么不一样

201
00:08:48,520 --> 00:08:50,400
下面有具体的一句话

202
00:08:50,400 --> 00:08:53,400
无论是123它还是一句话

203
00:08:53,400 --> 00:08:57,200
而这个冒号就相当于我们刚才讲BERT的时候一个MASK

204
00:08:57,200 --> 00:09:00,200
Second Law of Robot什么呢

205
00:09:00,200 --> 00:09:03,240
Second Law of Robot什么A呢

206
00:09:03,240 --> 00:09:06,960
Second Law of Robot什么Air Robot呢

207
00:09:06,960 --> 00:09:08,880
话呢还是那句话

208
00:09:08,880 --> 00:09:11,120
但是它里面的Mask就不一样了

209
00:09:11,120 --> 00:09:13,400
它的Mask是预测下一个单词

210
00:09:13,400 --> 00:09:18,160
而给出的prompt和提示就更加详细或者直接挖空

211
00:09:18,160 --> 00:09:22,240
使得GPT-3具有一个Zero shot和One shot的功能

212
00:09:22,240 --> 00:09:24,600
这个就是它最大的区别

213
00:09:24,600 --> 00:09:27,120
而这里面的语料非常丰富

214
00:09:27,120 --> 00:09:30,080
Transformer的层数套了96层

215
00:09:30,080 --> 00:09:33,560
我们刚才讲的BTER Large其实只有24层

216
00:09:33,560 --> 00:09:35,560
这里面的语料进入不增大了

217
00:09:35,560 --> 00:09:36,680
通过promote方式

218
00:09:36,680 --> 00:09:40,400
我们把一句话可能变成3,4,5,6句话非常多

219
00:09:40,400 --> 00:09:43,040
所以我们需要经过96层Transformer

220
00:09:43,040 --> 00:09:45,400
最后输出预测结果

221
00:09:45,400 --> 00:09:48,280
这就是GPT-3的一个最重要的概念

222
00:09:48,280 --> 00:09:52,200
就是它大部分是一个概念和数据处理流程的创新

223
00:09:52,200 --> 00:09:55,200
对我们的网络模型结构就是不断的堆掉

224
00:09:55,200 --> 00:09:59,720
我们这年不可枯饰的就是要训练一个具有96层的Transformer

225
00:09:59,720 --> 00:10:02,040
一张卡肯定是塞不下的

226
00:10:02,040 --> 00:10:04,320
所以我们需要分布是并行的一些工作

227
00:10:04,320 --> 00:10:08,080
这些工作我们会在后面的章节里面详细的去展开

228
00:10:09,080 --> 00:10:13,720
Switch Transformer是首个突破万亿规模的大模型

229
00:10:13,720 --> 00:10:14,600
万亿规模

230
00:10:14,600 --> 00:10:16,360
万亿规模是非常夸张的

231
00:10:16,680 --> 00:10:18,360
Switch Transformer最重要的工作

232
00:10:18,360 --> 00:10:22,120
就是把我们的开关添加Transformer的自注意力层之间

233
00:10:22,120 --> 00:10:25,200
我们可以看到Transformer里面有一个FFN

234
00:10:25,200 --> 00:10:28,480
就是经过MultiHead之后它会有个FFN

235
00:10:28,600 --> 00:10:31,360
这里面把FFN当作Export

236
00:10:31,360 --> 00:10:34,760
然后通过我们的路由机制去选定某个Export

237
00:10:35,160 --> 00:10:38,440
在Switch Transformer标题里面就明确说了

238
00:10:38,440 --> 00:10:41,960
Scaling to Trillion Primiton Models

239
00:10:41,960 --> 00:10:46,000
然后用一个非常高效和简单的稀疏结构

240
00:10:46,000 --> 00:10:49,640
我们可以看到稀疏系数结构主要是讲MOE

241
00:10:49,640 --> 00:10:54,280
它把MOE的方式放在我们的Transformer的结构里面

242
00:10:54,280 --> 00:10:56,680
但是它不是代替我们的QKV

243
00:10:56,680 --> 00:10:58,480
而是代替我们的FFN

244
00:10:58,480 --> 00:11:01,080
把FFN结构当成它的Export

245
00:11:01,080 --> 00:11:03,360
但是它跟传统的MOE不一样

246
00:11:03,360 --> 00:11:06,400
传统的MOE是通过Export进行预测的

247
00:11:06,400 --> 00:11:09,000
而这里面通过一个Export进行预测的

248
00:11:09,000 --> 00:11:10,880
是因为网络的规模太大了

249
00:11:10,880 --> 00:11:13,080
如果通过多个Export进行预测的时候

250
00:11:13,080 --> 00:11:14,440
这篇文章就发现了

251
00:11:14,440 --> 00:11:17,320
它会进一步的加大我们计算的开销

252
00:11:17,320 --> 00:11:18,320
通讯的开销

253
00:11:18,320 --> 00:11:20,440
而通过单个Export进行预测

254
00:11:20,440 --> 00:11:24,120
可以有效的加快我们训练的时间和训练的收敛性

255
00:11:24,600 --> 00:11:27,640
2.1章节里面就去讲讲我们稀疏吸塑路由

256
00:11:27,640 --> 00:11:29,160
具体是怎么去实现的

257
00:11:29,160 --> 00:11:31,000
它的计算公式是怎么算的

258
00:11:31,720 --> 00:11:34,080
而这里面就是我们的稀疏的选择

259
00:11:34,080 --> 00:11:37,240
这里面就是专家选择的一种具体的方式

260
00:11:37,240 --> 00:11:41,040
再往下就是分布式的一个稀疏专家的选择

261
00:11:41,040 --> 00:11:44,600
当然了这篇文章不仅也是提供了非常多的实验过程

262
00:11:44,600 --> 00:11:46,400
这个万亿规模的网络模型

263
00:11:46,400 --> 00:11:48,360
我们从这张图里面可以看到

264
00:11:48,360 --> 00:11:51,080
这个就是稠密的计算的时候它的精度

265
00:11:51,120 --> 00:11:54,400
上面蓝色的点就是SwitchTransform的性能

266
00:11:54,400 --> 00:11:59,120
实际上它的下标代表的是101种语言的翻译任务

267
00:11:59,120 --> 00:12:01,960
都有非常好的性能的提升

268
00:12:01,960 --> 00:12:06,520
通过一个网络模型就解决了100多种任务的性能提升

269
00:12:06,520 --> 00:12:11,080
所以这是一个非常Outstanding或者非常Sota的一篇文章

270
00:12:11,080 --> 00:12:15,160
而要训练一个万亿规模的网络模型其实没有那么简单

271
00:12:15,160 --> 00:12:17,520
而这里面又有一个更详细的展开

272
00:12:17,560 --> 00:12:19,240
FFN作用的Expert

273
00:12:19,240 --> 00:12:22,880
FFN的输出给我们的是QKV这个参数

274
00:12:22,880 --> 00:12:25,600
但实际上我们要训练一个万亿一规模的参数

275
00:12:25,600 --> 00:12:29,040
其实我们刚才只是停留在去讲我们的网络模型结构

276
00:12:29,040 --> 00:12:30,560
具体是怎么实现的

277
00:12:30,560 --> 00:12:35,080
但是万亿规模的参数实际上没有那么简单的去训练起来

278
00:12:35,080 --> 00:12:37,680
这里面就用到了数据并行

279
00:12:37,680 --> 00:12:38,440
模型并行

280
00:12:38,440 --> 00:12:40,920
模型和数据的这种混合并行

281
00:12:40,920 --> 00:12:44,080
另外它还提出了专家和数据的并行

282
00:12:44,080 --> 00:12:45,680
图里面的不同颜色

283
00:12:45,720 --> 00:12:47,480
就代表不同的专家

284
00:12:47,480 --> 00:12:49,240
专家和模型和数据并行

285
00:12:49,240 --> 00:12:51,440
就像这种方式具体组成

286
00:12:51,440 --> 00:12:56,880
所以说这篇文章不仅仅是讲了一些网络模型的结构的革新或者一些算法

287
00:12:56,880 --> 00:12:59,880
它还提出了专家和混合并行的具体的方式

288
00:12:59,880 --> 00:13:02,800
而这篇文章同样是出自于谷歌之手

289
00:13:02,800 --> 00:13:07,480
所以Tensor4或者谷歌的TPU是非常值得我们去研究

290
00:13:07,480 --> 00:13:09,200
但是我们在真正编码的时候

291
00:13:09,200 --> 00:13:12,080
可能会选择MindSpore或者PyTorch去实现

292
00:13:12,080 --> 00:13:14,480
因为TensorFlow确实太难学了

293
00:13:14,520 --> 00:13:20,760
最后一个网络模型就是GLaM 1.211参数的一个通用的系数语言模型

294
00:13:20,760 --> 00:13:21,720
既然是通用

295
00:13:21,720 --> 00:13:26,520
所以它的Zero-Shot或者One-Shot的性能肯定是越来越好

296
00:13:26,520 --> 00:13:29,080
比刚才的Switch Transformer更好

297
00:13:29,080 --> 00:13:32,240
这里面它对每一层的MOE进行控制

298
00:13:32,240 --> 00:13:34,440
里面只有64个专家

299
00:13:34,440 --> 00:13:36,320
而有32层MOE

300
00:13:36,320 --> 00:13:38,720
这篇文章我们就不详细的去展开

301
00:13:38,720 --> 00:13:42,480
大家有兴趣的可以去看看GLaM这篇文章

302
00:13:42,520 --> 00:13:43,880
现在我们来总结一下

303
00:13:43,880 --> 00:13:45,760
今天的内容可能稍微长了一点

304
00:13:45,760 --> 00:13:48,840
就是Attention is all you need的这种Transformer

305
00:13:48,840 --> 00:13:51,960
引发了AI迈进大模型的时代

306
00:13:51,960 --> 00:13:54,160
有Transformer才有大模型

307
00:13:54,160 --> 00:13:56,440
第二个就是稀疏门控的MOE

308
00:13:56,440 --> 00:14:01,160
有MOE我们的网络模型才有资格迈向万亿规模

309
00:14:01,160 --> 00:14:03,760
只是稠密的Transformer的计算是没办法

310
00:14:03,760 --> 00:14:06,720
让我们的网络模型迈入万亿规模的

311
00:14:06,720 --> 00:14:09,480
第三个我们就讲BERT网络模型

312
00:14:09,480 --> 00:14:12,040
这个芝麻街的椒麻鸡兄弟

313
00:14:12,040 --> 00:14:14,680
Transformer的双向无监督编码的结构

314
00:14:14,680 --> 00:14:18,120
使得我们的网络模型可以进行一个预训练

315
00:14:18,160 --> 00:14:21,320
而GPT-3用了96层Transformer

316
00:14:21,320 --> 00:14:25,000
实现了一个自回归的千亿规模的语言模型

317
00:14:25,000 --> 00:14:27,760
就直接用Zero Shpt就可以实现了

318
00:14:27,760 --> 00:14:30,320
而不需要像BERT一样做Fine Turning

319
00:14:30,320 --> 00:14:35,360
而SwitchTransformer将MOE和Transformer的结构结合在一起

320
00:14:35,360 --> 00:14:39,240
把Transformer的FFT当作MOE的一个专家

321
00:14:39,240 --> 00:14:42,120
最后就是有谷歌的GLaM

322
00:14:42,120 --> 00:14:46,760
在COSOP的领域里面直接打败了GPD-3

323
00:14:46,800 --> 00:14:48,160
讲了这么多大模型

324
00:14:48,160 --> 00:14:50,960
其实我们并没有深入的去看CV大模型

325
00:14:50,960 --> 00:14:53,240
NLP大模型和多模态大模型

326
00:14:53,240 --> 00:14:56,640
而是重点去讲讲我们的网络模型的结构

327
00:14:56,640 --> 00:15:00,040
对我们的大模型去提出了新的挑战

328
00:15:00,040 --> 00:15:02,240
和新的一些奠基的功能

329
00:15:02,240 --> 00:15:05,720
从MOE Transformer使得我们的大模型爆发了

330
00:15:05,720 --> 00:15:07,680
出现了BERT和GPT-3

331
00:15:07,680 --> 00:15:10,960
使得我们大模型的精度进行了进一步的突破

332
00:15:10,960 --> 00:15:12,880
和解决了碎片化的问题

333
00:15:12,880 --> 00:15:16,720
所以我们今天的分享主要还是围绕着大模型的网络模型结构

334
00:15:16,720 --> 00:15:18,960
然后模型的结构模型的参数量

335
00:15:18,960 --> 00:15:22,280
如何一步步的增大去进行展开

336
00:15:22,280 --> 00:15:25,720
对大模型结构有兴趣的同学可以多阅读原论文

