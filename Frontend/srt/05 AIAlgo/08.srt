1
00:00:00,000 --> 00:00:04,680
Baba Baa Baa Baa Baa Baa Baa Baa

2
00:00:04,680 --> 00:00:06,400
Hello 大家好

3
00:00:06,400 --> 00:00:08,760
我们又回来了

4
00:00:08,760 --> 00:00:09,880
然后我是钟鸣

5
00:00:09,880 --> 00:00:13,200
我们今天来聊一聊分布式算法这个系列

6
00:00:13,200 --> 00:00:14,200
那现在呢

7
00:00:14,200 --> 00:00:18,280
我们已经来到了分布式算法里面的最后一个小内容

8
00:00:18,280 --> 00:00:19,920
分布式的大模型算法

9
00:00:19,920 --> 00:00:22,200
我们不会在这里面讲太多

10
00:00:22,200 --> 00:00:26,440
而是重点讲讲几个比较有影响力的SOTA大模型

11
00:00:26,440 --> 00:00:27,840
那首先第一个就是Bird

12
00:00:27,840 --> 00:00:29,560
引起我们的预训练的

13
00:00:29,560 --> 00:00:32,160
第二个就是GtpX-3

14
00:00:32,160 --> 00:00:33,720
作为一个SILVERSOT LEARNING

15
00:00:33,720 --> 00:00:36,080
非常之有名的网络模型

16
00:00:36,080 --> 00:00:39,520
我们的模型参数量也是上了千亿规模的

17
00:00:39,520 --> 00:00:42,040
最后来一个上万亿规模参数量的

18
00:00:42,040 --> 00:00:44,920
用了MOE结构的Switch Transformer

19
00:00:45,880 --> 00:00:46,880
Hi 大家好

20
00:00:46,880 --> 00:00:47,840
我是钟鸣

21
00:00:47,840 --> 00:00:53,040
我们又来到了大模型和分布式训练这个系列里面的分享内容

22
00:00:53,040 --> 00:00:57,080
今天我想给大家一起去分享一下大模型的一个算法结构

23
00:00:57,120 --> 00:01:00,280
去看看我们的模型的发展的情况

24
00:01:00,280 --> 00:01:01,920
从我们最熟悉的模型

25
00:01:01,920 --> 00:01:05,360
其实一开始并没有提模型的参数量有多大

26
00:01:05,360 --> 00:01:08,040
一般来说模型的进步有多好而已

27
00:01:08,040 --> 00:01:11,080
后来等我们引入了大模型之后

28
00:01:11,080 --> 00:01:15,040
就开始的去提我们的模型到底是百亿规模的

29
00:01:15,040 --> 00:01:16,280
还是千亿规模的

30
00:01:16,280 --> 00:01:17,960
还是万亿规模的

31
00:01:17,960 --> 00:01:20,400
可以看到网络模型的规模越大

32
00:01:20,400 --> 00:01:21,960
一台GtpX放不下

33
00:01:21,960 --> 00:01:23,360
所以面向大模型

34
00:01:23,360 --> 00:01:26,160
我们要进行大规模分布式训练

35
00:01:26,200 --> 00:01:29,960
但是今天我们主要是聊聊大模型的一些结构

36
00:01:29,960 --> 00:01:31,440
而在后面的分享里面

37
00:01:31,440 --> 00:01:34,240
我们才去看看怎么把这些模型

38
00:01:34,240 --> 00:01:36,840
并行的切分到不同的机器上面

39
00:01:36,840 --> 00:01:38,800
去做一个训练的加速

40
00:01:38,800 --> 00:01:42,520
Transformer和MOE实际上是大模型的一个电机

41
00:01:42,520 --> 00:01:43,800
而真正的大模型

42
00:01:43,800 --> 00:01:45,960
我们从BUT网络模型开始

43
00:01:45,960 --> 00:01:50,080
然后首个突破十亿规模的NLP大模型

44
00:01:50,080 --> 00:01:52,680
它采用的是一个双向编码的方式

45
00:01:52,680 --> 00:01:55,920
来改进我们的整个网络模型的架构

46
00:01:55,920 --> 00:01:59,280
蓝色的这个框实际上是Transformer的一个视力

47
00:01:59,280 --> 00:02:01,760
里面我们可以看到有非常多的连线

48
00:02:01,760 --> 00:02:03,880
每一条连线都向左向右

49
00:02:03,880 --> 00:02:06,080
这里面就是BUT提出的一个概念

50
00:02:06,080 --> 00:02:07,560
双向编码

51
00:02:07,560 --> 00:02:09,920
这里面Transformer带来一个很重要的概念

52
00:02:09,920 --> 00:02:12,400
就是预训练的网络模型

53
00:02:12,400 --> 00:02:16,400
然后进行一些简单的效用任务的微调就可以了

54
00:02:16,400 --> 00:02:19,520
现在我们来看看BUT网络模型这篇论文

55
00:02:19,520 --> 00:02:22,120
我们经常会在一些论文或者博客里面

56
00:02:22,120 --> 00:02:23,920
看到这么一个小玩具

57
00:02:23,920 --> 00:02:27,320
这个呆呆的样子实际上是芝麻街兄弟里面的一员

58
00:02:27,320 --> 00:02:29,040
他的名字叫做BUT

59
00:02:29,040 --> 00:02:30,520
我们看看论文

60
00:02:30,520 --> 00:02:32,840
在论文里面的BUT它其实是

61
00:02:32,840 --> 00:02:36,760
Deep Break Direction Transformer for Language Understanding

62
00:02:36,760 --> 00:02:40,360
而BERT取决于双向incorder

63
00:02:40,360 --> 00:02:42,120
Representation Transformer

64
00:02:42,120 --> 00:02:45,000
这几个英文单词的首字母大写

65
00:02:45,000 --> 00:02:47,640
这篇论文同样我们简单的来翻一翻

66
00:02:47,640 --> 00:02:49,760
首先是我们的Introduction

67
00:02:49,760 --> 00:02:53,360
接着去理解一下什么叫做预训练网络模型

68
00:02:53,360 --> 00:02:54,920
或者Find Tuning

69
00:02:54,920 --> 00:02:57,160
接着去讲讲一些相关的工作

70
00:02:57,160 --> 00:02:59,440
第一个就是无监督学习

71
00:02:59,440 --> 00:03:02,480
无监督学习也是BERT里面很重要的一个概念

72
00:03:02,480 --> 00:03:03,920
第二个就是Find Tuning

73
00:03:03,920 --> 00:03:05,800
就是我们的微调任务

74
00:03:05,800 --> 00:03:07,160
可以看到Pretraining

75
00:03:07,160 --> 00:03:08,560
就是我们的预训练里面

76
00:03:08,560 --> 00:03:11,080
用的是一个无监督的学习的方法

77
00:03:11,080 --> 00:03:13,280
而后面针对不同的效用任务

78
00:03:13,280 --> 00:03:15,720
使用的是一个Find Tuning的工作

79
00:03:15,720 --> 00:03:18,920
在第三步就会详细的去介绍BUT网络模型的

80
00:03:18,920 --> 00:03:21,240
组织结构网络模型的方式

81
00:03:21,280 --> 00:03:24,760
3.1里面就讲了Pretraining BUT里面分为两个text

82
00:03:24,760 --> 00:03:26,760
第一个text就是maxlm

83
00:03:26,760 --> 00:03:29,000
第二个text就是next-sequent predict

84
00:03:29,000 --> 00:03:32,440
就是下一句话或者下一个单词的预测

85
00:03:32,440 --> 00:03:34,840
这个图我们将会在后面进行展开

86
00:03:34,840 --> 00:03:35,560
Experience

87
00:03:35,560 --> 00:03:38,320
Experience主要是基于GRUE这个benchmark

88
00:03:38,320 --> 00:03:41,440
就是GRUE这个下游任务去进行对比的

89
00:03:41,440 --> 00:03:43,840
而BUT网络模型在各个下游任务里

90
00:03:43,840 --> 00:03:46,480
都取得了一个非常舒坦的结果

91
00:03:46,480 --> 00:03:49,960
同样BUT网络模型后面又有非常多的附录

92
00:03:50,000 --> 00:03:53,720
而附录也是非常值得我们深入的去研究这个网络模型

93
00:03:53,720 --> 00:03:55,360
具体长什么样子的

94
00:03:55,360 --> 00:03:57,880
会提供了很多详细的信息

95
00:03:57,880 --> 00:03:59,360
从论文里面我们知道

96
00:03:59,360 --> 00:04:02,000
BUT网络模型是一个双向编码的网络模型

97
00:04:02,000 --> 00:04:05,880
但是这个双向编码实际上它只是一个概念

98
00:04:05,880 --> 00:04:09,320
在transformer的结构还是那个transformer的结构

99
00:04:09,320 --> 00:04:12,640
那我们看看它具体双向表现在哪里

100
00:04:12,640 --> 00:04:15,640
首先它网络模型分为一个Pretraining和Find Tuning

101
00:04:15,640 --> 00:04:16,760
Find Tuning我们先不管

102
00:04:16,760 --> 00:04:18,200
我们看看Pretraining

103
00:04:18,240 --> 00:04:22,960
数据的输出和输入就取决于我们的网络模型的训练的过程当中

104
00:04:22,960 --> 00:04:26,720
它到底是一个监督学习或者无监督学习的方式

105
00:04:26,720 --> 00:04:29,520
这里面有几个内容可能我们要稍微注意一下的

106
00:04:29,520 --> 00:04:31,360
这一个就是CLS

107
00:04:31,360 --> 00:04:33,040
第二个SEP

108
00:04:33,040 --> 00:04:34,840
第三个就是MAX

109
00:04:34,840 --> 00:04:37,400
SEP和CLS只是一个标志位

110
00:04:37,400 --> 00:04:39,400
标志我是同句子的开头

111
00:04:39,400 --> 00:04:40,560
句子的段距

112
00:04:40,560 --> 00:04:43,360
而MAX这个才是最重要的概念

113
00:04:43,360 --> 00:04:44,920
假设我们现在输一段话

114
00:04:44,920 --> 00:04:46,200
MyDot is cute

115
00:04:46,200 --> 00:04:48,760
那这个Cute我把它MAX就是遮住

116
00:04:48,760 --> 00:04:50,760
然后输进去我们的网络模型

117
00:04:50,760 --> 00:04:52,920
输出的时候我把MyDot is cute

118
00:04:52,920 --> 00:04:54,800
然后同样作为我的输出

119
00:04:54,800 --> 00:04:58,800
然后让我们的神经网络模型去预测这个Cute

120
00:04:58,800 --> 00:05:00,800
从而实现自监督的功能

121
00:05:00,800 --> 00:05:03,680
就我的数据已经不用严工的去标了

122
00:05:03,680 --> 00:05:08,120
让我们的神经网络模型自动的去从我们的数据之间发现它的规律

123
00:05:08,120 --> 00:05:09,960
而双向代表的什么意思呢

124
00:05:09,960 --> 00:05:12,960
假设我去预测是它的前后的关系

125
00:05:13,000 --> 00:05:14,720
前面是Dot还是Cute呢

126
00:05:14,720 --> 00:05:17,240
就是前后左右我都会进行一个预测

127
00:05:17,240 --> 00:05:19,600
假设是我们Transformer的Q

128
00:05:19,600 --> 00:05:21,000
我们的KE可能是Dot

129
00:05:21,000 --> 00:05:21,800
K20是Cute

130
00:05:21,800 --> 00:05:23,520
K30是HE

131
00:05:23,520 --> 00:05:26,720
因为Transformer网络模型QKV的特殊结构

132
00:05:26,720 --> 00:05:28,000
所以通过MAX之后

133
00:05:28,000 --> 00:05:30,960
它就变得有一种双向的概念

134
00:05:30,960 --> 00:05:33,440
所以双向是来自于这

135
00:05:33,440 --> 00:05:39,120
通过输进去Bot网络模型去预测单词到底是什么

136
00:05:39,120 --> 00:05:40,800
同样我们除了MAX单词

137
00:05:40,800 --> 00:05:42,640
我们还可以MAX下一个句子

138
00:05:43,480 --> 00:05:47,960
那第一个任务就是在句子里面随机的去支柱一部分单词

139
00:05:47,960 --> 00:05:51,640
然后去预测这部分单词到底是什么内容

140
00:05:51,640 --> 00:05:54,360
那第二任务就是Next Sequence Predict

141
00:05:54,360 --> 00:05:57,120
就是预测下一个句子是什么

142
00:05:57,960 --> 00:06:02,280
为什么我们说Bot网络模型对于大模型来说非常重要呢

143
00:06:02,840 --> 00:06:06,120
是因为Bot网络模型穿了非常多Transformer的Encoder

144
00:06:06,120 --> 00:06:08,800
它实际上通过超大规模的数据

145
00:06:08,800 --> 00:06:11,080
还有非常大的网络模型结构

146
00:06:11,080 --> 00:06:14,320
并且还用了谷歌的TPU去进行计算的

147
00:06:14,320 --> 00:06:17,920
在11个NLP应用里面都取得了非常好的结果

148
00:06:17,920 --> 00:06:19,760
这里面除了网络模型的大

149
00:06:19,760 --> 00:06:21,440
还用了超大的数据

150
00:06:21,440 --> 00:06:23,600
另外训练还分为了两个阶段

151
00:06:23,600 --> 00:06:26,320
第一个是预训练的阶段和Fight Tool的阶段

152
00:06:26,320 --> 00:06:31,760
预训练阶段这个概念把我们的网络模型引入了一个更高规模的规格

153
00:06:33,680 --> 00:06:35,040
这些算法讲的好累

154
00:06:36,160 --> 00:06:37,520
虽然我很努力的在讲

155
00:06:38,800 --> 00:06:41,120
但是可能你仍然还是听不明白

156
00:06:41,120 --> 00:06:42,440
听不明白没关系

157
00:06:42,440 --> 00:06:44,880
其实我更希望大家去阅读原论文

158
00:06:44,880 --> 00:06:48,840
去仔细的去研究一下这个网络模型到底有什么用

159
00:06:48,840 --> 00:06:50,720
具体理念是怎么去实现的

160
00:06:50,720 --> 00:06:53,320
下面我们来到第四个重要的网络模型

161
00:06:53,320 --> 00:06:55,400
就是我们的GPT-3

162
00:06:55,400 --> 00:07:00,520
前面的GPT-1和GPT-2实际上并没有太多的Outstanding的工作

163
00:07:00,520 --> 00:07:06,520
但是当GPT-3的网络模型的参数量都已经上到了接近2100一规模的时候

164
00:07:06,560 --> 00:07:09,600
它带来的是一个全新的语言模型

165
00:07:09,600 --> 00:07:12,760
这个全新的语言模型跟我们刚才的Bert

166
00:07:12,760 --> 00:07:14,920
它需要经过微调不一样

167
00:07:14,920 --> 00:07:16,120
GTP-3

168
00:07:16,120 --> 00:07:18,960
居于刚才的Bert做了一个无监督之外

169
00:07:18,960 --> 00:07:20,760
它还实现了一个Servo-Sort

170
00:07:20,760 --> 00:07:22,720
就是我不需要Fight Turing了

171
00:07:22,720 --> 00:07:24,680
我直接进行个Servo-Sort

172
00:07:24,680 --> 00:07:28,920
然后我的预训练模型直接可以对应到具体的效任务

173
00:07:28,920 --> 00:07:30,400
这里面右边的这个图

174
00:07:30,400 --> 00:07:34,720
就是讲GPT-3的一个Fuel-Sort、Win-Sort和Servo-Sort具体的效果

175
00:07:34,720 --> 00:07:38,040
可以看到它的Fuel-Sort的效果是非常好的

176
00:07:38,040 --> 00:07:40,920
至于Servo可能还是有点差别

177
00:07:40,920 --> 00:07:43,800
Fuel-Sort就代表我需要有一些引导的单词

178
00:07:43,800 --> 00:07:45,360
有一些引导的向下文

179
00:07:45,360 --> 00:07:47,120
让它可以预测的更好

180
00:07:47,120 --> 00:07:49,600
而Servo-Sort就我不需要任何引导

181
00:07:49,600 --> 00:07:51,760
我直接去对效任务进行推理

182
00:07:51,760 --> 00:07:53,800
它的区别就在于这

183
00:07:53,800 --> 00:07:56,200
最近印度小哥真的非常火

184
00:07:56,200 --> 00:07:58,600
除了当上了印度首相之外

185
00:07:58,600 --> 00:08:01,800
他们在AI的分享知识和AI理论的创新方面

186
00:08:01,800 --> 00:08:03,760
还是有非常多的工作的

187
00:08:03,800 --> 00:08:08,240
下面我们来看看这个印度小哥的一个分享

188
00:08:08,240 --> 00:08:13,160
现在具体一起来看看GPT-3是怎么去实现的

189
00:08:13,160 --> 00:08:15,920
GPT-3的数现在变成了一个Promote

190
00:08:15,920 --> 00:08:17,480
是一个英文的句子

191
00:08:17,480 --> 00:08:19,880
输出也是一个英文的句子

192
00:08:19,880 --> 00:08:22,800
具体是通过一个Unsuperized Pre-training

193
00:08:22,800 --> 00:08:25,320
就是无监督的一个训练学习

194
00:08:25,320 --> 00:08:29,160
训练的数据集有300 Billion Token of Text

195
00:08:29,160 --> 00:08:32,920
而训练的目标就是简单的去预测下一个单词

196
00:08:32,920 --> 00:08:35,360
所以它是非常简单的模式

197
00:08:35,360 --> 00:08:37,480
而Robot must什么呢

198
00:08:37,480 --> 00:08:39,200
它就是预测一个单词

199
00:08:39,200 --> 00:08:41,200
然后给GPT-3进行训练

200
00:08:41,200 --> 00:08:45,240
然后最终输出我们GPT-3的一个网络模型

201
00:08:45,240 --> 00:08:48,520
现在我们来看看具体的Sentence有什么不一样

202
00:08:48,520 --> 00:08:50,400
下面有具体的一句话

203
00:08:50,400 --> 00:08:53,400
无论是123它还是一句话

204
00:08:53,400 --> 00:08:57,200
而这个冒号就相当于我们刚才讲Bird的时候一个Must

205
00:08:57,200 --> 00:09:00,200
Second Law of Robot什么呢

206
00:09:00,200 --> 00:09:03,240
Second Law of Robot什么A呢

207
00:09:03,240 --> 00:09:06,960
Second Law of Robot什么Air Robot呢

208
00:09:06,960 --> 00:09:08,880
话呢还是那句话

209
00:09:08,880 --> 00:09:11,120
但是它里面的Must就不一样了

210
00:09:11,120 --> 00:09:13,400
它的Must是预测下一个单词

211
00:09:13,400 --> 00:09:18,160
而给出的抛幕和提示就更加详细或者直接挖空

212
00:09:18,160 --> 00:09:22,240
使得GPT-3具有一个Several sort和One sort的功能

213
00:09:22,240 --> 00:09:24,600
这个就是它最大的区别

214
00:09:24,600 --> 00:09:27,120
而这里面的语料非常丰富

215
00:09:27,120 --> 00:09:30,080
Transformer的层数套了96层

216
00:09:30,080 --> 00:09:33,560
我们刚才讲的ButtLarge其实只有24层

217
00:09:33,560 --> 00:09:35,560
这里面的语料进入不增大了

218
00:09:35,560 --> 00:09:36,680
通过promote方式

219
00:09:36,680 --> 00:09:40,400
我们把一句话可能变成3,4,5,6句话非常多

220
00:09:40,400 --> 00:09:43,040
所以我们需要经过96层Transformer

221
00:09:43,040 --> 00:09:45,400
最后输出预测结果

222
00:09:45,400 --> 00:09:48,280
这就是GPT-3的一个最重要的概念

223
00:09:48,280 --> 00:09:52,200
就是它大部分是一个概念和数据处理流程的创新

224
00:09:52,200 --> 00:09:55,200
对我们的网络模型结构就是不断的堆掉

225
00:09:55,200 --> 00:09:59,720
我们这年不可枯饰的就是要训练一个具有96层的Transformer

226
00:09:59,720 --> 00:10:02,040
一张卡肯定是塞不下的

227
00:10:02,040 --> 00:10:04,320
所以我们需要分布是并行的一些工作

228
00:10:04,320 --> 00:10:08,080
这些工作我们会在后面的章节里面详细的去展开

229
00:10:09,080 --> 00:10:13,720
Switch Transformer是首个突破万亿规模的大模型

230
00:10:13,720 --> 00:10:14,600
万亿规模

231
00:10:14,600 --> 00:10:16,360
万亿规模是非常夸张的

232
00:10:16,680 --> 00:10:18,360
Switch Transformer最重要的工作

233
00:10:18,360 --> 00:10:22,120
就是把我们的该关添加Transformer的自助Ed层之间

234
00:10:22,120 --> 00:10:25,200
我们可以看到Transformer里面有一个FFN

235
00:10:25,200 --> 00:10:28,480
就是经过MultiHat之后它会有个FFN

236
00:10:28,600 --> 00:10:31,360
这里面把FFN当作Esp

237
00:10:31,360 --> 00:10:34,760
然后通过我们的路由机制去选定某个Esp

238
00:10:35,160 --> 00:10:38,440
在Switch Transformer标题里面就明确说了

239
00:10:38,440 --> 00:10:41,960
Scaling to Trillion Primiton Models

240
00:10:41,960 --> 00:10:46,000
然后用一个非常高效和简单的系数结构

241
00:10:46,000 --> 00:10:49,640
我们可以看到系数结构主要是讲MOE

242
00:10:49,640 --> 00:10:54,280
它把MOE的方式放在我们的Transformer的结构里面

243
00:10:54,280 --> 00:10:56,680
但是它不是代替我们的QKV

244
00:10:56,680 --> 00:10:58,480
而是代替我们的FFN

245
00:10:58,480 --> 00:11:01,080
把FFN结构当成它的Esp

246
00:11:01,080 --> 00:11:03,360
但是它跟传统的MOE不一样

247
00:11:03,360 --> 00:11:06,400
传统的MOE是通过Esp进行预测的

248
00:11:06,400 --> 00:11:09,000
而这里面通过一个Esp进行预测的

249
00:11:09,000 --> 00:11:10,880
是因为网络的规模太大了

250
00:11:10,880 --> 00:11:13,080
如果通过多个Esp进行预测的时候

251
00:11:13,080 --> 00:11:14,440
这篇文章就发现了

252
00:11:14,440 --> 00:11:17,320
它会进一步的加大我们计算的开销

253
00:11:17,320 --> 00:11:18,320
通讯的开销

254
00:11:18,320 --> 00:11:20,440
而通过单个Esp进行预测

255
00:11:20,440 --> 00:11:24,120
可以有效的加快我们训练的时间和训练的收敛性

256
00:11:24,600 --> 00:11:27,640
2.1章节里面就去讲讲我们吸塑路由

257
00:11:27,640 --> 00:11:29,160
具体是怎么去实现的

258
00:11:29,160 --> 00:11:31,000
它的计算公式是怎么算的

259
00:11:31,720 --> 00:11:34,080
而这里面就是我们的吸塑的选择

260
00:11:34,080 --> 00:11:37,240
这里面就是专家选择的一种具体的方式

261
00:11:37,240 --> 00:11:41,040
再往下就是分布式的一个吸塑专家的选择

262
00:11:41,040 --> 00:11:44,600
当然了这篇文章不仅也是提供了非常多的实验过程

263
00:11:44,600 --> 00:11:46,400
这个万一规模的网络模型

264
00:11:46,400 --> 00:11:48,360
我们从这张图里面可以看到

265
00:11:48,360 --> 00:11:51,080
这个就是筹密的计算的时候它的精度

266
00:11:51,120 --> 00:11:54,400
上面蓝色的点就是SwitchTransform的性能

267
00:11:54,400 --> 00:11:59,120
实际上它的下标代表的是101种语言的翻译用户

268
00:11:59,120 --> 00:12:01,960
都有非常好的性能的提升

269
00:12:01,960 --> 00:12:06,520
通过一个网络模型就解决了100多种任务的性能提升

270
00:12:06,520 --> 00:12:11,080
所以这是一个非常Outstanding或者非常Sota的一篇文章

271
00:12:11,080 --> 00:12:15,160
而要训练一个万一规模的网络模型其实没有那么简单

272
00:12:15,160 --> 00:12:17,520
而这里面又有一个更详细的展开

273
00:12:17,560 --> 00:12:19,240
FFN作用的Expert

274
00:12:19,240 --> 00:12:22,880
FFN的输出给我们的是QKV这个参数

275
00:12:22,880 --> 00:12:25,600
但实际上我们要训练一个万一规模的参数

276
00:12:25,600 --> 00:12:29,040
其实我们刚才只是停留在去讲我们的网络模型结构

277
00:12:29,040 --> 00:12:30,560
具体是怎么实现的

278
00:12:30,560 --> 00:12:35,080
但是万一规模的参数实际上没有那么简单的去训练起来

279
00:12:35,080 --> 00:12:37,680
这里面就用到了数据并行

280
00:12:37,680 --> 00:12:38,440
模型并行

281
00:12:38,440 --> 00:12:40,920
模型和数据的这种缓和并行

282
00:12:40,920 --> 00:12:44,080
另外它还提出了专家和数据的并行

283
00:12:44,080 --> 00:12:45,680
图里面的不同颜色

284
00:12:45,720 --> 00:12:47,480
就代表不同的专家

285
00:12:47,480 --> 00:12:49,240
专家和模型和数据并行

286
00:12:49,240 --> 00:12:51,440
就像这种方式具体组成

287
00:12:51,440 --> 00:12:56,880
所以说这篇文章不仅仅是讲了一些网络模型的结构的革新或者一些算法

288
00:12:56,880 --> 00:12:59,880
它还提出了专家和缓和并行的具体的方式

289
00:12:59,880 --> 00:13:02,800
而这篇文章同样是出自于谷歌之手

290
00:13:02,800 --> 00:13:07,480
所以Tensor4或者谷歌的TPU是非常值得我们去研究

291
00:13:07,480 --> 00:13:09,200
但是我们在真正编码的时候

292
00:13:09,200 --> 00:13:12,080
可能会选择MindSpot或者PyTorch去实现

293
00:13:12,080 --> 00:13:14,480
因为Tensor4确实太难学了

294
00:13:14,520 --> 00:13:20,760
最后一个网络模型就是GLAM 1.211参数的一个通用的系数语言模型

295
00:13:20,760 --> 00:13:21,720
既然是通用

296
00:13:21,720 --> 00:13:26,520
所以它的CWSWP或者WNSWP的性能肯定是越来越好

297
00:13:26,520 --> 00:13:29,080
比刚才的Switch选手更好

298
00:13:29,080 --> 00:13:32,240
这里面它对每一层的MOE进行控制

299
00:13:32,240 --> 00:13:34,440
里面只有64个专家

300
00:13:34,440 --> 00:13:36,320
而有32层MOE

301
00:13:36,320 --> 00:13:38,720
这篇文章我们就不详细的去展开

302
00:13:38,720 --> 00:13:42,480
大家有兴趣的可以去看看GLAM这篇文章

303
00:13:42,520 --> 00:13:43,880
现在我们来总结一下

304
00:13:43,880 --> 00:13:45,760
今天的内容可能稍微长了一点

305
00:13:45,760 --> 00:13:48,840
就是Attention is all you need的这种Transformer

306
00:13:48,840 --> 00:13:51,960
引发了AI迈进大模型的时代

307
00:13:51,960 --> 00:13:54,160
有Transformer才有大模型

308
00:13:54,160 --> 00:13:56,440
第二个就是西书门控的MOE

309
00:13:56,440 --> 00:14:01,160
有MOE我们的网络模型才有资格迈向万亿规模

310
00:14:01,160 --> 00:14:03,760
只是稠密的Transformer的计算是没办法

311
00:14:03,760 --> 00:14:06,720
让我们的网络模型迈入万亿规模的

312
00:14:06,720 --> 00:14:09,480
第三个我们就讲Bert网络模型

313
00:14:09,480 --> 00:14:12,040
这个芝麻街的浇麻鸡兄弟

314
00:14:12,040 --> 00:14:14,680
Transformer的双向无监督编码的结构

315
00:14:14,680 --> 00:14:18,120
使得我们的网络模型可以进行一个预训练

316
00:14:18,160 --> 00:14:21,320
而GPD-3用了96层Transformer

317
00:14:21,320 --> 00:14:25,000
实现了一个自回归的千亿规模的语言模型

318
00:14:25,000 --> 00:14:27,760
就直接用COSOP就可以实现了

319
00:14:27,760 --> 00:14:30,320
而不需要像Bert一样做Fight Tune

320
00:14:30,320 --> 00:14:35,360
而SwitchTransformer将MOE和Transformer的结构结合在一起

321
00:14:35,360 --> 00:14:39,240
把Transformer的FFT当作MOE的一个专家

322
00:14:39,240 --> 00:14:42,120
最后就是有谷歌的GALM

323
00:14:42,120 --> 00:14:46,760
在COSOP的领域里面直接打败了GPD-3

324
00:14:46,800 --> 00:14:48,160
讲了这么多大模型

325
00:14:48,160 --> 00:14:50,960
其实我们并没有深入的去看CV大模型

326
00:14:50,960 --> 00:14:53,240
NLP大模型和多模态大模型

327
00:14:53,240 --> 00:14:56,640
而是重点去讲讲我们的网络模型的结构

328
00:14:56,640 --> 00:15:00,040
对我们的大模型去提出了新的挑战

329
00:15:00,040 --> 00:15:02,240
和新的一些电机的功能

330
00:15:02,240 --> 00:15:05,720
从MOE Transformer使得我们的大模型爆发了

331
00:15:05,720 --> 00:15:07,680
出现了Bert和GPD-3

332
00:15:07,680 --> 00:15:10,960
使得我们大模型的精度进行了进一步的突破

333
00:15:10,960 --> 00:15:12,880
和解决了碎片化的问题

334
00:15:12,880 --> 00:15:16,720
所以我们今天的分享主要还是围绕着大模型的网络模型结构

335
00:15:16,720 --> 00:15:18,960
然后模型的结构模型的参数量

336
00:15:18,960 --> 00:15:22,280
如何一步步的增大去进行展开

337
00:15:22,280 --> 00:15:25,720
对大模型结构有兴趣的同学可以多阅读原论文

