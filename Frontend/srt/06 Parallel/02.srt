1
00:00:00,000 --> 00:00:05,000
嗨兄弟们,迷失了,这是大模型与分布式训练系列里面的数据并行

2
00:00:05,000 --> 00:00:07,000
实际上我们之前讲了大模型的算法

3
00:00:07,000 --> 00:00:09,000
但是只是聚焦于算法的结构本身并没有去展开这些算法

4
00:00:09,000 --> 00:00:11,000
在大规模服务器或者集训里面是怎么样并行操作的

5
00:00:11,000 --> 00:00:13,000
接下来的内容我们希望去分享一下分布式并行的一些具体的操作

6
00:00:13,000 --> 00:00:15,000
分布式并行其实主要是在分布式并行的一个设计上

7
00:00:15,000 --> 00:00:17,000
分布式并行的一个设计上

8
00:00:17,000 --> 00:00:19,000
分布式并行的一个设计上

9
00:00:19,000 --> 00:00:21,000
分布式并行的一个设计上

10
00:00:21,000 --> 00:00:23,000
分布式并行的一个设计上

11
00:00:23,000 --> 00:00:25,340
下面接下来的内容我们希望去分享一些曲铺比 settlement

12
00:00:25,340 --> 00:00:26,680
下面接下来的内容我们希望去分享一些曲铺比 athletics

13
00:00:26,680 --> 00:00:28,680
下面接下来的内容我们希望去分享一些曲铺比

14
00:00:32,680 --> 00:00:35,100
из

15
00:00:36,980 --> 00:00:38,020
最后我们把之前学到的几个并行的操作

16
00:00:38,020 --> 00:00:40,280
最后我们把之前学到的几个并行的操作

17
00:00:40,280 --> 00:00:41,680
最后我们把之前学到的几个并行的操作

18
00:00:42,740 --> 00:00:43,380
做一个混合,让我们之前聊到了大模型的算法结构

19
00:00:43,380 --> 00:00:45,320
做一个混合,让我们之前聊到的大模型的算法结构

20
00:00:45,320 --> 00:00:47,320
做一个混合,让我们之前聊到了大模型的算法结构

21
00:00:47,320 --> 00:00:49,320
让我们更好更快的训练起来

22
00:00:49,320 --> 00:00:51,320
让我们更好更快的训练起来

23
00:00:51,320 --> 00:00:51,320


24
00:00:51,320 --> 00:00:53,320
当我们手头上只有一个intel 酷睿i3的笔记本的时候

25
00:00:53,320 --> 00:00:55,320
当我们手头上只有一个intel 酷睿i3的笔记本的时候

26
00:00:55,320 --> 00:00:57,320
想玩大模型,还是洗洗碎吧

27
00:00:57,320 --> 00:00:59,320
想玩大模型,还是洗洗碎吧

28
00:00:59,320 --> 00:01:01,320
但是当我们的钱已经多到花不完了

29
00:01:01,320 --> 00:01:03,320
但是当我们的钱已经多到花不完了

30
00:01:03,320 --> 00:01:05,320
那我们可以去购买我们的生腾服务器或者VBAC

31
00:01:05,320 --> 00:01:07,320
那我们可以去购买我们的生腾服务器或者VBAC

32
00:01:07,320 --> 00:01:09,320
或者用华为云的AI集群

33
00:01:09,320 --> 00:01:11,320
那我们就可以去享受分布式训练给我们带来的乐趣

34
00:01:11,320 --> 00:01:13,320
那我们就可以去享受分布式训练给我们带来的乐趣

35
00:01:13,320 --> 00:01:15,320
但是如果我有那么多钱的时候

36
00:01:15,320 --> 00:01:17,320
但是如果我有那么多钱的时候

37
00:01:17,320 --> 00:01:19,320
我为什么要当算法工程师呢?

38
00:01:19,320 --> 00:01:21,320
我为什么要当算法工程师呢?

39
00:01:21,320 --> 00:01:23,320
玩笑归玩笑啊

40
00:01:23,320 --> 00:01:25,320
我们可以看到今天我们主要来讲的内容呢

41
00:01:25,320 --> 00:01:27,320
就是我们的数据并行

42
00:01:27,320 --> 00:01:29,320
那数据并行呢

43
00:01:29,320 --> 00:01:31,320
其实它有很多种数据并行的方式

44
00:01:31,320 --> 00:01:33,320
我一开始对数据并行理解的概念

45
00:01:33,320 --> 00:01:35,320
是非常之粗俗的

46
00:01:35,320 --> 00:01:37,320
我以为数据并行只是简单的

47
00:01:37,320 --> 00:01:39,320
对我们的一些Data

48
00:01:39,320 --> 00:01:41,320
就是我们训练的数据进行一个并行的操作

49
00:01:41,320 --> 00:01:43,320
随着我对这个知识的深入的了解

50
00:01:43,320 --> 00:01:45,320
随着我对这个知识的深入的了解

51
00:01:45,320 --> 00:01:47,320
我发现数据并行还没有那么简单

52
00:01:47,320 --> 00:01:49,320
第一个呢就是我刚才聊到的

53
00:01:49,320 --> 00:01:51,320
我们需要对数据数的数据进行并行

54
00:01:51,320 --> 00:01:53,320
第二个我们会对网络模型的参数进行并行

55
00:01:53,320 --> 00:01:55,320
第二个我们会对网络模型的参数进行并行

56
00:01:55,320 --> 00:01:57,320
第三个呢我们计算完反向的时候呢

57
00:01:57,320 --> 00:01:59,320
第三个呢我们计算完反向的时候呢

58
00:01:59,320 --> 00:02:01,320
我们会对我们的T度进行并行

59
00:02:01,320 --> 00:02:03,320
第四个呢我们还会对优化器的状态进行并行

60
00:02:03,320 --> 00:02:05,320
所以数据并行的操作和方式有非常多

61
00:02:05,320 --> 00:02:07,320
所以数据并行的操作和方式有非常多

62
00:02:07,320 --> 00:02:09,320
这里面呢又分为简单的数据并行

63
00:02:09,320 --> 00:02:11,320
这里面呢又分为简单的数据并行

64
00:02:11,320 --> 00:02:13,320
还有分布式数据并行

65
00:02:13,320 --> 00:02:15,320
还有全切变数据并行

66
00:02:15,320 --> 00:02:17,320
下面呢我们会以PyTouchC三种并行的方式

67
00:02:17,320 --> 00:02:19,320
去了解一下数据并行的不同形态

68
00:02:19,320 --> 00:02:21,320
去了解一下数据并行的不同形态

69
00:02:21,320 --> 00:02:23,320
在正式的内容开始之前呢

70
00:02:23,320 --> 00:02:25,320
在正式的内容开始之前呢

71
00:02:25,320 --> 00:02:27,320
我想提一个疑问

72
00:02:27,320 --> 00:02:29,320
一般我们的网络模型或者CV卷机网络模型呢

73
00:02:29,320 --> 00:02:31,320
一般是输入一张图片

74
00:02:31,320 --> 00:02:33,320
我们的256x256长和宽

75
00:02:33,320 --> 00:02:35,320
再乘以一个三通道的图片

76
00:02:35,320 --> 00:02:37,320
给我们的神经网络进行训练的

77
00:02:37,320 --> 00:02:39,320
但是像我们下面的这些图啊

78
00:02:39,320 --> 00:02:41,320
其实都是一些卫星图

79
00:02:41,320 --> 00:02:43,320
卫星的图像是非常的大的

80
00:02:43,320 --> 00:02:45,320
卫星的图像是非常的大的

81
00:02:45,320 --> 00:02:47,320
那像卫星的图片呢可能我们以一个例子

82
00:02:47,320 --> 00:02:49,320
就是我现在有一张卫星的图片

83
00:02:49,320 --> 00:02:51,320
左边的那种图层也是非常多的

84
00:02:51,320 --> 00:02:53,320
他的图片的大小也是非常夸张的

85
00:02:53,320 --> 00:02:55,320
他的图片的大小也是非常夸张的

86
00:02:55,320 --> 00:02:57,320
已经到万极成以万极了

87
00:02:57,320 --> 00:02:59,320
那这个时候我们对这个卫星的图像

88
00:02:59,320 --> 00:03:01,320
丢给我们的神经网络去处理的时候

89
00:03:01,320 --> 00:03:03,320
他还是做数据并行吗

90
00:03:03,320 --> 00:03:05,320
这个问题呢等我们分享完这一节内容的时候

91
00:03:05,320 --> 00:03:07,320
这个问题呢等我们分享完这一节内容的时候

92
00:03:07,320 --> 00:03:09,320
我们再来一起回顾一下

93
00:03:13,320 --> 00:03:15,320
首先要分享的第一个内容就是我们的数据并行

94
00:03:15,320 --> 00:03:17,320
首先要分享的第一个内容就是我们的数据并行

95
00:03:17,320 --> 00:03:19,320
最原始的数据并行呢假设我们现在有两台4倍

96
00:03:19,320 --> 00:03:21,320
最原始的数据并行呢假设我们现在有两台4倍

97
00:03:21,320 --> 00:03:23,320
然后我们把数据切成一半

98
00:03:23,320 --> 00:03:25,320
一半的数据呢就给我们的4倍1

99
00:03:25,320 --> 00:03:27,320
另外一半的数据给我们的4倍2

100
00:03:27,320 --> 00:03:29,320
通过前向的计算反向的计算我们得到T度

101
00:03:29,320 --> 00:03:31,320
通过前向的计算反向的计算我们得到T度

102
00:03:31,320 --> 00:03:33,320
最后我们对数据进行同步和T度累积之后呢

103
00:03:33,320 --> 00:03:35,320
最后我们对数据进行同步和T度累积之后呢

104
00:03:35,320 --> 00:03:37,320
我们就完成了数据并行的第一个step训练

105
00:03:37,320 --> 00:03:39,320
我们就完成了数据并行的第一个step训练

106
00:03:39,320 --> 00:03:41,320
简单的对训练的数据进行并行呢

107
00:03:41,320 --> 00:03:43,320
简单的对训练的数据进行并行呢

108
00:03:43,320 --> 00:03:45,320
它的代码实现起来还是比较简单的

109
00:03:45,320 --> 00:03:47,320
这是我们实施的过程的第一个

110
00:03:47,320 --> 00:03:49,320
在原理上每台机器都会有一个独立的模型

111
00:03:49,320 --> 00:03:51,320
在原理上每台机器都会有一个独立的模型

112
00:03:51,320 --> 00:03:53,320
我们只是利用了集群的单节点的算力

113
00:03:53,320 --> 00:03:55,320
我们只是利用了集群的单节点的算力

114
00:03:55,320 --> 00:03:57,320
这样就会引起一个问题

115
00:03:57,320 --> 00:03:59,320
每台机器计算完之后都会有自己独立的T度

116
00:03:59,320 --> 00:04:01,320
我们需要把这些T度进行汇合

117
00:04:01,320 --> 00:04:03,320
然后做T度累积

118
00:04:03,320 --> 00:04:05,320
那谈到T度累积我们就会引起两个问题

119
00:04:05,320 --> 00:04:07,320
那谈到T度累积我们就会引起两个问题

120
00:04:07,320 --> 00:04:09,320
第一个就是T度累积

121
00:04:09,320 --> 00:04:11,320
第二个就是T度累积

122
00:04:11,320 --> 00:04:13,320
第三个就是T度累积

123
00:04:13,320 --> 00:04:15,320
第三个就是T度累积

124
00:04:15,320 --> 00:04:17,320
我们提了好几次T度累积

125
00:04:17,320 --> 00:04:19,320
但是T度累积到底是什么呢

126
00:04:19,320 --> 00:04:21,320
我们提了好几次T度累积

127
00:04:21,320 --> 00:04:23,320
但是T度累积到底是什么呢

128
00:04:23,320 --> 00:04:25,320
层次的这个就是我们的数据

129
00:04:25,320 --> 00:04:27,320
现在我们把数据分成mini-batch

130
00:04:27,320 --> 00:04:29,320
现在我们把数据分成mini-batch

131
00:04:29,320 --> 00:04:31,320
对数据进行分布式并行操作

132
00:04:31,320 --> 00:04:33,320
对数据进行分布式并行操作

133
00:04:33,320 --> 00:04:35,320
放在不同的机器去执行

134
00:04:35,320 --> 00:04:37,320
那每一台机器就会有一个分布式

135
00:04:37,320 --> 00:04:39,320
那每一台机器就会有一个分布式

136
00:04:39,320 --> 00:04:41,320
那每一台机器就会有一个分布式

137
00:04:41,320 --> 00:04:43,320
那每一台机器就会有一个分布式

138
00:04:43,320 --> 00:04:45,320
那每一台机器就会维护自己的网络模型

139
00:04:45,320 --> 00:04:47,320
那每一台机器就会维护自己的网络模型

140
00:04:47,320 --> 00:04:49,320
进行一个前向的计算

141
00:04:49,320 --> 00:04:51,320
反向的计算,就是计算我们的T度

142
00:04:53,320 --> 00:04:55,320
T度累积就是把我们不同4倍的T度

143
00:04:55,320 --> 00:04:57,320
进行一个求和汇总

144
00:04:57,320 --> 00:04:59,320
然后更新服务器参数的网络模型

145
00:04:59,320 --> 00:05:01,320
最后分发给每一台机器

146
00:05:01,320 --> 00:05:03,320
然后再进行下一次数据的迭代

147
00:05:03,320 --> 00:05:05,320
然后再进行下一次数据的迭代

148
00:05:05,320 --> 00:05:07,320
下面的这一部分就是我们的T度累积的过程和操作

149
00:05:07,320 --> 00:05:09,320
下面的这一部分就是我们的T度累积的过程和操作

150
00:05:09,320 --> 00:05:11,320
T度累积的时间节点分为两种

151
00:05:11,320 --> 00:05:13,320
一种是同步T度累积的方式

152
00:05:13,320 --> 00:05:15,320
一种是同步T度累积的方式

153
00:05:15,320 --> 00:05:17,320
同步的T度累积就是每一台机器都算完自己的T度之后

154
00:05:17,320 --> 00:05:19,320
就是每一台机器都算完自己的T度之后

155
00:05:19,320 --> 00:05:21,320
聚集到我们的PS服务器里面

156
00:05:21,320 --> 00:05:23,320
然后统一进行更新

157
00:05:23,320 --> 00:05:25,320
这里面就会把三个箭头进行统一更新

158
00:05:25,320 --> 00:05:27,320
这里面就会把三个箭头进行统一更新

159
00:05:27,320 --> 00:05:29,320
这种方式就是严格的按照时间序列来去进行执行

160
00:05:29,320 --> 00:05:31,320
这种方式就是严格的按照时间序列来去进行执行

161
00:05:31,320 --> 00:05:33,320
保证我们的网络模型的收敛能够得到保证

162
00:05:33,320 --> 00:05:35,320
但是快速可以看到像Devices2

163
00:05:35,320 --> 00:05:37,320
它的计算时间特别长

164
00:05:37,320 --> 00:05:39,320
而Devices1计算时间特别短

165
00:05:39,320 --> 00:05:41,320
而Devices1计算时间特别短

166
00:05:41,320 --> 00:05:43,320
这里面就会出现严重的资源损耗

167
00:05:43,320 --> 00:05:45,320
这里面就会出现严重的资源损耗

168
00:05:45,320 --> 00:05:47,320
里面的白色的这些框框

169
00:05:47,320 --> 00:05:49,320
我们叫做bubble

170
00:05:49,320 --> 00:05:51,320
出现了大量的bubble大量的时间浪费

171
00:05:51,320 --> 00:05:53,320
我们还会浪费大量的通讯的开销时间

172
00:05:53,320 --> 00:05:55,320
我们还会浪费大量的通讯的开销时间

173
00:05:55,320 --> 00:05:57,320
另外一种T度累积的方式就是

174
00:05:57,320 --> 00:05:59,320
一步更新

175
00:05:59,320 --> 00:06:01,320
一步更新比较简单

176
00:06:01,320 --> 00:06:03,320
所以我们的网络模型都是单独的去更新自己的服务器参数

177
00:06:03,320 --> 00:06:05,320
所以我们的网络模型都是单独的去更新自己的服务器参数

178
00:06:05,320 --> 00:06:07,320
这种方式带来的好处就是没有了刚才白色的bubble

179
00:06:07,320 --> 00:06:09,320
这种方式带来的好处就是没有了刚才白色的bubble

180
00:06:09,320 --> 00:06:11,320
Devices1进行完前向计算

181
00:06:11,320 --> 00:06:13,320
再进行反向的计算

182
00:06:13,320 --> 00:06:15,320
再进行正向的计算

183
00:06:15,320 --> 00:06:17,320
再进行反向的计算

184
00:06:17,320 --> 00:06:19,320
极大的减少了通讯的过程

185
00:06:19,320 --> 00:06:21,320
但是带来的问题就是我的网络模型会很难去收敛

186
00:06:21,320 --> 00:06:23,320
但是带来的问题就是我的网络模型会很难去收敛

187
00:06:23,320 --> 00:06:25,320
所以我们要计算的过程是不断的去迭代

188
00:06:25,320 --> 00:06:27,320
不断的去覆盖原来的副本的

189
00:06:29,320 --> 00:06:31,320
因此在实际的训练环节里面

190
00:06:31,320 --> 00:06:33,320
我们更多的是采用同步的T度更新的方式

191
00:06:33,320 --> 00:06:35,320
我们更多的是采用同步的T度更新的方式

192
00:06:35,320 --> 00:06:37,320
下面我们来看看T度累积的通讯具体的方式

193
00:06:37,320 --> 00:06:39,320
下面我们来看看T度累积的通讯具体的方式

194
00:06:39,320 --> 00:06:41,320
假设下面我们以左边的GPU0

195
00:06:41,320 --> 00:06:43,320
作为参数服务器也就是左边的这块卡

196
00:06:43,320 --> 00:06:45,320
作为参数服务器

197
00:06:45,320 --> 00:06:47,320
我们在做T度累积的时候

198
00:06:47,320 --> 00:06:49,320
我们会把Walk2、Walk4、Walk3的数据

199
00:06:49,320 --> 00:06:51,320
都同时回传到我们的MasterWalk1里面

200
00:06:51,320 --> 00:06:53,320
都同时回传到我们的MasterWalk1里面

201
00:06:53,320 --> 00:06:55,320
然后进行统一的更新之后

202
00:06:55,320 --> 00:06:57,320
再分发给不同的Walk

203
00:06:57,320 --> 00:06:59,320
这种就是GPU0作为参数服务器的同步更新方式

204
00:06:59,320 --> 00:07:01,320
这种就是GPU0作为参数服务器的同步更新方式

205
00:07:01,320 --> 00:07:03,320
另外一种就是参数服务器分布在所有GPU里面

206
00:07:03,320 --> 00:07:05,320
另外一种就是参数服务器分布在所有GPU里面

207
00:07:05,320 --> 00:07:07,320
那我们的所有GPU实际上我们的网络会形成一个环

208
00:07:07,320 --> 00:07:09,320
那我们的所有GPU实际上我们的网络会形成一个环

209
00:07:09,320 --> 00:07:11,320
通过这个环Walk1的参数

210
00:07:11,320 --> 00:07:13,320
会给Walk2进行同步

211
00:07:13,320 --> 00:07:15,320
Walk2给Walk4、Walk3给Walk1

212
00:07:15,320 --> 00:07:17,320
Walk2给Walk4、Walk3给Walk1

213
00:07:17,320 --> 00:07:19,320
通过我们之前的分享内容

214
00:07:19,320 --> 00:07:21,320
通过我们之前的分享内容只要进行两次变例环之后

215
00:07:21,320 --> 00:07:23,320
只要进行两次变例环之后

216
00:07:23,320 --> 00:07:25,320
我们四个参数服务器都有所有数据的备份

217
00:07:25,320 --> 00:07:27,320
这时候就完成整个梯度累积的同步了

218
00:07:27,320 --> 00:07:29,320
这时候就完成整个梯度累积的同步了

219
00:07:31,320 --> 00:07:33,320
现在我们来看看Pytorch的分布式数据并行的具体的实现方式

220
00:07:33,320 --> 00:07:35,320
现在我们来看看Pytorch的分布式数据并行的具体的实现方式

221
00:07:35,320 --> 00:07:37,320
DTP采用的是多进程的实现方式

222
00:07:37,320 --> 00:07:39,320
DTP采用的是多进程的实现方式

223
00:07:39,320 --> 00:07:41,320
所以没有了Python的GIL锁

224
00:07:41,320 --> 00:07:43,320
更加灵活、更加方便的去启用多个进程

225
00:07:43,320 --> 00:07:45,320
更加灵活、更加方便的去启用多个进程

226
00:07:45,320 --> 00:07:47,320
第二个就是每个进程并不是同步所有的参数

227
00:07:47,320 --> 00:07:49,320
第二个就是每个进程并不是同步所有的参数

228
00:07:49,320 --> 00:07:51,320
而是同步我们梯度的误差

229
00:07:51,320 --> 00:07:53,320
而是同步我们梯度的误差

230
00:07:53,320 --> 00:07:55,320
带来的好处就是减少了我们通讯的数据

231
00:07:55,320 --> 00:07:57,320
带来的好处就是减少了我们通讯的数据

232
00:07:57,320 --> 00:07:59,320
第三个就采用Win Overduce的方式去提升我们的通讯效率

233
00:07:59,320 --> 00:08:01,320
右边的这个图就是TDP的原理图

234
00:08:01,320 --> 00:08:03,320
右边的这个图就是TDP的原理图

235
00:08:03,320 --> 00:08:05,320
跟我们的DP的原理是不一样的

236
00:08:05,320 --> 00:08:07,320
跟我们的DP的原理是不一样的

237
00:08:07,320 --> 00:08:09,320
DP刚才只是对我们训练的数据进行同步

238
00:08:09,320 --> 00:08:11,320
DP刚才只是对我们的训练的数据进行同步

239
00:08:11,320 --> 00:08:13,320
DDP就是对我们的梯度进行更新

240
00:08:13,320 --> 00:08:15,320
就是我们的规点

241
00:08:15,320 --> 00:08:17,320
可以看到层次的这个就是我们的规点

242
00:08:17,320 --> 00:08:19,320
而梯度的同步并不是算完整个网络模型

243
00:08:19,320 --> 00:08:21,320
而梯度的同步并不是算完整个网络模型

244
00:08:21,320 --> 00:08:23,320
最后把所有的服务器进行更新的

245
00:08:23,320 --> 00:08:25,320
而是每一次进行一个分统

246
00:08:25,320 --> 00:08:27,320
所以我们可以在图中看到

247
00:08:27,320 --> 00:08:29,320
有好几个Bunkit

248
00:08:29,320 --> 00:08:31,320
然后每一台服务器都有自己的Bunkit

249
00:08:31,320 --> 00:08:33,320
这就是第一步操作

250
00:08:33,320 --> 00:08:35,320
对我们的梯度进行分统

251
00:08:35,320 --> 00:08:37,320
第二点就是对我们的梯度进行逆向排序

252
00:08:37,320 --> 00:08:39,320
去确定我们什么时候

253
00:08:39,320 --> 00:08:41,320
哪个梯度先进行更新

254
00:08:41,320 --> 00:08:43,320
第三个就是跳过一些

255
00:08:43,320 --> 00:08:45,320
跳过一些已经很久没有更新

256
00:08:45,320 --> 00:08:47,320
或者时间太慢的一些梯度

257
00:08:47,320 --> 00:08:49,320
第四点就是进行一个集合通讯

258
00:08:49,320 --> 00:08:51,320
第四点就是进行一个集合通讯

259
00:08:51,320 --> 00:08:53,320
让我们从右边的这个图来看看

260
00:08:53,320 --> 00:08:55,320
首先我们在进行计算的时候

261
00:08:55,320 --> 00:08:57,320
就会有很多梯度

262
00:08:57,320 --> 00:08:59,320
那梯度我们会把它们放在不同的桶里面

263
00:08:59,320 --> 00:09:01,320
假设我鼠标所在的AdMM2这一层

264
00:09:01,320 --> 00:09:03,320
已经完成了计算了

265
00:09:03,320 --> 00:09:05,320
那我Bunkit的数据两边都已经具备了

266
00:09:05,320 --> 00:09:07,320
这时候我就会直接进行一个Ovidus的通讯

267
00:09:07,320 --> 00:09:09,320
在通讯的同时

268
00:09:09,320 --> 00:09:11,320
我的AdMM1这一层

269
00:09:11,320 --> 00:09:13,320
已经开始计算了

270
00:09:13,320 --> 00:09:15,320
这样就极大地去利用了

271
00:09:15,320 --> 00:09:17,320
我们执行时间的异步

272
00:09:17,320 --> 00:09:19,320
最大化地利用了我们通讯的空窄时间

273
00:09:19,320 --> 00:09:21,320
最大化地利用了我们通讯的空窄时间

274
00:09:21,320 --> 00:09:23,320
最大化地利用了我们通讯的空窄时间

275
00:09:23,320 --> 00:09:25,320
下面的这个图会更加清晰

276
00:09:25,320 --> 00:09:27,320
每一次我们都会对规点不同的梯度进行分桶

277
00:09:27,320 --> 00:09:29,320
分完桶之后再进行一个同步的操作

278
00:09:29,320 --> 00:09:31,320
然后下次计算的时候再进行分桶

279
00:09:31,320 --> 00:09:33,320
然后再进行同步

280
00:09:33,320 --> 00:09:35,320
就利用了我们计算跟通讯的时间

281
00:09:35,320 --> 00:09:37,320
在通讯方式里面

282
00:09:37,320 --> 00:09:39,320
我们之前已经讲过了

283
00:09:39,320 --> 00:09:41,320
多对多的同步的通讯方式

284
00:09:41,320 --> 00:09:43,320
其实可以由一对多或者多对一进行组成的

285
00:09:43,320 --> 00:09:45,320
而这里面Ovidus的通讯方式

286
00:09:45,320 --> 00:09:47,320
其实可以由一对多或者多对一进行组成的

287
00:09:47,320 --> 00:09:49,320
而这里面Ovidus的通讯方式

288
00:09:49,320 --> 00:09:51,320
其实可以由一对多或者多对一进行组成的

289
00:09:51,320 --> 00:09:53,320
而这里面Ovidus的这种方式

290
00:09:53,320 --> 00:09:55,320
可以由VidusGet加Orget的两种方式

291
00:09:55,320 --> 00:09:57,320
刚才无论是DP还是DTP

292
00:09:57,320 --> 00:09:59,320
使用都是Ovidus的集合通讯的方式

293
00:09:59,320 --> 00:10:01,320
对我们的梯度进行同步更新

294
00:10:01,320 --> 00:10:03,320
实际上这一步操作

295
00:10:03,320 --> 00:10:05,320
可以拆解为VidusGet和Orget

296
00:10:05,320 --> 00:10:07,320
回顾一下我们DP是对训练的数据进行并行

297
00:10:07,320 --> 00:10:09,320
而DDP是对我们的梯度的数据进行并行

298
00:10:09,320 --> 00:10:11,320
而我们的网络模型

299
00:10:11,320 --> 00:10:13,320
除了这两种数据

300
00:10:13,320 --> 00:10:15,320
我们的DDP是对我们的梯度的数据进行并行

301
00:10:15,320 --> 00:10:17,320
而DDP是对我们的梯度的数据进行并行

302
00:10:17,320 --> 00:10:19,320
而我们的网络模型

303
00:10:19,320 --> 00:10:21,320
所以我们的网络模型除了这两种数据

304
00:10:21,320 --> 00:10:23,320
其实还有我们的网络模型的参数

305
00:10:23,320 --> 00:10:25,320
和优化器的状态

306
00:10:25,320 --> 00:10:30,320
同样我们可以对所有网络模型的数据进行并行更新

307
00:10:30,320 --> 00:10:33,320
这时候我们又引入了一个新的数据并行的方式

308
00:10:33,320 --> 00:10:37,320
叫做FSDP全数据切片的并行方式

309
00:10:38,320 --> 00:10:41,320
这种数据并行方式就把我们刚才讲到的

310
00:10:41,320 --> 00:10:43,320
网络模型的参数、梯度

311
00:10:43,320 --> 00:10:45,320
还有优化器的状态都进行并行更新

312
00:10:45,320 --> 00:10:48,320
另外我们还会对静态的内存

313
00:10:48,320 --> 00:10:50,320
去卸载到我们的CPU里面

314
00:10:50,320 --> 00:10:53,320
进一步的去提升我们NPU的内存

315
00:10:53,320 --> 00:10:56,320
下面这个图就是FSDP具体的执行方式

316
00:10:56,320 --> 00:10:59,320
首先第一种就是我们会对数据进行简单的并行

317
00:10:59,320 --> 00:11:01,320
进行一个前向的计算

318
00:11:01,320 --> 00:11:03,320
然后再进行一个反向的计算

319
00:11:03,320 --> 00:11:04,320
反向计算完之后

320
00:11:04,320 --> 00:11:06,320
我们会对网络模型的全数参数

321
00:11:06,320 --> 00:11:09,320
进行Reduce Scatter的同步方式

322
00:11:09,320 --> 00:11:12,320
然后去更新我们的网络模型全数

323
00:11:12,320 --> 00:11:13,320
更新完之后

324
00:11:13,320 --> 00:11:16,320
在这里面我们会执行一个Orgator

325
00:11:16,320 --> 00:11:19,320
把我们的数据同步给每一块卡

326
00:11:19,320 --> 00:11:20,320
对比起BBT

327
00:11:20,320 --> 00:11:24,320
我们把Org reduced分开两个通讯的操作

328
00:11:24,320 --> 00:11:26,320
变成了Reduce Scatter加上Orgator

329
00:11:26,320 --> 00:11:32,320
另外还会对网络模型的参数和优化器的状态进行同步更新

330
00:11:32,320 --> 00:11:37,320
进一步的去利用了我们计算时候网络通讯空窄的时间

331
00:11:37,320 --> 00:11:40,320
更进一步的利用了我们集群的性能

332
00:11:40,320 --> 00:11:44,320
下面这个图就是PyTorch去实现FSDP的

333
00:11:44,320 --> 00:11:46,320
一个详细的流程图

334
00:11:46,320 --> 00:11:50,320
可以看到首先我们会把我们的数据进行并行切分

335
00:11:50,320 --> 00:11:52,320
接着去进行一个前向的计算

336
00:11:52,320 --> 00:11:53,320
反向的计算

337
00:11:53,320 --> 00:11:55,320
然后去进行Reduce Scatter

338
00:11:55,320 --> 00:11:58,320
接着把一些不需要用到的全重参数

339
00:11:58,320 --> 00:12:01,320
去offload到我们的CPU里面

340
00:12:01,320 --> 00:12:03,320
等我们需要用到的时候

341
00:12:03,320 --> 00:12:07,320
再把我们的全重参数load到我们的NPU里面

342
00:12:07,320 --> 00:12:09,320
接着再做一个Orgator的操作

343
00:12:09,320 --> 00:12:10,320
如此往复

344
00:12:11,320 --> 00:12:13,320
中间通过Orgator跟Reduce Scatter

345
00:12:13,320 --> 00:12:16,320
对T度进行累积同步更新

346
00:12:16,320 --> 00:12:22,320
讲完DP、DDP、FSDP三种PyTorch的不同的数据并行的操作

347
00:12:22,320 --> 00:12:24,320
下面我们来简单的去看看

348
00:12:24,320 --> 00:12:27,320
PyTorch具体实现的架构和逻辑

349
00:12:27,320 --> 00:12:31,320
在最简单的DP就是对我们训练的数据进行并行

350
00:12:31,320 --> 00:12:34,320
它用的是多进程的方式去实现的

351
00:12:34,320 --> 00:12:37,320
在DDP分布式数据并行里面

352
00:12:37,320 --> 00:12:39,320
具体是使用多线程的方式

353
00:12:39,320 --> 00:12:43,320
这时候在机区里面就涉及到多卡和夸砌器进行通讯

354
00:12:43,320 --> 00:12:47,320
这时候可能会用MPI、NCCL、HCCL或者GLOO

355
00:12:47,320 --> 00:12:52,320
其中一种的方式对数据进行集合通讯

356
00:12:52,320 --> 00:12:53,320
最后就是FSDP

357
00:12:53,320 --> 00:12:59,320
而FSDP实际上是使用的RPC远程过程调用的协议进行计算的

358
00:12:59,320 --> 00:13:03,320
而PyTorch里面就提供了RPC Core、RPC FC

359
00:13:03,320 --> 00:13:08,320
还有分布式AutoGrid、分布式优夸器等API去实现我们的FSDP

360
00:13:08,320 --> 00:13:13,320
好了,在这一节里面我们讲了数据并行的三种方式

361
00:13:13,320 --> 00:13:17,320
一种是DP,一种是DDP,另外一种是FSDP

362
00:13:17,320 --> 00:13:19,320
不管是哪种数据并行的方式

363
00:13:19,320 --> 00:13:25,320
我们希望对我们训练的数据、对我们的网络模型的T2权重、参数和优化器的状态

364
00:13:25,320 --> 00:13:29,320
都作为我们的网络模型的数据进行并行操作

365
00:13:29,320 --> 00:13:36,320
而DP和DDP和FSDP分别使用了不同的数据并行方式去实现

366
00:13:36,320 --> 00:13:38,320
谢谢各位,拜拜

