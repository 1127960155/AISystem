1
00:00:00,140 --> 00:00:04,790
爸爸爸爸叭叭叭叭叭
2
00:00:04,790 --> 00:00:05,900
哈喽大家好
3
00:00:05,900 --> 00:00:06,920
我是钟米
4
00:00:06,920 --> 00:00:08,540
我们来到了大漠行语
5
00:00:08,540 --> 00:00:09,760
分布式训练
6
00:00:09,760 --> 00:00:14,320
分布式分布式并行里面的最后一个环节了
7
00:00:14,320 --> 00:00:17,020
接近最后一个环节就是我们的流水并行
8
00:00:17,020 --> 00:00:18,460
pipeline并行
9
00:00:18,460 --> 00:00:21,640
那我们之前呢了解了分布式并行的数据并行
10
00:00:21,640 --> 00:00:22,360
模型并行
11
00:00:22,360 --> 00:00:23,620
或者叫做张量并行
12
00:00:23,620 --> 00:00:25,520
然后到流水线并行
13
00:00:25,520 --> 00:00:27,680
最后我们还会有一个环节
14
00:00:27,680 --> 00:00:31,420
就是把刚才前面的这些并行呢组合起来
15
00:00:31,420 --> 00:00:32,680
变成一个混合并行
16
00:00:32,680 --> 00:00:37,560
我们今天的内容呢主要是集中在模型并行
17
00:00:37,560 --> 00:00:37,980
里面的
18
00:00:37,980 --> 00:00:38,820
流水线并行
19
00:00:38,820 --> 00:00:40,920
又叫做pi paralys
20
00:00:40,920 --> 00:00:43,380
那这里面呢我们会分开三个内容来讲
21
00:00:43,380 --> 00:00:45,420
第一个就是最原始的流水并行
22
00:00:45,420 --> 00:00:46,760
到底是个什么东西
23
00:00:46,760 --> 00:00:48,560
然后呢我们会分享两篇论文
24
00:00:48,560 --> 00:00:50,810
第一篇就是谷歌的gpine
25
00:00:50,810 --> 00:00:54,360
第二篇就是微软的pg
26
00:00:54,520 --> 00:00:57,300
下面我们来看看流水并行的一个概念
27
00:00:57,300 --> 00:00:58,800
其实我们上一个视频呢
28
00:00:58,800 --> 00:01:02,260
已经简单的去给大家去分享了一个流水并行
29
00:01:02,260 --> 00:01:04,660
流水并行呢主要是层间的并行
30
00:01:04,660 --> 00:01:06,800
我把网络模型的不同的层
31
00:01:06,800 --> 00:01:09,290
放在不同的设备上面去执行
32
00:01:09,290 --> 00:01:13,050
这里面呢引入一个概念叫做stage或者叫做阶段
33
00:01:13,050 --> 00:01:15,270
我可能d vs 11可以把第一层
34
00:01:15,270 --> 00:01:16,380
第二层放进去
35
00:01:16,380 --> 00:01:18,240
这个呢我们叫做stage 1
36
00:01:18,240 --> 00:01:20,640
把第三层放在devices 2的时候
37
00:01:20,640 --> 00:01:22,320
我们可能会叫做stage 2
38
00:01:22,320 --> 00:01:23,710
以此类推
39
00:01:23,710 --> 00:01:25,450
俺也一样
40
00:01:25,450 --> 00:01:29,260
接下来呢我们换一个角度去看待这个问题
41
00:01:29,260 --> 00:01:32,780
因为流水并行呢实际上是跟时间有关系的
42
00:01:32,780 --> 00:01:35,300
所以我们现在换了一种表达的方式
43
00:01:35,300 --> 00:01:37,020
由右边的这幅图所示
44
00:01:37,020 --> 00:01:38,640
左边的这个示例图里面呢
45
00:01:38,640 --> 00:01:40,800
我们把每一层当做一个stage
46
00:01:40,800 --> 00:01:42,960
放在一台不同的设备里面
47
00:01:42,960 --> 00:01:45,900
首先我会做一个正向的计算
48
00:01:45,900 --> 00:01:48,690
然后通过我的loss做一个反向的计算
49
00:01:48,690 --> 00:01:51,040
然后放在三个不同的卡
50
00:01:51,040 --> 00:01:52,750
放到右边的这个图呢
51
00:01:52,750 --> 00:01:53,920
f代表我的正向
52
00:01:53,920 --> 00:01:56,640
forward b呢代表我的反向by work
53
00:01:56,640 --> 00:01:59,220
可以看到每一块机器都去执行
54
00:01:59,220 --> 00:02:01,130
自己层之间的一个计算
55
00:02:01,130 --> 00:02:04,490
最后再统一对四个卡进行更新
56
00:02:04,490 --> 00:02:08,180
那这种呢就是最nice的并行方式
57
00:02:08,180 --> 00:02:11,420
nee的papi并行呢其实有一个很大的问题
58
00:02:11,420 --> 00:02:14,780
就是会有大量的等待和阻塞的时间
59
00:02:14,780 --> 00:02:16,900
可以看到中间的空载的时间
60
00:02:16,900 --> 00:02:19,000
就是我的机器啥都不执行
61
00:02:19,000 --> 00:02:20,620
就在那等待的时间
62
00:02:20,620 --> 00:02:24,770
我刚才黄线画的这些地方是非常的多的
63
00:02:24,770 --> 00:02:28,420
科学家们呢又引入了另外一种并行模式
64
00:02:28,420 --> 00:02:31,000
叫做mini batch的pine并行
65
00:02:31,000 --> 00:02:33,700
刚才f0 呢是整一个连在一起的
66
00:02:33,700 --> 00:02:35,830
他执行的是一个batch
67
00:02:35,830 --> 00:02:39,440
现在我们把一个batch拆分成很多小的batch
68
00:02:39,440 --> 00:02:42,230
通过小的batch去进行一个计算
69
00:02:42,230 --> 00:02:44,720
这个时候我们就可以充分的利用了
70
00:02:44,720 --> 00:02:46,960
我们计算和通讯的时间
71
00:02:46,960 --> 00:02:50,860
把一个大的batch拆分成很多小的mini batch之后
72
00:02:50,860 --> 00:02:52,750
或者叫做michael batch之后呢
73
00:02:52,750 --> 00:02:55,020
这个ai框架或者ai系统
74
00:02:55,020 --> 00:02:58,440
就可以充分的去利用我们芯片的计算时间
75
00:02:58,440 --> 00:03:00,780
还有运算的空载的时间
76
00:03:00,780 --> 00:03:06,020
使得我们的bubble机器空载的时间呢越少
77
00:03:06,020 --> 00:03:07,220
另外一个好处就是
78
00:03:07,220 --> 00:03:11,180
机器跟机器之间的相互等待的时间也越少
79
00:03:11,180 --> 00:03:12,500
这里面可以看到
80
00:03:12,500 --> 00:03:15,560
f10 跟f01 之间是并行去操作的
81
00:03:15,560 --> 00:03:18,020
devices 0在执行计算的时候
82
00:03:18,020 --> 00:03:19,940
devices一也开始执行了
83
00:03:19,940 --> 00:03:23,370
就是我一个小的batch在f00 里面执行完之后
84
00:03:23,370 --> 00:03:26,720
我去把下一个数据传给我的f10 
85
00:03:26,720 --> 00:03:27,320
f10 
86
00:03:27,320 --> 00:03:29,000
计算完之后传给我的f20 
87
00:03:29,000 --> 00:03:30,680
然后再传给我的f30 
88
00:03:30,680 --> 00:03:33,260
而mini batch里面最经典的一篇文章呢
89
00:03:33,260 --> 00:03:34,340
就是gpl
90
00:03:34,340 --> 00:03:35,960
谷歌2019年的时候
91
00:03:35,960 --> 00:03:38,880
发明的一个流水线并行的一篇文章
92
00:03:38,880 --> 00:03:42,180
这篇文章提出了三个比较重要的概念
93
00:03:42,180 --> 00:03:43,920
第一个是partition stage
94
00:03:43,920 --> 00:03:46,760
就是把我们的网络模型分开不同的阶段
95
00:03:46,760 --> 00:03:50,570
第二个呢就是在pine里面去引入了michael batch
96
00:03:50,570 --> 00:03:55,240
第三个就是从计算下面一起来看看这篇文章
97
00:03:55,240 --> 00:03:59,020
接下来让我们有请下一位学员登场
98
00:03:59,020 --> 00:04:00,760
徐派蓬来资源文章了
99
00:04:00,760 --> 00:04:04,100
叫做ec gling with michael batch
100
00:04:04,100 --> 00:04:07,250
其中michael bench是一个比较重要的概念
101
00:04:07,250 --> 00:04:10,910
第二个比较重要的概念就是pipeline paralyze
102
00:04:10,910 --> 00:04:15,040
流水线并行里面很重要的一点就是
103
00:04:15,040 --> 00:04:17,500
网络模型的切分成不同的stage
104
00:04:17,500 --> 00:04:19,399
不是按照一层一层的切的
105
00:04:19,399 --> 00:04:23,460
而是可能会两三层变成一个stage进行切分
106
00:04:23,460 --> 00:04:27,600
第二点就是把mini batch呢进一步的划分成为
107
00:04:27,600 --> 00:04:28,960
michael batch
108
00:04:28,960 --> 00:04:32,600
充分的去利用流水线并行的一个效率
109
00:04:32,600 --> 00:04:34,460
这个我们刚才已经讲过了
110
00:04:34,460 --> 00:04:36,500
我f00 执行完之后呢
111
00:04:36,500 --> 00:04:39,270
就可以传给下一层执行f10 
112
00:04:39,270 --> 00:04:40,800
f10 算完之后呢
113
00:04:40,800 --> 00:04:43,050
就给在下一场f20 
114
00:04:43,050 --> 00:04:45,500
然后等整一层算完之后
115
00:04:45,500 --> 00:04:48,020
等所有的mini batch执行完之后呢
116
00:04:48,020 --> 00:04:50,740
就开始按照michael batch的方式
117
00:04:50,740 --> 00:04:54,130
在进行每一个数据的反向计算
118
00:04:54,130 --> 00:04:56,600
最后呢进行一个统一的更新
119
00:04:56,600 --> 00:04:59,090
这个就是michael batch带来的好处
120
00:04:59,090 --> 00:05:02,020
而文章里面就详细的说明了
121
00:05:02,020 --> 00:05:05,320
michael batch可以使得网络模型的bubble
122
00:05:05,320 --> 00:05:09,419
我们的计算的空载时间压缩的非常的小
123
00:05:09,419 --> 00:05:11,639
在网络模型优化的阶段呢
124
00:05:11,639 --> 00:05:14,489
g pi提出了一个从计算的概念
125
00:05:14,489 --> 00:05:16,679
也就是在反向计算的时候呢
126
00:05:16,679 --> 00:05:18,300
我计算b03 的时候
127
00:05:18,300 --> 00:05:22,260
实际上我是依赖于f03 的前一项的结果
128
00:05:22,260 --> 00:05:25,530
f03 前项的计算的结果的输出
129
00:05:25,530 --> 00:05:27,720
需要缓存到我们的内存里面
130
00:05:27,720 --> 00:05:32,510
网络模型的激活计算的数据量是非常大的
131
00:05:32,510 --> 00:05:35,480
如果把f00010203 
132
00:05:35,480 --> 00:05:38,160
包括前面的所有都缓存起来
133
00:05:38,160 --> 00:05:40,740
然后再给后面去计计算的时候
134
00:05:40,740 --> 00:05:44,020
我们的动态内存就会急剧的增加
135
00:05:44,020 --> 00:05:45,520
所以呢文章里面呢
136
00:05:45,520 --> 00:05:48,070
刚才就提出了一个从计算的概念
137
00:05:48,070 --> 00:05:49,960
就是我在反向的时候呢
138
00:05:49,960 --> 00:05:51,960
不用正向的时候的结果
139
00:05:51,960 --> 00:05:54,740
而是重新算一遍正向的结果
140
00:05:54,740 --> 00:05:58,010
我们的晶体管的计算的速率是非常高的
141
00:05:58,010 --> 00:05:59,900
远大于我们的内存的消耗
142
00:05:59,900 --> 00:06:01,360
还有内存的搬运
143
00:06:01,360 --> 00:06:04,810
所以呢这里面就提出了一个简单的优化的概念
144
00:06:04,810 --> 00:06:08,780
使得gpine里面可以存放更大的网络模型
145
00:06:08,780 --> 00:06:10,970
那我们从实验结果可以看到啊
146
00:06:10,970 --> 00:06:13,520
一个简单的原始的模型在哪
147
00:06:13,520 --> 00:06:17,800
if的情况下最多只能存下6.26个gb
148
00:06:17,800 --> 00:06:20,140
但是在巴卡优化的情况下
149
00:06:20,140 --> 00:06:21,910
可以存下26个gp
150
00:06:21,910 --> 00:06:23,920
而使用tpu这个硬件
151
00:06:23,920 --> 00:06:26,500
使用内男if的pine并行呢
152
00:06:26,500 --> 00:06:28,440
最多只能存3.15g
153
00:06:28,440 --> 00:06:31,800
但是使用gpp line在128卡里面呢
154
00:06:31,800 --> 00:06:35,240
可以塞得下800个gb的sm的网络模型
155
00:06:35,240 --> 00:06:37,340
所以这是非常夸张的一个概念
156
00:06:39,260 --> 00:06:41,570
了解完了if的流水并行
157
00:06:41,570 --> 00:06:44,180
还有其pipeline的流水并行之后呢
158
00:06:44,180 --> 00:06:47,030
我们现在来看看流水并行其实有两种模式啊
159
00:06:47,030 --> 00:06:49,760
刚才我们讲的都是aff等
160
00:06:49,760 --> 00:06:53,250
b就是for a diambulwark这种模式
161
00:06:53,250 --> 00:06:55,080
先算完每台机器
162
00:06:55,080 --> 00:06:56,820
每个mini batch的正向
163
00:06:56,820 --> 00:06:59,550
然后再算每个mini batch的反向
164
00:06:59,550 --> 00:07:02,190
这里面呢可能用一个batch来去代替
165
00:07:02,190 --> 00:07:05,400
但实际上在jp poly里面是用了michael batch
166
00:07:05,400 --> 00:07:07,680
那同样的在第二个stage的时候呢
167
00:07:07,680 --> 00:07:09,520
我还是算完我的正向
168
00:07:09,520 --> 00:07:11,470
然后再上我们的反向
169
00:07:11,470 --> 00:07:15,040
所以这里面呢叫做f等b for a than bel
170
00:07:15,040 --> 00:07:16,960
微软在2021年之后呢
171
00:07:16,960 --> 00:07:20,800
就提出了一个one forward and when bel的这种模式
172
00:07:20,800 --> 00:07:23,500
就是我算完一个正向之后
173
00:07:23,500 --> 00:07:24,920
全部算完一个正向
174
00:07:24,920 --> 00:07:28,310
我马上就开始一个反向的计算了
175
00:07:28,310 --> 00:07:29,840
在第二个michael batch的时候
176
00:07:29,840 --> 00:07:32,780
我算完第二个前项的时候呢
177
00:07:32,780 --> 00:07:34,880
马上进行第二个后项
178
00:07:34,880 --> 00:07:37,340
以这种方式去进行计算
179
00:07:37,340 --> 00:07:41,710
可以看到极大的去减少了机器的空载的时间
180
00:07:41,710 --> 00:07:45,319
也就是中间的bubble少了非常的多
181
00:07:45,319 --> 00:07:47,329
但是大家有没有发现呢
182
00:07:47,329 --> 00:07:52,289
在当update就是我初始的阶段或者能启动的阶段
183
00:07:52,289 --> 00:07:55,139
流水线的序列还是比较有序的
184
00:07:55,139 --> 00:07:57,200
但是到后面的阶段
185
00:07:57,200 --> 00:08:00,740
基本上就密密麻麻的越来越乱了
186
00:08:00,740 --> 00:08:02,900
那下面呢我们来看看paper dream
187
00:08:02,900 --> 00:08:05,400
这篇文章到底是怎么解决这个问题的
188
00:08:05,400 --> 00:08:09,540
同时我们也去看一下gpio遇到哪些问题
189
00:08:09,540 --> 00:08:12,760
所以微软的人才会提出pg
190
00:08:12,810 --> 00:08:15,580
现在我们已经打开了paper讯这篇文章
191
00:08:15,580 --> 00:08:17,320
我们翻到有图的地方
192
00:08:17,320 --> 00:08:18,460
一般没有涂的地方了
193
00:08:18,460 --> 00:08:20,720
我觉得唉很难去看这篇文章的
194
00:08:20,720 --> 00:08:22,570
或者已经看不下去了啊
195
00:08:22,570 --> 00:08:23,950
诶嘿嘿嘿
196
00:08:23,950 --> 00:08:25,540
那我们翻到有图的地方
197
00:08:25,540 --> 00:08:29,050
这个树林呢就是gpine的一种具体的格式
198
00:08:29,050 --> 00:08:31,490
我们放大一点去看一看啊
199
00:08:31,490 --> 00:08:34,880
可以看到其实我们分了非常多的mini batch
200
00:08:34,880 --> 00:08:36,860
1234反向的时候呢
201
00:08:36,860 --> 00:08:38,180
为什么会两个一呢
202
00:08:38,180 --> 00:08:41,840
是因为有一个一呢是重新计算纵向
203
00:08:41,840 --> 00:08:43,610
然后再进行反向计算的
204
00:08:43,610 --> 00:08:46,400
那这里面呢只是比gpine里面的那个图
205
00:08:46,400 --> 00:08:47,420
画的更清楚
206
00:08:47,420 --> 00:08:49,910
实际上它还是gpine的一个格式
207
00:08:49,910 --> 00:08:52,880
我们可以看到gponline最重要的一个概念
208
00:08:52,880 --> 00:08:54,500
就是提出了michael batch
209
00:08:54,500 --> 00:08:57,180
就是我的网络模型的batch
210
00:08:57,180 --> 00:08:58,980
切分得越细越好
211
00:08:58,980 --> 00:09:01,540
中间的空载的时间就会越少
212
00:09:01,540 --> 00:09:04,210
但是这就会引起一个问题哦
213
00:09:04,210 --> 00:09:07,150
会引起了大量的前向的重计算
214
00:09:07,150 --> 00:09:11,220
另外的话还会引起了频繁的流水线的交互
215
00:09:11,220 --> 00:09:15,060
使得我们的运算的时间进一步的拖长
216
00:09:15,060 --> 00:09:19,160
就是我的效率会降d第二就是大量的重计算
217
00:09:19,160 --> 00:09:21,260
会导致我们的网络模型的权重
218
00:09:21,260 --> 00:09:24,660
还有激活的中间变量急剧的上升
219
00:09:24,660 --> 00:09:26,580
我们本来做重计算的
220
00:09:26,580 --> 00:09:29,400
是为了解决我们的内存下降的问题
221
00:09:29,400 --> 00:09:32,520
使得我们的动态内存少很多
222
00:09:32,520 --> 00:09:35,800
通过计算去换取我们动态内存的空间
223
00:09:35,800 --> 00:09:37,780
但是我的micro batch越多
224
00:09:37,780 --> 00:09:39,460
我要做大量的重计算
225
00:09:39,460 --> 00:09:42,420
这个时候呢就会引起我们整个ai系统里面的
226
00:09:42,420 --> 00:09:45,870
权重和激活的中间变量变得更多
227
00:09:45,870 --> 00:09:47,580
这时候我们的静态内存
228
00:09:47,580 --> 00:09:49,530
款的动态内存就提上去了
229
00:09:49,530 --> 00:09:51,460
为了解决这两个问题呢
230
00:09:51,460 --> 00:09:55,740
所以微软呢就提出了paper dream的这个解决方案
231
00:09:55,740 --> 00:09:59,100
pgm呢对训练的阶段划分成为两个
232
00:09:59,100 --> 00:10:00,990
第一个呢是start up的阶段
233
00:10:00,990 --> 00:10:03,440
第二个呢就是steady的阶段
234
00:10:03,440 --> 00:10:05,390
就是我们的稳态和初始态
235
00:10:05,390 --> 00:10:06,620
初始态的时候呢
236
00:10:06,620 --> 00:10:09,980
可能还是正常的分开很多个michael batch
237
00:10:09,980 --> 00:10:11,140
或者叫做mini batch
238
00:10:11,140 --> 00:10:14,950
拍部剧里面的mini batch是等同于gpio的michael batch的
239
00:10:14,950 --> 00:10:17,200
我去划分成很多的michael batch
240
00:10:17,200 --> 00:10:19,640
然后再进行反向的计算
241
00:10:19,640 --> 00:10:21,140
但是反向的计算呢
242
00:10:21,140 --> 00:10:24,760
我是使用刚才介绍的wf wb这种方式
243
00:10:24,760 --> 00:10:26,500
ai系统执行一个正向
244
00:10:26,500 --> 00:10:28,000
马上执行一个反向
245
00:10:28,000 --> 00:10:29,020
执行一个正向
246
00:10:29,020 --> 00:10:31,100
马上再执行一个反向
247
00:10:31,100 --> 00:10:32,660
通过这种方式
248
00:10:32,660 --> 00:10:35,900
使得在steady state就是稳态的时候呢
249
00:10:35,900 --> 00:10:38,420
使得我们的ai系统啊基本上没有buble
250
00:10:38,420 --> 00:10:40,580
可以看到后面密密麻麻的
251
00:10:40,580 --> 00:10:43,990
基本上做完跟前项要进行后项ai芯片
252
00:10:43,990 --> 00:10:48,070
升腾的ai芯片在study day的时候是非常的繁忙的
253
00:10:48,070 --> 00:10:50,220
基本上都一直在进行计算
254
00:10:50,220 --> 00:10:52,950
另外我们的通讯也是非常的繁忙的
255
00:10:52,950 --> 00:10:56,480
不断的去进行一个通讯的同步和数据的传输
256
00:10:56,480 --> 00:10:57,800
图还是这个图
257
00:10:57,800 --> 00:11:00,720
虽然steady state呢让我们的foo进一步减少了
258
00:11:00,720 --> 00:11:02,850
但是大家有没有发现一个问题
259
00:11:02,850 --> 00:11:05,790
后面的这一坨非常的乱
260
00:11:05,790 --> 00:11:08,660
我的权重到底什么时候更新
261
00:11:08,660 --> 00:11:11,200
我的梯度什么时候同步
262
00:11:11,200 --> 00:11:14,050
这引起了一个很严峻的问题
263
00:11:14,050 --> 00:11:18,340
所以在pg这篇文章又提出了两个概念
264
00:11:18,340 --> 00:11:21,400
第一个解决方案呢就是way station
265
00:11:21,400 --> 00:11:23,830
中文我们叫做权重隐藏
266
00:11:23,830 --> 00:11:27,020
第二个解决方案就是vertical sync
267
00:11:27,020 --> 00:11:29,450
垂直同步两个概念
268
00:11:29,450 --> 00:11:34,840
下面我们来逐个的去看一下vc的血呢
269
00:11:34,840 --> 00:11:37,660
可以看到右边的我们多了一个红色的框框
270
00:11:37,660 --> 00:11:40,160
它意味着我们为每一个激活
271
00:11:40,160 --> 00:11:43,490
或者每一个计算的输出都保存100
272
00:11:43,490 --> 00:11:44,900
前项计算的时候呢
273
00:11:44,900 --> 00:11:47,440
每个都是用最新的参数数据处理
274
00:11:47,440 --> 00:11:48,880
我们的mini batch的
275
00:11:48,880 --> 00:11:51,520
然后把这份参数呢保存下来
276
00:11:51,520 --> 00:11:54,740
用于同一个mini fish的后项的计算
277
00:11:54,740 --> 00:11:58,360
我们对第五个mini batch进行前向计算的时候
278
00:11:58,360 --> 00:12:02,800
用的是前一个最新的反向计算的一个激活
279
00:12:02,800 --> 00:12:05,680
去更新我们第五个的正向计算
280
00:12:05,680 --> 00:12:08,740
然后呢第五个的反向计算的时候呢
281
00:12:08,740 --> 00:12:11,200
保留了前项计算一的这份权重
282
00:12:11,200 --> 00:12:13,030
然后呢丢给五去计算
283
00:12:13,030 --> 00:12:15,820
但是呢五的时候已经经过了234了
284
00:12:15,820 --> 00:12:19,000
所以系统里面会同时去维护权重
285
00:12:19,000 --> 00:12:21,620
1234这几份摩卡三
286
00:12:21,620 --> 00:12:24,500
在第五个mini batch前项计算的时候呢
287
00:12:24,500 --> 00:12:28,270
用的是mini batch 3的反向的权重参数
288
00:12:28,270 --> 00:12:31,690
然后mini batch 3的反向的权重参数
289
00:12:31,690 --> 00:12:34,150
mini batch 5反向的时候呢
290
00:12:34,150 --> 00:12:37,560
同样更新的时候用的是mini batch 3
291
00:12:37,560 --> 00:12:39,000
反向的权重参数
292
00:12:39,000 --> 00:12:42,600
还有mini batch 5正向的权重
293
00:12:42,600 --> 00:12:45,420
所以ai系统呢会维护一个mini batch 3
294
00:12:45,420 --> 00:12:48,619
和mini batch 4的权重权重版本
295
00:12:48,619 --> 00:12:51,619
保证我的正向和反向是相关联的
296
00:12:51,619 --> 00:12:53,869
而不算我的二的反向的时候呢
297
00:12:53,869 --> 00:12:55,440
用的是五的正向
298
00:12:55,440 --> 00:12:58,060
我穿四的反向的时候用的是七的正向
299
00:12:58,060 --> 00:12:59,170
不是这么来的
300
00:12:59,170 --> 00:13:02,500
我算五的反向的时候用的是五的正向
301
00:13:02,500 --> 00:13:06,100
这样才能够使得我们的权重参数能够对应上来
302
00:13:06,100 --> 00:13:08,600
不会随便的被刷新
303
00:13:08,600 --> 00:13:10,580
使得网络模型不收敛
304
00:13:10,580 --> 00:13:12,140
计算mini batch 5的时候
305
00:13:12,140 --> 00:13:14,740
我用的是mini batch一去更新的
306
00:13:14,740 --> 00:13:16,300
所以在workout的时候
307
00:13:16,300 --> 00:13:18,550
我去计算mini batch 5的正向
308
00:13:18,550 --> 00:13:21,160
同样mini batch一的反向
309
00:13:21,160 --> 00:13:24,400
在walker 3 mini bh 5计算的时候
310
00:13:24,400 --> 00:13:28,460
我用的仍然是walker 3 mini batch一的反向
311
00:13:28,460 --> 00:13:29,630
以此类推
312
00:13:29,630 --> 00:13:31,370
在论文的实验里面的
313
00:13:31,370 --> 00:13:34,040
作者就告诉我们一个很重要的概念
314
00:13:34,040 --> 00:13:36,080
就是前面的westash
315
00:13:36,080 --> 00:13:38,540
我们的权重隐藏这个功能
316
00:13:38,540 --> 00:13:41,660
其实已经很好的对我们的网络模型的参数
317
00:13:41,660 --> 00:13:42,800
进行了同步了
318
00:13:42,800 --> 00:13:44,060
所以一般来说呢
319
00:13:44,060 --> 00:13:46,620
在超大规模网络模型训练的时候
320
00:13:46,620 --> 00:13:50,460
如果真的不是因为流水线并行引起的不收敛
321
00:13:50,460 --> 00:13:54,130
那他们默认是关闭垂直同步的
322
00:13:54,130 --> 00:13:56,080
只使用了权重隐藏
323
00:13:56,080 --> 00:14:00,350
westash这个功能在pjm这篇文章里面
324
00:14:00,350 --> 00:14:04,880
毫无悬念的就是它的效果性能还是非常的好的
325
00:14:04,880 --> 00:14:08,320
这里面呢我们就不跟大家一起去看实验结果了
326
00:14:08,320 --> 00:14:10,240
下面呢我们来总结一下
327
00:14:10,240 --> 00:14:12,700
今天流水线并行的一个概念
328
00:14:12,700 --> 00:14:16,540
首先呢模型并行主要分为张量并行和流水并行
329
00:14:16,540 --> 00:14:18,080
也叫做流水线并行啊
330
00:14:18,080 --> 00:14:20,120
三根并行呢是层内并行
331
00:14:20,120 --> 00:14:23,360
流水线并行呢用作呈肩并行
332
00:14:23,360 --> 00:14:26,120
那流水线并行作为模型并行的一部分呢
333
00:14:26,120 --> 00:14:28,020
一般不会单独去使用的
334
00:14:28,020 --> 00:14:29,400
而是通过混合
335
00:14:29,400 --> 00:14:32,640
张量并行和数据并行进行一起使用的
336
00:14:32,640 --> 00:14:36,000
刚才我们只是单单的去拎出了流水线并行
337
00:14:36,000 --> 00:14:38,000
来去讨论这个原理
338
00:14:38,000 --> 00:14:39,920
在下一个内容里面
339
00:14:39,920 --> 00:14:41,200
我们就会讲讲
340
00:14:41,200 --> 00:14:43,990
如何执行一个混合并行的网络模型
341
00:14:43,990 --> 00:14:46,120
那另外的话流水线并行呢
342
00:14:46,120 --> 00:14:49,660
从最难if的方式到f等于win的模式
343
00:14:49,660 --> 00:14:51,160
逐渐发展到wf
344
00:14:51,160 --> 00:14:53,960
wb就是一个正向一个反向的模式
345
00:14:53,960 --> 00:14:55,640
从另外一个方面来看呢
346
00:14:55,640 --> 00:14:58,180
从mini batch发展到michael batch
347
00:14:58,180 --> 00:15:02,000
每批次处理数据的力度呢会更加的细
348
00:15:02,000 --> 00:15:04,760
这个视频呢讲的时间会非常的短
349
00:15:04,760 --> 00:15:07,550
只是给大家带来一个概念上的认知
350
00:15:07,550 --> 00:15:10,440
或者一起去探讨更新的一些技术
351
00:15:10,440 --> 00:15:14,640
周米更加希望大家真的去看一下gpl和pg
352
00:15:14,640 --> 00:15:15,780
这两篇文章呢
353
00:15:15,780 --> 00:15:18,990
是有非常多的知识值得我们去学习的
354
00:15:18,990 --> 00:15:20,220
卷的不行了
355
00:15:20,220 --> 00:15:21,120
卷的不行了
356
00:15:21,120 --> 00:15:22,760
记得一键三连加关注哦
357
00:15:22,760 --> 00:15:24,260
所有的内容都会开源
358
00:15:24,260 --> 00:15:27,200
在下面这条链接里面摆了个拜
