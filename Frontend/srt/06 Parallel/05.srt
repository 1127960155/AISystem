1
00:00:00,000 --> 00:00:06,040
Hello 大家好

2
00:00:06,040 --> 00:00:07,040
我是中米

3
00:00:07,040 --> 00:00:09,800
我们来到了大模型与分步式训练

4
00:00:09,800 --> 00:00:11,040
分步式

5
00:00:11,040 --> 00:00:14,400
分步式并行里面的最后一个环节了

6
00:00:14,400 --> 00:00:15,640
接近最后一个环节

7
00:00:15,640 --> 00:00:17,160
就是我们的流水并行

8
00:00:17,160 --> 00:00:18,480
Pipe online 并行

9
00:00:18,480 --> 00:00:21,680
我们之前了解了分步式并行的数据并行

10
00:00:21,680 --> 00:00:22,400
模型并行

11
00:00:22,400 --> 00:00:23,640
或者叫做张量并行

12
00:00:23,640 --> 00:00:25,400
然后到流水线并行

13
00:00:25,400 --> 00:00:27,800
最后我们还会有一个环节

14
00:00:27,800 --> 00:00:30,800
就是把刚才前面的这些并行

15
00:00:30,800 --> 00:00:32,720
组合起来变成一个混合并行

16
00:00:32,720 --> 00:00:35,320
我们今天的内容主要是集中在

17
00:00:36,840 --> 00:00:38,920
模型并行里面的流水线并行

18
00:00:38,920 --> 00:00:41,080
又叫做Pipe online parallelism

19
00:00:41,080 --> 00:00:43,600
这里面我们会分开三个内容来讲

20
00:00:43,600 --> 00:00:45,480
第一个就是最原始的流水并行

21
00:00:45,480 --> 00:00:46,800
到底是个什么东西

22
00:00:46,800 --> 00:00:48,640
然后我们会分享两篇论文

23
00:00:48,640 --> 00:00:50,960
第一篇就是谷歌的GPipe online

24
00:00:50,960 --> 00:00:53,080
第二篇就是Rayron的Pipe online

25
00:00:54,800 --> 00:00:57,520
下面我们来看看流水并行的一个概念

26
00:00:57,520 --> 00:00:58,720
其实我们上一个视频

27
00:00:58,720 --> 00:01:02,440
已经简单的去给大家去分享了一个流水并行

28
00:01:02,440 --> 00:01:04,720
流水并行主要是层间的并行

29
00:01:04,720 --> 00:01:06,960
我把网络模型的不同的层

30
00:01:06,960 --> 00:01:09,560
放在不同的设备上面去执行

31
00:01:09,560 --> 00:01:11,840
这里面引入一个概念叫做stage

32
00:01:11,840 --> 00:01:13,200
或者叫做阶段

33
00:01:13,200 --> 00:01:16,480
我可能Devices 1可以把第一层第二层放进去

34
00:01:16,480 --> 00:01:18,440
这个我们叫做stage 1

35
00:01:18,440 --> 00:01:20,760
把第三层放在Devices 2的时候

36
00:01:20,760 --> 00:01:22,440
我们可能会叫做stage 2

37
00:01:22,440 --> 00:01:23,680
以此类推

38
00:01:23,920 --> 00:01:25,280
蛮严一啊

39
00:01:25,880 --> 00:01:29,400
接下来我们换一个角度去看待这个问题

40
00:01:29,400 --> 00:01:32,920
因为流水并行实际上是跟时间有关系的

41
00:01:32,920 --> 00:01:35,360
所以我们现在换了一种表达的方式

42
00:01:35,360 --> 00:01:36,800
由右边的这个图所示

43
00:01:37,200 --> 00:01:38,560
左边的这个势力图里面

44
00:01:38,560 --> 00:01:40,880
我们把每一层当做一个stage

45
00:01:40,880 --> 00:01:43,000
放在一台不同的设备里面

46
00:01:43,000 --> 00:01:46,000
首先我会做一个正向的计算

47
00:01:46,000 --> 00:01:48,840
然后通过我的loss做一个反向的计算

48
00:01:48,840 --> 00:01:50,880
然后放在三个不同的卡

49
00:01:51,320 --> 00:01:52,720
放到右边的这个图

50
00:01:52,720 --> 00:01:54,600
F代表我的正向forward

51
00:01:54,600 --> 00:01:56,840
B代表我的反向backward

52
00:01:56,840 --> 00:02:01,480
可以看到每一块机器都去执行自己层之间的一个计算

53
00:02:01,480 --> 00:02:04,680
最后再统一对四个卡进行更新

54
00:02:04,680 --> 00:02:08,520
这种就是最Nine-if的并行方式

55
00:02:08,520 --> 00:02:11,520
Nine-if的Pepeline并行其实有一个很大的问题

56
00:02:11,520 --> 00:02:14,840
就是会有大量的等待和阻塞的时间

57
00:02:14,840 --> 00:02:17,000
可以看到中间的空窄的时间

58
00:02:17,000 --> 00:02:19,080
就是我的机器啥都不执行

59
00:02:19,080 --> 00:02:20,800
就在那等待的时间

60
00:02:20,800 --> 00:02:24,520
我刚才黄线画的这些地方是非常的多的

61
00:02:24,520 --> 00:02:28,480
科学家们又引入了另外一种并行模式

62
00:02:28,480 --> 00:02:31,120
叫做mini-batch的Pepeline并行

63
00:02:31,120 --> 00:02:33,720
刚才F0是整一个连在一起的

64
00:02:33,720 --> 00:02:35,880
它执行的是一个batch

65
00:02:35,880 --> 00:02:39,560
现在我们把一个batch拆分成很多小的batch

66
00:02:39,560 --> 00:02:42,480
通过小的batch去进行一个计算

67
00:02:42,480 --> 00:02:47,160
这个时候我们就可以充分的利用了我们计算和通讯的时间

68
00:02:47,200 --> 00:02:51,000
把一个大的batch拆分成很多小的mini-batch之后

69
00:02:51,000 --> 00:02:52,880
或者叫做micro-batch之后

70
00:02:52,880 --> 00:02:55,160
这个AI框架或者AI系统

71
00:02:55,160 --> 00:02:58,600
就可以充分的去利用我们芯片的计算时间

72
00:02:58,600 --> 00:03:00,960
还有运算的空窄的时间

73
00:03:00,960 --> 00:03:06,200
使得我们的bubble机器空窄的时间越少

74
00:03:06,200 --> 00:03:11,320
另外一个好处就是机器跟机器之间的相互等待的时间也越少

75
00:03:11,320 --> 00:03:15,800
这里面可以看到F10跟F01之间是并行去操作的

76
00:03:15,800 --> 00:03:18,080
Devices 0在执行计算的时候

77
00:03:18,080 --> 00:03:20,000
Devices 1也开始执行了

78
00:03:20,000 --> 00:03:23,520
就是我一个小的batch在F00里面执行完之后

79
00:03:23,520 --> 00:03:26,920
我去把下一个数据传给我的F10

80
00:03:26,920 --> 00:03:29,080
F10计算完之后传给我的F20

81
00:03:29,080 --> 00:03:30,880
然后再传给我的F30

82
00:03:30,880 --> 00:03:33,320
而mini-batch里面最经典的一篇文章

83
00:03:33,320 --> 00:03:39,320
就是GPiPoLine谷歌2019年的时候发明的一个流水线并行的一篇文章

84
00:03:39,320 --> 00:03:42,280
这篇文章提出了三个比较重要的概念

85
00:03:42,280 --> 00:03:44,000
第一个是partition stage

86
00:03:44,000 --> 00:03:46,920
就是把我们的网络模型分开不同的阶段

87
00:03:46,920 --> 00:03:50,680
第二个就是在Pipeline里面去引入了microbatch

88
00:03:50,680 --> 00:03:53,160
第三个就是从计算

89
00:03:53,160 --> 00:03:55,600
下面一起来看看这篇文章

90
00:03:55,600 --> 00:03:59,240
接下来让我们有请下一位学员登场

91
00:03:59,240 --> 00:04:04,160
GPiPoLine这篇文章叫做Easy Scaling with Microbatch

92
00:04:04,160 --> 00:04:07,440
其中Microbatch是一个比较重要的概念

93
00:04:07,440 --> 00:04:13,440
第二个比较重要的概念就是Pipeline Parallelize流水线并行

94
00:04:13,480 --> 00:04:17,560
里面很重要的一点就是网络模型的切分成不同的stage

95
00:04:17,560 --> 00:04:19,600
不是按照一层一层的切的

96
00:04:19,600 --> 00:04:23,960
而是可能会两三层变成一个stage进行切分

97
00:04:23,960 --> 00:04:29,240
第二点就是把mini-batch进一步的划分成为microbatch

98
00:04:29,240 --> 00:04:32,840
充分的去利用流水线并行的一个效率

99
00:04:32,840 --> 00:04:34,600
这个我们刚才已经讲过了

100
00:04:34,600 --> 00:04:38,280
我f00执行完之后就可以传给下一层

101
00:04:38,280 --> 00:04:43,360
执行f10,f10算完之后就给在下一层f20

102
00:04:43,360 --> 00:04:45,720
然后等整一层算完之后

103
00:04:45,720 --> 00:04:48,120
等所有的mini-batch执行完之后

104
00:04:48,120 --> 00:04:50,920
就开始按照microbatch的方式

105
00:04:50,920 --> 00:04:54,320
再进行每一个数据的反向计算

106
00:04:54,320 --> 00:04:56,800
最后进行一个统一的更新

107
00:04:56,800 --> 00:04:59,320
这个就是microbatch带来的好处

108
00:04:59,320 --> 00:05:02,160
而文章里面就详细的说明了

109
00:05:02,160 --> 00:05:05,400
microbatch可以使得网络模型的bubble

110
00:05:05,400 --> 00:05:09,640
我们的计算的空窄时间压缩的非常的小

111
00:05:09,640 --> 00:05:11,760
在网络模型优化的阶段

112
00:05:11,800 --> 00:05:14,760
GPAP提出了一个从计算的概念

113
00:05:14,760 --> 00:05:16,720
也就是在反向计算的时候

114
00:05:16,720 --> 00:05:18,440
我计算b03的时候

115
00:05:18,440 --> 00:05:22,480
实际上我是依赖于f03的前项的结果

116
00:05:22,480 --> 00:05:25,720
f03前项的计算的结果的输出

117
00:05:25,720 --> 00:05:27,960
需要缓存到我们的内存里面

118
00:05:27,960 --> 00:05:32,720
网络模型的激活计算的数据量是非常的大的

119
00:05:32,720 --> 00:05:35,560
如果把f000,01,02,03

120
00:05:35,560 --> 00:05:38,240
包括前面的所有都缓存起来

121
00:05:38,240 --> 00:05:40,920
然后再给后面去计算的时候

122
00:05:40,920 --> 00:05:44,280
我们的动态内存就会急剧的增加

123
00:05:44,280 --> 00:05:48,240
所以文章里面刚才就提出了一个从计算的概念

124
00:05:48,240 --> 00:05:52,160
就是我在反向的时候不用正向的时候的结果

125
00:05:52,160 --> 00:05:54,880
而是重新算一遍正向的结果

126
00:05:54,880 --> 00:05:58,120
我们的晶体管的计算的数率是非常高的

127
00:05:58,120 --> 00:06:01,520
远大于我们的内存的消耗还有内存的搬运

128
00:06:01,520 --> 00:06:04,960
所以这里面就提出了一个简单的优化的概念

129
00:06:04,960 --> 00:06:08,960
使得GPAP里面可以存放更大的网络模型

130
00:06:09,000 --> 00:06:11,120
我们从实验结果可以看到

131
00:06:11,120 --> 00:06:14,960
一个简单的原始的模型在Ni-IF的情况下

132
00:06:14,960 --> 00:06:18,040
最多只能存下6.26GB

133
00:06:18,040 --> 00:06:22,120
但是在8卡优化的情况下可以存下26GB

134
00:06:22,120 --> 00:06:24,040
而使用TPU这个硬件

135
00:06:24,040 --> 00:06:28,720
使用Ni-IF的Pipeline并行最多只能存3.15G

136
00:06:28,720 --> 00:06:32,080
但是使用GPAP-9在128卡里面

137
00:06:32,080 --> 00:06:35,360
可以塞得下800个GB的Transformer的网络模型

138
00:06:35,360 --> 00:06:37,520
所以这是非常夸张的一个概念

139
00:06:39,360 --> 00:06:41,680
了解完Ni-IF的流水并行

140
00:06:41,680 --> 00:06:44,160
还有GPAP-9的流水并行之后

141
00:06:44,280 --> 00:06:47,120
我们现在来看看流水并行其实有两种模式

142
00:06:47,120 --> 00:06:50,440
刚才我们讲的都是F-B

143
00:06:50,440 --> 00:06:53,360
就是Fall-Bedwalk这种模式

144
00:06:53,360 --> 00:06:56,960
先算完每台机器每个mini-batch的正向

145
00:06:56,960 --> 00:06:59,720
然后再算每个mini-batch的反向

146
00:06:59,720 --> 00:07:02,280
这里面可能用一个batch来去代替

147
00:07:02,280 --> 00:07:05,480
但实际上在GPAP-9里面是用了micro-batch

148
00:07:05,480 --> 00:07:07,640
同样的在第二个stage的时候

149
00:07:07,640 --> 00:07:09,640
我还是算完我的正向

150
00:07:09,640 --> 00:07:11,600
然后再算我们的反向

151
00:07:11,600 --> 00:07:15,080
所以这里面叫做F-B-Fall-Bedwalk

152
00:07:15,080 --> 00:07:16,920
微软在2021年之后

153
00:07:17,080 --> 00:07:20,880
就提出了一个Want-Fall-Bedwalk的这种模式

154
00:07:20,880 --> 00:07:23,560
就是我算完一个正向之后

155
00:07:23,560 --> 00:07:25,040
全部算完一个正向

156
00:07:25,040 --> 00:07:28,400
我马上就开始一个反向的计算了

157
00:07:28,400 --> 00:07:29,960
在第二个micro-batch的时候

158
00:07:29,960 --> 00:07:32,600
我算完第二个前向的时候

159
00:07:32,600 --> 00:07:35,000
马上进行第二个后向

160
00:07:35,000 --> 00:07:37,440
以这种方式去进行计算

161
00:07:37,480 --> 00:07:41,840
可以看到极大的去减少了机器的空窄的时间

162
00:07:41,840 --> 00:07:45,480
也就是中间的bubble少了非常的多

163
00:07:45,480 --> 00:07:47,480
但是大家有没有发现

164
00:07:47,480 --> 00:07:49,080
在start-up stage

165
00:07:49,080 --> 00:07:52,400
就是我初始的阶段或者能启动的阶段

166
00:07:52,400 --> 00:07:55,160
流水线的序列还是比较有序的

167
00:07:55,160 --> 00:07:57,280
但是到后面的阶段

168
00:07:57,280 --> 00:08:00,760
基本上就密密麻麻的越来越乱了

169
00:08:00,760 --> 00:08:03,560
下面我们来去看看Python Dream这篇文章

170
00:08:03,560 --> 00:08:05,480
到底是怎么解决这个问题的

171
00:08:05,600 --> 00:08:09,640
同时我们也去看一下GPypo遇到哪些问题

172
00:08:09,640 --> 00:08:12,480
所以微软的人才会提出Pypo Dream

173
00:08:13,360 --> 00:08:15,760
现在我们已经打开了Pypo Dream这篇文章

174
00:08:15,760 --> 00:08:17,360
我们翻到有图的地方

175
00:08:17,360 --> 00:08:18,520
一般没有图的地方

176
00:08:18,520 --> 00:08:20,760
我觉得很难去看这篇文章

177
00:08:20,760 --> 00:08:22,200
或者已经看不下去了

178
00:08:24,280 --> 00:08:25,560
我们翻到有图的地方

179
00:08:25,560 --> 00:08:29,120
figure 3就是GPypo line的一种具体的格式

180
00:08:29,120 --> 00:08:31,360
我们放大一点去看一看

181
00:08:31,360 --> 00:08:34,920
可以看到其实我们分了非常多的mini-batch

182
00:08:34,920 --> 00:08:35,920
1234

183
00:08:35,920 --> 00:08:38,200
反向的时候为什么会两个1呢

184
00:08:38,200 --> 00:08:41,960
是因为有一个1是重新计算正向

185
00:08:41,960 --> 00:08:43,800
然后再进行反向计算的

186
00:08:43,800 --> 00:08:47,520
这里面只是比GPypo line里面的图画的更清楚

187
00:08:47,520 --> 00:08:50,160
实际上它还是GPypo line的一个格式

188
00:08:50,160 --> 00:08:53,000
我们可以看到GPypo line最重要的一个概念

189
00:08:53,000 --> 00:08:54,560
就是提出了micro-batch

190
00:08:54,560 --> 00:08:59,200
就是我的网络模型的batch切分的越细越好

191
00:08:59,200 --> 00:09:01,720
中间空窄的时间就会越少

192
00:09:01,720 --> 00:09:03,800
但是这就会引起一个问题

193
00:09:04,360 --> 00:09:07,280
会引起了大量的前向的重计算

194
00:09:07,280 --> 00:09:11,240
另外的话还会引起了频繁的流水线的交互

195
00:09:11,240 --> 00:09:15,320
使得我们的运算的时间进一步的拖长

196
00:09:15,320 --> 00:09:17,560
就是我的效率会降低

197
00:09:17,560 --> 00:09:19,200
第二就是大量的重计算

198
00:09:19,200 --> 00:09:21,400
会导致我们的网络模型的权重

199
00:09:21,400 --> 00:09:24,840
还有激活的中间变量急剧的上升

200
00:09:24,840 --> 00:09:26,640
我们本来做重计算

201
00:09:26,640 --> 00:09:29,520
是为了解决我们的内存下降的问题

202
00:09:29,520 --> 00:09:32,720
使得我们的动态内存少很多

203
00:09:32,720 --> 00:09:36,000
通过计算去换取我们动态内存的空间

204
00:09:36,000 --> 00:09:37,880
但是我的micro-batch越多

205
00:09:37,880 --> 00:09:39,560
我要做大量的重计算

206
00:09:39,560 --> 00:09:42,600
这个时候就会引起我们整个AI系统里面的

207
00:09:42,600 --> 00:09:46,400
权重和激活的中间变量变得更多

208
00:09:46,400 --> 00:09:49,680
这时候我们的静态内存和动态内存又提上去了

209
00:09:49,680 --> 00:09:51,600
为了解决这两个问题

210
00:09:51,600 --> 00:09:56,160
所以微软就提出了Pypo Dream的解决方案

211
00:09:56,160 --> 00:09:59,160
Pypo Dream对训练的阶段划分成为两个

212
00:09:59,160 --> 00:10:01,200
第一个是start up的阶段

213
00:10:01,200 --> 00:10:03,560
第二个就是steady的阶段

214
00:10:03,560 --> 00:10:05,520
就是我们的稳态和初始态

215
00:10:05,520 --> 00:10:08,200
初始态的时候可能还是正常的

216
00:10:08,200 --> 00:10:11,240
分开很多个micro-batch或者叫做mini-batch

217
00:10:11,240 --> 00:10:15,040
Pypo Dream里面的mini-batch是等同于Gpypo的micro-batch的

218
00:10:15,040 --> 00:10:17,280
我去划分成很多的micro-batch

219
00:10:17,280 --> 00:10:19,840
然后再进行反向的计算

220
00:10:19,840 --> 00:10:21,080
但是反向的计算

221
00:10:21,080 --> 00:10:24,920
我是使用刚才介绍的1F1B这种方式

222
00:10:24,920 --> 00:10:26,520
AI系统执行一个正向

223
00:10:26,520 --> 00:10:28,080
马上执行一个反向

224
00:10:28,080 --> 00:10:29,080
执行一个正向

225
00:10:29,080 --> 00:10:31,080
马上再执行一个反向

226
00:10:31,360 --> 00:10:34,920
通过这种方式使得在steady state

227
00:10:34,920 --> 00:10:35,920
就是稳态的时候

228
00:10:35,920 --> 00:10:38,520
使得我们的AI系统基本上没有bubble

229
00:10:38,520 --> 00:10:40,600
可以看到后面密密麻麻的

230
00:10:40,600 --> 00:10:43,040
基本上做完一个前向又再进行后向

231
00:10:43,040 --> 00:10:44,080
AI芯片

232
00:10:44,080 --> 00:10:46,480
生腾的AI芯片在steady state的时候

233
00:10:46,480 --> 00:10:48,200
是非常的繁忙的

234
00:10:48,200 --> 00:10:50,360
基本上都一直在进行计算

235
00:10:50,360 --> 00:10:53,160
另外我们的通讯也是非常的繁忙的

236
00:10:53,160 --> 00:10:56,640
不断的去进行一个通讯的同步和数据的传输

237
00:10:56,640 --> 00:10:57,880
图还是这个图

238
00:10:57,880 --> 00:11:00,840
虽然steady state让我们的bubble进一步减少了

239
00:11:00,840 --> 00:11:03,160
但是大家有没有发现一个问题

240
00:11:03,160 --> 00:11:06,080
后面的这一坨非常的乱

241
00:11:06,080 --> 00:11:08,760
我的权重到底什么时候更新

242
00:11:08,760 --> 00:11:11,400
我的T度什么时候同步

243
00:11:11,400 --> 00:11:14,200
这引起了一个很严峻的问题

244
00:11:14,200 --> 00:11:17,880
所以在PypoGym这篇文章又提出了两个概念

245
00:11:18,840 --> 00:11:21,520
第一个解决方案就是way station

246
00:11:21,520 --> 00:11:24,160
中文我们叫做权重隐藏

247
00:11:24,160 --> 00:11:27,320
第二个解决方案就是vertical sync

248
00:11:27,320 --> 00:11:28,680
垂折同步

249
00:11:28,680 --> 00:11:29,560
两个概念

250
00:11:29,600 --> 00:11:31,560
下面我们来足格的去看一下

251
00:11:34,080 --> 00:11:37,760
way station可以看到右边我们多了一个红色的框框

252
00:11:37,760 --> 00:11:40,360
它意味着我们为每一个激活

253
00:11:40,360 --> 00:11:43,680
或者每一个计算的输出都保存一半

254
00:11:43,680 --> 00:11:44,920
前向计算的时候

255
00:11:44,920 --> 00:11:49,000
每个stage都是用最新的参数去处理我们的mini-batch的

256
00:11:49,000 --> 00:11:51,680
然后把这份参数保存下来

257
00:11:51,680 --> 00:11:55,240
用于同一个mini-batch的后向的计算

258
00:11:55,240 --> 00:11:58,520
我们对第五个mini-batch进行前向计算的时候

259
00:11:58,520 --> 00:12:02,960
用的是前一个最新的反向计算的一个激活

260
00:12:02,960 --> 00:12:05,840
去更新我们第五个的正向计算

261
00:12:05,840 --> 00:12:08,920
然后第五个的反向计算的时候

262
00:12:08,920 --> 00:12:11,320
保留了前向计算1的这份权重

263
00:12:11,320 --> 00:12:13,160
然后丢给5去计算

264
00:12:13,160 --> 00:12:15,960
但是5的时候已经经过了2、3、4了

265
00:12:15,960 --> 00:12:21,160
所以系统里面会同时去维护权重1、2、3、4这几份

266
00:12:21,160 --> 00:12:24,520
莫可三在第五个mini-batch前向计算的时候

267
00:12:24,520 --> 00:12:28,480
用的是mini-batch3的反向的权重参数

268
00:12:28,480 --> 00:12:32,000
然后mini-batch3的反向的权重参数

269
00:12:32,000 --> 00:12:34,320
mini-batch5反向的时候

270
00:12:34,320 --> 00:12:39,160
同样更新的时候用的是mini-batch3反向的权重参数

271
00:12:39,160 --> 00:12:42,760
还有mini-batch5正向的权重

272
00:12:42,760 --> 00:12:49,000
所以AI系统会维护一个mini-batch3和mini-batch4的权重版本

273
00:12:49,000 --> 00:12:51,760
保证我的正向和反向是相关联的

274
00:12:51,760 --> 00:12:54,040
而不算我的2的反向的时候

275
00:12:54,040 --> 00:12:55,560
用的是5的正向

276
00:12:55,560 --> 00:12:58,240
我穿4的反向的时候用的是7的正向

277
00:12:58,240 --> 00:12:59,280
不是这么来的

278
00:12:59,280 --> 00:13:02,640
我穿5的反向的时候用的是5的正向

279
00:13:02,640 --> 00:13:06,160
这样才能够使得我们的权重参数能够对应上来

280
00:13:06,160 --> 00:13:08,880
不会随便的被刷新

281
00:13:08,880 --> 00:13:10,800
使得网络模型不收敛

282
00:13:10,800 --> 00:13:14,880
计算mini-batch5的时候我用的是mini-batch1去更新的

283
00:13:14,880 --> 00:13:18,680
所以在wok2的时候我去计算mini-batch5的正向

284
00:13:18,680 --> 00:13:21,320
同样mini-batch1的反向

285
00:13:21,320 --> 00:13:26,760
在wok3mini-batch5计算的时候我用的仍然是wok3

286
00:13:26,800 --> 00:13:30,040
mini-batch1的反向以此类推

287
00:13:30,040 --> 00:13:31,480
在论文的实验里面

288
00:13:31,480 --> 00:13:34,200
作者就告诉我们一个很重要的概念

289
00:13:34,200 --> 00:13:37,760
就是前面的waste-我们的权重隐藏

290
00:13:37,760 --> 00:13:40,000
这个功能其实已经很好的

291
00:13:40,000 --> 00:13:42,960
对我们的网络模型的参数进行了同步了

292
00:13:42,960 --> 00:13:44,080
所以一般来说

293
00:13:44,080 --> 00:13:46,760
在超大规模网络模型训练的时候

294
00:13:46,760 --> 00:13:50,640
如果真的不是因为流水线并行引起的不收敛

295
00:13:50,640 --> 00:13:54,240
他们默认是关闭垂直同步的

296
00:13:54,240 --> 00:13:58,040
只使用了权重隐藏waste-这个功能

297
00:13:58,720 --> 00:14:00,480
在pipoGM这篇文章里面

298
00:14:00,480 --> 00:14:05,000
毫无悬念的就是它的效果性能还是非常的好的

299
00:14:05,000 --> 00:14:08,320
这里面我们就不跟大家一起去看实验结果了

300
00:14:08,320 --> 00:14:12,720
下面我们来总结一下今天流水线并行的概念

301
00:14:12,720 --> 00:14:16,640
首先模型并行主要分为张量并行和流水并行

302
00:14:16,640 --> 00:14:17,800
也叫做流水线并行

303
00:14:17,800 --> 00:14:20,240
张量并行是层类并行

304
00:14:20,240 --> 00:14:23,440
流水线并行用作层间并行

305
00:14:23,600 --> 00:14:26,160
流水线并行作为模型并行的一部分

306
00:14:26,160 --> 00:14:28,160
一般不会单独去使用的

307
00:14:28,160 --> 00:14:32,760
而是通过混合张量并行和数据并行进行一起使用的

308
00:14:32,760 --> 00:14:36,240
刚才我们只是单单去拎出了流水线并行

309
00:14:36,240 --> 00:14:38,200
去讨论这个原理

310
00:14:38,200 --> 00:14:40,040
在下一个内容里面

311
00:14:40,040 --> 00:14:44,240
我们就会讲讲如何执行一个混合并行的网络模型

312
00:14:44,240 --> 00:14:45,120
另外的话

313
00:14:45,120 --> 00:14:49,720
流水线并行从最naive的方式到f1的模式

314
00:14:49,720 --> 00:14:51,680
逐渐发展到1f1b

315
00:14:51,680 --> 00:14:54,120
就是一个正向一个反向的模式

316
00:14:54,120 --> 00:14:55,680
从另外一个方面来看

317
00:14:55,680 --> 00:14:58,440
从mini-batch发展到micro-batch

318
00:14:58,440 --> 00:15:02,480
每批次处理数据的力度会更加的细

319
00:15:02,480 --> 00:15:04,840
这个视频讲的时间会非常的短

320
00:15:04,840 --> 00:15:07,720
只是给大家带来一个概念上的认知

321
00:15:07,720 --> 00:15:10,640
或者一起去探讨更新的一些技术

322
00:15:10,640 --> 00:15:14,800
zoom也更加希望大家真的去看一下Gpipeline和Pipeline Gym

323
00:15:14,800 --> 00:15:18,560
这两篇文章是有非常多的知识值得我们去学习的

324
00:15:19,520 --> 00:15:21,160
卷的不行

325
00:15:21,200 --> 00:15:22,960
记得一键三连加关注

326
00:15:22,960 --> 00:15:26,560
所有的内容都会开源在下面这条链接里面

327
00:15:26,560 --> 00:15:27,840
拜了个拜

