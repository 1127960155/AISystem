1
00:00:00,000 --> 00:00:04,800
巴巴巴巴巴巴巴巴巴

2
00:00:04,800 --> 00:00:07,000
Hello 大家好,我是ZOMI

3
00:00:07,000 --> 00:00:10,000
今天我们来到大模型与分布式训练里面的

4
00:00:10,000 --> 00:00:13,000
张亮并行这个内容里面

5
00:00:13,000 --> 00:00:16,400
那之前搁了一段时间呢是因为公司到年底了

6
00:00:16,400 --> 00:00:19,000
各种汇报、对齐、开会

7
00:00:19,000 --> 00:00:20,600
占了非常多的时间

8
00:00:20,600 --> 00:00:24,800
明显更新的速率呢要比之前的慢了很多

9
00:00:24,800 --> 00:00:26,400
大模型要真正训练起来

10
00:00:26,400 --> 00:00:29,000
其实我们不仅仅需要非常多的money

11
00:00:29,000 --> 00:00:30,400
还有我们的服务器集群

12
00:00:30,400 --> 00:00:33,200
我们更多的是需要分布式并行

13
00:00:33,200 --> 00:00:35,200
或者分布式训练的能力

14
00:00:35,200 --> 00:00:38,000
分布式训练里面呢又分为模型并行

15
00:00:38,000 --> 00:00:41,000
而模型并行里面有两个非常重要的

16
00:00:41,000 --> 00:00:43,000
就是我们对模型进行切分

17
00:00:43,000 --> 00:00:46,400
今天我们主要来去聊聊张亮并行的这个概念

18
00:00:46,400 --> 00:00:49,600
首先我们会从一个张亮并行的一个原理来讲

19
00:00:49,600 --> 00:00:54,000
然后用三个算子去讲讲我们张亮是怎么做并行的

20
00:00:54,000 --> 00:00:56,600
最后呢去讲讲张亮重排

21
00:00:56,600 --> 00:00:58,800
在Mathwell里面非常重要的一个概念

22
00:00:59,200 --> 00:01:02,000
在上一节里面呢我们讲了数据并行

23
00:01:02,000 --> 00:01:07,600
数据并行其实分为DP DDP和FSDP三种不同的并行模式

24
00:01:07,600 --> 00:01:10,400
那之前呢我们留了一个疑问题

25
00:01:10,400 --> 00:01:12,800
假设我输的图片的通道有非常多

26
00:01:12,800 --> 00:01:14,800
我输的图片呢也是非常大

27
00:01:14,800 --> 00:01:17,800
那这时候呢训练的数据这么大的情况下

28
00:01:17,800 --> 00:01:20,000
我们是属于数据并行吗

29
00:01:21,200 --> 00:01:24,200
哎这个问题呢我们上一节没有回答哦

30
00:01:24,200 --> 00:01:25,800
这里面真正来回答一下

31
00:01:26,200 --> 00:01:28,200
不管是训练输入的数据呢

32
00:01:28,400 --> 00:01:31,200
还是我们神经网络里面去执行的数据

33
00:01:31,200 --> 00:01:33,000
我们统一都称为张亮

34
00:01:33,000 --> 00:01:35,800
所以对这个这么大的数据进行切分呢

35
00:01:35,800 --> 00:01:37,800
我们叫做张亮并行

36
00:01:37,800 --> 00:01:41,600
下面我们来看看张亮并行和流水线并行的不同之处啊

37
00:01:41,600 --> 00:01:43,600
第一个就是我们的流水线并行

38
00:01:43,600 --> 00:01:46,400
可以看到左边的这个图呢就是流水线并行

39
00:01:46,400 --> 00:01:50,800
流水线并行是按照模型的layer进行切分到不同的机器的

40
00:01:50,800 --> 00:01:52,800
我第一层切分到第一台机器

41
00:01:52,800 --> 00:01:54,800
第二层切分到第二台机器

42
00:01:55,200 --> 00:01:59,000
那这种层间的并行呢我们叫做流水线并行

43
00:01:59,000 --> 00:02:01,000
下个内容呢我们会来介绍的

44
00:02:01,000 --> 00:02:03,400
张亮并行呢就像右边的这个图

45
00:02:03,400 --> 00:02:07,800
我们将计算图的层内的不同的参数切分到不同的机器

46
00:02:07,800 --> 00:02:09,200
假设是同一层的

47
00:02:09,200 --> 00:02:12,200
我把这一层的参数或者这一套的张亮

48
00:02:12,200 --> 00:02:15,400
切分成devices1和devices2去执行

49
00:02:15,400 --> 00:02:18,400
那这种层内的并行我们叫做张亮并行

50
00:02:19,000 --> 00:02:22,400
张亮并行呢实际上我们会引起两个重要的思考

51
00:02:22,400 --> 00:02:24,400
如何切分我的网络模型

52
00:02:24,600 --> 00:02:26,200
我是从这个位置进行切分呢

53
00:02:26,200 --> 00:02:28,600
还是这个位置对半切分呢

54
00:02:29,200 --> 00:02:30,800
那第二个值得思考的就是

55
00:02:30,800 --> 00:02:34,200
怎么样去保证我切分完之后的正确性呢

56
00:02:34,200 --> 00:02:36,400
devices1第一层的神经元

57
00:02:36,400 --> 00:02:40,200
其实是跟devices2最后一层的神经元有相关联联系的

58
00:02:40,200 --> 00:02:42,600
如何保证切分之后的正确性呢

59
00:02:43,200 --> 00:02:45,800
这两个问题都是非常值得大家去思考的

60
00:02:46,800 --> 00:02:49,200
讲完背景知识之后呢

61
00:02:49,200 --> 00:02:51,800
我们正式的来去了解一下数学原理

62
00:02:51,800 --> 00:02:53,600
数学原理其实很简单了

63
00:02:53,600 --> 00:02:56,800
张亮的切分方式呢主要分为两种

64
00:02:56,800 --> 00:02:58,800
第一种呢就是行的切分

65
00:02:58,800 --> 00:03:01,600
假设我们现在有一个二层二的矩阵

66
00:03:01,600 --> 00:03:05,000
按行的切分呢很明显就是直接按行切分

67
00:03:05,000 --> 00:03:08,200
那第二种切分方式呢就按列来切分

68
00:03:08,200 --> 00:03:11,000
那第三种呢其实它不属于一种切分方式

69
00:03:11,000 --> 00:03:12,400
它是一种切分方式

70
00:03:13,400 --> 00:03:16,000
那第三种呢其实它不属于一种切分方式

71
00:03:16,000 --> 00:03:17,600
它更多是一个复制

72
00:03:17,600 --> 00:03:21,000
就把一个原始的矩阵复制到不同的机器里面

73
00:03:21,000 --> 00:03:23,000
一句话来说数学原理很简单

74
00:03:23,000 --> 00:03:25,000
但是组合起来就很复杂

75
00:03:25,000 --> 00:03:27,000
X乘以A等于Y

76
00:03:27,000 --> 00:03:30,400
那X呢我们假设是网络模型的输入或者激活

77
00:03:30,400 --> 00:03:31,600
A就是我们的权重

78
00:03:31,600 --> 00:03:33,000
Y就是我们的输出

79
00:03:33,000 --> 00:03:36,400
我们把A就是我们的权重呢按列来进行切分

80
00:03:36,400 --> 00:03:38,200
那我就切分了成两个

81
00:03:38,200 --> 00:03:39,600
一个A1一个A2

82
00:03:39,600 --> 00:03:42,200
X乘以A1A2呢就等于我们的Y

83
00:03:42,200 --> 00:03:44,800
那第二个呢我A按行的切分

84
00:03:44,800 --> 00:03:46,000
我按行来切分

85
00:03:46,000 --> 00:03:49,800
这个时候我X就必须按照列进行切分

86
00:03:49,800 --> 00:03:51,200
1乘以2的矩阵

87
00:03:51,200 --> 00:03:53,200
再乘以2乘以1的矩阵

88
00:03:53,200 --> 00:03:56,200
最后才能形成一个1乘以1的矩阵

89
00:03:56,200 --> 00:04:00,000
这个时候我的数呢就被迫按照列来进行切分

90
00:04:00,000 --> 00:04:04,400
按照不同的切分方式会影响到我前后的输出处

91
00:04:04,400 --> 00:04:06,800
下面呢我们正式的去一个MathMall

92
00:04:06,800 --> 00:04:10,800
就是矩阵层或者GMMLiner这种矩阵层的算子

93
00:04:11,000 --> 00:04:12,600
并行作为一个例子

94
00:04:12,600 --> 00:04:15,000
那同样的公式X乘以A等于Y

95
00:04:15,000 --> 00:04:16,000
X作为输入

96
00:04:16,000 --> 00:04:17,800
A作为算子的权重

97
00:04:17,800 --> 00:04:20,400
现在呢我把A按照列进行切分

98
00:04:20,400 --> 00:04:21,800
就竖着来切

99
00:04:21,800 --> 00:04:24,800
这时候呢我就分开一个A1和A2

100
00:04:24,800 --> 00:04:28,600
X乘以A1A2等于Y1Y2两个数

101
00:04:28,600 --> 00:04:30,600
最后两个数我要把它拼起来

102
00:04:30,600 --> 00:04:32,000
变成一个矩阵Y

103
00:04:32,000 --> 00:04:34,400
我需要通过一个Orgd的方式

104
00:04:34,400 --> 00:04:36,400
因为Y1在Devices1

105
00:04:36,400 --> 00:04:37,800
Y2在Devices2

106
00:04:37,800 --> 00:04:40,200
我把它拼起来的时候就需要一个通讯

107
00:04:40,200 --> 00:04:42,800
所以这里面用了Orgd的通讯方式

108
00:04:43,400 --> 00:04:46,800
第二种方式就是我A按照行的方式进行切分

109
00:04:46,800 --> 00:04:49,800
那我的X就必须按照列进行切分

110
00:04:49,800 --> 00:04:51,400
所以我们看看下面这个图

111
00:04:51,400 --> 00:04:53,400
我首先X要进行一个Split

112
00:04:53,600 --> 00:04:56,200
这时候我的X就需要进行通讯

113
00:04:56,200 --> 00:04:58,200
放在两半放在不同的机器

114
00:04:58,200 --> 00:05:00,600
然后A按照正常的进行切分

115
00:05:00,600 --> 00:05:03,000
X1乘以A1等于我们的Y1

116
00:05:03,000 --> 00:05:05,800
X2乘以A2等于我的IY2

117
00:05:05,800 --> 00:05:07,200
得到Y1Y2之后

118
00:05:07,200 --> 00:05:09,600
现在还是在两台不同的机器

119
00:05:09,800 --> 00:05:12,800
最后我们要执行一个Orgd的通讯

120
00:05:12,800 --> 00:05:14,800
才能执行加号的操作

121
00:05:16,600 --> 00:05:17,800
有了MATMUL的理解

122
00:05:18,000 --> 00:05:20,200
我们来看看Transformer的NLP

123
00:05:20,200 --> 00:05:21,600
其实Transformer的NLP

124
00:05:21,600 --> 00:05:24,000
只是多了一个激活在这里面

125
00:05:24,200 --> 00:05:26,200
这里面我们还是按照刚才的方式

126
00:05:26,200 --> 00:05:28,400
第一种就是按行进行切分

127
00:05:28,400 --> 00:05:29,800
按行的切分的方式意味着

128
00:05:29,800 --> 00:05:33,000
我的X就必须要被迫着按照列进行切分

129
00:05:33,000 --> 00:05:35,600
但是后面我们有一个激活函数

130
00:05:35,600 --> 00:05:37,000
那在执行激活函数的时候

131
00:05:37,200 --> 00:05:39,400
我需要进行一个加的操作

132
00:05:39,600 --> 00:05:42,000
这意味着我的Y1Y2

133
00:05:42,000 --> 00:05:45,800
就必须要进行一个All Reduced Sum的通讯

134
00:05:46,000 --> 00:05:47,400
所以从右边的图

135
00:05:47,400 --> 00:05:49,200
我们这里面通讯就有两次了

136
00:05:49,200 --> 00:05:51,000
第一次就是在Split这个地方

137
00:05:51,000 --> 00:05:52,800
把X分成两个

138
00:05:52,800 --> 00:05:55,400
第二个就是在执行GELU之前

139
00:05:55,600 --> 00:05:57,200
再做一个聚合的通讯

140
00:05:57,400 --> 00:06:00,600
第二种方式就是A按照列的方式进行切分

141
00:06:00,600 --> 00:06:02,800
我把A切分成A1A2

142
00:06:03,000 --> 00:06:05,400
这个时候我的GELU就可以独立起来

143
00:06:05,400 --> 00:06:06,800
得到Y1Y2

144
00:06:06,800 --> 00:06:08,800
最后我再做一个Orgd的方式

145
00:06:08,800 --> 00:06:10,800
对Y1Y2进行拼接起来

146
00:06:11,400 --> 00:06:13,400
但是又但是了

147
00:06:13,400 --> 00:06:15,800
Transformer里面的NLP还没有这么简单

148
00:06:15,800 --> 00:06:18,400
它后面还会接一个Dropout

149
00:06:18,400 --> 00:06:20,400
这个时候我的Y1Y2出来之后

150
00:06:20,400 --> 00:06:23,800
我们还需要乘以另外一个权重

151
00:06:23,800 --> 00:06:25,600
就是我们的B得到J1J2

152
00:06:25,600 --> 00:06:26,800
然后再合并

153
00:06:26,800 --> 00:06:29,400
那这个时候我之前的Y1Y2

154
00:06:29,600 --> 00:06:31,000
已经是两个了

155
00:06:31,000 --> 00:06:34,200
我最后我的B只要按照行切分就可以了

156
00:06:34,200 --> 00:06:35,800
我A按照列来切分

157
00:06:35,800 --> 00:06:38,000
我的B按照行的方式来切分

158
00:06:38,400 --> 00:06:39,600
最后执行NLP

159
00:06:39,600 --> 00:06:43,000
只是在开始跟结尾的时候做一个通讯

160
00:06:43,000 --> 00:06:46,200
第一个就是对X进行广播或者Copy

161
00:06:46,200 --> 00:06:49,000
第二个就是Orgd把它聚合起来

162
00:06:49,000 --> 00:06:50,200
然后做一个Dropout

163
00:06:50,200 --> 00:06:54,600
在集群里面通讯的成本会比计算的成本要高很多

164
00:06:54,600 --> 00:06:56,200
因为通讯的时间很慢

165
00:06:56,200 --> 00:06:58,600
计算我们现在已经到了5纳米的

166
00:06:58,600 --> 00:07:01,200
晶体管的计算速率是非常高的

167
00:07:01,200 --> 00:07:03,800
所以我们在网络模型或者计算图里面

168
00:07:03,800 --> 00:07:05,600
对我们的张量进行切分

169
00:07:05,600 --> 00:07:06,800
张量并行的时候

170
00:07:07,000 --> 00:07:10,600
要考虑的就是我的整个网络模型怎么切分

171
00:07:10,600 --> 00:07:14,400
才能够让我们的通讯时间更短更少

172
00:07:14,400 --> 00:07:16,800
使得整网的训练效率更高

173
00:07:19,000 --> 00:07:20,200
那在Transformer里面

174
00:07:20,400 --> 00:07:22,400
第二个重要的就是Self-Attention

175
00:07:22,400 --> 00:07:24,600
我们来看看Self-Attention的具体执行

176
00:07:24,600 --> 00:07:26,600
Self-Attention其实比较简单

177
00:07:26,600 --> 00:07:29,000
主要是由QKV进行一个矩阵层

178
00:07:29,000 --> 00:07:31,000
然后加个Softmax Dropout

179
00:07:31,000 --> 00:07:32,000
得到我的Y1

180
00:07:32,000 --> 00:07:34,800
最后再执行一个Dropout得到我的J

181
00:07:34,800 --> 00:07:38,000
那这个方式跟刚才的MLP其实类似的

182
00:07:38,000 --> 00:07:42,200
同样我的QKV是按列的方式进行切分

183
00:07:42,200 --> 00:07:44,400
我的B按行进行切分

184
00:07:44,400 --> 00:07:46,800
使得我只要一开始做一个通讯

185
00:07:46,800 --> 00:07:49,000
结尾的时候再做一次通讯

186
00:07:49,000 --> 00:07:51,600
就执行完我一个Self-Attention的层了

187
00:07:54,800 --> 00:07:56,400
Transformer一开始推出的时候

188
00:07:56,600 --> 00:07:58,400
主要是处理NLP用户的

189
00:07:58,400 --> 00:08:01,400
而NLP用户的第一个数就是我的Embedding层

190
00:08:01,400 --> 00:08:03,800
一般用来压缩我的Vocabulary

191
00:08:03,800 --> 00:08:06,600
或者作为Vocabulary Size的第一层输入

192
00:08:06,600 --> 00:08:08,400
在大模型GPT-2里面

193
00:08:08,400 --> 00:08:11,000
Vocabulary Size就已经有5万多个了

194
00:08:11,000 --> 00:08:12,400
再加上Hidden Size

195
00:08:12,400 --> 00:08:15,800
其实第一层的网络模型就已经非常的巨大了

196
00:08:15,800 --> 00:08:18,200
那这个时候我们想把它切分到

197
00:08:18,200 --> 00:08:19,800
不同的机器里面

198
00:08:19,800 --> 00:08:22,800
才能够塞得多更多的词汇表

199
00:08:22,800 --> 00:08:24,600
那一般的推荐的方式

200
00:08:24,600 --> 00:08:26,400
就是按列的方式进行切分

201
00:08:26,400 --> 00:08:29,000
就是按照词表的方式进行切分

202
00:08:29,000 --> 00:08:31,800
前两万个单词我就放在Devices

203
00:08:32,000 --> 00:08:35,800
前后两个单词我就放在Devices2里面

204
00:08:35,800 --> 00:08:37,200
通过列的切分方式

205
00:08:37,200 --> 00:08:38,600
然后得到Y1 Y2

206
00:08:38,600 --> 00:08:40,400
最后执行一个Orgat通讯

207
00:08:40,400 --> 00:08:41,400
得到我们的Y

208
00:08:41,400 --> 00:08:44,200
再输给我们的Transformer的网络模型的结构层里面

209
00:08:47,000 --> 00:08:49,600
但是Embedding它不仅仅是用在

210
00:08:49,600 --> 00:08:52,000
LLM大规模语言模型里面

211
00:08:52,400 --> 00:08:55,000
它还用在推荐模型里面

212
00:08:55,200 --> 00:08:56,400
所以推荐模型里面

213
00:08:56,600 --> 00:08:58,800
又有了一个Embedding的切分方式

214
00:08:58,800 --> 00:09:00,600
那下面有两个例子

215
00:09:01,000 --> 00:09:03,400
在推荐领域我们有两种切分方式

216
00:09:03,400 --> 00:09:05,800
一种是Tablewise的切分方式

217
00:09:05,800 --> 00:09:08,000
一种是Clumwise的切分方式

218
00:09:08,600 --> 00:09:10,600
假设在推荐网络模型里面

219
00:09:10,800 --> 00:09:13,200
我们有很多物体的输入特征

220
00:09:13,200 --> 00:09:16,800
Tablewise就是按照物体的特征进行切分的

221
00:09:16,800 --> 00:09:21,000
我可能把特征1 2 3 0切分到我们的GPU0里面

222
00:09:21,200 --> 00:09:23,800
我把特征3 1切分到Devices2里面

223
00:09:23,800 --> 00:09:26,000
那这是一种Tablewise的切分方式

224
00:09:26,200 --> 00:09:29,200
第二种Clumwise的是最通用最常用的

225
00:09:29,200 --> 00:09:32,000
我们会在混合并行里面详细去介绍的

226
00:09:32,000 --> 00:09:36,600
Clumwise这种方式就是我把所有的特征融合到一起

227
00:09:37,000 --> 00:09:38,600
然后把特征的Embedding表

228
00:09:38,600 --> 00:09:40,600
按照列的方式进行切分

229
00:09:40,600 --> 00:09:42,800
可以看到一个物体的Embedding特征

230
00:09:42,800 --> 00:09:45,400
我可以放在GPU0 GPU1里面

231
00:09:45,600 --> 00:09:48,200
这种就是按照Clumwise进行一个切分的

232
00:09:48,800 --> 00:09:51,800
接下来这个内容可能会稍微复杂了一点

233
00:09:51,800 --> 00:09:53,800
就是Cross Entropy Lost

234
00:09:53,800 --> 00:09:56,200
就是我们的损失函数的并行

235
00:09:56,600 --> 00:10:00,000
对损失函数进行并行有两种场景

236
00:10:00,400 --> 00:10:03,200
第一种就是在我们的大词汇表或者语言模型里面

237
00:10:03,400 --> 00:10:05,600
我的Logics的规模是非常大的

238
00:10:05,800 --> 00:10:10,200
就是我们刚才讲的我的VocabularySize是非常多的

239
00:10:10,400 --> 00:10:14,000
我在计算的时候同样需要把这个VocabularySize

240
00:10:14,200 --> 00:10:15,400
传给我们的Lost函数

241
00:10:15,600 --> 00:10:16,800
Label也是非常多的

242
00:10:16,800 --> 00:10:18,400
因为Label对应我们的词表

243
00:10:19,200 --> 00:10:21,000
所以我们要考虑到把整个词表

244
00:10:21,000 --> 00:10:23,200
拆分到不同的基因上面

245
00:10:24,200 --> 00:10:27,800
第二种场景就是我的分类场景

246
00:10:27,800 --> 00:10:30,200
一些极端的高难度的挑战任务里面

247
00:10:30,200 --> 00:10:34,600
可能对图像的分类就有上万种上十万种

248
00:10:34,600 --> 00:10:36,400
这个时候如果类别非常的大

249
00:10:36,400 --> 00:10:39,400
也会导致我们的单卡或者单芯片里面

250
00:10:39,600 --> 00:10:42,600
没法储存和计算我们的Logics这个矩阵

251
00:10:42,600 --> 00:10:45,800
所以我们就必须要按照类别的维度进行切分

252
00:10:45,800 --> 00:10:47,800
不管按照类的方式来进行切分

253
00:10:48,000 --> 00:10:50,200
还是按照词表的方式进行切分

254
00:10:50,200 --> 00:10:52,800
都是按照列的切分方式

255
00:10:53,200 --> 00:10:56,200
下面一起来回顾一下交叉商损失函数

256
00:10:56,200 --> 00:10:57,600
在二分类的时候

257
00:10:57,800 --> 00:11:02,000
我们每个类别的预测概率是P和1-P

258
00:11:02,200 --> 00:11:03,800
真实的预测值是Yi

259
00:11:04,000 --> 00:11:06,200
另外一个预测不到的就是1-Yi

260
00:11:06,200 --> 00:11:07,600
这是二分类的情况

261
00:11:07,800 --> 00:11:11,400
把它拓展到多分类情况里面就有两个求和了

262
00:11:11,600 --> 00:11:12,800
对于多分类的方式

263
00:11:12,800 --> 00:11:15,000
假设我们想有M个类别PIC

264
00:11:15,200 --> 00:11:17,200
就是每一个类别的预测概率

265
00:11:17,600 --> 00:11:19,800
这里面以语言模型作为例子

266
00:11:19,800 --> 00:11:21,200
我们可以看到这里面有一个

267
00:11:21,200 --> 00:11:22,400
Vocabulary size

268
00:11:22,400 --> 00:11:24,800
第一步就是要进行数据的拆分

269
00:11:24,800 --> 00:11:28,600
我们把输入的数据按Vocabulary size进行拆分

270
00:11:28,600 --> 00:11:30,600
假设我现在有四台机器

271
00:11:30,600 --> 00:11:34,200
我可能会把数据按照列的方式进行拆分

272
00:11:34,600 --> 00:11:36,400
而Enable就是我们的真值

273
00:11:36,400 --> 00:11:39,000
一开始它可能是一些具体的单词

274
00:11:40,200 --> 00:11:41,800
首先会对具体的单词

275
00:11:41,800 --> 00:11:43,600
执行一个1-hot向量的操作

276
00:11:43,600 --> 00:11:46,200
然后再把它scatter到不同的机器上面

277
00:11:46,400 --> 00:11:48,600
就变成我们右边的图logic

278
00:11:48,600 --> 00:11:50,600
就是我们的输入有四个部分

279
00:11:50,800 --> 00:11:53,000
然后我们的label也对应有四个部分

280
00:11:54,000 --> 00:11:56,200
第二步就是最大值同步了

281
00:11:56,200 --> 00:11:58,800
最大值同步主要是对我们的输入

282
00:11:58,800 --> 00:12:00,800
xi减去最大值后

283
00:12:01,200 --> 00:12:03,800
求softmax中间求最大值的时候

284
00:12:04,000 --> 00:12:06,400
就通过always use max去保证

285
00:12:06,400 --> 00:12:08,600
获取的是一个全局的最大值

286
00:12:08,600 --> 00:12:10,400
而不是某个机器的最大值

287
00:12:10,600 --> 00:12:14,000
第三步就是express sum和softmax的计算了

288
00:12:15,400 --> 00:12:17,600
我们继续来打开看一看第三步

289
00:12:17,600 --> 00:12:20,200
第三步首先我进行一个指数的运算

290
00:12:20,400 --> 00:12:22,000
求一个当地local的最大值

291
00:12:22,000 --> 00:12:24,000
然后再求全局的最大值

292
00:12:24,000 --> 00:12:25,800
拿到全局的最大值

293
00:12:25,800 --> 00:12:27,800
然后我再进行一个指数的求和

294
00:12:27,800 --> 00:12:29,400
得到softmax的值了

295
00:12:30,000 --> 00:12:32,600
最后一步就是真正执行我们的lost

296
00:12:33,000 --> 00:12:34,600
我们会把输入的logist

297
00:12:34,600 --> 00:12:36,600
还有我们的label进行相乘

298
00:12:36,600 --> 00:12:37,600
并且求和

299
00:12:37,600 --> 00:12:41,200
得到我们label所对应的一个位置的值

300
00:12:41,200 --> 00:12:43,600
然后再进行一个always use sum

301
00:12:43,800 --> 00:12:45,000
全局的同步

302
00:12:45,000 --> 00:12:47,200
然后去计算lock softmax

303
00:12:47,200 --> 00:12:49,200
然后加上负号的操作

304
00:12:49,200 --> 00:12:53,000
就得到了分布式交叉商的损失函数值了

305
00:12:53,000 --> 00:12:53,600
cut

306
00:12:54,200 --> 00:12:55,000
到这一步为止

307
00:12:55,200 --> 00:12:58,200
我们可以看到只是简单的模型并行

308
00:12:58,200 --> 00:13:00,600
我们会对输入的数据进行并行

309
00:13:00,600 --> 00:13:03,000
还会对输入数据的第一层

310
00:13:03,000 --> 00:13:04,400
embedded层进行并行

311
00:13:04,400 --> 00:13:07,200
中间的transformer和NLP或者矩阵层

312
00:13:07,200 --> 00:13:08,600
也会做并行

313
00:13:08,600 --> 00:13:10,800
包括我的损失函数

314
00:13:10,800 --> 00:13:12,600
也会做张量并行

315
00:13:13,000 --> 00:13:15,800
总的来说张量并行无处不在

316
00:13:16,600 --> 00:13:19,400
下面我们来看一个额外的知识点

317
00:13:19,400 --> 00:13:21,400
就是我们的随机控制的问题

318
00:13:21,400 --> 00:13:24,600
第一个就是参数初始化的随机性的问题

319
00:13:25,800 --> 00:13:27,800
假设我们现在只有一块卡的时候

320
00:13:27,800 --> 00:13:31,000
可能我只需要设置一个简单的随机种子

321
00:13:31,000 --> 00:13:33,600
然后对11进行一个随机数就可以了

322
00:13:33,600 --> 00:13:36,400
但是我把相同的数据切换成两半

323
00:13:36,400 --> 00:13:37,800
放在不同的机器里面

324
00:13:38,200 --> 00:13:40,400
我两台机器的随机种子都是p的时候

325
00:13:40,400 --> 00:13:42,200
就造成了数学上不等价

326
00:13:42,200 --> 00:13:44,600
失去了真正的随机性了

327
00:13:44,600 --> 00:13:47,800
也就是可能我的11跟12里面

328
00:13:47,800 --> 00:13:49,800
初始化的数据都是相同的

329
00:13:49,800 --> 00:13:52,600
但是我刚才只有一台机器的时候

330
00:13:52,600 --> 00:13:56,200
两半部分的随机生成的数据肯定是不同的

331
00:13:56,200 --> 00:13:58,200
于是我们在做多卡切分的时候

332
00:13:58,600 --> 00:14:01,800
就会去使用不同的随机种子

333
00:14:01,800 --> 00:14:04,200
对不同的机器进行初始化

334
00:14:05,400 --> 00:14:07,000
第二个随机控制的问题

335
00:14:07,000 --> 00:14:09,600
就是我们算子计算的一个随机性

336
00:14:09,600 --> 00:14:11,600
可以看到神经网络里面

337
00:14:11,600 --> 00:14:13,400
不仅仅只有初始化的时候

338
00:14:13,400 --> 00:14:14,800
会引入随机问题

339
00:14:14,800 --> 00:14:15,800
我们在dropout

340
00:14:15,800 --> 00:14:17,400
还有trunkies normalization

341
00:14:17,400 --> 00:14:19,200
standard normalization

342
00:14:19,200 --> 00:14:20,800
还有standard Laplace

343
00:14:20,800 --> 00:14:23,600
还有GAMM等不同的情况下

344
00:14:23,600 --> 00:14:26,200
都会引入我们的随机种子

345
00:14:26,200 --> 00:14:29,400
这个时候就会造成在网络模型当中

346
00:14:29,400 --> 00:14:31,400
会引起的随机性的问题

347
00:14:32,000 --> 00:14:33,800
假设我把一个dropout的算子

348
00:14:33,800 --> 00:14:35,200
进行张量并行

349
00:14:35,400 --> 00:14:38,600
这个时候我可能会设置不同的随机种子

350
00:14:38,600 --> 00:14:41,200
但是如果我把所有的数据都allget

351
00:14:41,200 --> 00:14:42,000
或者allreduce

352
00:14:42,200 --> 00:14:45,200
这个时候我使用相同一个随机种子就可以了

353
00:14:45,200 --> 00:14:46,400
讲这么多原理

354
00:14:46,400 --> 00:14:48,200
实际上不是希望我们的用户

355
00:14:48,200 --> 00:14:50,600
或者我们的开发者自己去试着这些

356
00:14:50,600 --> 00:14:52,600
而是告诉大家这些内容

357
00:14:52,600 --> 00:14:54,200
AI框架

358
00:14:54,200 --> 00:14:57,400
AI系统都会帮我们自动的去处理好

359
00:14:57,400 --> 00:14:59,600
所以大家不用去担心或者纠结

360
00:15:00,000 --> 00:15:03,400
在一个878卡64块MPU的集群里面

361
00:15:04,200 --> 00:15:05,200
如果初始化的时候

362
00:15:05,200 --> 00:15:07,000
使用同一个随机种子

363
00:15:07,600 --> 00:15:10,400
网络模型最后训练出来的精度

364
00:15:10,600 --> 00:15:12,000
是没有我初始化的时候

365
00:15:12,000 --> 00:15:14,600
使用不同的随机种子的精度要高的

366
00:15:14,600 --> 00:15:15,800
从这个图里面

367
00:15:15,800 --> 00:15:18,600
我们可以看出随机性控制对我们的精度

368
00:15:18,600 --> 00:15:19,800
还有loss的收敛

369
00:15:19,800 --> 00:15:21,600
还是非常具有影响力的

370
00:15:21,600 --> 00:15:23,400
今天的内容稍微多了一点

371
00:15:23,400 --> 00:15:24,600
我们来总结一下

372
00:15:24,600 --> 00:15:27,400
模型并行分为张量并行和流水线并行

373
00:15:27,400 --> 00:15:29,800
而张量并行就是层内并行

374
00:15:29,800 --> 00:15:33,200
流水线并行是一个层间并行的概念

375
00:15:33,200 --> 00:15:37,000
第二就是张量并行主要是对数据进行切分

376
00:15:37,000 --> 00:15:40,600
切分的方式有按行按列的方式进行切分

377
00:15:40,600 --> 00:15:43,000
另外最常见的张量并行就是

378
00:15:43,000 --> 00:15:44,400
MATMUL的张量并行

379
00:15:44,800 --> 00:15:47,400
通过MATMUL这个张量并行的算子原理

380
00:15:47,400 --> 00:15:49,400
我们很快的去把它拓展到了

381
00:15:49,400 --> 00:15:52,000
embedding MLP还有transformer等

382
00:15:52,000 --> 00:15:53,800
不同的算子的并行

383
00:15:53,800 --> 00:15:57,400
可能还会涉及到损失函数的并行

384
00:15:58,400 --> 00:16:00,600
最后一个就是张量并行的时候

385
00:16:00,800 --> 00:16:03,600
要注意随机性的问题

386
00:16:03,600 --> 00:16:06,200
开发AI框架或者AI系统的时候

387
00:16:06,600 --> 00:16:09,400
要注意随机性种子的设置

388
00:16:09,400 --> 00:16:11,400
卷的不行了卷的不行了

389
00:16:11,400 --> 00:16:13,200
记得一键三连加关注哦

390
00:16:13,200 --> 00:16:16,200
所有的内容都会开源在下面这条链接里面

391
00:16:16,800 --> 00:16:17,400
拜了拜拜

