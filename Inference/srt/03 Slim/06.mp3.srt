0
0:00:00.000 --> 0:00:04.480


1
0:00:04.480 --> 0:00:07.760
哈喽大家好我是宗米

2
0:00:07.760 --> 0:00:09.600
因为普通话咬字不清楚

3
0:00:09.600 --> 0:00:12.120
所以录一门课呢我一般都会NG很多遍

4
0:00:12.120 --> 0:00:14.480
然后重复很多遍

5
0:00:14.480 --> 0:00:18.800
那今天的我们来到推力引擎模型压缩的导暑

6
0:00:18.800 --> 0:00:22.920
第二个内容啊就是知识征流Knowledge Distillation

7
0:00:22.920 --> 0:00:25.760
那好了现在我们来到知识征流这个环节

8
0:00:25.760 --> 0:00:29.920
我们看一下今天知识征流要给大家汇报哪几个内容

9
0:00:29.920 --> 0:00:33.720
首先呢就是知识增留的背景布莱广

10
0:00:33.720 --> 0:00:35.320
然后呢我们可以看一下

11
0:00:35.320 --> 0:00:37.720
因为我们需要对知识进行增留嘛

12
0:00:37.720 --> 0:00:39.720
那这里面的知识就很重要了

13
0:00:39.720 --> 0:00:42.820
我们了解一下所谓的知识的形式

14
0:00:42.820 --> 0:00:44.720
就是knowledge的format

15
0:00:44.720 --> 0:00:47.820
然后呢我们可能会分开两个视频来看

16
0:00:47.820 --> 0:00:50.320
因为一个视频可能讲完这些内容了有点长

17
0:00:50.320 --> 0:00:53.820
第二个视频的内容里面我们会讲讲distillation

18
0:00:53.820 --> 0:00:55.820
就是增留的具体的方法

19
0:00:55.820 --> 0:00:59.720
上面的内容我们更多的聚焦于knowledge

20
0:00:59.720 --> 0:01:04.420
下面的内容我们更多的聚焦于distillation

21
0:01:04.420 --> 0:01:07.620
希腾最经典的一个知识增留的算法进行解读

22
0:01:07.620 --> 0:01:14.220
因为希腾他写了一篇文章才正式的去把这个知识点呢明确下来

23
0:01:14.220 --> 0:01:19.020
下面呢我们看一下所在的位置知识增留还是在模型压锁的这个模块

24
0:01:19.020 --> 0:01:26.520
但实际上啊很多知识增留的算法块量化算法还有减资的算法包括二字化的算法都是在我们的AI框架的一个特性

25
0:01:26.520 --> 0:01:30.420
真正我们在做完模型转换之后变成自己的AI

26
0:01:30.420 --> 0:01:39.720
然后呢用模型压缩去对我们AI框架关于模型压缩的特性进行处理才得到一个小型化的模型

27
0:01:39.720 --> 0:01:44.120
得到小型化的模型再给真正的推理的

28
0:01:44.120 --> 0:01:52.120
注意一下推定引擎架构里面的压缩模块它是有限定的它只是做一个模型的小型化的工作

29
0:01:52.120 --> 0:01:58.420
很多内容呢都是在我们AI框架里面去处理的哦

30
0:01:58.420 --> 0:02:11.720
下面呢我们看一个比较重要的内容就是知识增留那knowledge distillation的kd我们后面呢都可以简称kd最初呢是希腾呢在这篇文章里面去发表的

31
0:02:11.720 --> 0:02:21.720
里面呢就提出了soft label和hard label然后知识增留呢使用soft label来训练教师网络和学生网络

32
0:02:21.720 --> 0:02:25.920
通过教师网络把知识呢交给学生

33
0:02:25.920 --> 0:02:34.720
它的算法原理呢一句话比较简单就是把教师网络学习到的知识增留或者交给学生网络

34
0:02:34.720 --> 0:02:41.920
像教师网络呢一般都会比较大学生网络呢一般都会比较小

35
0:02:41.920 --> 0:02:51.620
两者之间呢是相互竞争的关系直到学生啊可以学到跟老师网络差不多的网络模型的精度

36
0:02:52.120 --> 0:03:00.520
下面这个图呢就很清楚地说明这个情况了首先我们有三个内容一个是老师的网络那老师网络应该会比较大

37
0:03:00.520 --> 0:03:12.320
因为老师呢他知识量呢比较丰富这里面呢很重要的一个概念就是知识通过知识的转换之后呢得到我们的student model学生网络

38
0:03:12.320 --> 0:03:20.620
学生网络呢一般都会默认比教师网络要小所以我们说这个可以通过知识增留的方式做一个模型的压缩

39
0:03:20.620 --> 0:03:32.020
从大模型变成小模型从老师模型变成学生模型而知识怎么转换就取决于我们的增留的算法

40
0:03:32.020 --> 0:03:42.520
不过有点注意的就是我们所使用的数据都是相同的数据都是一样的有可能还会增加一些数据给学生网络

41
0:03:42.520 --> 0:03:47.420
这也是整体知识增留的算法架构

42
0:03:47.420 --> 0:04:08.220
我们对刚才的一个知识增留的算法架构呢再进一步地抽象成为三个内容那第一个就是增留的知识knowledge第二个呢就是增留的算法district algorithm第三个就是师生网络的架构就是老师跟学生之间的关系架构组成

43
0:04:10.220 --> 0:04:16.420
下面我们看一下第二个内容知识增留的方式那这个方式呢其实我们参考这一篇文章

44
0:04:16.420 --> 0:04:23.420
knowledge distillation而survive通常一些survive或者white paper呢都是一个很好的宗数和参考哦

45
0:04:25.420 --> 0:04:45.420
下面我们来聚焦一下一个点就是knowledge知识的方式说白了虽然知识的方式说得很抽象就是我这些知识我要增留的知识从哪里来怎么来这个问题我们要解开的它有三种方式啊responsible basefeature based relation based还有architecture based

46
0:04:45.420 --> 0:04:49.420
那一般来说architecture based呢却是用得比较少更多的是前三种

47
0:04:51.420 --> 0:04:56.420
我们逐一种来去打开首先就是responsible based

48
0:04:57.420 --> 0:05:03.420
它主要呢是指teacher model教师模型输出层的特征就是最后的一个feature map的特征

49
0:05:04.420 --> 0:05:13.420
希望让我们的学生模型呢直接学习到教师模型预测的结果真正地直接把结果给学生去学习

50
0:05:13.420 --> 0:05:25.420
通俗地来讲就是一个知识点老师充分地学习完了然后把结论告诉学生就好啦这个就是最终的输出下面我们看一下具体的公式啊假设呢

51
0:05:25.420 --> 0:05:39.420
这一期呢就是教师模型的输出而这一期呢就是学生模型的输出这里面的输出都是指最后一层的输出而responsible based knowledge增留的形式呢就可以用下面这条公式来去描述

52
0:05:39.420 --> 0:05:40.420
回到这个图呢我们看一下responsible based在哪里在这儿那这就证明我们最后一层的模型的输出的知识就是老师已经学完了然后把这个知识呢传授给学生下面我们看一下responsible based的一个学习流程图红色的代表的是老师绿色的代表的是学生我们会把teacher model输出输出的特征给到学生网络模型去学

53
0:06:09.420 --> 0:06:34.420
学习那这里面呢我们会拿出最后一层的特征然后呢通过distillation loss让学生最后一层的特征呢去学习老师的输出的特征那老师的输出特征呢应该是比较固定的而学生他是不太固定需要去学习的于是呢我们通过一个损失函数去模拟去减少去学习使得我们两个的logist越小越好

54
0:06:35.420 --> 0:06:52.420
第二种就是feature based knowledge实际上我们的网络模型有很多的层每一层呢都会有对应的feature map有非常多的特征有非常多的知识可以让我们去学习

55
0:06:52.420 --> 0:07:02.420
所以feature based knowledge呢主要是把教授和学生的特征激活连接起来的这种方式呢可以用下面这条公式来表示

56
0:07:02.420 --> 0:07:09.420
我们看一下知识的图像feature based knowledge呢就在这里面

57
0:07:09.420 --> 0:07:15.420
我们会把网络模型中间学习到的激活还有权重把它作为一个feature

58
0:07:15.420 --> 0:07:23.420
然后通过这个feature呢建立学生跟老师之间的关系这种就是feature based knowledge

59
0:07:23.420 --> 0:07:29.420
下面我们看一下feature based knowledge的一个整体的或者工作的流程图

60
0:07:30.420 --> 0:07:38.420
这里面distribution loss呢更多是建立在teacher model跟student model的中间层通过中间层呢去建立一个链接关系

61
0:07:39.420 --> 0:07:51.420
这种算法的好处就是老师网络呢可以给学生网络提供非常大量的有用的参考信息但是怎么有效地从老师网络这里面这么多提示层去给到学生

62
0:07:51.420 --> 0:07:54.420
因为student model的网络层数呢会比teacher model要少

63
0:07:55.420 --> 0:08:02.420
我们应该从teacher model里面提取哪几层有效的作为引导层这一步呢是前年的研究

64
0:08:02.420 --> 0:08:17.420
缺点呢就是提示层和引导层的大小啊存在明显的差异说白了就是teacher model跟student model的网络层数中间的feature嘛差异比较大如何正确地匹配两者之间的关系也是需要一个探索和研究的

65
0:08:17.420 --> 0:08:20.420
而业界呢在这个整体上面还没有成熟

66
0:08:21.420 --> 0:08:34.420
最后一种就是relation based knowledge基于关系的知识呢主要是进一步探索不同层或者数据样本之间的一个关系从而提取一些相关的知识

67
0:08:34.420 --> 0:08:38.420
我们看一下这个图relation based knowledge呢就在这里面

68
0:08:39.420 --> 0:08:46.420
希望通过研究不同的样本不同的输出层之间的关系呢从而建立整个知识金流的知识点

69
0:08:47.420 --> 0:08:54.420
下面呢我们又来到算法的架构图里面我们有相同的数据然后有一个teacher model和student model

70
0:08:54.420 --> 0:09:01.420
那distillation knowledge呢就不仅仅是去学习我们刚才讲到的网络模型中间的特征还有最后一层的特征而且

71
0:09:01.420 --> 0:09:05.420
它会学习数据样本还有网络模型层之间的一个关系

72
0:09:06.420 --> 0:09:12.420
好了今天给大家汇报了两个知识一个就是知识增留的背景我们为什么要去增留增留的好处

73
0:09:12.420 --> 0:09:19.420
接着呢我们看一下增留的知识的形态就是更多的关于knowledge怎么提取knowledge从哪里来


