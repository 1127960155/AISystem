0
0:00:00.000 --> 0:00:07.800
Hello大家好,我是周米

1
0:00:07.800 --> 0:00:09.560
因为普通话咬字不清楚

2
0:00:09.560 --> 0:00:10.480
所以录一门课

3
0:00:10.480 --> 0:00:12.200
我一般都会NG很多遍

4
0:00:12.200 --> 0:00:13.960
然后重复很多遍

5
0:00:14.720 --> 0:00:18.120
那今天我们来到推理引擎模型压缩的

6
0:00:18.120 --> 0:00:19.520
倒数第二个内容

7
0:00:19.680 --> 0:00:21.280
就是知识蒸馏

8
0:00:21.280 --> 0:00:22.880
Knowledge Distillation

9
0:00:22.880 --> 0:00:23.400
那好了

10
0:00:23.400 --> 0:00:25.720
现在我们来到知识蒸馏这个环节

11
0:00:25.720 --> 0:00:28.560
我们看一下今天知识蒸馏要给大家汇报

12
0:00:28.560 --> 0:00:29.920
哪几个内容

13
0:00:30.840 --> 0:00:33.120
首先就是知识蒸馏的背景

14
0:00:33.120 --> 0:00:33.720
Blackboard

15
0:00:33.720 --> 0:00:35.240
然后我们可以看一下

16
0:00:35.240 --> 0:00:37.480
因为我们需要对知识进行蒸馏

17
0:00:37.720 --> 0:00:39.640
这里面的知识就很重要了

18
0:00:39.640 --> 0:00:42.760
我们了解一下所谓的知识的形式

19
0:00:42.760 --> 0:00:44.600
就是Knowledge的format

20
0:00:44.840 --> 0:00:47.720
然后我们可能会分开两个视频来看

21
0:00:47.720 --> 0:00:49.520
因为一个视频可能讲完这些内容了

22
0:00:49.520 --> 0:00:50.320
有点长

23
0:00:50.760 --> 0:00:52.000
第二个视频的内容里面

24
0:00:52.000 --> 0:00:52.880
我们会讲讲

25
0:00:52.880 --> 0:00:55.800
Distillation就是蒸馏的具体的方法

26
0:00:56.320 --> 0:00:57.440
上面的内容

27
0:00:57.440 --> 0:00:59.680
我们更多的聚焦于Knowledge

28
0:00:59.800 --> 0:01:00.560
下面的内容

29
0:01:00.560 --> 0:01:03.320
我们更多的聚焦于Distillation

30
0:01:04.600 --> 0:01:07.680
Hiton最经典的一个知识蒸馏的算法进行解读

31
0:01:07.680 --> 0:01:09.800
因为Hiton他写了一篇文章

32
0:01:09.800 --> 0:01:13.280
才正式的去把这个知识点明确下来

33
0:01:14.280 --> 0:01:16.640
下面我们看一下所在的位置

34
0:01:16.640 --> 0:01:19.040
知识蒸馏还是在模型压缩的模块

35
0:01:19.040 --> 0:01:21.320
但实际上很多知识蒸馏的算法

36
0:01:21.320 --> 0:01:21.960
包括量化算法

37
0:01:21.960 --> 0:01:23.040
还有简直的算法

38
0:01:23.040 --> 0:01:24.160
包括二次化的算法

39
0:01:24.160 --> 0:01:26.440
都是在我们的AI框架的一个特性

40
0:01:26.440 --> 0:01:29.040
真正我们在做完模型转换之后

41
0:01:29.040 --> 0:01:30.400
变成自己的IR

42
0:01:30.400 --> 0:01:32.400
然后用模型压缩

43
0:01:33.000 --> 0:01:34.120
去对我们AI框架

44
0:01:34.120 --> 0:01:36.920
关于模型压缩的特性进行处理

45
0:01:36.920 --> 0:01:39.680
才得到一个小型化的模型

46
0:01:39.680 --> 0:01:40.720
得到小型化的模型

47
0:01:40.720 --> 0:01:43.000
再给真正的推理的

48
0:01:44.200 --> 0:01:44.760
注意一下

49
0:01:44.760 --> 0:01:47.440
推进引擎架构里面的压缩模块

50
0:01:47.440 --> 0:01:48.680
它是有限定的

51
0:01:48.680 --> 0:01:51.840
它只是做一个模型的小型化的工作

52
0:01:52.080 --> 0:01:54.320
很多内容都是在我们AI框架里面

53
0:01:54.320 --> 0:01:54.920
去处理的

54
0:01:54.960 --> 0:01:56.960
下面我们看一个比较重要的内容

55
0:01:56.960 --> 0:01:58.120
就是知识蒸馏

56
0:01:58.120 --> 0:02:00.920
Knowledge Distillation KD

57
0:02:00.920 --> 0:02:03.400
我们后面都可以简称KD

58
0:02:03.400 --> 0:02:06.960
最初是Hitten在这篇文章里面去发表的

59
0:02:08.360 --> 0:02:11.680
里面就提出了soft label和hard label

60
0:02:11.680 --> 0:02:13.880
然后知识蒸馏使用soft label

61
0:02:13.880 --> 0:02:16.440
来训练教师网络和学生网络

62
0:02:18.320 --> 0:02:21.800
通过教师网络把知识交给学生

63
0:02:21.800 --> 0:02:23.400
然后学生就能够

64
0:02:23.400 --> 0:02:25.240
把知识交给学生

65
0:02:25.920 --> 0:02:28.440
它的算法原理一句话比较简单

66
0:02:28.440 --> 0:02:30.960
就是把教师网络学习到的知识

67
0:02:30.960 --> 0:02:33.960
增留或者交给学生网络

68
0:02:34.680 --> 0:02:36.840
像教师网络一般都会比较大

69
0:02:36.840 --> 0:02:39.760
学生网络一般都会比较小

70
0:02:41.880 --> 0:02:44.640
两者之间是相互竞争的关系

71
0:02:44.640 --> 0:02:47.760
直到学生可以学到跟老师网络

72
0:02:47.760 --> 0:02:50.480
差不多的网络模型的精度

73
0:02:51.480 --> 0:02:55.360
下面这个图就很清楚的说明这个情况了

74
0:02:55.360 --> 0:02:56.600
首先我们有三个内容

75
0:02:56.600 --> 0:02:58.800
一个是老师的网络

76
0:02:58.800 --> 0:03:00.480
老师网络应该会比较大

77
0:03:00.480 --> 0:03:03.520
因为老师他知识量比较丰富

78
0:03:04.360 --> 0:03:07.440
这里面很重要的一个概念就是知识

79
0:03:07.440 --> 0:03:08.640
通过知识的转换之后

80
0:03:08.760 --> 0:03:11.720
得到我们的student model学生网络

81
0:03:12.320 --> 0:03:15.880
学生网络一般都会默认比教师网络要小

82
0:03:15.880 --> 0:03:17.920
所以我们说这个可以通过知识

83
0:03:17.920 --> 0:03:20.560
增优的方式做一个模型的压缩

84
0:03:20.560 --> 0:03:23.240
从大模型变成小模型

85
0:03:23.240 --> 0:03:25.840
从老师模型变成学生模型

86
0:03:25.840 --> 0:03:28.640
而知识怎幺转换就取决于我们的

87
0:03:28.640 --> 0:03:30.720
增优的算法

88
0:03:32.000 --> 0:03:34.240
不过有一点注意的就是我们所使用的

89
0:03:34.240 --> 0:03:36.160
数据都是相同的

90
0:03:36.160 --> 0:03:37.680
数据都是一样的

91
0:03:37.680 --> 0:03:41.360
有可能还会增加一些数据给学生网络

92
0:03:42.480 --> 0:03:46.000
这也是整体知识增优的算法架构

93
0:03:47.000 --> 0:03:49.400
我们在刚才的一个知识增优的算法架构

94
0:03:49.400 --> 0:03:52.840
再进一步的抽象成为三个内容

95
0:03:52.840 --> 0:03:56.520
第一个就是增优的知识knowledge

96
0:03:56.520 --> 0:04:00.880
第二个就是增优的算法distillate algorithm

97
0:04:00.880 --> 0:04:04.360
第三个就是师生网络的架构

98
0:04:04.360 --> 0:04:08.080
就是老师跟学生之间的关系架构组成

99
0:04:10.040 --> 0:04:12.320
下面我们看一下第二个内容

100
0:04:12.320 --> 0:04:13.840
知识增优的方式

101
0:04:13.880 --> 0:04:16.480
这个方式其实我们参考这一篇文章

102
0:04:16.480 --> 0:04:18.640
knowledge,distillation,survive

103
0:04:18.640 --> 0:04:20.800
通常一些survive或者white paper

104
0:04:20.800 --> 0:04:23.520
都是一个很好的宗述和参考

105
0:04:25.520 --> 0:04:27.040
下面我们来聚焦一下一个点

106
0:04:27.040 --> 0:04:29.760
就是knowledge知识的方式

107
0:04:30.360 --> 0:04:32.960
说白了虽然知识的方式说得很抽象

108
0:04:32.960 --> 0:04:35.920
就是我这些知识我要增优的知识从哪里来

109
0:04:35.920 --> 0:04:37.120
怎幺来

110
0:04:38.280 --> 0:04:40.000
这个问题我们要解开的

111
0:04:40.000 --> 0:04:41.720
它有三种方式

112
0:04:41.760 --> 0:04:44.280
responsible based,feature based,relation based

113
0:04:44.280 --> 0:04:45.400
还有architecture based

114
0:04:45.400 --> 0:04:46.840
一般来说architecture based

115
0:04:46.840 --> 0:04:47.920
确实用的比较少

116
0:04:47.920 --> 0:04:49.880
更多的是前三种

117
0:04:51.720 --> 0:04:54.040
我们逐一种来去打开

118
0:04:54.040 --> 0:04:56.720
首先就是responsible based

119
0:04:57.440 --> 0:05:01.240
它主要是指teacher model教师模型输出层的特征

120
0:05:01.240 --> 0:05:04.280
就是最后的一个feature map的特征

121
0:05:04.880 --> 0:05:06.840
希望让我们的学生模型

122
0:05:07.040 --> 0:05:10.200
直接学习到教师模型预测的结果

123
0:05:10.200 --> 0:05:13.640
真正的直接把结果给学生去学习

124
0:05:14.320 --> 0:05:16.080
通俗的来讲就是一个知识点

125
0:05:16.080 --> 0:05:17.680
老师充分的学习完了

126
0:05:17.680 --> 0:05:20.240
然后把结论告诉学生就好了

127
0:05:20.240 --> 0:05:22.520
这个就是最终的输出

128
0:05:22.520 --> 0:05:24.720
下面我们看一下具体的公式

129
0:05:24.920 --> 0:05:27.960
假设JT就是教师模型的输出

130
0:05:27.960 --> 0:05:30.480
而JS就是学生模型的输出

131
0:05:30.480 --> 0:05:33.200
这里面的输出都是指最后一层的输出

132
0:05:33.880 --> 0:05:35.520
而responsible based knowledge

133
0:05:35.520 --> 0:05:39.840
增优的形式就可以用下面这条公式来去描述

134
0:05:41.200 --> 0:05:42.160
回到这个图

135
0:05:42.320 --> 0:05:44.640
我们看一下responsible based在哪里

136
0:05:44.640 --> 0:05:45.560
在这儿

137
0:05:45.920 --> 0:05:49.480
这就证明我们最后一层的模型的输出的知识

138
0:05:49.480 --> 0:05:51.200
就是老师已经学完了

139
0:05:51.200 --> 0:05:53.960
然后把这个知识传授给学生

140
0:05:55.520 --> 0:05:56.320
下面我们看一下

141
0:05:56.320 --> 0:05:59.280
responsible based的一个学习流程图

142
0:05:59.280 --> 0:06:01.040
红色代表的是老师

143
0:06:01.040 --> 0:06:02.960
绿色代表的是学生

144
0:06:02.960 --> 0:06:06.680
我们会把teacher model输出的特征

145
0:06:06.680 --> 0:06:09.920
给到学生网络模型去学习

146
0:06:10.040 --> 0:06:12.360
这里面我们会拿出最后一层的特征

147
0:06:12.360 --> 0:06:14.720
然后通过distillation loss

148
0:06:14.720 --> 0:06:16.480
让学生最后一层的特征

149
0:06:16.480 --> 0:06:19.200
去学习老师的输出的特征

150
0:06:19.200 --> 0:06:21.680
老师的输出特征应该是比较固定的

151
0:06:21.680 --> 0:06:23.600
而学生它是不太固定

152
0:06:23.600 --> 0:06:24.840
需要去学习的

153
0:06:24.840 --> 0:06:27.840
于是我们通过一个损失函数去模拟

154
0:06:27.840 --> 0:06:30.200
去减少去学习

155
0:06:30.200 --> 0:06:34.120
使得我们两个的logist越小越好

156
0:06:34.960 --> 0:06:40.240
第二种就是feature based knowledge

157
0:06:40.240 --> 0:06:44.520
实际上我们的网络模型有很多的层

158
0:06:44.520 --> 0:06:46.880
每一层都会有对应的feature map

159
0:06:46.880 --> 0:06:48.960
有非常多的特征

160
0:06:48.960 --> 0:06:51.920
有非常多的知识可以让我们去学习

161
0:06:51.920 --> 0:06:54.840
所以feature based knowledge

162
0:06:54.840 --> 0:06:57.840
主要是把教授和学生的特征激活

163
0:06:57.840 --> 0:06:59.520
连接起来

164
0:06:59.520 --> 0:07:02.320
这种方式可以用下面这条公式来表示

165
0:07:02.720 --> 0:07:04.920
我们看一下知识的图

166
0:07:04.920 --> 0:07:08.080
像feature based knowledge就在这里面

167
0:07:09.640 --> 0:07:12.640
我们会把网络模型中间学习到的激活

168
0:07:12.640 --> 0:07:13.800
还有权重

169
0:07:13.800 --> 0:07:15.480
把它作为一个feature

170
0:07:15.480 --> 0:07:17.080
然后通过这个feature

171
0:07:17.080 --> 0:07:19.720
鉴密学生跟老师之间的关系

172
0:07:19.720 --> 0:07:22.120
这种就是feature based knowledge

173
0:07:23.760 --> 0:07:26.320
下面我们看一下feature based knowledge的

174
0:07:26.320 --> 0:07:29.160
一个整体的或者工作的流程图

175
0:07:30.160 --> 0:07:31.720
这里面的studio loss

176
0:07:31.720 --> 0:07:33.480
更多是创建在teacher model

177
0:07:33.480 --> 0:07:35.440
跟student model的中间层

178
0:07:35.440 --> 0:07:38.440
通过中间层去创建一个连接关系

179
0:07:39.560 --> 0:07:42.080
这种算法的好处就是老师网络

180
0:07:42.080 --> 0:07:44.800
可以给学生网络提供非常大量的

181
0:07:44.800 --> 0:07:46.240
有用的参考信息

182
0:07:46.240 --> 0:07:48.280
但是怎幺有效的从老师网络

183
0:07:48.280 --> 0:07:51.360
这里面这幺多提示层去给到学生

184
0:07:51.360 --> 0:07:52.960
因为student model的网络层数

185
0:07:52.960 --> 0:07:54.720
会比teacher model要少

186
0:07:55.400 --> 0:07:56.880
我们应该从teacher model里面

187
0:07:56.920 --> 0:07:59.880
提取哪几层有效的作为引导层

188
0:07:59.880 --> 0:08:02.440
这一步是前年的研究

189
0:08:03.000 --> 0:08:05.520
缺点就是提示层和引导层的大小

190
0:08:05.520 --> 0:08:07.160
存在明显的差异

191
0:08:07.160 --> 0:08:08.360
说白了就是teacher model

192
0:08:08.360 --> 0:08:10.160
跟student model的网络层数

193
0:08:10.160 --> 0:08:12.360
中间的feature map差异比较大

194
0:08:12.360 --> 0:08:14.800
如何正确的匹配两者之间的关系

195
0:08:14.800 --> 0:08:17.320
也是需要一个探索和研究的

196
0:08:17.320 --> 0:08:20.000
而业界在整体上面还没有成熟

197
0:08:21.640 --> 0:08:25.160
最后一种就是relation based knowledge

198
0:08:25.960 --> 0:08:26.960
基于关系的知识

199
0:08:27.160 --> 0:08:29.120
主要是进一步探索不同层

200
0:08:29.120 --> 0:08:31.760
或者数据样本之间的一个关系

201
0:08:31.760 --> 0:08:34.240
从而提出一些相关的知识

202
0:08:34.680 --> 0:08:36.000
我们看一下这个图

203
0:08:36.000 --> 0:08:37.200
relation based knowledge

204
0:08:37.320 --> 0:08:38.680
就在这里面

205
0:08:39.520 --> 0:08:41.080
希望通过研究不同的样本

206
0:08:41.080 --> 0:08:42.720
不同的输出层之间的关系

207
0:08:43.080 --> 0:08:46.640
从而创建整个知识金流的知识点

208
0:08:47.560 --> 0:08:50.800
下面我们又来到算法的架构图里面

209
0:08:50.800 --> 0:08:52.280
我们有相同的数据

210
0:08:52.280 --> 0:08:54.400
然后有一个teacher model和student model

211
0:08:54.720 --> 0:08:55.560
distillation loss

212
0:08:55.680 --> 0:08:56.960
就不仅仅是去学习

213
0:08:56.960 --> 0:08:58.320
我们刚才讲到的

214
0:08:58.320 --> 0:08:59.880
网络模型中间的特征

215
0:08:59.880 --> 0:09:00.960
还有最后一层的特征

216
0:09:00.960 --> 0:09:02.680
而且他会学习数据样本

217
0:09:02.680 --> 0:09:05.160
还有网络模型层之间的一个关系

218
0:09:06.280 --> 0:09:06.800
好了

219
0:09:06.800 --> 0:09:08.680
今天给大家汇报了两个知识

220
0:09:08.680 --> 0:09:10.480
一个就是知识增流的背景

221
0:09:10.480 --> 0:09:11.760
我们为什幺要去增流

222
0:09:11.760 --> 0:09:13.000
增流的好处

223
0:09:13.000 --> 0:09:14.240
接着我们看一下

224
0:09:14.240 --> 0:09:15.600
增流的知识的形态

225
0:09:15.600 --> 0:09:17.240
就是更多的关注于knowledge

226
0:09:17.240 --> 0:09:18.720
怎幺提取knowledge

227
0:09:18.720 --> 0:09:19.800
从哪里来

228
0:09:19.800 --> 0:09:20.080
好

229
0:09:20.080 --> 0:09:20.800
谢谢各位

230
0:09:20.800 --> 0:09:21.880
拜了个拜

231
0:09:22.400 --> 0:09:24.120
卷的不行了

232
0:09:24.160 --> 0:09:25.520
记得一键三连加关注

233
0:09:25.880 --> 0:09:27.240
所有的内容都会开源

234
0:09:27.240 --> 0:09:29.120
在下面这条链接里面

235
0:09:29.480 --> 0:09:30.440
拜了个拜

