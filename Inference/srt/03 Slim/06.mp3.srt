1
00:00:00,000 --> 00:00:07,800
Hello大家好,我是周米

2
00:00:07,800 --> 00:00:09,560
因为普通话咬字不清楚

3
00:00:09,560 --> 00:00:10,480
所以录一门课

4
00:00:10,480 --> 00:00:12,200
我一般都会NG很多遍

5
00:00:12,200 --> 00:00:13,960
然后重复很多遍

6
00:00:14,720 --> 00:00:18,120
那今天我们来到推理引擎模型压缩的

7
00:00:18,120 --> 00:00:19,520
倒数第二个内容

8
00:00:19,680 --> 00:00:21,280
就是知识蒸馏

9
00:00:21,280 --> 00:00:22,880
Knowledge Distillation

10
00:00:22,880 --> 00:00:23,400
那好了

11
00:00:23,400 --> 00:00:25,720
现在我们来到知识蒸馏这个环节

12
00:00:25,720 --> 00:00:28,560
我们看一下今天知识蒸馏要给大家汇报

13
00:00:28,560 --> 00:00:29,920
哪几个内容

14
00:00:30,840 --> 00:00:33,120
首先就是知识蒸馏的背景

15
00:00:33,120 --> 00:00:33,720
Blackboard

16
00:00:33,720 --> 00:00:35,240
然后我们可以看一下

17
00:00:35,240 --> 00:00:37,480
因为我们需要对知识进行蒸馏

18
00:00:37,720 --> 00:00:39,640
这里面的知识就很重要了

19
00:00:39,640 --> 00:00:42,760
我们了解一下所谓的知识的形式

20
00:00:42,760 --> 00:00:44,600
就是Knowledge的format

21
00:00:44,840 --> 00:00:47,720
然后我们可能会分开两个视频来看

22
00:00:47,720 --> 00:00:49,520
因为一个视频可能讲完这些内容了

23
00:00:49,520 --> 00:00:50,320
有点长

24
00:00:50,760 --> 00:00:52,000
第二个视频的内容里面

25
00:00:52,000 --> 00:00:52,880
我们会讲讲

26
00:00:52,880 --> 00:00:55,800
Distillation就是蒸馏的具体的方法

27
00:00:56,320 --> 00:00:57,440
上面的内容

28
00:00:57,440 --> 00:00:59,680
我们更多的聚焦于Knowledge

29
00:00:59,800 --> 00:01:00,560
下面的内容

30
00:01:00,560 --> 00:01:03,320
我们更多的聚焦于Distillation

31
00:01:04,600 --> 00:01:07,680
Hiton最经典的一个知识蒸馏的算法进行解读

32
00:01:07,680 --> 00:01:09,800
因为Hiton他写了一篇文章

33
00:01:09,800 --> 00:01:13,280
才正式的去把这个知识点明确下来

34
00:01:14,280 --> 00:01:16,640
下面我们看一下所在的位置

35
00:01:16,640 --> 00:01:19,040
知识蒸馏还是在模型压缩的模块

36
00:01:19,040 --> 00:01:21,320
但实际上很多知识蒸馏的算法

37
00:01:21,320 --> 00:01:21,960
包括量化算法

38
00:01:21,960 --> 00:01:23,040
还有简直的算法

39
00:01:23,040 --> 00:01:24,160
包括二次化的算法

40
00:01:24,160 --> 00:01:26,440
都是在我们的AI框架的一个特性

41
00:01:26,440 --> 00:01:29,040
真正我们在做完模型转换之后

42
00:01:29,040 --> 00:01:30,400
变成自己的IR

43
00:01:30,400 --> 00:01:32,400
然后用模型压缩

44
00:01:33,000 --> 00:01:34,120
去对我们AI框架

45
00:01:34,120 --> 00:01:36,920
关于模型压缩的特性进行处理

46
00:01:36,920 --> 00:01:39,680
才得到一个小型化的模型

47
00:01:39,680 --> 00:01:40,720
得到小型化的模型

48
00:01:40,720 --> 00:01:43,000
再给真正的推理的

49
00:01:44,200 --> 00:01:44,760
注意一下

50
00:01:44,760 --> 00:01:47,440
推进引擎架构里面的压缩模块

51
00:01:47,440 --> 00:01:48,680
它是有限定的

52
00:01:48,680 --> 00:01:51,840
它只是做一个模型的小型化的工作

53
00:01:52,080 --> 00:01:54,320
很多内容都是在我们AI框架里面

54
00:01:54,320 --> 00:01:54,920
去处理的

55
00:01:54,960 --> 00:01:56,960
下面我们看一个比较重要的内容

56
00:01:56,960 --> 00:01:58,120
就是知识蒸馏

57
00:01:58,120 --> 00:02:00,920
Knowledge Distillation KD

58
00:02:00,920 --> 00:02:03,400
我们后面都可以简称KD

59
00:02:03,400 --> 00:02:06,960
最初是Hitten在这篇文章里面去发表的

60
00:02:08,360 --> 00:02:11,680
里面就提出了soft label和hard label

61
00:02:11,680 --> 00:02:13,880
然后知识蒸馏使用soft label

62
00:02:13,880 --> 00:02:16,440
来训练教师网络和学生网络

63
00:02:18,320 --> 00:02:21,800
通过教师网络把知识交给学生

64
00:02:21,800 --> 00:02:23,400
然后学生就能够

65
00:02:23,400 --> 00:02:25,240
把知识交给学生

66
00:02:25,920 --> 00:02:28,440
它的算法原理一句话比较简单

67
00:02:28,440 --> 00:02:30,960
就是把教师网络学习到的知识

68
00:02:30,960 --> 00:02:33,960
增留或者交给学生网络

69
00:02:34,680 --> 00:02:36,840
像教师网络一般都会比较大

70
00:02:36,840 --> 00:02:39,760
学生网络一般都会比较小

71
00:02:41,880 --> 00:02:44,640
两者之间是相互竞争的关系

72
00:02:44,640 --> 00:02:47,760
直到学生可以学到跟老师网络

73
00:02:47,760 --> 00:02:50,480
差不多的网络模型的精度

74
00:02:51,480 --> 00:02:55,360
下面这个图就很清楚的说明这个情况了

75
00:02:55,360 --> 00:02:56,600
首先我们有三个内容

76
00:02:56,600 --> 00:02:58,800
一个是老师的网络

77
00:02:58,800 --> 00:03:00,480
老师网络应该会比较大

78
00:03:00,480 --> 00:03:03,520
因为老师他知识量比较丰富

79
00:03:04,360 --> 00:03:07,440
这里面很重要的一个概念就是知识

80
00:03:07,440 --> 00:03:08,640
通过知识的转换之后

81
00:03:08,760 --> 00:03:11,720
得到我们的student model学生网络

82
00:03:12,320 --> 00:03:15,880
学生网络一般都会默认比教师网络要小

83
00:03:15,880 --> 00:03:17,920
所以我们说这个可以通过知识

84
00:03:17,920 --> 00:03:20,560
增优的方式做一个模型的压缩

85
00:03:20,560 --> 00:03:23,240
从大模型变成小模型

86
00:03:23,240 --> 00:03:25,840
从老师模型变成学生模型

87
00:03:25,840 --> 00:03:28,640
而知识怎幺转换就取决于我们的

88
00:03:28,640 --> 00:03:30,720
增优的算法

89
00:03:32,000 --> 00:03:34,240
不过有一点注意的就是我们所使用的

90
00:03:34,240 --> 00:03:36,160
数据都是相同的

91
00:03:36,160 --> 00:03:37,680
数据都是一样的

92
00:03:37,680 --> 00:03:41,360
有可能还会增加一些数据给学生网络

93
00:03:42,480 --> 00:03:46,000
这也是整体知识增优的算法架构

94
00:03:47,000 --> 00:03:49,400
我们在刚才的一个知识增优的算法架构

95
00:03:49,400 --> 00:03:52,840
再进一步的抽象成为三个内容

96
00:03:52,840 --> 00:03:56,520
第一个就是增优的知识knowledge

97
00:03:56,520 --> 00:04:00,880
第二个就是增优的算法distillate algorithm

98
00:04:00,880 --> 00:04:04,360
第三个就是师生网络的架构

99
00:04:04,360 --> 00:04:08,080
就是老师跟学生之间的关系架构组成

100
00:04:10,040 --> 00:04:12,320
下面我们看一下第二个内容

101
00:04:12,320 --> 00:04:13,840
知识增优的方式

102
00:04:13,880 --> 00:04:16,480
这个方式其实我们参考这一篇文章

103
00:04:16,480 --> 00:04:18,640
knowledge,distillation,survive

104
00:04:18,640 --> 00:04:20,800
通常一些survive或者white paper

105
00:04:20,800 --> 00:04:23,520
都是一个很好的宗述和参考

106
00:04:25,520 --> 00:04:27,040
下面我们来聚焦一下一个点

107
00:04:27,040 --> 00:04:29,760
就是knowledge知识的方式

108
00:04:30,360 --> 00:04:32,960
说白了虽然知识的方式说得很抽象

109
00:04:32,960 --> 00:04:35,920
就是我这些知识我要增优的知识从哪里来

110
00:04:35,920 --> 00:04:37,120
怎幺来

111
00:04:38,280 --> 00:04:40,000
这个问题我们要解开的

112
00:04:40,000 --> 00:04:41,720
它有三种方式

113
00:04:41,760 --> 00:04:44,280
responsible based,feature based,relation based

114
00:04:44,280 --> 00:04:45,400
还有architecture based

115
00:04:45,400 --> 00:04:46,840
一般来说architecture based

116
00:04:46,840 --> 00:04:47,920
确实用的比较少

117
00:04:47,920 --> 00:04:49,880
更多的是前三种

118
00:04:51,720 --> 00:04:54,040
我们逐一种来去打开

119
00:04:54,040 --> 00:04:56,720
首先就是responsible based

120
00:04:57,440 --> 00:05:01,240
它主要是指teacher model教师模型输出层的特征

121
00:05:01,240 --> 00:05:04,280
就是最后的一个feature map的特征

122
00:05:04,880 --> 00:05:06,840
希望让我们的学生模型

123
00:05:07,040 --> 00:05:10,200
直接学习到教师模型预测的结果

124
00:05:10,200 --> 00:05:13,640
真正的直接把结果给学生去学习

125
00:05:14,320 --> 00:05:16,080
通俗的来讲就是一个知识点

126
00:05:16,080 --> 00:05:17,680
老师充分的学习完了

127
00:05:17,680 --> 00:05:20,240
然后把结论告诉学生就好了

128
00:05:20,240 --> 00:05:22,520
这个就是最终的输出

129
00:05:22,520 --> 00:05:24,720
下面我们看一下具体的公式

130
00:05:24,920 --> 00:05:27,960
假设JT就是教师模型的输出

131
00:05:27,960 --> 00:05:30,480
而JS就是学生模型的输出

132
00:05:30,480 --> 00:05:33,200
这里面的输出都是指最后一层的输出

133
00:05:33,880 --> 00:05:35,520
而responsible based knowledge

134
00:05:35,520 --> 00:05:39,840
增优的形式就可以用下面这条公式来去描述

135
00:05:41,200 --> 00:05:42,160
回到这个图

136
00:05:42,320 --> 00:05:44,640
我们看一下responsible based在哪里

137
00:05:44,640 --> 00:05:45,560
在这儿

138
00:05:45,920 --> 00:05:49,480
这就证明我们最后一层的模型的输出的知识

139
00:05:49,480 --> 00:05:51,200
就是老师已经学完了

140
00:05:51,200 --> 00:05:53,960
然后把这个知识传授给学生

141
00:05:55,520 --> 00:05:56,320
下面我们看一下

142
00:05:56,320 --> 00:05:59,280
responsible based的一个学习流程图

143
00:05:59,280 --> 00:06:01,040
红色代表的是老师

144
00:06:01,040 --> 00:06:02,960
绿色代表的是学生

145
00:06:02,960 --> 00:06:06,680
我们会把teacher model输出的特征

146
00:06:06,680 --> 00:06:09,920
给到学生网络模型去学习

147
00:06:10,040 --> 00:06:12,360
这里面我们会拿出最后一层的特征

148
00:06:12,360 --> 00:06:14,720
然后通过distillation loss

149
00:06:14,720 --> 00:06:16,480
让学生最后一层的特征

150
00:06:16,480 --> 00:06:19,200
去学习老师的输出的特征

151
00:06:19,200 --> 00:06:21,680
老师的输出特征应该是比较固定的

152
00:06:21,680 --> 00:06:23,600
而学生它是不太固定

153
00:06:23,600 --> 00:06:24,840
需要去学习的

154
00:06:24,840 --> 00:06:27,840
于是我们通过一个损失函数去模拟

155
00:06:27,840 --> 00:06:30,200
去减少去学习

156
00:06:30,200 --> 00:06:34,120
使得我们两个的logist越小越好

157
00:06:34,960 --> 00:06:40,240
第二种就是feature based knowledge

158
00:06:40,240 --> 00:06:44,520
实际上我们的网络模型有很多的层

159
00:06:44,520 --> 00:06:46,880
每一层都会有对应的feature map

160
00:06:46,880 --> 00:06:48,960
有非常多的特征

161
00:06:48,960 --> 00:06:51,920
有非常多的知识可以让我们去学习

162
00:06:51,920 --> 00:06:54,840
所以feature based knowledge

163
00:06:54,840 --> 00:06:57,840
主要是把教授和学生的特征激活

164
00:06:57,840 --> 00:06:59,520
连接起来

165
00:06:59,520 --> 00:07:02,320
这种方式可以用下面这条公式来表示

166
00:07:02,720 --> 00:07:04,920
我们看一下知识的图

167
00:07:04,920 --> 00:07:08,080
像feature based knowledge就在这里面

168
00:07:09,640 --> 00:07:12,640
我们会把网络模型中间学习到的激活

169
00:07:12,640 --> 00:07:13,800
还有权重

170
00:07:13,800 --> 00:07:15,480
把它作为一个feature

171
00:07:15,480 --> 00:07:17,080
然后通过这个feature

172
00:07:17,080 --> 00:07:19,720
鉴密学生跟老师之间的关系

173
00:07:19,720 --> 00:07:22,120
这种就是feature based knowledge

174
00:07:23,760 --> 00:07:26,320
下面我们看一下feature based knowledge的

175
00:07:26,320 --> 00:07:29,160
一个整体的或者工作的流程图

176
00:07:30,160 --> 00:07:31,720
这里面的studio loss

177
00:07:31,720 --> 00:07:33,480
更多是创建在teacher model

178
00:07:33,480 --> 00:07:35,440
跟student model的中间层

179
00:07:35,440 --> 00:07:38,440
通过中间层去创建一个连接关系

180
00:07:39,560 --> 00:07:42,080
这种算法的好处就是老师网络

181
00:07:42,080 --> 00:07:44,800
可以给学生网络提供非常大量的

182
00:07:44,800 --> 00:07:46,240
有用的参考信息

183
00:07:46,240 --> 00:07:48,280
但是怎幺有效的从老师网络

184
00:07:48,280 --> 00:07:51,360
这里面这幺多提示层去给到学生

185
00:07:51,360 --> 00:07:52,960
因为student model的网络层数

186
00:07:52,960 --> 00:07:54,720
会比teacher model要少

187
00:07:55,400 --> 00:07:56,880
我们应该从teacher model里面

188
00:07:56,920 --> 00:07:59,880
提取哪几层有效的作为引导层

189
00:07:59,880 --> 00:08:02,440
这一步是前年的研究

190
00:08:03,000 --> 00:08:05,520
缺点就是提示层和引导层的大小

191
00:08:05,520 --> 00:08:07,160
存在明显的差异

192
00:08:07,160 --> 00:08:08,360
说白了就是teacher model

193
00:08:08,360 --> 00:08:10,160
跟student model的网络层数

194
00:08:10,160 --> 00:08:12,360
中间的feature map差异比较大

195
00:08:12,360 --> 00:08:14,800
如何正确的匹配两者之间的关系

196
00:08:14,800 --> 00:08:17,320
也是需要一个探索和研究的

197
00:08:17,320 --> 00:08:20,000
而业界在整体上面还没有成熟

198
00:08:21,640 --> 00:08:25,160
最后一种就是relation based knowledge

199
00:08:25,960 --> 00:08:26,960
基于关系的知识

200
00:08:27,160 --> 00:08:29,120
主要是进一步探索不同层

201
00:08:29,120 --> 00:08:31,760
或者数据样本之间的一个关系

202
00:08:31,760 --> 00:08:34,240
从而提出一些相关的知识

203
00:08:34,680 --> 00:08:36,000
我们看一下这个图

204
00:08:36,000 --> 00:08:37,200
relation based knowledge

205
00:08:37,320 --> 00:08:38,680
就在这里面

206
00:08:39,520 --> 00:08:41,080
希望通过研究不同的样本

207
00:08:41,080 --> 00:08:42,720
不同的输出层之间的关系

208
00:08:43,080 --> 00:08:46,640
从而创建整个知识金流的知识点

209
00:08:47,560 --> 00:08:50,800
下面我们又来到算法的架构图里面

210
00:08:50,800 --> 00:08:52,280
我们有相同的数据

211
00:08:52,280 --> 00:08:54,400
然后有一个teacher model和student model

212
00:08:54,720 --> 00:08:55,560
distillation loss

213
00:08:55,680 --> 00:08:56,960
就不仅仅是去学习

214
00:08:56,960 --> 00:08:58,320
我们刚才讲到的

215
00:08:58,320 --> 00:08:59,880
网络模型中间的特征

216
00:08:59,880 --> 00:09:00,960
还有最后一层的特征

217
00:09:00,960 --> 00:09:02,680
而且他会学习数据样本

218
00:09:02,680 --> 00:09:05,160
还有网络模型层之间的一个关系

219
00:09:06,280 --> 00:09:06,800
好了

220
00:09:06,800 --> 00:09:08,680
今天给大家汇报了两个知识

221
00:09:08,680 --> 00:09:10,480
一个就是知识增流的背景

222
00:09:10,480 --> 00:09:11,760
我们为什幺要去增流

223
00:09:11,760 --> 00:09:13,000
增流的好处

224
00:09:13,000 --> 00:09:14,240
接着我们看一下

225
00:09:14,240 --> 00:09:15,600
增流的知识的形态

226
00:09:15,600 --> 00:09:17,240
就是更多的关注于knowledge

227
00:09:17,240 --> 00:09:18,720
怎幺提取knowledge

228
00:09:18,720 --> 00:09:19,800
从哪里来

229
00:09:19,800 --> 00:09:20,080
好

230
00:09:20,080 --> 00:09:20,800
谢谢各位

231
00:09:20,800 --> 00:09:21,880
拜了个拜

232
00:09:22,400 --> 00:09:24,120
卷的不行了

233
00:09:24,160 --> 00:09:25,520
记得一键三连加关注

234
00:09:25,880 --> 00:09:27,240
所有的内容都会开源

235
00:09:27,240 --> 00:09:29,120
在下面这条链接里面

236
00:09:29,480 --> 00:09:30,440
拜了个拜

