1
00:00:00,000 --> 00:00:07,500
Hello 大家好,我是曾铭

2
00:00:07,500 --> 00:00:13,500
我们现在的位置处于推理引擎这个系列里面的模型压缩

3
00:00:13,500 --> 00:00:16,700
今天我要给大家去带来一个新的内容

4
00:00:16,700 --> 00:00:18,500
应该不算新的内容了

5
00:00:18,500 --> 00:00:22,500
我们在量化里面再进一步的去展开杆子量化训练

6
00:00:22,500 --> 00:00:25,700
也就是大家经常说到的QAT

7
00:00:26,200 --> 00:00:31,700
现在我们来到了第一笔特量化这个系列课程里面的第二个内容

8
00:00:31,700 --> 00:00:34,700
杆子量化QAT

9
00:00:34,700 --> 00:00:39,200
下面我们再来看一下在整个推理引擎架构里面

10
00:00:39,200 --> 00:00:42,200
杆子量化其实处于这个阶段

11
00:00:42,200 --> 00:00:46,200
讲错了,量化是处于这个阶段

12
00:00:46,200 --> 00:00:49,200
但是杆子量化是跟训练墙相关的

13
00:00:49,200 --> 00:00:51,700
所以这个内容是我们看不到的

14
00:00:52,200 --> 00:00:54,700
在AI框架上面的一个特性

15
00:00:54,700 --> 00:00:57,700
例如PyTorch也有自己的杆子量化训练的一些特性

16
00:00:57,700 --> 00:01:01,700
还有Maxwell也会推出自己杆子量化训练的特性

17
00:01:01,700 --> 00:01:05,200
接下来我们来一起看一下杆子量化训练

18
00:01:05,200 --> 00:01:07,200
它到底是怎幺实现的

19
00:01:07,200 --> 00:01:11,700
简单的一句话就是我们会在一个正常的网络模型栏中

20
00:01:11,700 --> 00:01:15,200
去插入一些伪量化的算子或者节点

21
00:01:15,200 --> 00:01:17,700
这个节点我们叫做FakeQuant

22
00:01:17,700 --> 00:01:19,200
因为它不是真正的量化

23
00:01:19,200 --> 00:01:23,700
而是用来模拟量化的时候引入的一些误差

24
00:01:23,700 --> 00:01:25,700
而在真正端次推理的时候

25
00:01:25,700 --> 00:01:29,200
我们需要把这些FakeQuant去进行一个直叠

26
00:01:29,200 --> 00:01:30,700
那为啥叫直叠呢?

27
00:01:30,700 --> 00:01:33,200
是因为它确实只剩下一些常量

28
00:01:33,200 --> 00:01:35,700
我们需要对这些常量进行直叠

29
00:01:35,700 --> 00:01:39,700
然后把相关的属性或者信息变成我们Tensor的信息

30
00:01:39,700 --> 00:01:41,700
最后再进行一个推理

31
00:01:41,700 --> 00:01:44,200
那我们看看下面这个图

32
00:01:44,200 --> 00:01:46,700
下面这个是我们的一个计算图

33
00:01:46,700 --> 00:01:48,700
输进去的是一些数据

34
00:01:48,700 --> 00:01:52,200
我们需要在输进去的数据的时候插入一个伪量化的算子

35
00:01:52,200 --> 00:01:56,700
接着我们可能还需要对我们的权重插入一个伪量化的算子

36
00:01:56,700 --> 00:02:01,200
完成卷集计算之后就会给一个编程进行一个学习

37
00:02:01,200 --> 00:02:04,200
学习完之后就真正的去进入了一个韵录了

38
00:02:04,200 --> 00:02:08,200
在出去韵录之前我们也会插一个伪量化的节点在这里面

39
00:02:08,200 --> 00:02:11,700
像这种确实在我们的一个正常的计算图里面

40
00:02:11,700 --> 00:02:13,700
去插入各种伪量化的节点

41
00:02:13,700 --> 00:02:16,700
这种方式我们叫做QAT

42
00:02:17,200 --> 00:02:22,200
接下来我们去看看刚才大量的去提到一些伪量化的节点FadeCode

43
00:02:22,200 --> 00:02:25,200
那FadeCode的节点有什幺用吗?

44
00:02:25,200 --> 00:02:27,200
下面就有两个比较大的作用了

45
00:02:27,200 --> 00:02:29,700
这也是伪量化节点的一个具体的作用

46
00:02:29,700 --> 00:02:33,700
首先这个伪量化节点主要是找到输入数据的分布

47
00:02:33,700 --> 00:02:38,200
也就是找到我们数据的一个最大值和最小值

48
00:02:38,200 --> 00:02:41,700
第二个点就是我们刚才简单的提到过的

49
00:02:41,700 --> 00:02:43,200
它去模拟我们量化操作

50
00:02:43,200 --> 00:02:50,200
就是把我们的一些FPSensor量化到我们的int8这种低比特的时候的一些精度的损失

51
00:02:50,200 --> 00:02:53,700
把这些损失在网络模型训练或者FileTuning的时候

52
00:02:53,700 --> 00:02:57,700
作用到整个网络模型当中传递给损失函数

53
00:02:57,700 --> 00:02:58,700
就是我们的Loss

54
00:02:58,700 --> 00:03:01,700
让优化器在训练或者FileTuning的过程当中

55
00:03:01,700 --> 00:03:07,700
对我们因为量化所造成的损失进行一个优化和学习

56
00:03:07,700 --> 00:03:14,700
那至于第二个就是我们真正的一个伪量化节点或者伪量化算子它的内核作用了

57
00:03:14,700 --> 00:03:21,700
我们下面去看看伪量化节点的一个正向传播具体是怎幺算的

58
00:03:21,700 --> 00:03:25,700
其实为了求出我们网络模型的输入

59
00:03:25,700 --> 00:03:28,700
就是我们的Tensor一个比较精确的min和max

60
00:03:28,700 --> 00:03:33,700
所以我们会在一个网络模型训练的时候插入伪量化节点

61
00:03:33,700 --> 00:03:36,200
也就是我们需要获取那个计算图

62
00:03:36,200 --> 00:03:40,200
然后对这个计算图进行改造插入我们希望的节点

63
00:03:40,200 --> 00:03:44,200
然后模拟误差得到数据的分布

64
00:03:44,200 --> 00:03:48,200
而对每一个算子我们都会去求输入的数据x

65
00:03:48,200 --> 00:03:50,200
它的一个最小值还有最大值

66
00:03:50,200 --> 00:03:53,200
还记得我们在上一节课里面去讲量化原理的时候

67
00:03:53,200 --> 00:03:55,200
有了最小值和最大值之后

68
00:03:55,200 --> 00:03:58,200
我们就可以去求我们量化的scale

69
00:03:58,200 --> 00:04:03,200
通过这个scale我们就可以把我们的输入数据直接量化成我们的int8

70
00:04:03,200 --> 00:04:05,700
那正向的forward的时候我们就会做这个工作

71
00:04:05,700 --> 00:04:07,700
除了记录最大值和最小值

72
00:04:07,700 --> 00:04:11,700
它还要做一个量化模拟的操作

73
00:04:11,700 --> 00:04:13,700
假设我们之前的数据是一个平滑的数据

74
00:04:13,700 --> 00:04:15,700
类似一条线性的

75
00:04:15,700 --> 00:04:19,700
经过伪量化算子进行模拟的时候就变成了有阶梯形状

76
00:04:19,700 --> 00:04:22,700
把大部分的数据都直接消掉了

77
00:04:22,700 --> 00:04:25,700
从episodic数据变成int8的数据

78
00:04:25,700 --> 00:04:28,700
那这个就是伪量化算子的正向传播

79
00:04:28,700 --> 00:04:31,700
有正向是不是应该有反向啊

80
00:04:31,700 --> 00:04:34,700
那我们现在来看看反向传播

81
00:04:34,700 --> 00:04:36,700
这个伪量化算子具体怎幺实现

82
00:04:36,700 --> 00:04:38,700
按照刚才正向传播的公式

83
00:04:38,700 --> 00:04:40,700
如果我们反向的时候呢

84
00:04:40,700 --> 00:04:42,700
对刚才正向的那条公式求导数

85
00:04:42,700 --> 00:04:44,700
肯定会导致我们的权重为0

86
00:04:44,700 --> 00:04:47,700
权重为0就没有办法去学习了

87
00:04:47,700 --> 00:04:48,700
于是呢反向的时候呢

88
00:04:48,700 --> 00:04:51,700
我们相当于一个直通的连通器

89
00:04:51,700 --> 00:04:53,700
把delta in直接给delta out

90
00:04:53,700 --> 00:04:55,700
但是有点治愈的就是

91
00:04:55,700 --> 00:04:57,700
我们的输入的数据x呢

92
00:04:57,700 --> 00:04:59,700
必须要在我们的量化范围之内

93
00:04:59,700 --> 00:05:01,700
如果不在的我们把它截断

94
00:05:01,700 --> 00:05:04,700
于是呢最终反向传播的fakecoin呢

95
00:05:04,700 --> 00:05:06,700
一般来说我们都会对它的数据

96
00:05:06,700 --> 00:05:09,700
进行截断式的处理

97
00:05:10,700 --> 00:05:12,700
了解完伪量化算子呢

98
00:05:12,700 --> 00:05:14,700
我们现在呢伪量化算子

99
00:05:14,700 --> 00:05:15,700
还有一个很重要的工作

100
00:05:15,700 --> 00:05:17,700
就是更新min和max

101
00:05:17,700 --> 00:05:20,700
因为每一次训练每一轮迭代

102
00:05:20,700 --> 00:05:22,700
每一个apple每个step呢

103
00:05:22,700 --> 00:05:24,700
它都会有不同的min和max

104
00:05:24,700 --> 00:05:26,700
它都有不同的数据的输入

105
00:05:26,700 --> 00:05:28,700
有点类似于我们的边算子

106
00:05:28,700 --> 00:05:30,700
或者LayerLoop算子呢

107
00:05:30,700 --> 00:05:33,700
去更新beta和gamma的这种方式

108
00:05:33,700 --> 00:05:35,700
通过一个winlimin winlimax

109
00:05:35,700 --> 00:05:37,700
还有movinmin movinmax进行计算

110
00:05:37,700 --> 00:05:40,700
如果大家看不懂没关系

111
00:05:40,700 --> 00:05:41,700
去看看边这个算子呢

112
00:05:41,700 --> 00:05:43,700
怎幺去更新beta和gamma的

113
00:05:43,700 --> 00:05:45,700
如果大家确实很有兴趣

114
00:05:45,700 --> 00:05:46,700
去了解伪量化算子

115
00:05:46,700 --> 00:05:48,700
真正的一些代码实现呢

116
00:05:48,700 --> 00:05:51,700
也可以去看一下Pytorch的具体实现

117
00:05:51,700 --> 00:05:53,700
那下面呢我们去

118
00:05:53,700 --> 00:05:56,700
下面呢我们让观众提两个问题

119
00:05:57,700 --> 00:05:58,700
作弊老师你好啊

120
00:05:58,700 --> 00:05:59,700
我想问一下

121
00:05:59,700 --> 00:06:02,700
在什幺地方或者什幺位置

122
00:06:02,700 --> 00:06:06,700
去插入FakeQuant伪量化这个节点呢

123
00:06:08,700 --> 00:06:09,700
诶

124
00:06:09,700 --> 00:06:11,700
小心你这个问题确实是灵魂拷问的

125
00:06:11,700 --> 00:06:14,700
我们刚才只是简单的去给大家讲了

126
00:06:14,700 --> 00:06:16,700
FakeQuant这个伪量化的算子

127
00:06:16,700 --> 00:06:17,700
是怎幺去实现的

128
00:06:17,700 --> 00:06:20,700
正向怎幺把它进行一个伪量化的学习

129
00:06:20,700 --> 00:06:23,700
反向呢怎幺对它进行一个截断

130
00:06:23,700 --> 00:06:24,700
那一般来说呢

131
00:06:24,700 --> 00:06:26,700
我们会在一些密集的计算的算子

132
00:06:26,700 --> 00:06:28,700
说到密集计算算子呢

133
00:06:28,700 --> 00:06:29,700
其实并不多

134
00:06:29,700 --> 00:06:31,700
有点类似于华为生腾里面的一个Cube

135
00:06:31,700 --> 00:06:32,700
这些算子

136
00:06:32,700 --> 00:06:34,700
举个简单的几个例子

137
00:06:34,700 --> 00:06:36,700
就是GMM矩阵的相乘

138
00:06:36,700 --> 00:06:37,700
还有卷机

139
00:06:37,700 --> 00:06:40,700
那这些呢就是密集计算的算子

140
00:06:40,700 --> 00:06:42,700
另外我们还会激活算子之后

141
00:06:42,700 --> 00:06:44,700
或者之前进行插入的

142
00:06:44,700 --> 00:06:46,700
最后呢我们还会在网络模型输入

143
00:06:46,700 --> 00:06:49,700
输出的地方进行插入伪量化算子

144
00:06:49,700 --> 00:06:52,700
下面呢我们看一个更加具体的例子

145
00:06:52,700 --> 00:06:54,700
那虽然看上去左边的这个图呢

146
00:06:54,700 --> 00:06:55,700
很复杂

147
00:06:55,700 --> 00:06:56,700
但实际上啊

148
00:06:56,700 --> 00:06:58,700
这个是我们的一个卷机BMV路

149
00:06:58,700 --> 00:07:01,700
三个简单的算子的一个计算图

150
00:07:01,700 --> 00:07:02,700
因为边层呢

151
00:07:02,700 --> 00:07:04,700
它有很多的不同的参数

152
00:07:04,700 --> 00:07:05,700
它要更新Gamma了

153
00:07:05,700 --> 00:07:06,700
要更新Beta了

154
00:07:06,700 --> 00:07:08,700
所以你看上去很复杂

155
00:07:08,700 --> 00:07:10,700
我们插入伪量化算子呢

156
00:07:10,700 --> 00:07:12,700
就会在这个图的input里面插入

157
00:07:12,700 --> 00:07:13,700
在我们的wait里面插入

158
00:07:13,700 --> 00:07:16,700
可能在对我们的W进行输出的时候

159
00:07:16,700 --> 00:07:17,700
插入一个

160
00:07:17,700 --> 00:07:18,700
另外的话

161
00:07:18,700 --> 00:07:21,700
我们还会对我们的激活后面插入一个

162
00:07:21,700 --> 00:07:23,700
这个呢就是一般的插入方式

163
00:07:23,700 --> 00:07:24,700
值得注意的就是

164
00:07:24,700 --> 00:07:26,700
如果你研究感知量化算法呢

165
00:07:26,700 --> 00:07:29,700
你可能会提出很多不同的插入的方式

166
00:07:29,700 --> 00:07:30,700
去学习

167
00:07:30,700 --> 00:07:33,700
也可能会自己造一个伪量化的算子

168
00:07:33,700 --> 00:07:35,700
或者把伪量化算子呢改掉

169
00:07:35,700 --> 00:07:36,700
那这个呢是最原始

170
00:07:36,700 --> 00:07:38,700
或者最naive的一种方式

171
00:07:39,700 --> 00:07:40,700
好了

172
00:07:40,700 --> 00:07:43,700
我们刚才只是简单的去给大家讲了

173
00:07:43,700 --> 00:07:44,700
感知量化训练呢

174
00:07:44,700 --> 00:07:46,700
一般的通用性的算法

175
00:07:46,700 --> 00:07:49,700
还讲了伪量化算子是怎幺实现的

176
00:07:49,700 --> 00:07:50,700
包括正向反向

177
00:07:50,700 --> 00:07:51,700
另外还有伪量化算子呢

178
00:07:51,700 --> 00:07:53,700
是怎幺插入到我们的计算图里面

179
00:07:53,700 --> 00:07:56,700
接下来我想提出几个问题

180
00:07:56,700 --> 00:07:58,700
让大家一起去思考思考

181
00:07:58,700 --> 00:07:59,700
第一个点呢

182
00:07:59,700 --> 00:08:03,700
就是优和平滑的计算伪量化阶段的两个值

183
00:08:03,700 --> 00:08:04,700
mean和max

184
00:08:04,700 --> 00:08:06,700
我们刚才提到batch normalization

185
00:08:06,700 --> 00:08:07,700
这种算子

186
00:08:07,700 --> 00:08:09,700
它其实也有很多参数

187
00:08:09,700 --> 00:08:11,700
需要根据我们的数的数据

188
00:08:11,700 --> 00:08:13,700
还有输出的数据进行学习的

189
00:08:13,700 --> 00:08:14,700
像编程呢

190
00:08:14,700 --> 00:08:17,700
一般都会有一个平滑计算的过程

191
00:08:17,700 --> 00:08:19,700
在具体算子或者kernel实现的时候呢

192
00:08:19,700 --> 00:08:20,700
就会有一个moving mean

193
00:08:20,700 --> 00:08:21,700
moving variance

194
00:08:21,700 --> 00:08:23,700
去进行一个平滑

195
00:08:23,700 --> 00:08:25,700
那另外一点就是

196
00:08:25,700 --> 00:08:27,700
我去看一些论文呢

197
00:08:27,700 --> 00:08:31,700
基本上我们都会对bn进行一个矫正

198
00:08:31,700 --> 00:08:34,700
而且还会有一个贝塞尔矫正

199
00:08:34,700 --> 00:08:37,700
为什幺我们需要做这些矫正呢

200
00:08:37,700 --> 00:08:38,700
那这一点呢

201
00:08:38,700 --> 00:08:41,700
其实是跟我们的样本有关系的

202
00:08:41,700 --> 00:08:43,700
我也非常欢迎大家去看一下

203
00:08:43,700 --> 00:08:46,700
或者去搜索一下相关的原理

204
00:08:46,700 --> 00:08:47,700
最后一点呢

205
00:08:47,700 --> 00:08:49,700
如果我们要对bn进行指点

206
00:08:49,700 --> 00:08:51,700
那计算公式或者kernel

207
00:08:51,700 --> 00:08:53,700
会不会有新的变化呢

208
00:08:53,700 --> 00:08:55,700
在我们的感知量化训练的过程当中

209
00:08:55,700 --> 00:08:57,700
确实是有的

210
00:08:57,700 --> 00:08:58,700
在真正计算的时候呢

211
00:08:58,700 --> 00:09:02,700
我们确实需要对它进行一个特殊的插入

212
00:09:02,700 --> 00:09:03,700
可以看到左边呢

213
00:09:03,700 --> 00:09:05,700
这个就是在没有融合之前

214
00:09:05,700 --> 00:09:06,700
右边这个呢

215
00:09:06,700 --> 00:09:07,700
就是在融合之后的

216
00:09:07,700 --> 00:09:09,700
算子融合的过程当中呢

217
00:09:09,700 --> 00:09:11,700
有同学去问我

218
00:09:11,700 --> 00:09:12,700
上面这条公式呢

219
00:09:12,700 --> 00:09:13,700
变成下面这条公式

220
00:09:13,700 --> 00:09:15,700
具体有什幺不一样呢

221
00:09:15,700 --> 00:09:18,700
很明显大家看我们的计算图就知道了

222
00:09:18,700 --> 00:09:21,700
我们整体的计算公式或者整体的计算形态啊

223
00:09:21,700 --> 00:09:23,700
确实改变了很多

224
00:09:23,700 --> 00:09:25,700
回到我们AI系统

225
00:09:25,700 --> 00:09:27,700
AI框架里面去看一下

226
00:09:27,700 --> 00:09:29,700
QAT的一个工作流程

227
00:09:29,700 --> 00:09:33,700
我们首先会有很多预训练的模型

228
00:09:33,700 --> 00:09:34,700
或者已经训练好的模型

229
00:09:34,700 --> 00:09:36,700
获取这个计算图之后

230
00:09:36,700 --> 00:09:38,700
我们需要对我们的网络模型呢

231
00:09:38,700 --> 00:09:39,700
进行一个改造

232
00:09:39,700 --> 00:09:42,700
去插入一些刚才讲到的

233
00:09:42,700 --> 00:09:43,700
伪量化的算子

234
00:09:43,700 --> 00:09:44,700
或者这里面呢

235
00:09:44,700 --> 00:09:45,700
叫做观察算子

236
00:09:45,700 --> 00:09:48,700
那我们统一称为伪量化算子就好了

237
00:09:48,700 --> 00:09:51,700
另外我们还要准备一些训练的数据

238
00:09:51,700 --> 00:09:52,700
那这个时候呢

239
00:09:52,700 --> 00:09:55,700
这里面输进去的就是已经经过改造的

240
00:09:55,700 --> 00:09:58,700
插入了伪量化算子之后的一个计算图

241
00:09:58,700 --> 00:09:59,700
另外一边呢

242
00:09:59,700 --> 00:10:01,700
我们需要Fine Tuning或者Training的数据

243
00:10:01,700 --> 00:10:02,700
接着呢

244
00:10:02,700 --> 00:10:04,700
真正的去执行Training和Fine Tuning

245
00:10:04,700 --> 00:10:05,700
最后呢

246
00:10:05,700 --> 00:10:06,700
在学习的过程当中

247
00:10:06,700 --> 00:10:08,700
我们就会不断的去学习

248
00:10:08,700 --> 00:10:10,700
因为量化所造成的一些误差

249
00:10:10,700 --> 00:10:11,700
把这些误差呢

250
00:10:11,700 --> 00:10:12,700
通过学习的方式

251
00:10:12,700 --> 00:10:14,700
把它变得更小更好

252
00:10:14,700 --> 00:10:15,700
最后呢

253
00:10:15,700 --> 00:10:17,700
就输出了我们QAT的网络模型

254
00:10:17,700 --> 00:10:19,700
那这个QAT的网络模型

255
00:10:19,700 --> 00:10:22,700
还真没有办法马上去执行的哦

256
00:10:22,700 --> 00:10:24,700
它要经过推理系统的一个转换模块

257
00:10:24,700 --> 00:10:27,700
然后去掉一些勇于的伪量化的算子

258
00:10:27,700 --> 00:10:29,700
才能够正常的推理

259
00:10:29,700 --> 00:10:30,700
那这一部分呢

260
00:10:30,700 --> 00:10:33,700
我们会在下一个内容里面去给大家介绍

261
00:10:34,700 --> 00:10:37,700
接下来我们来聊一聊QAT的衍生研究

262
00:10:37,700 --> 00:10:39,700
那QAT的衍生研究非常多啊

263
00:10:39,700 --> 00:10:40,700
而且这方面呢

264
00:10:40,700 --> 00:10:42,700
确实有很多新的算法

265
00:10:42,700 --> 00:10:43,700
或者新的idea提出

266
00:10:43,700 --> 00:10:45,700
例如好像这篇文章

267
00:10:45,700 --> 00:10:47,700
例如好像这篇文章

268
00:10:47,700 --> 00:10:49,700
中文名我也翻译不出来

269
00:10:49,700 --> 00:10:50,700
它这里面呢

270
00:10:50,700 --> 00:10:52,700
就做了一个新的伪量化的算子

271
00:10:52,700 --> 00:10:53,700
那正向呢

272
00:10:53,700 --> 00:10:55,700
可能跟我们刚才讲的差不多

273
00:10:55,700 --> 00:10:56,700
但是反向呢

274
00:10:56,700 --> 00:10:57,700
它就不一样了

275
00:10:57,700 --> 00:10:59,700
它不是一个简单的节段

276
00:10:59,700 --> 00:11:00,700
那这种方式呢

277
00:11:00,700 --> 00:11:02,700
就是对伪量化算子进行一个创新的

278
00:11:03,700 --> 00:11:04,700
另外呢

279
00:11:04,700 --> 00:11:06,700
有一些科研类的创新的文章呢

280
00:11:06,700 --> 00:11:08,700
会对我们的计算图啊

281
00:11:08,700 --> 00:11:10,700
或者对我们的量化的流程

282
00:11:10,700 --> 00:11:12,700
进行一个改进

283
00:11:12,700 --> 00:11:13,700
那最后呢

284
00:11:13,700 --> 00:11:14,700
我们再看一下

285
00:11:14,700 --> 00:11:16,700
其实我们在做量化的时候呢

286
00:11:16,700 --> 00:11:17,700
刚才只是很笼统的

287
00:11:17,700 --> 00:11:19,700
说我们的伪量化算子呢

288
00:11:19,700 --> 00:11:21,700
只是对我们的数的数据呢

289
00:11:21,700 --> 00:11:23,700
模拟我们的伪量化的过程当中

290
00:11:23,700 --> 00:11:25,700
但是我们的数据的形态是很多的

291
00:11:25,700 --> 00:11:27,700
有NCHW

292
00:11:27,700 --> 00:11:28,700
那这个时候呢

293
00:11:28,700 --> 00:11:29,700
我们可以对Patreon进行伪量化

294
00:11:29,700 --> 00:11:30,700
或者Patreon进行量化

295
00:11:30,700 --> 00:11:33,700
我们也可以对Potential进行量化

296
00:11:33,700 --> 00:11:35,700
所以量化的方式和种类

297
00:11:35,700 --> 00:11:36,700
还有不同层次

298
00:11:36,700 --> 00:11:37,700
有非常的多

299
00:11:37,700 --> 00:11:38,700
这里面呢

300
00:11:38,700 --> 00:11:39,700
周米更欢迎大家去看看

301
00:11:39,700 --> 00:11:41,700
相关更多的论文

302
00:11:41,700 --> 00:11:43,700
和最新的量化的研究

303
00:11:45,700 --> 00:11:47,700
最后我非常欢迎大家

304
00:11:47,700 --> 00:11:48,700
去阅读一下

305
00:11:48,700 --> 00:11:50,700
我这里面提到过的一些论文

306
00:11:50,700 --> 00:11:51,700
这里面这些论文呢

307
00:11:51,700 --> 00:11:53,700
也是比较新的

308
00:11:53,700 --> 00:11:55,700
一些关于量化相关的一些论文

309
00:11:55,700 --> 00:11:56,700
好了

310
00:11:56,700 --> 00:11:57,700
今天的内容呢

311
00:11:57,700 --> 00:11:58,700
就到这里为止

312
00:11:58,700 --> 00:11:59,700
谢谢各位

313
00:11:59,700 --> 00:12:00,700
卷的不行了

314
00:12:00,700 --> 00:12:01,700
卷的不行了

315
00:12:01,700 --> 00:12:02,700
记得一键三连加关注哦

316
00:12:02,700 --> 00:12:04,700
所有的内容都会开源在

317
00:12:04,700 --> 00:12:06,700
下面这条链接里面

318
00:12:06,700 --> 00:12:07,700
拜了个拜

