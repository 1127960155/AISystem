0
0:00:00.000 --> 0:00:05.280


1
0:00:05.360 --> 0:00:07.400
哈喽大家好我是宗密

2
0:00:07.600 --> 0:00:13.440
那我们现在的位置处于推理引擎这个系列里面的模型压缩

3
0:00:13.760 --> 0:00:18.360
今天我要给大家去带来一个新的内容应该不算新的内容了

4
0:00:18.720 --> 0:00:25.440
我们在量化里面呢在进一步的去展开杆子量化训练也就是大家经常说到的QAT

5
0:00:25.440 --> 0:00:31.540
现在呢我们来到了第一笔特量化这个系列课程里面的第二个内容

6
0:00:31.540 --> 0:00:34.540
感知量化QAT

7
0:00:34.540 --> 0:00:39.040
下面呢我们再来看一下在整个推定型架构里面

8
0:00:39.040 --> 0:00:42.040
感知量化呀其实处于这个阶段

9
0:00:42.040 --> 0:00:46.540
哦讲错了量化呢是处于这个阶段

10
0:00:46.540 --> 0:00:49.540
但是感知量化呢是跟训练墙相关的

11
0:00:49.540 --> 0:00:52.040
所以这个内容呢是我们看不到的

12
0:00:52.040 --> 0:00:54.640
在AI框架上面的一个特性

13
0:00:54.640 --> 0:00:57.640
例如派套取也有自己的感知量化训练的一些特性

14
0:00:57.640 --> 0:01:01.640
还有脉搏也会推出自己感知量化训练的特性

15
0:01:01.640 --> 0:01:07.140
接下来我们来一起看一下感知量化训练它到底是怎么实现的

16
0:01:07.140 --> 0:01:11.640
简单的一句话就是我们会在一个正常的网络模型当中呢

17
0:01:11.640 --> 0:01:15.140
去插入一些伪量化的算子或者节点

18
0:01:15.140 --> 0:01:17.640
那这个节点呢我们叫做fake quote

19
0:01:17.640 --> 0:01:20.640
因为它不是真正的量化而是用来模拟

20
0:01:20.740 --> 0:01:23.740
量化的时候引入的一些误差

21
0:01:23.740 --> 0:01:25.740
而在真正端出推理的时候呢

22
0:01:25.740 --> 0:01:29.240
我们需要把这些fake quote呢去进行一个直迭

23
0:01:29.240 --> 0:01:33.240
那为啥叫直迭呢是因为它确实只剩下一些藏量

24
0:01:33.240 --> 0:01:36.240
我们需要对这些藏量呢进行直迭

25
0:01:36.240 --> 0:01:39.740
然后把相关的属性或者信息呢变成我们探测的信息

26
0:01:39.740 --> 0:01:41.740
最后再进行一个推理

27
0:01:41.740 --> 0:01:44.240
让我们看看下面这个图哦

28
0:01:44.240 --> 0:01:47.240
而下面的这个是我们的一个计算图

29
0:01:47.240 --> 0:01:48.740
输进去的是一些数据

30
0:01:48.740 --> 0:01:52.340
我们需要在输进去的数据的时候呢插入一个伪量化的算子

31
0:01:52.340 --> 0:01:56.840
接着呢我们可能还需要对我们的权重啊插入一个伪量化的算子

32
0:01:56.840 --> 0:02:01.040
完成卷集计算之后呢就会给一个边层进行一个学习

33
0:02:01.040 --> 0:02:03.840
学习完之后就真正地去进入了一个围录了

34
0:02:03.840 --> 0:02:08.340
那在出去围录之前呢我们也会插一个伪量化的节点在这里面

35
0:02:08.340 --> 0:02:11.840
那像这种呢确实在我们的一个正常的计算图里面

36
0:02:11.840 --> 0:02:13.840
去插入各种伪量化的节点

37
0:02:13.840 --> 0:02:16.840
那这种方式我们叫做qat

38
0:02:17.440 --> 0:02:22.440
接下来我们去看看刚才大量地去提到一些伪量化的节点qat

39
0:02:22.440 --> 0:02:24.940
那qat的节点有什么用吗

40
0:02:24.940 --> 0:02:26.940
下面呢就有两个比较大的作用了

41
0:02:26.940 --> 0:02:29.940
这也是伪量化节点的一个具体的作用

42
0:02:29.940 --> 0:02:33.940
首先呢这个伪量化节点主要是找到输入数据的分布

43
0:02:33.940 --> 0:02:37.940
也就是找到我们数据的一个最大值和最小值

44
0:02:37.940 --> 0:02:41.940
第二个点呢就是我们刚才简单地提到过的

45
0:02:41.940 --> 0:02:44.940
它去模拟我们量化操作就是把我们的一些fp三手

46
0:02:45.040 --> 0:02:50.040
量化到我们的int八这种低比特的时候的一些精度的损失

47
0:02:50.040 --> 0:02:54.040
把这些损失呢在网络模型训练或者发球点的时候呢

48
0:02:54.040 --> 0:02:59.040
作用到整个网络模型当中传递给损失函数就我们的loss

49
0:02:59.040 --> 0:03:02.040
让优化器呢在训练或者发球点的过程当中呢

50
0:03:02.040 --> 0:03:08.040
对我们因为量化所造成的损失进行一个优化和学习

51
0:03:08.040 --> 0:03:12.040
那至于第二个就是我们真正的一个伪量化节点

52
0:03:12.140 --> 0:03:14.140
伪量化算子它的核心作用了

53
0:03:14.140 --> 0:03:21.140
我们下面去看看伪量化节点的一个正向传播具体是怎么算的哦

54
0:03:21.140 --> 0:03:26.140
其实为了求出我们网络模型的输入就是我们的tensor

55
0:03:26.140 --> 0:03:29.140
一个比较精确的min和max呢

56
0:03:29.140 --> 0:03:33.140
所以我们会在一个网络模型训练的时候呢插入伪量化节点

57
0:03:33.140 --> 0:03:36.140
也就是我们需要获取那个计算图

58
0:03:36.140 --> 0:03:40.140
然后对这个计算图呢进行改造插入我们希望的节点

59
0:03:40.240 --> 0:03:44.240
然后模拟误差得到数据的分布

60
0:03:44.240 --> 0:03:48.240
而对每一个算子呢我们都会去求输入的数据x

61
0:03:48.240 --> 0:03:50.240
它的一个最小值还有最大值

62
0:03:50.240 --> 0:03:53.240
还记得我们在上一节课里面去讲量化原理的时候

63
0:03:53.240 --> 0:03:55.240
有了最小值最大值之后呢

64
0:03:55.240 --> 0:03:58.240
我们就可以去求我们量化的scale

65
0:03:58.240 --> 0:04:03.240
通过这个scale呢我们就可以把我们的输出去直接量化成我们的int八

66
0:04:03.240 --> 0:04:08.240
那正向的forward的时候呢我们就会做这个工作除了记录最大值和最小值

67
0:04:08.240 --> 0:04:11.340
它还要做一个量化模拟的操作

68
0:04:11.340 --> 0:04:15.340
假设我们之前的数据呢是一个平滑的数据类似一条线性的

69
0:04:15.340 --> 0:04:20.340
经过伪量化算子进行模拟的时候呢就变成了有阶梯形状

70
0:04:20.340 --> 0:04:22.340
把大部分的数据呢都直接消掉了

71
0:04:22.340 --> 0:04:25.340
从二批三十二的数据呢变成int八的数据

72
0:04:25.340 --> 0:04:28.340
那这个呢就是伪量化算子的正向传播

73
0:04:28.340 --> 0:04:31.340
有正向是不是应该有反向啊

74
0:04:31.340 --> 0:04:36.340
那我们现在来看看反向传播这个伪量化算子具体怎么实现

75
0:04:36.440 --> 0:04:40.440
按照刚才正向传播的公式如果我们反向的时候呢

76
0:04:40.440 --> 0:04:45.440
对刚才正向的那条公式求导数肯定会导致我们的权重为零

77
0:04:45.440 --> 0:04:47.440
权重为零就没有办法去学习了

78
0:04:47.440 --> 0:04:51.440
于是呢反向的时候呢我们相当于一个直通的联通器

79
0:04:51.440 --> 0:04:53.440
把delta一呢直接给delta二

80
0:04:53.440 --> 0:04:59.440
但是有点治愈的就是我们的输的数据x呢必须要在我们的量化范围之内

81
0:04:59.440 --> 0:05:01.440
如果不在的我们把它截断

82
0:05:01.440 --> 0:05:05.440
于是呢最终反向传播的非矿的呢

83
0:05:05.540 --> 0:05:09.540
一般来说我们都会对它的数据进行阶段式的处理

84
0:05:11.540 --> 0:05:16.540
了解完伪量化算子呢我们现在的伪量化算子还有一个很重要的工作

85
0:05:16.540 --> 0:05:18.540
就是更新min和max

86
0:05:18.540 --> 0:05:25.540
因为每一次训练每一轮迭代每一个apple每个step呢它都会有不同的min和max

87
0:05:25.540 --> 0:05:27.540
它都有不同的数据的输入

88
0:05:27.540 --> 0:05:33.540
有点类似于我们的边算子或者layer log算子呢去更新beta和gamma的这种方式

89
0:05:33.640 --> 0:05:37.640
通过一个分类min分类max还有无分min无分max进行计算

90
0:05:37.640 --> 0:05:44.640
如果大家看不懂没关系去看看边这个算子呢怎么去更新beta和gamma的

91
0:05:44.640 --> 0:05:49.640
如果大家确实很有兴趣去了解伪量化算子真正的一些代码实现呢

92
0:05:49.640 --> 0:05:51.640
也可以去看一下派套许的具体实现

93
0:05:51.640 --> 0:05:56.640
那下面呢我们去下面呢我们让观众提两个问题

94
0:05:57.640 --> 0:06:02.640
说明老师你好哦我想问一下在什么地方或者什么位置

95
0:06:02.740 --> 0:06:06.740
去插入fakecoin伪量化这个节点呢

96
0:06:08.740 --> 0:06:11.740
哎小心你这个问题确实是灵魂拷问的

97
0:06:11.740 --> 0:06:14.740
我们刚才只是简单的去给大家讲了

98
0:06:14.740 --> 0:06:17.740
fakecoin这个伪量化的算子是怎么去实现的

99
0:06:17.740 --> 0:06:23.740
正向怎么把它进行一个伪量化的学习反向了怎么对它进行一个阶段

100
0:06:23.740 --> 0:06:26.740
那一般来说呢我们会在一些密集的计算的算子

101
0:06:26.740 --> 0:06:31.740
说到密集计算的算子呢其实并不多有点类似于华为生态里面的一个cube

102
0:06:31.840 --> 0:06:37.840
这些算子举个简单的几个例子就是gmm矩阵的乡城还有卷机

103
0:06:37.840 --> 0:06:43.840
那这些呢就是密集计算的算子另外我们还会激活算子之后或者之前进行插入的

104
0:06:43.840 --> 0:06:49.840
最后呢我们还会在网络模型输入输出的地方进行操用伪量化算子

105
0:06:49.840 --> 0:06:51.840
下面呢我们看一个更加具体的例子

106
0:06:51.840 --> 0:06:54.840
那虽然看上去左边的这个图呢很复杂

107
0:06:54.840 --> 0:07:00.840
但实际上啊这个是我们的一个卷机比恩比露三个简单的算子的一个计算

108
0:07:00.940 --> 0:07:08.940
图因为边城呢它有很多的不同的参数它要更新伽玛了要更新贝塔了所以你看上去很复杂

109
0:07:08.940 --> 0:07:17.940
我们操用伪量化算子呢就会在这个图的input子里面插入在我们的位置里面插入可能在对我们的W进行输出的时候插入一个

110
0:07:17.940 --> 0:07:23.940
另外的话我们还会对我们的激活后面插入一个这个呢就是一般的插入方式

111
0:07:23.940 --> 0:07:29.940
值得注意的就是如果你研究杆子量化算法呢你可能会提出很多不同的插入的方式去学习

112
0:07:29.940 --> 0:07:40.040
也可能会自己造一个伪量化的算子或者把伪量化算子呢改掉那这个呢是最原始或者最naive的一种方式

113
0:07:40.040 --> 0:07:54.040
好了我们刚才只是简单的去给大家讲了杆子量化训练呢一般的通用性的算法还讲了伪量化算子是怎么实现的包括正向反向另外还有伪量化算子呢是怎么操作到我们的计算图里面

114
0:07:54.040 --> 0:07:59.040
接下来我想提出几个问题让大家一起去思考思考

115
0:07:59.040 --> 0:08:24.140
第一个点呢就是如何平滑的计算伪量化阶段的两个值命和maxed我们刚才提到batch normalization这种算子它其实也有很多参数需要根据我们的输入的数据还有输出的数据进行学习的像编成呢一般都会有一个平滑计算的过程在具体算子或者科诺实现的时候呢就会有一个moving meanmoving variance去进行一个平滑

116
0:08:24.140 --> 0:08:24.840
那另外一点就是我去看一些论文啊基本上我们都会对比恩进行一个矫正而且还会有一个贝萨尔矫正为什么我们需要做这些矫正呢那这一点呢其实是跟我们的样本有关系的我也非常欢迎大家去看一下或者去搜索一下相关的原理最后一点呢如果我们要对比恩进行直迭那计算公式或者科诺会不会有新的变化呢在我们

117
0:08:54.140 --> 0:09:24.140
的杆子量化训练的过程当中确实是有的在真正计算的时候呢我们确实需要对它进行一个特殊的插入可以看到左边的这个就是在没有融合之前右边这个呢就是在融合之后的算子融合的过程当中呢有同学去问我上面这套公司呢变成下面这套公司具体有什么不一样呢呃很明显大家看我们的计算图就知道了我们整体的计算公式或者整体的计算形态呀确实改变了很多

118
0:09:24.140 --> 0:09:54.140
回到我们AI系统AI框架里面去看一下QAT的一个工作流程我们首先会有很多预训练的模型或者已经训练好的模型获取这个计算图之后我们需要对我们的网络模型呢进行一个改造去插入一些刚才讲到的伪量化的算子或者这里面呢叫做观察算子那我们统一称为伪量化算子就好了另外我们还要准备一些训练的数据那这个时候呢这里面输进去的

119
0:09:54.140 --> 0:10:24.140
就是已经经过改造的插入了伪量化算子之后的一个计算图另外一边呢我们需要fine tuning或者training的数据接着呢真正的去执行training和fine tuning最后呢在学习的过程当中我们就会不断地去学习因为量化所造成的一些误差把这些误差呢通过学习的方式把它变得更小更好最后呢就输出了我们QAT的网络模型那这个QAT的网络模型还真没有办法马上去执行的哦

120
0:10:24.140 --> 0:10:54.140
系统的一个转换模块然后去掉一些勇于的伪量化的算子才能够正常地推理那这一部分呢我们会在下一个内容里面去给大家介绍接下来我们来聊一聊QAT的衍生研究那QAT的衍生研究非常多而且这方面呢确实有很多新的算法或者新的idea提出例如好像这篇文章例如好像这篇文章啊中文名我也翻译不出来它这里面呢就做了一个新的伪量化的算子

121
0:10:54.140 --> 0:11:24.140
跟我们刚才讲的差不多但是反向了它就不一样了它不是一个简单的阶段那这种方式呢就是对伪量化算子进行一个创新的另外呢有一些科研类的创新的文章呢会对我们的计算图啊或者对我们的量化的流程进行一个改进那最后呢我们再看一下其实我们在做量化的时候呢刚才只是很笼统地说我们的伪量化算子呢只是对我们的数的数据呢模拟我们的伪量化的过程当中

122
0:11:24.140 --> 0:11:54.140
但是我们的数据的形态是很多的有NCHW那这个时候呢我们可以对per channel进行伪量化或者per channel进行量化我们也可以对per tensor进行量化所以量化的方式和种类还有不同层次有非常的多这里面呢综明更欢迎大家去看看相关更多的论文和最新的量化的研究最后我非常欢迎大家去阅读一下我这里面提到过的一些论文这里面这些论文呢也是比较新的一些关于

123
0:11:54.140 --> 0:12:07.640
量化相关的一些论文好了今天的内容呢就到这里为止

