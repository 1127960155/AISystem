0
0:00:00.000 --> 0:00:07.500
Hello 大家好,我是曾铭

1
0:00:07.500 --> 0:00:13.500
我们现在的位置处于推理引擎这个系列里面的模型压缩

2
0:00:13.500 --> 0:00:16.700
今天我要给大家去带来一个新的内容

3
0:00:16.700 --> 0:00:18.500
应该不算新的内容了

4
0:00:18.500 --> 0:00:22.500
我们在量化里面再进一步的去展开杆子量化训练

5
0:00:22.500 --> 0:00:25.700
也就是大家经常说到的QAT

6
0:00:26.200 --> 0:00:31.700
现在我们来到了第一笔特量化这个系列课程里面的第二个内容

7
0:00:31.700 --> 0:00:34.700
杆子量化QAT

8
0:00:34.700 --> 0:00:39.200
下面我们再来看一下在整个推理引擎架构里面

9
0:00:39.200 --> 0:00:42.200
杆子量化其实处于这个阶段

10
0:00:42.200 --> 0:00:46.200
讲错了,量化是处于这个阶段

11
0:00:46.200 --> 0:00:49.200
但是杆子量化是跟训练墙相关的

12
0:00:49.200 --> 0:00:51.700
所以这个内容是我们看不到的

13
0:00:52.200 --> 0:00:54.700
在AI框架上面的一个特性

14
0:00:54.700 --> 0:00:57.700
例如PyTorch也有自己的杆子量化训练的一些特性

15
0:00:57.700 --> 0:01:01.700
还有Maxwell也会推出自己杆子量化训练的特性

16
0:01:01.700 --> 0:01:05.200
接下来我们来一起看一下杆子量化训练

17
0:01:05.200 --> 0:01:07.200
它到底是怎幺实现的

18
0:01:07.200 --> 0:01:11.700
简单的一句话就是我们会在一个正常的网络模型栏中

19
0:01:11.700 --> 0:01:15.200
去插入一些伪量化的算子或者节点

20
0:01:15.200 --> 0:01:17.700
这个节点我们叫做FakeQuant

21
0:01:17.700 --> 0:01:19.200
因为它不是真正的量化

22
0:01:19.200 --> 0:01:23.700
而是用来模拟量化的时候引入的一些误差

23
0:01:23.700 --> 0:01:25.700
而在真正端次推理的时候

24
0:01:25.700 --> 0:01:29.200
我们需要把这些FakeQuant去进行一个直叠

25
0:01:29.200 --> 0:01:30.700
那为啥叫直叠呢?

26
0:01:30.700 --> 0:01:33.200
是因为它确实只剩下一些常量

27
0:01:33.200 --> 0:01:35.700
我们需要对这些常量进行直叠

28
0:01:35.700 --> 0:01:39.700
然后把相关的属性或者信息变成我们Tensor的信息

29
0:01:39.700 --> 0:01:41.700
最后再进行一个推理

30
0:01:41.700 --> 0:01:44.200
那我们看看下面这个图

31
0:01:44.200 --> 0:01:46.700
下面这个是我们的一个计算图

32
0:01:46.700 --> 0:01:48.700
输进去的是一些数据

33
0:01:48.700 --> 0:01:52.200
我们需要在输进去的数据的时候插入一个伪量化的算子

34
0:01:52.200 --> 0:01:56.700
接着我们可能还需要对我们的权重插入一个伪量化的算子

35
0:01:56.700 --> 0:02:01.200
完成卷集计算之后就会给一个编程进行一个学习

36
0:02:01.200 --> 0:02:04.200
学习完之后就真正的去进入了一个韵录了

37
0:02:04.200 --> 0:02:08.200
在出去韵录之前我们也会插一个伪量化的节点在这里面

38
0:02:08.200 --> 0:02:11.700
像这种确实在我们的一个正常的计算图里面

39
0:02:11.700 --> 0:02:13.700
去插入各种伪量化的节点

40
0:02:13.700 --> 0:02:16.700
这种方式我们叫做QAT

41
0:02:17.200 --> 0:02:22.200
接下来我们去看看刚才大量的去提到一些伪量化的节点FadeCode

42
0:02:22.200 --> 0:02:25.200
那FadeCode的节点有什幺用吗?

43
0:02:25.200 --> 0:02:27.200
下面就有两个比较大的作用了

44
0:02:27.200 --> 0:02:29.700
这也是伪量化节点的一个具体的作用

45
0:02:29.700 --> 0:02:33.700
首先这个伪量化节点主要是找到输入数据的分布

46
0:02:33.700 --> 0:02:38.200
也就是找到我们数据的一个最大值和最小值

47
0:02:38.200 --> 0:02:41.700
第二个点就是我们刚才简单的提到过的

48
0:02:41.700 --> 0:02:43.200
它去模拟我们量化操作

49
0:02:43.200 --> 0:02:50.200
就是把我们的一些FPSensor量化到我们的int8这种低比特的时候的一些精度的损失

50
0:02:50.200 --> 0:02:53.700
把这些损失在网络模型训练或者FileTuning的时候

51
0:02:53.700 --> 0:02:57.700
作用到整个网络模型当中传递给损失函数

52
0:02:57.700 --> 0:02:58.700
就是我们的Loss

53
0:02:58.700 --> 0:03:01.700
让优化器在训练或者FileTuning的过程当中

54
0:03:01.700 --> 0:03:07.700
对我们因为量化所造成的损失进行一个优化和学习

55
0:03:07.700 --> 0:03:14.700
那至于第二个就是我们真正的一个伪量化节点或者伪量化算子它的内核作用了

56
0:03:14.700 --> 0:03:21.700
我们下面去看看伪量化节点的一个正向传播具体是怎幺算的

57
0:03:21.700 --> 0:03:25.700
其实为了求出我们网络模型的输入

58
0:03:25.700 --> 0:03:28.700
就是我们的Tensor一个比较精确的min和max

59
0:03:28.700 --> 0:03:33.700
所以我们会在一个网络模型训练的时候插入伪量化节点

60
0:03:33.700 --> 0:03:36.200
也就是我们需要获取那个计算图

61
0:03:36.200 --> 0:03:40.200
然后对这个计算图进行改造插入我们希望的节点

62
0:03:40.200 --> 0:03:44.200
然后模拟误差得到数据的分布

63
0:03:44.200 --> 0:03:48.200
而对每一个算子我们都会去求输入的数据x

64
0:03:48.200 --> 0:03:50.200
它的一个最小值还有最大值

65
0:03:50.200 --> 0:03:53.200
还记得我们在上一节课里面去讲量化原理的时候

66
0:03:53.200 --> 0:03:55.200
有了最小值和最大值之后

67
0:03:55.200 --> 0:03:58.200
我们就可以去求我们量化的scale

68
0:03:58.200 --> 0:04:03.200
通过这个scale我们就可以把我们的输入数据直接量化成我们的int8

69
0:04:03.200 --> 0:04:05.700
那正向的forward的时候我们就会做这个工作

70
0:04:05.700 --> 0:04:07.700
除了记录最大值和最小值

71
0:04:07.700 --> 0:04:11.700
它还要做一个量化模拟的操作

72
0:04:11.700 --> 0:04:13.700
假设我们之前的数据是一个平滑的数据

73
0:04:13.700 --> 0:04:15.700
类似一条线性的

74
0:04:15.700 --> 0:04:19.700
经过伪量化算子进行模拟的时候就变成了有阶梯形状

75
0:04:19.700 --> 0:04:22.700
把大部分的数据都直接消掉了

76
0:04:22.700 --> 0:04:25.700
从episodic数据变成int8的数据

77
0:04:25.700 --> 0:04:28.700
那这个就是伪量化算子的正向传播

78
0:04:28.700 --> 0:04:31.700
有正向是不是应该有反向啊

79
0:04:31.700 --> 0:04:34.700
那我们现在来看看反向传播

80
0:04:34.700 --> 0:04:36.700
这个伪量化算子具体怎幺实现

81
0:04:36.700 --> 0:04:38.700
按照刚才正向传播的公式

82
0:04:38.700 --> 0:04:40.700
如果我们反向的时候呢

83
0:04:40.700 --> 0:04:42.700
对刚才正向的那条公式求导数

84
0:04:42.700 --> 0:04:44.700
肯定会导致我们的权重为0

85
0:04:44.700 --> 0:04:47.700
权重为0就没有办法去学习了

86
0:04:47.700 --> 0:04:48.700
于是呢反向的时候呢

87
0:04:48.700 --> 0:04:51.700
我们相当于一个直通的连通器

88
0:04:51.700 --> 0:04:53.700
把delta in直接给delta out

89
0:04:53.700 --> 0:04:55.700
但是有点治愈的就是

90
0:04:55.700 --> 0:04:57.700
我们的输入的数据x呢

91
0:04:57.700 --> 0:04:59.700
必须要在我们的量化范围之内

92
0:04:59.700 --> 0:05:01.700
如果不在的我们把它截断

93
0:05:01.700 --> 0:05:04.700
于是呢最终反向传播的fakecoin呢

94
0:05:04.700 --> 0:05:06.700
一般来说我们都会对它的数据

95
0:05:06.700 --> 0:05:09.700
进行截断式的处理

96
0:05:10.700 --> 0:05:12.700
了解完伪量化算子呢

97
0:05:12.700 --> 0:05:14.700
我们现在呢伪量化算子

98
0:05:14.700 --> 0:05:15.700
还有一个很重要的工作

99
0:05:15.700 --> 0:05:17.700
就是更新min和max

100
0:05:17.700 --> 0:05:20.700
因为每一次训练每一轮迭代

101
0:05:20.700 --> 0:05:22.700
每一个apple每个step呢

102
0:05:22.700 --> 0:05:24.700
它都会有不同的min和max

103
0:05:24.700 --> 0:05:26.700
它都有不同的数据的输入

104
0:05:26.700 --> 0:05:28.700
有点类似于我们的边算子

105
0:05:28.700 --> 0:05:30.700
或者LayerLoop算子呢

106
0:05:30.700 --> 0:05:33.700
去更新beta和gamma的这种方式

107
0:05:33.700 --> 0:05:35.700
通过一个winlimin winlimax

108
0:05:35.700 --> 0:05:37.700
还有movinmin movinmax进行计算

109
0:05:37.700 --> 0:05:40.700
如果大家看不懂没关系

110
0:05:40.700 --> 0:05:41.700
去看看边这个算子呢

111
0:05:41.700 --> 0:05:43.700
怎幺去更新beta和gamma的

112
0:05:43.700 --> 0:05:45.700
如果大家确实很有兴趣

113
0:05:45.700 --> 0:05:46.700
去了解伪量化算子

114
0:05:46.700 --> 0:05:48.700
真正的一些代码实现呢

115
0:05:48.700 --> 0:05:51.700
也可以去看一下Pytorch的具体实现

116
0:05:51.700 --> 0:05:53.700
那下面呢我们去

117
0:05:53.700 --> 0:05:56.700
下面呢我们让观众提两个问题

118
0:05:57.700 --> 0:05:58.700
作弊老师你好啊

119
0:05:58.700 --> 0:05:59.700
我想问一下

120
0:05:59.700 --> 0:06:02.700
在什幺地方或者什幺位置

121
0:06:02.700 --> 0:06:06.700
去插入FakeQuant伪量化这个节点呢

122
0:06:08.700 --> 0:06:09.700
诶

123
0:06:09.700 --> 0:06:11.700
小心你这个问题确实是灵魂拷问的

124
0:06:11.700 --> 0:06:14.700
我们刚才只是简单的去给大家讲了

125
0:06:14.700 --> 0:06:16.700
FakeQuant这个伪量化的算子

126
0:06:16.700 --> 0:06:17.700
是怎幺去实现的

127
0:06:17.700 --> 0:06:20.700
正向怎幺把它进行一个伪量化的学习

128
0:06:20.700 --> 0:06:23.700
反向呢怎幺对它进行一个截断

129
0:06:23.700 --> 0:06:24.700
那一般来说呢

130
0:06:24.700 --> 0:06:26.700
我们会在一些密集的计算的算子

131
0:06:26.700 --> 0:06:28.700
说到密集计算算子呢

132
0:06:28.700 --> 0:06:29.700
其实并不多

133
0:06:29.700 --> 0:06:31.700
有点类似于华为生腾里面的一个Cube

134
0:06:31.700 --> 0:06:32.700
这些算子

135
0:06:32.700 --> 0:06:34.700
举个简单的几个例子

136
0:06:34.700 --> 0:06:36.700
就是GMM矩阵的相乘

137
0:06:36.700 --> 0:06:37.700
还有卷机

138
0:06:37.700 --> 0:06:40.700
那这些呢就是密集计算的算子

139
0:06:40.700 --> 0:06:42.700
另外我们还会激活算子之后

140
0:06:42.700 --> 0:06:44.700
或者之前进行插入的

141
0:06:44.700 --> 0:06:46.700
最后呢我们还会在网络模型输入

142
0:06:46.700 --> 0:06:49.700
输出的地方进行插入伪量化算子

143
0:06:49.700 --> 0:06:52.700
下面呢我们看一个更加具体的例子

144
0:06:52.700 --> 0:06:54.700
那虽然看上去左边的这个图呢

145
0:06:54.700 --> 0:06:55.700
很复杂

146
0:06:55.700 --> 0:06:56.700
但实际上啊

147
0:06:56.700 --> 0:06:58.700
这个是我们的一个卷机BMV路

148
0:06:58.700 --> 0:07:01.700
三个简单的算子的一个计算图

149
0:07:01.700 --> 0:07:02.700
因为边层呢

150
0:07:02.700 --> 0:07:04.700
它有很多的不同的参数

151
0:07:04.700 --> 0:07:05.700
它要更新Gamma了

152
0:07:05.700 --> 0:07:06.700
要更新Beta了

153
0:07:06.700 --> 0:07:08.700
所以你看上去很复杂

154
0:07:08.700 --> 0:07:10.700
我们插入伪量化算子呢

155
0:07:10.700 --> 0:07:12.700
就会在这个图的input里面插入

156
0:07:12.700 --> 0:07:13.700
在我们的wait里面插入

157
0:07:13.700 --> 0:07:16.700
可能在对我们的W进行输出的时候

158
0:07:16.700 --> 0:07:17.700
插入一个

159
0:07:17.700 --> 0:07:18.700
另外的话

160
0:07:18.700 --> 0:07:21.700
我们还会对我们的激活后面插入一个

161
0:07:21.700 --> 0:07:23.700
这个呢就是一般的插入方式

162
0:07:23.700 --> 0:07:24.700
值得注意的就是

163
0:07:24.700 --> 0:07:26.700
如果你研究感知量化算法呢

164
0:07:26.700 --> 0:07:29.700
你可能会提出很多不同的插入的方式

165
0:07:29.700 --> 0:07:30.700
去学习

166
0:07:30.700 --> 0:07:33.700
也可能会自己造一个伪量化的算子

167
0:07:33.700 --> 0:07:35.700
或者把伪量化算子呢改掉

168
0:07:35.700 --> 0:07:36.700
那这个呢是最原始

169
0:07:36.700 --> 0:07:38.700
或者最naive的一种方式

170
0:07:39.700 --> 0:07:40.700
好了

171
0:07:40.700 --> 0:07:43.700
我们刚才只是简单的去给大家讲了

172
0:07:43.700 --> 0:07:44.700
感知量化训练呢

173
0:07:44.700 --> 0:07:46.700
一般的通用性的算法

174
0:07:46.700 --> 0:07:49.700
还讲了伪量化算子是怎幺实现的

175
0:07:49.700 --> 0:07:50.700
包括正向反向

176
0:07:50.700 --> 0:07:51.700
另外还有伪量化算子呢

177
0:07:51.700 --> 0:07:53.700
是怎幺插入到我们的计算图里面

178
0:07:53.700 --> 0:07:56.700
接下来我想提出几个问题

179
0:07:56.700 --> 0:07:58.700
让大家一起去思考思考

180
0:07:58.700 --> 0:07:59.700
第一个点呢

181
0:07:59.700 --> 0:08:03.700
就是优和平滑的计算伪量化阶段的两个值

182
0:08:03.700 --> 0:08:04.700
mean和max

183
0:08:04.700 --> 0:08:06.700
我们刚才提到batch normalization

184
0:08:06.700 --> 0:08:07.700
这种算子

185
0:08:07.700 --> 0:08:09.700
它其实也有很多参数

186
0:08:09.700 --> 0:08:11.700
需要根据我们的数的数据

187
0:08:11.700 --> 0:08:13.700
还有输出的数据进行学习的

188
0:08:13.700 --> 0:08:14.700
像编程呢

189
0:08:14.700 --> 0:08:17.700
一般都会有一个平滑计算的过程

190
0:08:17.700 --> 0:08:19.700
在具体算子或者kernel实现的时候呢

191
0:08:19.700 --> 0:08:20.700
就会有一个moving mean

192
0:08:20.700 --> 0:08:21.700
moving variance

193
0:08:21.700 --> 0:08:23.700
去进行一个平滑

194
0:08:23.700 --> 0:08:25.700
那另外一点就是

195
0:08:25.700 --> 0:08:27.700
我去看一些论文呢

196
0:08:27.700 --> 0:08:31.700
基本上我们都会对bn进行一个矫正

197
0:08:31.700 --> 0:08:34.700
而且还会有一个贝塞尔矫正

198
0:08:34.700 --> 0:08:37.700
为什幺我们需要做这些矫正呢

199
0:08:37.700 --> 0:08:38.700
那这一点呢

200
0:08:38.700 --> 0:08:41.700
其实是跟我们的样本有关系的

201
0:08:41.700 --> 0:08:43.700
我也非常欢迎大家去看一下

202
0:08:43.700 --> 0:08:46.700
或者去搜索一下相关的原理

203
0:08:46.700 --> 0:08:47.700
最后一点呢

204
0:08:47.700 --> 0:08:49.700
如果我们要对bn进行指点

205
0:08:49.700 --> 0:08:51.700
那计算公式或者kernel

206
0:08:51.700 --> 0:08:53.700
会不会有新的变化呢

207
0:08:53.700 --> 0:08:55.700
在我们的感知量化训练的过程当中

208
0:08:55.700 --> 0:08:57.700
确实是有的

209
0:08:57.700 --> 0:08:58.700
在真正计算的时候呢

210
0:08:58.700 --> 0:09:02.700
我们确实需要对它进行一个特殊的插入

211
0:09:02.700 --> 0:09:03.700
可以看到左边呢

212
0:09:03.700 --> 0:09:05.700
这个就是在没有融合之前

213
0:09:05.700 --> 0:09:06.700
右边这个呢

214
0:09:06.700 --> 0:09:07.700
就是在融合之后的

215
0:09:07.700 --> 0:09:09.700
算子融合的过程当中呢

216
0:09:09.700 --> 0:09:11.700
有同学去问我

217
0:09:11.700 --> 0:09:12.700
上面这条公式呢

218
0:09:12.700 --> 0:09:13.700
变成下面这条公式

219
0:09:13.700 --> 0:09:15.700
具体有什幺不一样呢

220
0:09:15.700 --> 0:09:18.700
很明显大家看我们的计算图就知道了

221
0:09:18.700 --> 0:09:21.700
我们整体的计算公式或者整体的计算形态啊

222
0:09:21.700 --> 0:09:23.700
确实改变了很多

223
0:09:23.700 --> 0:09:25.700
回到我们AI系统

224
0:09:25.700 --> 0:09:27.700
AI框架里面去看一下

225
0:09:27.700 --> 0:09:29.700
QAT的一个工作流程

226
0:09:29.700 --> 0:09:33.700
我们首先会有很多预训练的模型

227
0:09:33.700 --> 0:09:34.700
或者已经训练好的模型

228
0:09:34.700 --> 0:09:36.700
获取这个计算图之后

229
0:09:36.700 --> 0:09:38.700
我们需要对我们的网络模型呢

230
0:09:38.700 --> 0:09:39.700
进行一个改造

231
0:09:39.700 --> 0:09:42.700
去插入一些刚才讲到的

232
0:09:42.700 --> 0:09:43.700
伪量化的算子

233
0:09:43.700 --> 0:09:44.700
或者这里面呢

234
0:09:44.700 --> 0:09:45.700
叫做观察算子

235
0:09:45.700 --> 0:09:48.700
那我们统一称为伪量化算子就好了

236
0:09:48.700 --> 0:09:51.700
另外我们还要准备一些训练的数据

237
0:09:51.700 --> 0:09:52.700
那这个时候呢

238
0:09:52.700 --> 0:09:55.700
这里面输进去的就是已经经过改造的

239
0:09:55.700 --> 0:09:58.700
插入了伪量化算子之后的一个计算图

240
0:09:58.700 --> 0:09:59.700
另外一边呢

241
0:09:59.700 --> 0:10:01.700
我们需要Fine Tuning或者Training的数据

242
0:10:01.700 --> 0:10:02.700
接着呢

243
0:10:02.700 --> 0:10:04.700
真正的去执行Training和Fine Tuning

244
0:10:04.700 --> 0:10:05.700
最后呢

245
0:10:05.700 --> 0:10:06.700
在学习的过程当中

246
0:10:06.700 --> 0:10:08.700
我们就会不断的去学习

247
0:10:08.700 --> 0:10:10.700
因为量化所造成的一些误差

248
0:10:10.700 --> 0:10:11.700
把这些误差呢

249
0:10:11.700 --> 0:10:12.700
通过学习的方式

250
0:10:12.700 --> 0:10:14.700
把它变得更小更好

251
0:10:14.700 --> 0:10:15.700
最后呢

252
0:10:15.700 --> 0:10:17.700
就输出了我们QAT的网络模型

253
0:10:17.700 --> 0:10:19.700
那这个QAT的网络模型

254
0:10:19.700 --> 0:10:22.700
还真没有办法马上去执行的哦

255
0:10:22.700 --> 0:10:24.700
它要经过推理系统的一个转换模块

256
0:10:24.700 --> 0:10:27.700
然后去掉一些勇于的伪量化的算子

257
0:10:27.700 --> 0:10:29.700
才能够正常的推理

258
0:10:29.700 --> 0:10:30.700
那这一部分呢

259
0:10:30.700 --> 0:10:33.700
我们会在下一个内容里面去给大家介绍

260
0:10:34.700 --> 0:10:37.700
接下来我们来聊一聊QAT的衍生研究

261
0:10:37.700 --> 0:10:39.700
那QAT的衍生研究非常多啊

262
0:10:39.700 --> 0:10:40.700
而且这方面呢

263
0:10:40.700 --> 0:10:42.700
确实有很多新的算法

264
0:10:42.700 --> 0:10:43.700
或者新的idea提出

265
0:10:43.700 --> 0:10:45.700
例如好像这篇文章

266
0:10:45.700 --> 0:10:47.700
例如好像这篇文章

267
0:10:47.700 --> 0:10:49.700
中文名我也翻译不出来

268
0:10:49.700 --> 0:10:50.700
它这里面呢

269
0:10:50.700 --> 0:10:52.700
就做了一个新的伪量化的算子

270
0:10:52.700 --> 0:10:53.700
那正向呢

271
0:10:53.700 --> 0:10:55.700
可能跟我们刚才讲的差不多

272
0:10:55.700 --> 0:10:56.700
但是反向呢

273
0:10:56.700 --> 0:10:57.700
它就不一样了

274
0:10:57.700 --> 0:10:59.700
它不是一个简单的节段

275
0:10:59.700 --> 0:11:00.700
那这种方式呢

276
0:11:00.700 --> 0:11:02.700
就是对伪量化算子进行一个创新的

277
0:11:03.700 --> 0:11:04.700
另外呢

278
0:11:04.700 --> 0:11:06.700
有一些科研类的创新的文章呢

279
0:11:06.700 --> 0:11:08.700
会对我们的计算图啊

280
0:11:08.700 --> 0:11:10.700
或者对我们的量化的流程

281
0:11:10.700 --> 0:11:12.700
进行一个改进

282
0:11:12.700 --> 0:11:13.700
那最后呢

283
0:11:13.700 --> 0:11:14.700
我们再看一下

284
0:11:14.700 --> 0:11:16.700
其实我们在做量化的时候呢

285
0:11:16.700 --> 0:11:17.700
刚才只是很笼统的

286
0:11:17.700 --> 0:11:19.700
说我们的伪量化算子呢

287
0:11:19.700 --> 0:11:21.700
只是对我们的数的数据呢

288
0:11:21.700 --> 0:11:23.700
模拟我们的伪量化的过程当中

289
0:11:23.700 --> 0:11:25.700
但是我们的数据的形态是很多的

290
0:11:25.700 --> 0:11:27.700
有NCHW

291
0:11:27.700 --> 0:11:28.700
那这个时候呢

292
0:11:28.700 --> 0:11:29.700
我们可以对Patreon进行伪量化

293
0:11:29.700 --> 0:11:30.700
或者Patreon进行量化

294
0:11:30.700 --> 0:11:33.700
我们也可以对Potential进行量化

295
0:11:33.700 --> 0:11:35.700
所以量化的方式和种类

296
0:11:35.700 --> 0:11:36.700
还有不同层次

297
0:11:36.700 --> 0:11:37.700
有非常的多

298
0:11:37.700 --> 0:11:38.700
这里面呢

299
0:11:38.700 --> 0:11:39.700
周米更欢迎大家去看看

300
0:11:39.700 --> 0:11:41.700
相关更多的论文

301
0:11:41.700 --> 0:11:43.700
和最新的量化的研究

302
0:11:45.700 --> 0:11:47.700
最后我非常欢迎大家

303
0:11:47.700 --> 0:11:48.700
去阅读一下

304
0:11:48.700 --> 0:11:50.700
我这里面提到过的一些论文

305
0:11:50.700 --> 0:11:51.700
这里面这些论文呢

306
0:11:51.700 --> 0:11:53.700
也是比较新的

307
0:11:53.700 --> 0:11:55.700
一些关于量化相关的一些论文

308
0:11:55.700 --> 0:11:56.700
好了

309
0:11:56.700 --> 0:11:57.700
今天的内容呢

310
0:11:57.700 --> 0:11:58.700
就到这里为止

311
0:11:58.700 --> 0:11:59.700
谢谢各位

312
0:11:59.700 --> 0:12:00.700
卷的不行了

313
0:12:00.700 --> 0:12:01.700
卷的不行了

314
0:12:01.700 --> 0:12:02.700
记得一键三连加关注哦

315
0:12:02.700 --> 0:12:04.700
所有的内容都会开源在

316
0:12:04.700 --> 0:12:06.700
下面这条链接里面

317
0:12:06.700 --> 0:12:07.700
拜了个拜

