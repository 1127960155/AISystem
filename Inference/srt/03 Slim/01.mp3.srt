1
00:00:00,000 --> 00:00:08,560
嗨大家好我是周米卷王来了卷王来了

2
00:00:08,560 --> 00:00:11,520
那我们今天呢给大家带来一个新的内容

3
00:00:11,520 --> 00:00:15,280
就是推理引擎或者推理系统里面的模型压缩

4
00:00:15,280 --> 00:00:19,080
也可以叫模型小型化或者模型轻量化这个工作

5
00:00:19,080 --> 00:00:22,880
那今天呢我们主要是给大家去带来一个新的内容

6
00:00:22,880 --> 00:00:25,840
那说到这个内容呢我可不困了

7
00:00:25,840 --> 00:00:29,440
因为这个内容我们叫做离线优化

8
00:00:29,440 --> 00:00:31,240
包括离线模型压缩

9
00:00:31,240 --> 00:00:34,880
里面呢主要去讲讲模型压缩的四件套

10
00:00:34,880 --> 00:00:36,160
哪四件套啊

11
00:00:36,160 --> 00:00:39,640
那第一个呢就是低比特的量化

12
00:00:39,640 --> 00:00:41,880
那量化有训练量化还有杆子量化

13
00:00:41,880 --> 00:00:43,840
这两个呢我们都会去讲

14
00:00:43,840 --> 00:00:47,840
接着呢我们去讲讲一个现在还在学术前沿的

15
00:00:47,840 --> 00:00:50,960
二值化网络模型

16
00:00:50,960 --> 00:00:54,800
所谓二值化网络模型其实就是-1 0 1

17
00:00:54,800 --> 00:00:56,240
这种状态啊

18
00:00:56,240 --> 00:00:59,000
去表示我们的网络模型的结构

19
00:00:59,000 --> 00:01:00,400
所以说很有意思

20
00:01:00,400 --> 00:01:03,280
它其实也属于低比特量化其中的一种

21
00:01:03,280 --> 00:01:05,160
不过呢由于它的特殊性

22
00:01:05,160 --> 00:01:07,080
所以我们单独把它拿出来

23
00:01:07,080 --> 00:01:10,360
接着呢我们去看看模型的剪辑

24
00:01:10,360 --> 00:01:11,560
模型的剪辑比较简单

25
00:01:11,560 --> 00:01:14,320
就是我们网络模型啊有非常多的连接

26
00:01:14,320 --> 00:01:15,440
还有权重

27
00:01:15,440 --> 00:01:17,760
怎幺把这些参数变小

28
00:01:17,760 --> 00:01:18,920
把它们剪掉

29
00:01:18,920 --> 00:01:20,840
那这个呢就是模型剪辑

30
00:01:20,840 --> 00:01:24,080
最后一个内容就是知识增留

31
00:01:24,080 --> 00:01:24,760
知识增留

32
00:01:24,760 --> 00:01:27,000
Knowledge Distribution

33
00:01:27,000 --> 00:01:28,800
不知道读的对不对啊

34
00:01:28,800 --> 00:01:32,200
这四个内容就是我们模型压缩的四件套

35
00:01:32,200 --> 00:01:36,000
那我们看一下在整个推理引擎架构里面

36
00:01:36,000 --> 00:01:38,640
模型压缩一般处于哪个步骤

37
00:01:41,280 --> 00:01:42,320
下面我们来看一下

38
00:01:42,320 --> 00:01:45,680
这个呢就是我们整个推理引擎的总体架构

39
00:01:45,680 --> 00:01:47,680
那我们简单的去复述一下

40
00:01:47,680 --> 00:01:51,280
其实我们之前已经做了一个很详细很详细的介绍了

41
00:01:52,880 --> 00:01:55,200
首先呢我们对上有个API的程

42
00:01:55,200 --> 00:01:57,600
接着呢我们有个模型的转换

43
00:01:57,600 --> 00:01:58,520
那模型的转换呢

44
00:01:58,520 --> 00:02:01,800
就会把我们从不同的AI框架训练出来的网络模型

45
00:02:01,800 --> 00:02:04,320
转换成为我们推理引擎的自己的AR

46
00:02:04,320 --> 00:02:05,640
或者自己的Screamer

47
00:02:05,640 --> 00:02:07,040
转换成为自己的AR之后呢

48
00:02:07,040 --> 00:02:09,720
我们就会经过模型压缩这个功能

49
00:02:09,720 --> 00:02:11,560
那可能我们会做一些量化

50
00:02:11,560 --> 00:02:12,160
增留

51
00:02:12,160 --> 00:02:12,680
剪辑

52
00:02:12,680 --> 00:02:13,160
二次化

53
00:02:14,360 --> 00:02:17,960
有可能我们会把模型压缩的四件套同时用起来

54
00:02:17,960 --> 00:02:18,720
那这个时候呢

55
00:02:18,720 --> 00:02:22,520
我们叫做多维混和压缩算法

56
00:02:22,520 --> 00:02:23,120
那没关系

57
00:02:23,120 --> 00:02:25,000
这些名字都是我们随便起的

58
00:02:25,000 --> 00:02:26,440
或者怎幺起的高大上

59
00:02:26,560 --> 00:02:28,400
显得怎幺有竞争力也好

60
00:02:28,400 --> 00:02:29,120
但是呢

61
00:02:29,120 --> 00:02:30,040
说白到底

62
00:02:30,040 --> 00:02:32,560
它还是只有这四种算法去牵引

63
00:02:33,960 --> 00:02:35,520
实现完这个模型压缩之后呢

64
00:02:35,520 --> 00:02:38,280
就真正的去把我们的网络模型给到OneTime

65
00:02:38,280 --> 00:02:41,600
还有Kernel去执行在我们不同的硬件上面

66
00:02:41,600 --> 00:02:43,760
那整体流程就是这样的

67
00:02:43,760 --> 00:02:44,400
下面呢

68
00:02:44,400 --> 00:02:46,520
我们今天最主要的关注点呢

69
00:02:46,520 --> 00:02:49,000
就是对模型进行压缩

70
00:02:49,000 --> 00:02:51,440
把我们的模型变得越小越好

71
00:02:51,440 --> 00:02:53,520
就减少我们的网络模型的大小

72
00:02:53,520 --> 00:02:54,080
那第二个呢

73
00:02:54,080 --> 00:02:57,400
就是加快我们整个的推理的速度

74
00:02:57,400 --> 00:02:58,160
不是训练哦

75
00:02:58,160 --> 00:02:59,320
是推理

76
00:02:59,320 --> 00:03:04,080
使得我们在推理引擎里面跑得越快越好

77
00:03:04,080 --> 00:03:05,320
但是呢

78
00:03:05,320 --> 00:03:07,880
凡事都有一个约束

79
00:03:07,880 --> 00:03:10,960
这就是我们要求保持相同的精度

80
00:03:10,960 --> 00:03:13,760
或者不怎幺掉精度的前提之下呢

81
00:03:13,760 --> 00:03:18,040
去减少网络模型的大小和加快我们的推理速度

82
00:03:18,040 --> 00:03:20,560
所以保持精度很重要

83
00:03:20,560 --> 00:03:21,720
你模型变小了

84
00:03:21,760 --> 00:03:24,560
但是你不能把我的精度给改掉了哦

85
00:03:24,560 --> 00:03:28,080
那下面我们来看看整体的在推理流程里面

86
00:03:28,080 --> 00:03:32,000
我们会把很多不同AI框架训练出来的网络模型呢

87
00:03:32,000 --> 00:03:34,160
转换成为我们推理的模型

88
00:03:34,160 --> 00:03:38,320
接着大部分的都会经过一个模型的压缩模块

89
00:03:38,320 --> 00:03:39,640
通过模型的压缩

90
00:03:39,640 --> 00:03:42,200
把我的模型变得又小

91
00:03:42,200 --> 00:03:43,600
运行起来又快

92
00:03:43,600 --> 00:03:45,440
而且精度还能无损

93
00:03:45,440 --> 00:03:46,640
那这是更好的

94
00:03:46,640 --> 00:03:47,760
整套流程呢

95
00:03:47,760 --> 00:03:50,280
就是在我们之前已经详细讲开过了

96
00:03:50,320 --> 00:03:53,280
这里面呢我们重点关注就是模型压缩

97
00:03:56,880 --> 00:03:59,720
好了今天的内容呢就到这里为止

98
00:03:59,720 --> 00:04:04,280
欢迎大家继续留意下一期我们真正的一些算法和内容

99
00:04:05,280 --> 00:04:06,080
拜了个拜

100
00:04:07,760 --> 00:04:09,440
卷的不行了卷的不行了

101
00:04:09,440 --> 00:04:11,240
记得一键三连加关注哦

102
00:04:11,240 --> 00:04:14,840
所有的内容都会开源在下面这条链接里面

103
00:04:14,840 --> 00:04:15,560
拜了个拜

