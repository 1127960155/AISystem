0
0:00:00.000 --> 0:00:08.560
嗨大家好我是周米卷王来了卷王来了

1
0:00:08.560 --> 0:00:11.520
那我们今天呢给大家带来一个新的内容

2
0:00:11.520 --> 0:00:15.280
就是推理引擎或者推理系统里面的模型压缩

3
0:00:15.280 --> 0:00:19.080
也可以叫模型小型化或者模型轻量化这个工作

4
0:00:19.080 --> 0:00:22.880
那今天呢我们主要是给大家去带来一个新的内容

5
0:00:22.880 --> 0:00:25.840
那说到这个内容呢我可不困了

6
0:00:25.840 --> 0:00:29.440
因为这个内容我们叫做离线优化

7
0:00:29.440 --> 0:00:31.240
包括离线模型压缩

8
0:00:31.240 --> 0:00:34.880
里面呢主要去讲讲模型压缩的四件套

9
0:00:34.880 --> 0:00:36.160
哪四件套啊

10
0:00:36.160 --> 0:00:39.640
那第一个呢就是低比特的量化

11
0:00:39.640 --> 0:00:41.880
那量化有训练量化还有杆子量化

12
0:00:41.880 --> 0:00:43.840
这两个呢我们都会去讲

13
0:00:43.840 --> 0:00:47.840
接着呢我们去讲讲一个现在还在学术前沿的

14
0:00:47.840 --> 0:00:50.960
二值化网络模型

15
0:00:50.960 --> 0:00:54.800
所谓二值化网络模型其实就是-1 0 1

16
0:00:54.800 --> 0:00:56.240
这种状态啊

17
0:00:56.240 --> 0:00:59.000
去表示我们的网络模型的结构

18
0:00:59.000 --> 0:01:00.400
所以说很有意思

19
0:01:00.400 --> 0:01:03.280
它其实也属于低比特量化其中的一种

20
0:01:03.280 --> 0:01:05.160
不过呢由于它的特殊性

21
0:01:05.160 --> 0:01:07.080
所以我们单独把它拿出来

22
0:01:07.080 --> 0:01:10.360
接着呢我们去看看模型的剪辑

23
0:01:10.360 --> 0:01:11.560
模型的剪辑比较简单

24
0:01:11.560 --> 0:01:14.320
就是我们网络模型啊有非常多的连接

25
0:01:14.320 --> 0:01:15.440
还有权重

26
0:01:15.440 --> 0:01:17.760
怎幺把这些参数变小

27
0:01:17.760 --> 0:01:18.920
把它们剪掉

28
0:01:18.920 --> 0:01:20.840
那这个呢就是模型剪辑

29
0:01:20.840 --> 0:01:24.080
最后一个内容就是知识增留

30
0:01:24.080 --> 0:01:24.760
知识增留

31
0:01:24.760 --> 0:01:27.000
Knowledge Distribution

32
0:01:27.000 --> 0:01:28.800
不知道读的对不对啊

33
0:01:28.800 --> 0:01:32.200
这四个内容就是我们模型压缩的四件套

34
0:01:32.200 --> 0:01:36.000
那我们看一下在整个推理引擎架构里面

35
0:01:36.000 --> 0:01:38.640
模型压缩一般处于哪个步骤

36
0:01:41.280 --> 0:01:42.320
下面我们来看一下

37
0:01:42.320 --> 0:01:45.680
这个呢就是我们整个推理引擎的总体架构

38
0:01:45.680 --> 0:01:47.680
那我们简单的去复述一下

39
0:01:47.680 --> 0:01:51.280
其实我们之前已经做了一个很详细很详细的介绍了

40
0:01:52.880 --> 0:01:55.200
首先呢我们对上有个API的程

41
0:01:55.200 --> 0:01:57.600
接着呢我们有个模型的转换

42
0:01:57.600 --> 0:01:58.520
那模型的转换呢

43
0:01:58.520 --> 0:02:01.800
就会把我们从不同的AI框架训练出来的网络模型

44
0:02:01.800 --> 0:02:04.320
转换成为我们推理引擎的自己的AR

45
0:02:04.320 --> 0:02:05.640
或者自己的Screamer

46
0:02:05.640 --> 0:02:07.040
转换成为自己的AR之后呢

47
0:02:07.040 --> 0:02:09.720
我们就会经过模型压缩这个功能

48
0:02:09.720 --> 0:02:11.560
那可能我们会做一些量化

49
0:02:11.560 --> 0:02:12.160
增留

50
0:02:12.160 --> 0:02:12.680
剪辑

51
0:02:12.680 --> 0:02:13.160
二次化

52
0:02:14.360 --> 0:02:17.960
有可能我们会把模型压缩的四件套同时用起来

53
0:02:17.960 --> 0:02:18.720
那这个时候呢

54
0:02:18.720 --> 0:02:22.520
我们叫做多维混和压缩算法

55
0:02:22.520 --> 0:02:23.120
那没关系

56
0:02:23.120 --> 0:02:25.000
这些名字都是我们随便起的

57
0:02:25.000 --> 0:02:26.440
或者怎幺起的高大上

58
0:02:26.560 --> 0:02:28.400
显得怎幺有竞争力也好

59
0:02:28.400 --> 0:02:29.120
但是呢

60
0:02:29.120 --> 0:02:30.040
说白到底

61
0:02:30.040 --> 0:02:32.560
它还是只有这四种算法去牵引

62
0:02:33.960 --> 0:02:35.520
实现完这个模型压缩之后呢

63
0:02:35.520 --> 0:02:38.280
就真正的去把我们的网络模型给到OneTime

64
0:02:38.280 --> 0:02:41.600
还有Kernel去执行在我们不同的硬件上面

65
0:02:41.600 --> 0:02:43.760
那整体流程就是这样的

66
0:02:43.760 --> 0:02:44.400
下面呢

67
0:02:44.400 --> 0:02:46.520
我们今天最主要的关注点呢

68
0:02:46.520 --> 0:02:49.000
就是对模型进行压缩

69
0:02:49.000 --> 0:02:51.440
把我们的模型变得越小越好

70
0:02:51.440 --> 0:02:53.520
就减少我们的网络模型的大小

71
0:02:53.520 --> 0:02:54.080
那第二个呢

72
0:02:54.080 --> 0:02:57.400
就是加快我们整个的推理的速度

73
0:02:57.400 --> 0:02:58.160
不是训练哦

74
0:02:58.160 --> 0:02:59.320
是推理

75
0:02:59.320 --> 0:03:04.080
使得我们在推理引擎里面跑得越快越好

76
0:03:04.080 --> 0:03:05.320
但是呢

77
0:03:05.320 --> 0:03:07.880
凡事都有一个约束

78
0:03:07.880 --> 0:03:10.960
这就是我们要求保持相同的精度

79
0:03:10.960 --> 0:03:13.760
或者不怎幺掉精度的前提之下呢

80
0:03:13.760 --> 0:03:18.040
去减少网络模型的大小和加快我们的推理速度

81
0:03:18.040 --> 0:03:20.560
所以保持精度很重要

82
0:03:20.560 --> 0:03:21.720
你模型变小了

83
0:03:21.760 --> 0:03:24.560
但是你不能把我的精度给改掉了哦

84
0:03:24.560 --> 0:03:28.080
那下面我们来看看整体的在推理流程里面

85
0:03:28.080 --> 0:03:32.000
我们会把很多不同AI框架训练出来的网络模型呢

86
0:03:32.000 --> 0:03:34.160
转换成为我们推理的模型

87
0:03:34.160 --> 0:03:38.320
接着大部分的都会经过一个模型的压缩模块

88
0:03:38.320 --> 0:03:39.640
通过模型的压缩

89
0:03:39.640 --> 0:03:42.200
把我的模型变得又小

90
0:03:42.200 --> 0:03:43.600
运行起来又快

91
0:03:43.600 --> 0:03:45.440
而且精度还能无损

92
0:03:45.440 --> 0:03:46.640
那这是更好的

93
0:03:46.640 --> 0:03:47.760
整套流程呢

94
0:03:47.760 --> 0:03:50.280
就是在我们之前已经详细讲开过了

95
0:03:50.320 --> 0:03:53.280
这里面呢我们重点关注就是模型压缩

96
0:03:56.880 --> 0:03:59.720
好了今天的内容呢就到这里为止

97
0:03:59.720 --> 0:04:04.280
欢迎大家继续留意下一期我们真正的一些算法和内容

98
0:04:05.280 --> 0:04:06.080
拜了个拜

99
0:04:07.760 --> 0:04:09.440
卷的不行了卷的不行了

100
0:04:09.440 --> 0:04:11.240
记得一键三连加关注哦

101
0:04:11.240 --> 0:04:14.840
所有的内容都会开源在下面这条链接里面

102
0:04:14.840 --> 0:04:15.560
拜了个拜

