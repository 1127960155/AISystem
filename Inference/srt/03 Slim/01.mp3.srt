0
0:00:00.000 --> 0:00:08.600
嗨大家好我是ZOMi卷王来了卷王来了

1
0:00:08.600 --> 0:00:11.520
那我们今天呢给大家带来一个新的内容

2
0:00:11.520 --> 0:00:15.320
就是推理引擎或者推理系统里面的模型压缩

3
0:00:15.320 --> 0:00:19.120
也可以叫模型小型化或者模型轻量化这个工作

4
0:00:19.120 --> 0:00:22.960
那今天的我们主要是给大家去带来一个新的内容

5
0:00:22.960 --> 0:00:25.880
那说到这个内容呢我可不困了

6
0:00:25.880 --> 0:00:31.160
因为这个内容我们叫做离线优化或离线模型压缩

7
0:00:31.160 --> 0:00:36.040
里面呢主要去讲讲模型压缩的四件套内四件套啊

8
0:00:36.040 --> 0:00:39.560
那第一个呢就是低比特的量化

9
0:00:39.560 --> 0:00:43.720
量化有训练量化还有杆子量化这两个呢我们都会去讲

10
0:00:43.720 --> 0:00:50.080
接着呢我们去讲讲一个现在还在学术前年的二子化网络模型

11
0:00:50.880 --> 0:00:56.200
所谓二子化网络模型其实就是负一零一这种状态啊

12
0:00:56.200 --> 0:00:58.920
去表示我们的网络模型的结构

13
0:00:58.920 --> 0:01:03.200
所以说很有意思它其实也属于低比特量化其中的一种

14
0:01:03.200 --> 0:01:07.000
不过呢由于它的特殊性所以我们单独把它拿出来

15
0:01:07.000 --> 0:01:10.240
接着呢我们去看看模型的减资

16
0:01:10.240 --> 0:01:15.360
模型的减资比较简单就是我们网络模型啊有非常多的连接还有权重

17
0:01:15.360 --> 0:01:18.840
怎么把这些参数变小把它们减掉

18
0:01:18.840 --> 0:01:23.960
那这个呢就是模型减资最后一个内容就是知识增留

19
0:01:23.960 --> 0:01:28.720
知识增留knowledge distribution不知道读的对不对啊

20
0:01:28.720 --> 0:01:32.160
这四个内容就是我们模型压缩的四件套

21
0:01:32.160 --> 0:01:41.200
让我们看一下在整个推理引擎架构里面模型压缩一般处于哪个步骤

22
0:01:41.200 --> 0:01:45.600
下面我们来看一下这个呢就是我们整个推理引擎的总体架构

23
0:01:45.600 --> 0:01:47.640
让我们简单的去复述一下

24
0:01:47.680 --> 0:01:51.240
其实我们之前已经做了一个很详细很详细的介绍了

25
0:01:52.880 --> 0:01:57.600
首先呢我们对上有个API的城接着呢我们有个模型的转换

26
0:01:57.600 --> 0:02:01.840
那模型的转换呢就会把我们从不同的AI框架训练出来的网络模型

27
0:02:01.840 --> 0:02:05.680
转换成为我们推理引擎的自己的AI或者自己的screen吧

28
0:02:05.680 --> 0:02:09.760
转换成为自己的AI之后呢我们就会经过模型压缩这个功能

29
0:02:09.760 --> 0:02:13.360
那可能我们会做一些量化增留减资二级化

30
0:02:14.360 --> 0:02:17.960
有可能我们会把模型压缩的四件套同时用起来

31
0:02:17.960 --> 0:02:22.480
那这个时候呢我们叫做我们叫做多维捆和压缩算法

32
0:02:22.480 --> 0:02:24.960
那没关系这些名字都是我们随便起的

33
0:02:24.960 --> 0:02:28.360
或者怎么起的高大上显得怎么有竞争力也好

34
0:02:28.360 --> 0:02:32.760
但是呢说白到底它还是只有这四种算法去牵引

35
0:02:33.960 --> 0:02:37.840
实现完这个模型压缩之后呢就真正的去把我们的网络模型给到

36
0:02:37.840 --> 0:02:41.600
温泰还有科诺去执行在我们不同的硬件上面

37
0:02:41.600 --> 0:02:43.760
那整体流程就是这样的

38
0:02:43.760 --> 0:02:49.000
下面呢我们今天最主要的关注点呢就是对模型进行压缩

39
0:02:49.000 --> 0:02:51.440
帮我们的模型变得越小越好

40
0:02:51.440 --> 0:02:53.480
就减少我们的网络模型的大小

41
0:02:53.480 --> 0:02:59.280
那第二个呢就是加快我们整个的推理的速度不是训练哦是推理

42
0:02:59.280 --> 0:03:04.040
使得我们在推理引擎里面跑得越快越好

43
0:03:04.040 --> 0:03:07.880
但是呢凡是都有一个约束

44
0:03:07.880 --> 0:03:13.760
这就是我们要求保持相同的精度或者不怎么掉精度的前提之下呢

45
0:03:13.760 --> 0:03:18.040
去减少网络模型的大小和加快我们的推理速度

46
0:03:18.040 --> 0:03:24.520
所以保持精度很重要你模型变小了但是你不能把我的精度给改掉咯

47
0:03:24.520 --> 0:03:32.000
那下面我们来看看整体的在推理流程里面我们会把很多不同AI框架训练出来的网络模型呢

48
0:03:32.000 --> 0:03:34.160
转换成为我们推理的模型

49
0:03:34.160 --> 0:03:39.600
接着大部分呢都会经过一个模型的压缩模块通过模型的压缩

50
0:03:39.600 --> 0:03:46.680
帮我们的模型变得又小运行起来又快而且精度还能无损那这是更好的

51
0:03:46.680 --> 0:03:53.360
整套流程呢就是在我们之前已经详细讲开过了这里面呢我们重点关注就是模型压缩

52
0:03:56.800 --> 0:04:04.120
好了今天的内容呢就到这里为止欢迎大家继续留意下一期我们真正的一些算法和内容
