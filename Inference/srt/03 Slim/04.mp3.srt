1
00:00:00,000 --> 00:00:04,560
巴巴巴巴巴巴巴巴巴巴

2
00:00:04,560 --> 00:00:06,080
好辣呀好辣呀

3
00:00:06,080 --> 00:00:08,640
今天吃了火锅特别的辣

4
00:00:08,640 --> 00:00:09,960
那至于辣哪里呢

5
00:00:09,960 --> 00:00:11,400
大家想想就知道了

6
00:00:11,400 --> 00:00:12,600
我是ZOMI

7
00:00:12,600 --> 00:00:16,400
今天呢我们是在推丁引擎里面的模型压缩

8
00:00:16,400 --> 00:00:18,320
训练后量化就是PTQ

9
00:00:18,320 --> 00:00:19,960
还有量化完之后呢

10
00:00:19,960 --> 00:00:23,720
我们怎么把这些量化后的网络模型进行部署起来

11
00:00:25,040 --> 00:00:26,960
ZOMI讲话的语速呢会稍微快了一点

12
00:00:26,960 --> 00:00:28,840
所以大家有不懂的或者我讲不明白的

13
00:00:28,840 --> 00:00:31,800
也欢迎大家去给我私信和贪慕留言

14
00:00:31,800 --> 00:00:34,480
现在呢我们来到这个内容

15
00:00:34,480 --> 00:00:36,440
Post Training Quantumization

16
00:00:36,440 --> 00:00:38,760
训练后量化我们又叫做PTQ

17
00:00:38,760 --> 00:00:42,440
还有量化之后怎么去把它真正部署起来

18
00:00:42,440 --> 00:00:44,160
两个比较重要的内容

19
00:00:45,800 --> 00:00:49,000
首先第一个内容就是训练后量化PTQ

20
00:00:49,000 --> 00:00:52,440
实际上呢PTQ它分为Static和Dynamic

21
00:00:52,440 --> 00:00:55,120
两个一个是静态一个是动态

22
00:00:55,800 --> 00:00:57,680
我们先看一个比较简单一点的

23
00:00:57,720 --> 00:01:00,960
就是动态脱机量化PTQ Dynamic

24
00:01:01,800 --> 00:01:03,320
PTQ Dynamic其实非常简单

25
00:01:03,320 --> 00:01:05,200
也是ZOMI做的第一个量化的项目

26
00:01:05,680 --> 00:01:08,960
简单的来说就是把我们的网络模型全种了

27
00:01:08,960 --> 00:01:13,720
直接从FP32简单的硬式成为INT8,INT16

28
00:01:13,720 --> 00:01:14,440
这种方式

29
00:01:15,720 --> 00:01:19,160
最重要的目的呢就是减少我们的网络模型的大小

30
00:01:19,160 --> 00:01:20,760
为什么叫做动态呢

31
00:01:20,760 --> 00:01:22,680
是因为我们的缩放因子Scale

32
00:01:22,680 --> 00:01:24,640
是一个动态去计算出来的

33
00:01:25,640 --> 00:01:26,840
因此这种量化方式呢

34
00:01:26,840 --> 00:01:29,080
是几种量化方式里面性能最差的

35
00:01:30,280 --> 00:01:32,480
我们看一下一个简单的流程

36
00:01:33,200 --> 00:01:34,680
PTQ Dynamic的算法流程呢

37
00:01:34,680 --> 00:01:36,320
主要有三个模块

38
00:01:36,320 --> 00:01:39,000
第一个就是我们拿到一个已经训练好的网络模型了

39
00:01:39,960 --> 00:01:43,280
接着我们把这个网络模型的FP32的网络模型的全种

40
00:01:43,280 --> 00:01:47,000
直接转换成为INT8这种量化的模型

41
00:01:47,000 --> 00:01:50,200
那最后呢就输出一个已经转换成为全种

42
00:01:50,200 --> 00:01:53,400
转换成为INT8的量化的模型对外进行输出

43
00:01:55,640 --> 00:01:57,920
接下来我们来到第二个内容

44
00:01:57,920 --> 00:02:00,080
那这个内容呢还是有点意思的

45
00:02:00,080 --> 00:02:04,400
像华为上唐Ken里面的推理引擎ACL

46
00:02:04,400 --> 00:02:08,160
还有英伟达的TensorRT里面的量化模块呢

47
00:02:08,160 --> 00:02:11,920
都是采用了静态脱机量化PTQ Static

48
00:02:12,920 --> 00:02:15,760
现在我们看一下这种量化方式呢有什么不一样啊

49
00:02:15,760 --> 00:02:18,720
现在大部分的推理引擎或者推理框架

50
00:02:18,720 --> 00:02:21,840
都会采用脱机量化的这种方式

51
00:02:21,840 --> 00:02:23,400
作为里面集成的一个模块

52
00:02:23,440 --> 00:02:26,480
所以这个模块呢我们稍微简单的展开一下

53
00:02:26,480 --> 00:02:29,280
那这个静态的脱机量化同时叫做

54
00:02:29,280 --> 00:02:31,880
校正量化或者数据级量化

55
00:02:31,880 --> 00:02:35,920
因为里面使用了少量没有标签的校准数据

56
00:02:35,920 --> 00:02:38,480
所以它叫做校正量化或者数据级量化

57
00:02:38,480 --> 00:02:41,440
我们统一都理解为静态脱机量化就好了

58
00:02:41,440 --> 00:02:44,000
这也是一个学术和官方的一种叫法

59
00:02:45,000 --> 00:02:47,520
而这里面呢使用无标签的校准数据呢

60
00:02:47,520 --> 00:02:50,240
主要是用在去计算我们的scale

61
00:02:50,680 --> 00:02:53,080
通过真实场景没有标签的校准数据

62
00:02:53,840 --> 00:02:55,200
因为我们在获取scale的时候呢

63
00:02:55,200 --> 00:02:57,000
没有必要去训练或者file to lane

64
00:02:57,000 --> 00:02:59,360
我们只需要简单的运行一些正向

65
00:02:59,360 --> 00:03:03,280
这个数据呢只需要有一些真实的数据场景就好了

66
00:03:03,280 --> 00:03:05,480
我们就可以根据真实的数据场景

67
00:03:05,480 --> 00:03:08,840
去获取我们的scale 缩放因子

68
00:03:09,960 --> 00:03:13,480
在ptq static静态脱机量化里面怎么去获取scale呢

69
00:03:13,480 --> 00:03:15,480
就是算法的内核

70
00:03:15,480 --> 00:03:17,160
里面呢有可以通过mean max呢

71
00:03:17,160 --> 00:03:21,240
klt呢adm呢eq等不同的方式去展开

72
00:03:22,240 --> 00:03:26,400
那下面呢我们看一下ptq static一个算法的主要流程

73
00:03:26,400 --> 00:03:29,720
首先我们已经有一个已经训练好的网络模型

74
00:03:29,720 --> 00:03:33,040
接着呢我们获取这个网络模型的计算图

75
00:03:33,040 --> 00:03:36,160
对这个计算图呢插入一些伪量化的算子

76
00:03:36,160 --> 00:03:37,640
那这个插入伪量化的算子

77
00:03:37,640 --> 00:03:41,120
跟刚才在讲感知量化训练的时候的伪量化算子

78
00:03:41,120 --> 00:03:42,600
意义上是相同的

79
00:03:42,600 --> 00:03:44,560
但是我们没有训练的流程

80
00:03:44,560 --> 00:03:46,280
我们只有校正的流程

81
00:03:46,280 --> 00:03:48,520
既然有校正我们就有校正的数据集

82
00:03:48,520 --> 00:03:51,200
而这个数据集呢不需要带标签

83
00:03:51,200 --> 00:03:53,120
只要从真实的数据场景里面

84
00:03:53,120 --> 00:03:56,200
去获取相关的一个小部分的数据集就好了

85
00:03:56,200 --> 00:03:58,360
然后呢就给到我们的校正算法

86
00:03:58,360 --> 00:04:01,000
真正的去做一个量化的工作

87
00:04:01,000 --> 00:04:05,520
最后呢就输出一个ptq-models的网络模型

88
00:04:05,520 --> 00:04:07,520
就经过量化后的网络模型

89
00:04:07,520 --> 00:04:11,200
然后呢就给推理部署平台进行一个真正的处理

90
00:04:15,200 --> 00:04:16,920
在静态理性量化里面呢

91
00:04:16,920 --> 00:04:19,880
为了更好的去得到我们的scale

92
00:04:19,880 --> 00:04:21,680
或者去计算我们的数据分布呢

93
00:04:21,680 --> 00:04:24,640
这里面我们用了一种算法叫做KL散度

94
00:04:24,640 --> 00:04:27,000
用KL散度去校准我们的数据集

95
00:04:27,000 --> 00:04:30,640
那这里面呢我们先看看KL散度的一个原理哦

96
00:04:30,640 --> 00:04:33,680
首先呢KL散度呢它也叫做相对商

97
00:04:33,680 --> 00:04:37,360
这里面这套公式呢就是KL散度的真实的公式

98
00:04:37,360 --> 00:04:40,680
我们主要是去对比两个分布之间的差异

99
00:04:40,680 --> 00:04:42,600
其中p呢就是真实的分布

100
00:04:42,600 --> 00:04:44,360
而q呢就是预测的分布

101
00:04:44,360 --> 00:04:46,520
KL散度呢就去对比真实的分布

102
00:04:46,520 --> 00:04:49,280
跟预测的分布之间的一个近似的值

103
00:04:49,800 --> 00:04:51,200
或者它们的差异的大小

104
00:04:51,200 --> 00:04:53,560
当然了它们的差异越小越好

105
00:04:54,320 --> 00:04:56,440
诶有了这个原理之后

106
00:04:56,440 --> 00:04:58,240
那事情就变得更加简单了

107
00:04:58,240 --> 00:05:01,120
我们看一下KL散度的一个具体的算法流程

108
00:05:03,480 --> 00:05:06,160
第一步我们需要准备一些带校准的数据集

109
00:05:06,160 --> 00:05:07,960
那这些数据呢不需要有标签

110
00:05:07,960 --> 00:05:10,520
我们从真实的场景或者验证集里面

111
00:05:10,520 --> 00:05:12,480
去选择一小部分就可以了

112
00:05:13,320 --> 00:05:16,760
然后做一个正向的就是FP32的一个推理

113
00:05:16,760 --> 00:05:17,560
大家要注意哦

114
00:05:17,560 --> 00:05:19,720
是FP32没有经过任何量化的

115
00:05:19,720 --> 00:05:22,520
下面的这个Fold步骤呢才是真正的量化

116
00:05:22,520 --> 00:05:24,360
那在FP32推理的时候呢

117
00:05:24,360 --> 00:05:27,800
我们就会对每一层做一个不同的工作

118
00:05:27,800 --> 00:05:29,200
针对每一层都要做

119
00:05:29,960 --> 00:05:32,400
那这里面呢我们需要去收集

120
00:05:32,400 --> 00:05:35,040
每一层的进货值的一个值方图

121
00:05:35,040 --> 00:05:38,280
就获取我们数据输出的一个分布

122
00:05:39,240 --> 00:05:40,760
在这里面的第二步

123
00:05:40,760 --> 00:05:43,160
就是我们需要去设置几个域子

124
00:05:43,160 --> 00:05:46,600
通过不同的域子呢去获取量化后的一个分布

125
00:05:47,120 --> 00:05:50,280
这里面呢我们设计了很多不同的域子数据

126
00:05:50,280 --> 00:05:52,600
所以会得到很多不同的量化的分布

127
00:05:54,080 --> 00:05:57,240
在最后一步呢我们就真正的去计算KL散度了

128
00:05:58,360 --> 00:06:01,560
而KL散度的两个数值就是刚才的第一步

129
00:06:01,560 --> 00:06:02,600
获取真实的数据

130
00:06:03,160 --> 00:06:04,880
第二步呢就是通过域子去产生

131
00:06:04,880 --> 00:06:06,800
不同的量化后的分布

132
00:06:06,800 --> 00:06:08,200
通过比较这两步

133
00:06:08,200 --> 00:06:10,040
然后我们得到一个最小值

134
00:06:10,840 --> 00:06:13,240
原始数据的分布跟量化后的数据的分布

135
00:06:13,240 --> 00:06:15,040
是比较相似的

136
00:06:15,080 --> 00:06:17,040
那这个具体的域子和相关的参数呢

137
00:06:17,040 --> 00:06:19,400
我们就作为最优的参数值

138
00:06:19,400 --> 00:06:22,440
那这里面有几个点需要去注意一下的

139
00:06:22,440 --> 00:06:24,920
就是我们需要准备一些小批量的数据

140
00:06:24,920 --> 00:06:28,000
也就是我们刚才所说的Calibration Data Set

141
00:06:28,000 --> 00:06:30,520
需要去校准的数据集

142
00:06:30,520 --> 00:06:34,680
这里面的一般都会采用500到1000张图片左右

143
00:06:34,680 --> 00:06:35,960
就够了不用太多

144
00:06:36,600 --> 00:06:38,240
朱米之前在一个项目里面

145
00:06:38,240 --> 00:06:39,720
采用了1万多张数据集

146
00:06:39,720 --> 00:06:43,080
后来发现其实跟500多张数据集差异不太大

147
00:06:43,080 --> 00:06:45,000
所以大家不用浪费太多的时间

148
00:06:45,000 --> 00:06:47,240
去调多少张不同的数据集

149
00:06:47,240 --> 00:06:49,680
更多的可以去调一调第二步

150
00:06:49,680 --> 00:06:51,240
通过什么域子什么参数

151
00:06:51,240 --> 00:06:53,200
去产生不同的数据的分布

152
00:06:56,040 --> 00:06:59,880
下面这段尾代码就是英伟达TensorRT的一个关于KL散度

153
00:06:59,880 --> 00:07:03,920
或者静态量化后训练的一个具体的尾代码

154
00:07:03,920 --> 00:07:05,160
我们就不一一解析了

155
00:07:05,760 --> 00:07:08,240
有兴趣的同学可以上朱米的Github里面

156
00:07:08,240 --> 00:07:10,520
去获取相关的数据

157
00:07:11,520 --> 00:07:15,320
接着我们来到第二个比较重要的内容

158
00:07:15,320 --> 00:07:17,320
就是端测量化的推理部署

159
00:07:17,320 --> 00:07:19,880
我们刚才讲了很多种不同的量化方式

160
00:07:19,880 --> 00:07:24,800
但是实际上真正的量化方式应该是怎么样的呢

161
00:07:31,960 --> 00:07:34,920
我量化完之后真正的要去推理部署了

162
00:07:34,920 --> 00:07:36,600
那在端测量化推理部署里面

163
00:07:36,600 --> 00:07:39,640
其实非常非常的讲究

164
00:07:39,920 --> 00:07:41,640
在真正推理部署的时候

165
00:07:41,840 --> 00:07:43,680
其实有很多种方式

166
00:07:43,880 --> 00:07:46,880
下面我们展开三个图有三种方式

167
00:07:46,880 --> 00:07:48,520
第一种就是我的输入

168
00:07:48,520 --> 00:07:52,000
假设这个输入它不是实际上的网络模型的输入

169
00:07:52,760 --> 00:07:55,000
假设我输入的是FP32

170
00:07:55,000 --> 00:07:57,840
接着我这个算子是一个int8的算子

171
00:07:57,840 --> 00:08:01,040
我这个卷集算子对应的参数也是int8

172
00:08:01,040 --> 00:08:04,520
这个时候我对输入的时候就需要做一个量化

173
00:08:04,960 --> 00:08:07,920
把输进去的FP32的数据量化成int8

174
00:08:08,040 --> 00:08:10,080
接着去用int8进行计算

175
00:08:10,080 --> 00:08:13,600
计算完之后我们的数据输出来肯定是int32的

176
00:08:14,600 --> 00:08:16,440
假设我们的算子是一个卷集

177
00:08:16,440 --> 00:08:17,920
卷集的输入是int8

178
00:08:17,920 --> 00:08:19,240
它的权重也是int8

179
00:08:19,240 --> 00:08:21,560
如果我的输出都是int8的话

180
00:08:21,560 --> 00:08:23,000
它就会造成一个溢出

181
00:08:23,000 --> 00:08:25,800
255×255肯定超过255

182
00:08:25,800 --> 00:08:28,160
一般的输出会变成一个int32

183
00:08:28,160 --> 00:08:30,440
这个时候我们需要de-portalization

184
00:08:30,440 --> 00:08:33,960
就是反量化回FP32再给下一层输入

185
00:08:34,880 --> 00:08:38,080
另外我们看一下第二种方式

186
00:08:38,560 --> 00:08:41,440
第二种方式它的输入是FP32

187
00:08:41,440 --> 00:08:43,360
输出是int8

188
00:08:43,360 --> 00:08:46,520
我们看一下里面的一个具体的内容

189
00:08:47,760 --> 00:08:49,400
FP32输进去

190
00:08:49,400 --> 00:08:52,640
我们肯定需要进行一个量化回int8

191
00:08:52,640 --> 00:08:55,360
接着我需要去做一个具体的计算

192
00:08:55,360 --> 00:08:57,760
计算完之后我们还是变成int32

193
00:08:58,240 --> 00:09:01,640
int32之后我们会做一个重量化

194
00:09:01,640 --> 00:09:06,440
把int32的数据量化成int8

195
00:09:06,440 --> 00:09:08,920
然后下一个算子如果也是一个卷集

196
00:09:08,920 --> 00:09:12,200
这个时候就直接串起来就行了

197
00:09:12,200 --> 00:09:14,920
所以引申成为第三种方式

198
00:09:14,920 --> 00:09:17,080
假设这个数据的输出

199
00:09:17,080 --> 00:09:20,680
是第三个数据算子的输入

200
00:09:20,680 --> 00:09:22,440
我输的是int8

201
00:09:22,440 --> 00:09:26,320
卷集算子计算之后输出是int32

202
00:09:26,320 --> 00:09:30,760
然后再做一个重量化变成int8再输出

203
00:09:30,760 --> 00:09:32,760
所以在我们整个计算图

204
00:09:32,760 --> 00:09:35,320
在整个网络模型的过程当中

205
00:09:35,320 --> 00:09:38,320
会遇到三种不同的方式

206
00:09:40,320 --> 00:09:44,280
下面这个图我把这三种不同的方式串起来

207
00:09:44,280 --> 00:09:46,920
我们的网络模型的数据的输进去

208
00:09:46,920 --> 00:09:48,560
肯定是FP32的

209
00:09:48,560 --> 00:09:51,200
于是我们就会经过一个量化

210
00:09:51,200 --> 00:09:53,240
接着就会超越一个量化的算子

211
00:09:53,240 --> 00:09:55,040
对这个数据进行一个量化

212
00:09:55,040 --> 00:09:57,800
然后真正的去做一些卷集

213
00:09:57,800 --> 00:10:00,280
GMM的计算做一个重量化

214
00:10:00,280 --> 00:10:02,760
下次给下一个卷集算子

215
00:10:02,760 --> 00:10:04,440
接着再做一个重量化

216
00:10:04,440 --> 00:10:06,200
给我们下个卷集的算子

217
00:10:06,200 --> 00:10:08,680
在网络模型真正的输出之前

218
00:10:08,680 --> 00:10:11,640
我们肯定需要有一个反量化的

219
00:10:11,640 --> 00:10:14,440
所以量化和反量化这两个算子

220
00:10:14,440 --> 00:10:16,200
一定会有重量化

221
00:10:16,200 --> 00:10:18,280
会不会有需要决定于我们的

222
00:10:18,280 --> 00:10:20,280
网络模型的具体的形态

223
00:10:22,200 --> 00:10:25,320
大家有没有发现一个比较典型的规律

224
00:10:25,320 --> 00:10:27,760
在我们端测量化推理部署的时候

225
00:10:27,800 --> 00:10:30,960
会多了几个不同的算子

226
00:10:30,960 --> 00:10:33,640
这些算子都是实时机器的算子

227
00:10:33,640 --> 00:10:35,640
一个就是量化的算子

228
00:10:35,640 --> 00:10:37,440
一个是反量化的算子

229
00:10:37,440 --> 00:10:39,840
还有重量化的算子

230
00:10:39,840 --> 00:10:43,160
这是跟我们没有量化之前最大的区别

231
00:10:43,160 --> 00:10:45,120
需要根据这些算子

232
00:10:45,120 --> 00:10:47,680
去计算量化的公式

233
00:10:49,000 --> 00:10:50,480
第一个就是量化

234
00:10:50,480 --> 00:10:53,880
我们需要把FP32的数据量化成int8

235
00:10:53,880 --> 00:10:55,880
在脱机转换工具转换的时候

236
00:10:55,880 --> 00:10:57,440
就需要根据刚才

237
00:10:57,880 --> 00:11:00,200
不管是杆子量化训练还是训练后量化

238
00:11:00,200 --> 00:11:01,600
主要是找到x的max

239
00:11:01,600 --> 00:11:02,360
x的密码

240
00:11:02,360 --> 00:11:03,800
g的分布

241
00:11:03,800 --> 00:11:05,600
就是我们的qmax qmin也需要

242
00:11:05,600 --> 00:11:06,600
找到数据的分布

243
00:11:06,600 --> 00:11:08,600
脱机转换工具的时候

244
00:11:08,600 --> 00:11:10,880
就会去计算scale还有offset

245
00:11:10,880 --> 00:11:12,400
这两个数据很重要

246
00:11:12,400 --> 00:11:13,880
在端测推理部署的时候

247
00:11:13,880 --> 00:11:15,720
就真正温探去运行的时候

248
00:11:15,720 --> 00:11:19,920
就会根据脱机转换工具得到的一个scale跟offset

249
00:11:19,920 --> 00:11:22,320
把我们的FP32的数据转成int8

250
00:11:22,320 --> 00:11:24,760
这个就是量化的具体的公式

251
00:11:24,760 --> 00:11:25,680
量化很简单

252
00:11:25,680 --> 00:11:26,920
计算方式也很简单

253
00:11:27,040 --> 00:11:29,320
但是scale跟offset的求取

254
00:11:29,320 --> 00:11:30,800
是一个比较麻烦的事情

255
00:11:30,800 --> 00:11:32,520
它有很多种求取的方式

256
00:11:35,480 --> 00:11:37,760
接着我们看一看第二个算子

257
00:11:37,760 --> 00:11:39,040
de-quantization

258
00:11:39,040 --> 00:11:40,600
反量化

259
00:11:40,600 --> 00:11:44,720
反量化它不是把int8反量化成为FP32

260
00:11:44,720 --> 00:11:49,080
而是把int32反量化成为FP32

261
00:11:49,080 --> 00:11:52,160
因为int8不管是相加相乘

262
00:11:52,160 --> 00:11:53,520
它肯定会溢出的

263
00:11:53,520 --> 00:11:56,520
所以我们需要用int32的格式进行存储

264
00:11:57,240 --> 00:12:00,800
可以看到下面的公式就变得非常的复杂

265
00:12:00,800 --> 00:12:02,160
虽然看上去很复杂

266
00:12:02,160 --> 00:12:04,160
我们可以看到一般的卷机的计算

267
00:12:04,160 --> 00:12:05,600
就是xxw

268
00:12:05,600 --> 00:12:07,680
这个是我推导的公式

269
00:12:07,680 --> 00:12:10,120
然后推导完就可以得到我们的xscale

270
00:12:10,120 --> 00:12:11,160
然后xwscale

271
00:12:11,160 --> 00:12:12,880
然后再xint32的result

272
00:12:12,880 --> 00:12:15,800
就得到反量化我们的y的结果了

273
00:12:15,800 --> 00:12:18,000
这里面也非常欢迎大家自己去推理一下

274
00:12:19,640 --> 00:12:22,560
最后一个就是重量化

275
00:12:22,560 --> 00:12:26,800
把我们int32的数据重量化为int8

276
00:12:26,800 --> 00:12:29,640
重量化的推导公式就如下面这条公式所示

277
00:12:29,640 --> 00:12:31,800
也是我慢慢的去推导的

278
00:12:31,800 --> 00:12:34,200
但是有一个点值得注意的

279
00:12:34,200 --> 00:12:35,440
在计算公式的时候

280
00:12:35,440 --> 00:12:38,080
我们不仅仅需要当前算子

281
00:12:38,080 --> 00:12:40,200
输入input或者权重的scale

282
00:12:40,200 --> 00:12:42,920
我们更加也需要下一个op

283
00:12:42,920 --> 00:12:46,640
就下一个算子的输入的一个scale和offset

284
00:12:46,640 --> 00:12:48,680
因此在运行量化推理的过程当中

285
00:12:48,680 --> 00:12:51,680
我们确实需要到全图的信息

286
00:12:51,680 --> 00:12:53,680
也就是整个计算图的信息

287
00:12:57,800 --> 00:13:00,840
下面在正式结束之前

288
00:13:00,840 --> 00:13:01,960
我想提几个疑问

289
00:13:01,960 --> 00:13:04,080
也想引起大家的一个思考

290
00:13:04,080 --> 00:13:06,800
为什么模型量化技术

291
00:13:06,800 --> 00:13:10,360
能够对实际的部署场景起到加速的作用

292
00:13:10,360 --> 00:13:12,560
这个其实我们之前已经讲过了

293
00:13:12,560 --> 00:13:15,080
也希望大家去思考回顾一下

294
00:13:15,080 --> 00:13:18,440
第二点就是为什么我们需要对网络模型

295
00:13:18,440 --> 00:13:20,120
进行量化压缩

296
00:13:20,120 --> 00:13:23,120
也就是量化压缩到底有什么好处

297
00:13:23,120 --> 00:13:26,720
第三点就是为什么不直接训练

298
00:13:26,720 --> 00:13:29,600
低精度的网络模型

299
00:13:29,600 --> 00:13:31,640
直接训练一个int8的网络模型

300
00:13:31,640 --> 00:13:32,760
可不可以呢

301
00:13:32,760 --> 00:13:35,480
直接训练一个二值化的网络模型

302
00:13:35,480 --> 00:13:36,560
可不可以呢

303
00:13:36,560 --> 00:13:40,360
针对大模型它有千亿百亿万亿规模

304
00:13:40,360 --> 00:13:42,640
我为什么要训练一个万亿规模的大模型

305
00:13:42,640 --> 00:13:45,120
我直接训练一个十亿规模的小模型

306
00:13:45,120 --> 00:13:47,040
不就好了吗

307
00:13:47,040 --> 00:13:51,000
最后一个问题就是在什么情况下不应该

308
00:13:51,000 --> 00:13:54,880
或者在什么情况下应该使用模型量化的技术

309
00:13:54,920 --> 00:13:57,280
这个问题也是我在第一节里面

310
00:13:57,280 --> 00:13:58,840
去给大家提问过的

311
00:13:58,840 --> 00:14:02,160
也希望大家去思考思考

312
00:14:02,160 --> 00:14:03,600
好了谢谢各位

313
00:14:03,600 --> 00:14:04,600
摆了个掰

