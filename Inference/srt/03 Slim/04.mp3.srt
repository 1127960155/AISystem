0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴

1
0:00:04.560 --> 0:00:06.080
好辣呀好辣呀

2
0:00:06.080 --> 0:00:08.640
今天吃了火锅特别的辣

3
0:00:08.640 --> 0:00:09.960
那至于辣哪里呢

4
0:00:09.960 --> 0:00:11.400
大家想想就知道了

5
0:00:11.400 --> 0:00:12.600
我是ZOMI

6
0:00:12.600 --> 0:00:16.400
今天呢我们是在推丁引擎里面的模型压缩

7
0:00:16.400 --> 0:00:18.320
训练后量化就是PTQ

8
0:00:18.320 --> 0:00:19.960
还有量化完之后呢

9
0:00:19.960 --> 0:00:23.720
我们怎么把这些量化后的网络模型进行部署起来

10
0:00:25.040 --> 0:00:26.960
ZOMI讲话的语速呢会稍微快了一点

11
0:00:26.960 --> 0:00:28.840
所以大家有不懂的或者我讲不明白的

12
0:00:28.840 --> 0:00:31.800
也欢迎大家去给我私信和贪慕留言

13
0:00:31.800 --> 0:00:34.480
现在呢我们来到这个内容

14
0:00:34.480 --> 0:00:36.440
Post Training Quantumization

15
0:00:36.440 --> 0:00:38.760
训练后量化我们又叫做PTQ

16
0:00:38.760 --> 0:00:42.440
还有量化之后怎么去把它真正部署起来

17
0:00:42.440 --> 0:00:44.160
两个比较重要的内容

18
0:00:45.800 --> 0:00:49.000
首先第一个内容就是训练后量化PTQ

19
0:00:49.000 --> 0:00:52.440
实际上呢PTQ它分为Static和Dynamic

20
0:00:52.440 --> 0:00:55.120
两个一个是静态一个是动态

21
0:00:55.800 --> 0:00:57.680
我们先看一个比较简单一点的

22
0:00:57.720 --> 0:01:00.960
就是动态脱机量化PTQ Dynamic

23
0:01:01.800 --> 0:01:03.320
PTQ Dynamic其实非常简单

24
0:01:03.320 --> 0:01:05.200
也是ZOMI做的第一个量化的项目

25
0:01:05.680 --> 0:01:08.960
简单的来说就是把我们的网络模型全种了

26
0:01:08.960 --> 0:01:13.720
直接从FP32简单的硬式成为INT8,INT16

27
0:01:13.720 --> 0:01:14.440
这种方式

28
0:01:15.720 --> 0:01:19.160
最重要的目的呢就是减少我们的网络模型的大小

29
0:01:19.160 --> 0:01:20.760
为什么叫做动态呢

30
0:01:20.760 --> 0:01:22.680
是因为我们的缩放因子Scale

31
0:01:22.680 --> 0:01:24.640
是一个动态去计算出来的

32
0:01:25.640 --> 0:01:26.840
因此这种量化方式呢

33
0:01:26.840 --> 0:01:29.080
是几种量化方式里面性能最差的

34
0:01:30.280 --> 0:01:32.480
我们看一下一个简单的流程

35
0:01:33.200 --> 0:01:34.680
PTQ Dynamic的算法流程呢

36
0:01:34.680 --> 0:01:36.320
主要有三个模块

37
0:01:36.320 --> 0:01:39.000
第一个就是我们拿到一个已经训练好的网络模型了

38
0:01:39.960 --> 0:01:43.280
接着我们把这个网络模型的FP32的网络模型的全种

39
0:01:43.280 --> 0:01:47.000
直接转换成为INT8这种量化的模型

40
0:01:47.000 --> 0:01:50.200
那最后呢就输出一个已经转换成为全种

41
0:01:50.200 --> 0:01:53.400
转换成为INT8的量化的模型对外进行输出

42
0:01:55.640 --> 0:01:57.920
接下来我们来到第二个内容

43
0:01:57.920 --> 0:02:00.080
那这个内容呢还是有点意思的

44
0:02:00.080 --> 0:02:04.400
像华为上唐Ken里面的推理引擎ACL

45
0:02:04.400 --> 0:02:08.160
还有英伟达的TensorRT里面的量化模块呢

46
0:02:08.160 --> 0:02:11.920
都是采用了静态脱机量化PTQ Static

47
0:02:12.920 --> 0:02:15.760
现在我们看一下这种量化方式呢有什么不一样啊

48
0:02:15.760 --> 0:02:18.720
现在大部分的推理引擎或者推理框架

49
0:02:18.720 --> 0:02:21.840
都会采用脱机量化的这种方式

50
0:02:21.840 --> 0:02:23.400
作为里面集成的一个模块

51
0:02:23.440 --> 0:02:26.480
所以这个模块呢我们稍微简单的展开一下

52
0:02:26.480 --> 0:02:29.280
那这个静态的脱机量化同时叫做

53
0:02:29.280 --> 0:02:31.880
校正量化或者数据级量化

54
0:02:31.880 --> 0:02:35.920
因为里面使用了少量没有标签的校准数据

55
0:02:35.920 --> 0:02:38.480
所以它叫做校正量化或者数据级量化

56
0:02:38.480 --> 0:02:41.440
我们统一都理解为静态脱机量化就好了

57
0:02:41.440 --> 0:02:44.000
这也是一个学术和官方的一种叫法

58
0:02:45.000 --> 0:02:47.520
而这里面呢使用无标签的校准数据呢

59
0:02:47.520 --> 0:02:50.240
主要是用在去计算我们的scale

60
0:02:50.680 --> 0:02:53.080
通过真实场景没有标签的校准数据

61
0:02:53.840 --> 0:02:55.200
因为我们在获取scale的时候呢

62
0:02:55.200 --> 0:02:57.000
没有必要去训练或者file to lane

63
0:02:57.000 --> 0:02:59.360
我们只需要简单的运行一些正向

64
0:02:59.360 --> 0:03:03.280
这个数据呢只需要有一些真实的数据场景就好了

65
0:03:03.280 --> 0:03:05.480
我们就可以根据真实的数据场景

66
0:03:05.480 --> 0:03:08.840
去获取我们的scale 缩放因子

67
0:03:09.960 --> 0:03:13.480
在ptq static静态脱机量化里面怎么去获取scale呢

68
0:03:13.480 --> 0:03:15.480
就是算法的内核

69
0:03:15.480 --> 0:03:17.160
里面呢有可以通过mean max呢

70
0:03:17.160 --> 0:03:21.240
klt呢adm呢eq等不同的方式去展开

71
0:03:22.240 --> 0:03:26.400
那下面呢我们看一下ptq static一个算法的主要流程

72
0:03:26.400 --> 0:03:29.720
首先我们已经有一个已经训练好的网络模型

73
0:03:29.720 --> 0:03:33.040
接着呢我们获取这个网络模型的计算图

74
0:03:33.040 --> 0:03:36.160
对这个计算图呢插入一些伪量化的算子

75
0:03:36.160 --> 0:03:37.640
那这个插入伪量化的算子

76
0:03:37.640 --> 0:03:41.120
跟刚才在讲感知量化训练的时候的伪量化算子

77
0:03:41.120 --> 0:03:42.600
意义上是相同的

78
0:03:42.600 --> 0:03:44.560
但是我们没有训练的流程

79
0:03:44.560 --> 0:03:46.280
我们只有校正的流程

80
0:03:46.280 --> 0:03:48.520
既然有校正我们就有校正的数据集

81
0:03:48.520 --> 0:03:51.200
而这个数据集呢不需要带标签

82
0:03:51.200 --> 0:03:53.120
只要从真实的数据场景里面

83
0:03:53.120 --> 0:03:56.200
去获取相关的一个小部分的数据集就好了

84
0:03:56.200 --> 0:03:58.360
然后呢就给到我们的校正算法

85
0:03:58.360 --> 0:04:01.000
真正的去做一个量化的工作

86
0:04:01.000 --> 0:04:05.520
最后呢就输出一个ptq-models的网络模型

87
0:04:05.520 --> 0:04:07.520
就经过量化后的网络模型

88
0:04:07.520 --> 0:04:11.200
然后呢就给推理部署平台进行一个真正的处理

89
0:04:15.200 --> 0:04:16.920
在静态理性量化里面呢

90
0:04:16.920 --> 0:04:19.880
为了更好的去得到我们的scale

91
0:04:19.880 --> 0:04:21.680
或者去计算我们的数据分布呢

92
0:04:21.680 --> 0:04:24.640
这里面我们用了一种算法叫做KL散度

93
0:04:24.640 --> 0:04:27.000
用KL散度去校准我们的数据集

94
0:04:27.000 --> 0:04:30.640
那这里面呢我们先看看KL散度的一个原理哦

95
0:04:30.640 --> 0:04:33.680
首先呢KL散度呢它也叫做相对商

96
0:04:33.680 --> 0:04:37.360
这里面这套公式呢就是KL散度的真实的公式

97
0:04:37.360 --> 0:04:40.680
我们主要是去对比两个分布之间的差异

98
0:04:40.680 --> 0:04:42.600
其中p呢就是真实的分布

99
0:04:42.600 --> 0:04:44.360
而q呢就是预测的分布

100
0:04:44.360 --> 0:04:46.520
KL散度呢就去对比真实的分布

101
0:04:46.520 --> 0:04:49.280
跟预测的分布之间的一个近似的值

102
0:04:49.800 --> 0:04:51.200
或者它们的差异的大小

103
0:04:51.200 --> 0:04:53.560
当然了它们的差异越小越好

104
0:04:54.320 --> 0:04:56.440
诶有了这个原理之后

105
0:04:56.440 --> 0:04:58.240
那事情就变得更加简单了

106
0:04:58.240 --> 0:05:01.120
我们看一下KL散度的一个具体的算法流程

107
0:05:03.480 --> 0:05:06.160
第一步我们需要准备一些带校准的数据集

108
0:05:06.160 --> 0:05:07.960
那这些数据呢不需要有标签

109
0:05:07.960 --> 0:05:10.520
我们从真实的场景或者验证集里面

110
0:05:10.520 --> 0:05:12.480
去选择一小部分就可以了

111
0:05:13.320 --> 0:05:16.760
然后做一个正向的就是FP32的一个推理

112
0:05:16.760 --> 0:05:17.560
大家要注意哦

113
0:05:17.560 --> 0:05:19.720
是FP32没有经过任何量化的

114
0:05:19.720 --> 0:05:22.520
下面的这个Fold步骤呢才是真正的量化

115
0:05:22.520 --> 0:05:24.360
那在FP32推理的时候呢

116
0:05:24.360 --> 0:05:27.800
我们就会对每一层做一个不同的工作

117
0:05:27.800 --> 0:05:29.200
针对每一层都要做

118
0:05:29.960 --> 0:05:32.400
那这里面呢我们需要去收集

119
0:05:32.400 --> 0:05:35.040
每一层的进货值的一个值方图

120
0:05:35.040 --> 0:05:38.280
就获取我们数据输出的一个分布

121
0:05:39.240 --> 0:05:40.760
在这里面的第二步

122
0:05:40.760 --> 0:05:43.160
就是我们需要去设置几个域子

123
0:05:43.160 --> 0:05:46.600
通过不同的域子呢去获取量化后的一个分布

124
0:05:47.120 --> 0:05:50.280
这里面呢我们设计了很多不同的域子数据

125
0:05:50.280 --> 0:05:52.600
所以会得到很多不同的量化的分布

126
0:05:54.080 --> 0:05:57.240
在最后一步呢我们就真正的去计算KL散度了

127
0:05:58.360 --> 0:06:01.560
而KL散度的两个数值就是刚才的第一步

128
0:06:01.560 --> 0:06:02.600
获取真实的数据

129
0:06:03.160 --> 0:06:04.880
第二步呢就是通过域子去产生

130
0:06:04.880 --> 0:06:06.800
不同的量化后的分布

131
0:06:06.800 --> 0:06:08.200
通过比较这两步

132
0:06:08.200 --> 0:06:10.040
然后我们得到一个最小值

133
0:06:10.840 --> 0:06:13.240
原始数据的分布跟量化后的数据的分布

134
0:06:13.240 --> 0:06:15.040
是比较相似的

135
0:06:15.080 --> 0:06:17.040
那这个具体的域子和相关的参数呢

136
0:06:17.040 --> 0:06:19.400
我们就作为最优的参数值

137
0:06:19.400 --> 0:06:22.440
那这里面有几个点需要去注意一下的

138
0:06:22.440 --> 0:06:24.920
就是我们需要准备一些小批量的数据

139
0:06:24.920 --> 0:06:28.000
也就是我们刚才所说的Calibration Data Set

140
0:06:28.000 --> 0:06:30.520
需要去校准的数据集

141
0:06:30.520 --> 0:06:34.680
这里面的一般都会采用500到1000张图片左右

142
0:06:34.680 --> 0:06:35.960
就够了不用太多

143
0:06:36.600 --> 0:06:38.240
朱米之前在一个项目里面

144
0:06:38.240 --> 0:06:39.720
采用了1万多张数据集

145
0:06:39.720 --> 0:06:43.080
后来发现其实跟500多张数据集差异不太大

146
0:06:43.080 --> 0:06:45.000
所以大家不用浪费太多的时间

147
0:06:45.000 --> 0:06:47.240
去调多少张不同的数据集

148
0:06:47.240 --> 0:06:49.680
更多的可以去调一调第二步

149
0:06:49.680 --> 0:06:51.240
通过什么域子什么参数

150
0:06:51.240 --> 0:06:53.200
去产生不同的数据的分布

151
0:06:56.040 --> 0:06:59.880
下面这段尾代码就是英伟达TensorRT的一个关于KL散度

152
0:06:59.880 --> 0:07:03.920
或者静态量化后训练的一个具体的尾代码

153
0:07:03.920 --> 0:07:05.160
我们就不一一解析了

154
0:07:05.760 --> 0:07:08.240
有兴趣的同学可以上朱米的Github里面

155
0:07:08.240 --> 0:07:10.520
去获取相关的数据

156
0:07:11.520 --> 0:07:15.320
接着我们来到第二个比较重要的内容

157
0:07:15.320 --> 0:07:17.320
就是端测量化的推理部署

158
0:07:17.320 --> 0:07:19.880
我们刚才讲了很多种不同的量化方式

159
0:07:19.880 --> 0:07:24.800
但是实际上真正的量化方式应该是怎么样的呢

160
0:07:31.960 --> 0:07:34.920
我量化完之后真正的要去推理部署了

161
0:07:34.920 --> 0:07:36.600
那在端测量化推理部署里面

162
0:07:36.600 --> 0:07:39.640
其实非常非常的讲究

163
0:07:39.920 --> 0:07:41.640
在真正推理部署的时候

164
0:07:41.840 --> 0:07:43.680
其实有很多种方式

165
0:07:43.880 --> 0:07:46.880
下面我们展开三个图有三种方式

166
0:07:46.880 --> 0:07:48.520
第一种就是我的输入

167
0:07:48.520 --> 0:07:52.000
假设这个输入它不是实际上的网络模型的输入

168
0:07:52.760 --> 0:07:55.000
假设我输入的是FP32

169
0:07:55.000 --> 0:07:57.840
接着我这个算子是一个int8的算子

170
0:07:57.840 --> 0:08:01.040
我这个卷集算子对应的参数也是int8

171
0:08:01.040 --> 0:08:04.520
这个时候我对输入的时候就需要做一个量化

172
0:08:04.960 --> 0:08:07.920
把输进去的FP32的数据量化成int8

173
0:08:08.040 --> 0:08:10.080
接着去用int8进行计算

174
0:08:10.080 --> 0:08:13.600
计算完之后我们的数据输出来肯定是int32的

175
0:08:14.600 --> 0:08:16.440
假设我们的算子是一个卷集

176
0:08:16.440 --> 0:08:17.920
卷集的输入是int8

177
0:08:17.920 --> 0:08:19.240
它的权重也是int8

178
0:08:19.240 --> 0:08:21.560
如果我的输出都是int8的话

179
0:08:21.560 --> 0:08:23.000
它就会造成一个溢出

180
0:08:23.000 --> 0:08:25.800
255×255肯定超过255

181
0:08:25.800 --> 0:08:28.160
一般的输出会变成一个int32

182
0:08:28.160 --> 0:08:30.440
这个时候我们需要de-portalization

183
0:08:30.440 --> 0:08:33.960
就是反量化回FP32再给下一层输入

184
0:08:34.880 --> 0:08:38.080
另外我们看一下第二种方式

185
0:08:38.560 --> 0:08:41.440
第二种方式它的输入是FP32

186
0:08:41.440 --> 0:08:43.360
输出是int8

187
0:08:43.360 --> 0:08:46.520
我们看一下里面的一个具体的内容

188
0:08:47.760 --> 0:08:49.400
FP32输进去

189
0:08:49.400 --> 0:08:52.640
我们肯定需要进行一个量化回int8

190
0:08:52.640 --> 0:08:55.360
接着我需要去做一个具体的计算

191
0:08:55.360 --> 0:08:57.760
计算完之后我们还是变成int32

192
0:08:58.240 --> 0:09:01.640
int32之后我们会做一个重量化

193
0:09:01.640 --> 0:09:06.440
把int32的数据量化成int8

194
0:09:06.440 --> 0:09:08.920
然后下一个算子如果也是一个卷集

195
0:09:08.920 --> 0:09:12.200
这个时候就直接串起来就行了

196
0:09:12.200 --> 0:09:14.920
所以引申成为第三种方式

197
0:09:14.920 --> 0:09:17.080
假设这个数据的输出

198
0:09:17.080 --> 0:09:20.680
是第三个数据算子的输入

199
0:09:20.680 --> 0:09:22.440
我输的是int8

200
0:09:22.440 --> 0:09:26.320
卷集算子计算之后输出是int32

201
0:09:26.320 --> 0:09:30.760
然后再做一个重量化变成int8再输出

202
0:09:30.760 --> 0:09:32.760
所以在我们整个计算图

203
0:09:32.760 --> 0:09:35.320
在整个网络模型的过程当中

204
0:09:35.320 --> 0:09:38.320
会遇到三种不同的方式

205
0:09:40.320 --> 0:09:44.280
下面这个图我把这三种不同的方式串起来

206
0:09:44.280 --> 0:09:46.920
我们的网络模型的数据的输进去

207
0:09:46.920 --> 0:09:48.560
肯定是FP32的

208
0:09:48.560 --> 0:09:51.200
于是我们就会经过一个量化

209
0:09:51.200 --> 0:09:53.240
接着就会超越一个量化的算子

210
0:09:53.240 --> 0:09:55.040
对这个数据进行一个量化

211
0:09:55.040 --> 0:09:57.800
然后真正的去做一些卷集

212
0:09:57.800 --> 0:10:00.280
GMM的计算做一个重量化

213
0:10:00.280 --> 0:10:02.760
下次给下一个卷集算子

214
0:10:02.760 --> 0:10:04.440
接着再做一个重量化

215
0:10:04.440 --> 0:10:06.200
给我们下个卷集的算子

216
0:10:06.200 --> 0:10:08.680
在网络模型真正的输出之前

217
0:10:08.680 --> 0:10:11.640
我们肯定需要有一个反量化的

218
0:10:11.640 --> 0:10:14.440
所以量化和反量化这两个算子

219
0:10:14.440 --> 0:10:16.200
一定会有重量化

220
0:10:16.200 --> 0:10:18.280
会不会有需要决定于我们的

221
0:10:18.280 --> 0:10:20.280
网络模型的具体的形态

222
0:10:22.200 --> 0:10:25.320
大家有没有发现一个比较典型的规律

223
0:10:25.320 --> 0:10:27.760
在我们端测量化推理部署的时候

224
0:10:27.800 --> 0:10:30.960
会多了几个不同的算子

225
0:10:30.960 --> 0:10:33.640
这些算子都是实时机器的算子

226
0:10:33.640 --> 0:10:35.640
一个就是量化的算子

227
0:10:35.640 --> 0:10:37.440
一个是反量化的算子

228
0:10:37.440 --> 0:10:39.840
还有重量化的算子

229
0:10:39.840 --> 0:10:43.160
这是跟我们没有量化之前最大的区别

230
0:10:43.160 --> 0:10:45.120
需要根据这些算子

231
0:10:45.120 --> 0:10:47.680
去计算量化的公式

232
0:10:49.000 --> 0:10:50.480
第一个就是量化

233
0:10:50.480 --> 0:10:53.880
我们需要把FP32的数据量化成int8

234
0:10:53.880 --> 0:10:55.880
在脱机转换工具转换的时候

235
0:10:55.880 --> 0:10:57.440
就需要根据刚才

236
0:10:57.880 --> 0:11:00.200
不管是杆子量化训练还是训练后量化

237
0:11:00.200 --> 0:11:01.600
主要是找到x的max

238
0:11:01.600 --> 0:11:02.360
x的密码

239
0:11:02.360 --> 0:11:03.800
g的分布

240
0:11:03.800 --> 0:11:05.600
就是我们的qmax qmin也需要

241
0:11:05.600 --> 0:11:06.600
找到数据的分布

242
0:11:06.600 --> 0:11:08.600
脱机转换工具的时候

243
0:11:08.600 --> 0:11:10.880
就会去计算scale还有offset

244
0:11:10.880 --> 0:11:12.400
这两个数据很重要

245
0:11:12.400 --> 0:11:13.880
在端测推理部署的时候

246
0:11:13.880 --> 0:11:15.720
就真正温探去运行的时候

247
0:11:15.720 --> 0:11:19.920
就会根据脱机转换工具得到的一个scale跟offset

248
0:11:19.920 --> 0:11:22.320
把我们的FP32的数据转成int8

249
0:11:22.320 --> 0:11:24.760
这个就是量化的具体的公式

250
0:11:24.760 --> 0:11:25.680
量化很简单

251
0:11:25.680 --> 0:11:26.920
计算方式也很简单

252
0:11:27.040 --> 0:11:29.320
但是scale跟offset的求取

253
0:11:29.320 --> 0:11:30.800
是一个比较麻烦的事情

254
0:11:30.800 --> 0:11:32.520
它有很多种求取的方式

255
0:11:35.480 --> 0:11:37.760
接着我们看一看第二个算子

256
0:11:37.760 --> 0:11:39.040
de-quantization

257
0:11:39.040 --> 0:11:40.600
反量化

258
0:11:40.600 --> 0:11:44.720
反量化它不是把int8反量化成为FP32

259
0:11:44.720 --> 0:11:49.080
而是把int32反量化成为FP32

260
0:11:49.080 --> 0:11:52.160
因为int8不管是相加相乘

261
0:11:52.160 --> 0:11:53.520
它肯定会溢出的

262
0:11:53.520 --> 0:11:56.520
所以我们需要用int32的格式进行存储

263
0:11:57.240 --> 0:12:00.800
可以看到下面的公式就变得非常的复杂

264
0:12:00.800 --> 0:12:02.160
虽然看上去很复杂

265
0:12:02.160 --> 0:12:04.160
我们可以看到一般的卷机的计算

266
0:12:04.160 --> 0:12:05.600
就是xxw

267
0:12:05.600 --> 0:12:07.680
这个是我推导的公式

268
0:12:07.680 --> 0:12:10.120
然后推导完就可以得到我们的xscale

269
0:12:10.120 --> 0:12:11.160
然后xwscale

270
0:12:11.160 --> 0:12:12.880
然后再xint32的result

271
0:12:12.880 --> 0:12:15.800
就得到反量化我们的y的结果了

272
0:12:15.800 --> 0:12:18.000
这里面也非常欢迎大家自己去推理一下

273
0:12:19.640 --> 0:12:22.560
最后一个就是重量化

274
0:12:22.560 --> 0:12:26.800
把我们int32的数据重量化为int8

275
0:12:26.800 --> 0:12:29.640
重量化的推导公式就如下面这条公式所示

276
0:12:29.640 --> 0:12:31.800
也是我慢慢的去推导的

277
0:12:31.800 --> 0:12:34.200
但是有一个点值得注意的

278
0:12:34.200 --> 0:12:35.440
在计算公式的时候

279
0:12:35.440 --> 0:12:38.080
我们不仅仅需要当前算子

280
0:12:38.080 --> 0:12:40.200
输入input或者权重的scale

281
0:12:40.200 --> 0:12:42.920
我们更加也需要下一个op

282
0:12:42.920 --> 0:12:46.640
就下一个算子的输入的一个scale和offset

283
0:12:46.640 --> 0:12:48.680
因此在运行量化推理的过程当中

284
0:12:48.680 --> 0:12:51.680
我们确实需要到全图的信息

285
0:12:51.680 --> 0:12:53.680
也就是整个计算图的信息

286
0:12:57.800 --> 0:13:00.840
下面在正式结束之前

287
0:13:00.840 --> 0:13:01.960
我想提几个疑问

288
0:13:01.960 --> 0:13:04.080
也想引起大家的一个思考

289
0:13:04.080 --> 0:13:06.800
为什么模型量化技术

290
0:13:06.800 --> 0:13:10.360
能够对实际的部署场景起到加速的作用

291
0:13:10.360 --> 0:13:12.560
这个其实我们之前已经讲过了

292
0:13:12.560 --> 0:13:15.080
也希望大家去思考回顾一下

293
0:13:15.080 --> 0:13:18.440
第二点就是为什么我们需要对网络模型

294
0:13:18.440 --> 0:13:20.120
进行量化压缩

295
0:13:20.120 --> 0:13:23.120
也就是量化压缩到底有什么好处

296
0:13:23.120 --> 0:13:26.720
第三点就是为什么不直接训练

297
0:13:26.720 --> 0:13:29.600
低精度的网络模型

298
0:13:29.600 --> 0:13:31.640
直接训练一个int8的网络模型

299
0:13:31.640 --> 0:13:32.760
可不可以呢

300
0:13:32.760 --> 0:13:35.480
直接训练一个二值化的网络模型

301
0:13:35.480 --> 0:13:36.560
可不可以呢

302
0:13:36.560 --> 0:13:40.360
针对大模型它有千亿百亿万亿规模

303
0:13:40.360 --> 0:13:42.640
我为什么要训练一个万亿规模的大模型

304
0:13:42.640 --> 0:13:45.120
我直接训练一个十亿规模的小模型

305
0:13:45.120 --> 0:13:47.040
不就好了吗

306
0:13:47.040 --> 0:13:51.000
最后一个问题就是在什么情况下不应该

307
0:13:51.000 --> 0:13:54.880
或者在什么情况下应该使用模型量化的技术

308
0:13:54.920 --> 0:13:57.280
这个问题也是我在第一节里面

309
0:13:57.280 --> 0:13:58.840
去给大家提问过的

310
0:13:58.840 --> 0:14:02.160
也希望大家去思考思考

311
0:14:02.160 --> 0:14:03.600
好了谢谢各位

312
0:14:03.600 --> 0:14:04.600
摆了个掰

