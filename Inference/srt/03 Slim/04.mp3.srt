0
0:00:00.000 --> 0:00:30.000


1
0:00:30.000 --> 0:00:31.440
私信和弹幕留言

2
0:00:32.440 --> 0:00:34.440
现在呢我们来到这个内容

3
0:00:34.440 --> 0:00:37.440
Post Training Organization训练后量化

4
0:00:37.440 --> 0:00:38.640
我们又叫做PTQ

5
0:00:39.240 --> 0:00:42.440
还有量化之后怎么去把它真正部署起来

6
0:00:42.440 --> 0:00:44.040
两个比较重要的内容

7
0:00:45.840 --> 0:00:49.040
首先第一个内容就是训练后量化PTQ啊

8
0:00:49.040 --> 0:00:52.440
实际上的PTQ它分为static和dynamic

9
0:00:52.440 --> 0:00:55.040
两个一个是静态一个是动态

10
0:00:55.840 --> 0:00:59.960
我们先看一个比较简单一点的就是动态离线量化PTQ

11
0:01:00.000 --> 0:01:01.200
dynamic

12
0:01:01.200 --> 0:01:05.200
嗯PTQ等等其实非常简单也是综艺做的第一个量化的项目

13
0:01:05.200 --> 0:01:10.200
呃简单的来说就是把我们的网络模型全重了直接从FP三十二

14
0:01:10.200 --> 0:01:15.200
简单地映射成为英特巴英特十六这种方式

15
0:01:15.200 --> 0:01:19.200
最重要的目的呢就是减少我们的网络模型的大小

16
0:01:19.200 --> 0:01:22.200
为什么叫做动态呢是因为我们的缩放音质scale

17
0:01:22.200 --> 0:01:25.200
是一个动态去计算出来的

18
0:01:25.200 --> 0:01:29.200
因此这种量化方式呢是几种量化方式里面性能最差的

19
0:01:30.200 --> 0:01:33.200
我们看一下一个简单的流程

20
0:01:33.200 --> 0:01:36.200
PQ dynamic的算法流程呢主要有三个模块

21
0:01:36.200 --> 0:01:39.200
第一个就是我们拿到一个已经训练好的网络模型了

22
0:01:39.200 --> 0:01:47.200
接着我们把这个网络模型的FP三十二的网络模型的全重直接转换成为英特巴这种量化的模型

23
0:01:47.200 --> 0:01:54.200
那最后呢就输出一个已经转换成为全重转换成为英特巴的量化的模型对外进行输出

24
0:01:56.200 --> 0:01:58.200
接下来我们来到第二个内容

25
0:01:58.440 --> 0:02:00.440
这个内容呢还是有点意思的

26
0:02:00.440 --> 0:02:12.440
像像华为生成片里面的推理引擎SL还有英伟达的天赛阿基里面的量化模块呢都是采用了静态离线量化PQ static

27
0:02:13.440 --> 0:02:16.440
现在我们看一下这种量化方式呢有什么不一样啊

28
0:02:16.440 --> 0:02:23.440
现在大部分的推理引擎或者推理框架都会采用离线量化的这种方式作为里面集成的一个模块

29
0:02:23.440 --> 0:02:26.440
所以这个模块呢我们稍微简单的展开一下

30
0:02:26.440 --> 0:02:31.680
那这个静态的离线量化同时叫做状态量化或者数据级量化

31
0:02:31.680 --> 0:02:38.680
英伟里面使用了少量没有标签的校准数据所以它叫做状态量化或者数据级量化

32
0:02:38.680 --> 0:02:43.680
我们统一都理解为静态离线量化就好了这也是一个学术和官方的一种叫法

33
0:02:44.680 --> 0:02:53.680
而这里面呢使用无标签的校准数据呢主要是用在去计算我们的scale通过真实场景没有标签的校准数据

34
0:02:53.680 --> 0:02:58.920
因为我们在获取scale的时候呢没有必要去训练或者发掘类我们只需要简单的执行一些正项

35
0:02:58.920 --> 0:03:08.920
这个数据呢只需要有一些真实的数据场景就好了我们就可以根据真实的数据场景去获取我们的scale所放英子

36
0:03:09.920 --> 0:03:20.920
在ptq static静态离线量化里面怎么去获取scale呢就是算法的核心里面呢有可以通过memex呢klt呢edm呢eq等不同的方式去展开

37
0:03:21.160 --> 0:03:29.160
那下面我们看一下ptq static一个算法的主要流程首先我们已经有一个已经训练好的网络模型

38
0:03:29.160 --> 0:03:35.160
接着呢我们获取这个网络模型的计算图对这个计算图呢插入一些微量化的算子

39
0:03:35.160 --> 0:03:48.160
那这个插入微量化的算子跟刚才在讲感知量化训练的时候的微量化算子意义上是相同的但是我们没有训练的流程我们只有校正的流程既然有校正我们就有校正的数据集

40
0:03:48.400 --> 0:04:00.400
而这个数据集呢不需要带标签只要从真实的数据场景里面去获取相关的一个小部分的数据就好了然后呢就给到我们的校正算法真正的去做一个量化的工作

41
0:04:01.400 --> 0:04:10.400
最后呢就输出一个ptq的models的网络模型就经过量化后的网络模型然后呢就给推理部署平台进行一个真正的处理

42
0:04:10.400 --> 0:04:26.640
在静态理性量化里面呢为了更好地去得到我们的scale或者去计算我们的数据分布呢这里面我们用了一种算法叫做KO散度用KO散度呢去校准我们的数据集

43
0:04:26.640 --> 0:04:39.640
那这里面呢我们先看看KO散度的一个原理哦首先呢KO散度呢它也叫做相对商这里面这套公司呢就是KO散度的真实的公司我们主要是去对比两个分布之间的差异

44
0:04:39.880 --> 0:04:52.880
其中p呢就是真实的分布而q呢就是预测的分布KO散度呢就去对比真实的分布跟预测的分布之间的一个近似的值或者它们的差异的大小当然了它们的差异越小越好

45
0:04:53.880 --> 0:05:00.880
哎有了这个原理之后那事情就变得更加简单了我们看一下KO散度的一个具体的算法流程

46
0:05:01.880 --> 0:05:08.880
第一步我们需要准备一些带校准的数据那这些数据呢不需要有标签我们从真实的场景或者验证

47
0:05:09.640 --> 0:05:37.880
集里面去选择一小部分就可以了然后做一个正向的就是IP三十二的一个推理大家要注意哦是IP三十二没有经过任何量化的下面的这个复的步骤呢才是真正量化那在IP三十二推理的时候呢我们就会对每一层做一个不同的工作针对每一层都要做哦那这里面呢我们需要去收集每一层的净货值的一个值方图就获取我们数据输出的一个分布

48
0:05:37.880 --> 0:06:07.880
在这里面的第二步就是我们需要去设定几个率子通过不同的率子呢去获取量化后的一个分布这边呢我们设计了很多不同的率子数据所以呢会得到很多不同的量化的分布在最后一步呢我们就真正的去计算KO散度了而KO散度的两个数值就是刚才的第一步获取真实的数据第二步呢就是通过率子去产生不同的量化后的分布通过比较这两个

49
0:06:07.880 --> 0:06:37.880
步然后我们得到一个最小值原始数据的分布跟量化后的数据的分布是比较相似的那这个具体的率子和相关的参数呢我们就作为最优的参数值那这里面有几个点需要去注意一下的就是我们需要准备一些小批量的数据也就是我们刚才所说的calculation data set需要去校准的数据集这里面的一般啊都会采用五百到一千张图片左右就够了不用太多

50
0:06:37.880 --> 0:07:07.880
这里面的采用了一万多张数据集后来发现其实跟五百多张数据差异不太大所以大家不用浪费太多的时间去调多少张不同的数据集更多的可以去调一调第二步通过什么率子什么参数去产生不同的数据的分布下面这段伪代码就是英伟达天塞阿梯的一个关于KO散度或者或者静态量化后训练的一个具体的伪代码我们就不一解析了有兴趣的同学呢可以上中米的github

51
0:07:07.880 --> 0:07:37.880
里面去获取相关的资料接着我们来到第二个比较重要的内容就是端测量化的推理部署我们刚才讲了很多种不同的量化方式但是但是实际上真正的量化方式应该是怎么样的呢我量化完之后真正的要去推理部署了那在端测量化推理部署里面呢其实

52
0:07:37.880 --> 0:08:07.880
非常非常的讲究在真正推理部署的时候呢其实有很多种方式啊下面我们呢展开三个图有三种方式第一种就是我的输入假设这个输入呢它不是实际上的网络模型的输入假设我输入的是FP三的时候接着我这个算子是一个英特巴的算子我这个转机算子对应的参数也是英特巴这个时候呢我对输入的时候呢就需要做一个量化把输进去的FP三时候的数据呢量化成英特巴

53
0:08:07.880 --> 0:08:37.880
接着去用英特巴进行计算那计算完之后呢我们的数据输出来肯定是英特三时候的假设我们的算子呢是一个转机转机的输入是英特巴它的权重也是英特巴如果我的输出都是英特巴的话它就会造成一个溢出二五五乘二五五肯定超过二五五一般的输出呢会变成一个英特三时候这个时候我们需要de-optimization就是反量化回FP三时候再给下一层输入另外呢我们看一下第二种方式

54
0:08:37.880 --> 0:09:07.880
那第二种方式呢它的输入是FP三时候输出是英特巴我们看一下里面的一个具体的内容FP三时候输进去我们肯定需要进行一个量化回英特巴接着呢我需要去做一个具体的计算计算完之后呢我们还是变成英特三时候英特三时候之后我们会做一个重量化把英特三时候的数据呢把英特三时候的数据呢量化成英特巴然后下一个算子

55
0:09:07.880 --> 0:09:37.880
也是一个卷机那这个时候呢就直接串起来就行了所以影响成为第三种方式假设这个数据的输出是这个第三个数据算子的输入那我输的是英特巴卷机算子计算之后呢输出是英特三时候然后呢再做一个重量化变成英特巴再输出所以在我们整个计算图在整个网络模型的过程当中会遇到三种不同的方法

56
0:09:37.880 --> 0:10:07.880
方式下面这个图呢我把这三种不同的方式串起来我们的网络模型的数据的输进去肯定是二批三时候的于是呢我们就会经过一个量化接着呢就会超越一个量化的算子对这个数据呢进行一个量化然后真正的去做一些卷机啊纪念的计算做一个重量化下次呢下一个卷机算子接着呢再做一个重量化给我们下个卷机的算子

57
0:10:07.880 --> 0:10:37.880
输出之前我们肯定需要有一个反量化的所以量化和反量化这两个算子一定会有重量化会不会有需要决定于我们的网络模型的具体的形态哎大家有没有发现一个比较典型的规律在我们端次量化推理部署的时候机上呢会多了几个不同的算子那这些算子呢都是实实继继的算子哦一个就是量化的算子一个是反量化的算子

58
0:10:37.880 --> 0:11:07.880
重量化的算子这是跟我们没有量化之前最大的区别需要根据这些算子去计算量化的公式第一个呢就是量化我们需要把IP三时候的数据量化成int八在离线转换工具转换的时候就需要根据刚才不管是杆子量化训练还是训练后量化呢主要是找到x的max的x的密码据的分布就是我们的qmaxq密也需要找到数据的分布呢

59
0:11:07.880 --> 0:11:37.880
这个时候呢就会去计算scale还有offset那这两个数据呢很重要在端次推理部署的时候就真正温探去执行的时候呢就会根据离线转换工具得到的一个scale跟offset呢把我们的IP三时候的数据转成int八那这个呢就是量化的具体的公式量化很简单计算方式也很简单但是呢scale跟offset的求取是一个比较麻烦的事情它有很多种求取的方式接着我们看看第二个算子

60
0:11:37.880 --> 0:12:07.880
quantization反量化反量化呢它不是把int八反量化成为IP三时候而是把int三时候呢反量化成为IP三时候因为int八不管是相加相乘啊它肯定会溢出的所以我们需要用int三时候的格式呢进行存储可以看到下面的公式就变得非常的复杂虽然看上去很复杂啊我们可以看到一般的转机的计算呢就是x乘以w而这个呢是我推导的公式

61
0:12:07.880 --> 0:12:37.880
然后推导完呢就可以得到我们的scale然后乘以wscale然后再乘以int三时候的without就得到反量化我们的y的结果了这里面也非常欢迎大家自己去推理一下最后一个就是从量化把我们int三时候的数据呢从量化为int八从量化的推导公式呢就如下面这条公式所示也是我慢慢地去推导的但是呢有一个点值得注意的在计算公式的时候呢我们不仅仅需要当前算

62
0:12:37.880 --> 0:13:07.880
字输入或者权重的scale我们更加也需要下一个op就下一个算子的输入的一个scale和offset因此呢在执行量化推理的过程当中呢我们确实需要到全图的信息也就是整个计算图的信息下面呢在正式结束之前呢我想提几个疑问也想引起大家的一个思考为什么模型量化技术能够对实际的

63
0:13:07.880 --> 0:13:37.880
部署场景起到加速的作用那这个其实我们之前已经讲过了也希望大家去思考回顾一下第二点就是为什么我们需要对网络模型进行量化压缩呢也就是量化压缩到底有什么好处第三点呢就是为什么不直接训练低精度的网络模型直接训练一个int八的网络模型可不可以呢直接训练一个二子化的网络模型可不可以呢那针对大模型

64
0:13:37.880 --> 0:14:07.880
它有千亿百亿万亿规模我为啥训练一个万亿规模的大模型啊我直接训练一个十亿规模的小模型不就好了吗最后一个问题呢就是在什么情况下不应该或者在什么情况下应该使用模型量化的技术那这个问题呢也是我在第一节里面去给大家提问过的也希望大家去思考思考


