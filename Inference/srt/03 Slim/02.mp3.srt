0
0:00:00.000 --> 0:00:04.380
Baba Baba Baba Baba

1
0:00:04.380 --> 0:00:07.100
嗨大家好我是周米

2
0:00:07.100 --> 0:00:08.500
卷王来了卷王来了

3
0:00:08.500 --> 0:00:11.500
那我们今天呢给大家带来一个新的内容

4
0:00:11.500 --> 0:00:15.260
就是推理引擎或者推理系统里面的模型压缩

5
0:00:15.260 --> 0:00:19.060
也可以叫模型小型化呀或者模型轻量化这个工作

6
0:00:19.060 --> 0:00:22.900
那今天呢我们主要是给大家去带来一个新的内容

7
0:00:22.900 --> 0:00:24.900
模型的量化

8
0:00:26.100 --> 0:00:28.860
回到我们今天课程的主角

9
0:00:28.860 --> 0:00:33.660
我们今天的课程呢主要是去讲讲低比特量化

10
0:00:33.660 --> 0:00:35.360
那在低比特量化里面呢

11
0:00:35.360 --> 0:00:39.660
周米呢可能会分开三节课给大家介绍的

12
0:00:41.160 --> 0:00:42.360
因为内容有点多

13
0:00:42.360 --> 0:00:45.060
所以我把低比特量化呢采访成三节课

14
0:00:45.060 --> 0:00:48.360
每节课呢尽量控制在十分钟以内

15
0:00:48.360 --> 0:00:50.460
让大家听得比较愉快

16
0:00:50.460 --> 0:00:51.960
心情比较愉悦

17
0:00:51.960 --> 0:00:55.260
那第一个呢就是最基本的量化基础啊

18
0:00:55.260 --> 0:00:56.360
Basic Concept

19
0:00:56.360 --> 0:00:58.660
第二个呢就是量化的原理

20
0:00:58.660 --> 0:01:02.960
去看看具体的量化的算法是怎幺实现的

21
0:01:02.960 --> 0:01:05.160
到底低比特指的是啥

22
0:01:05.160 --> 0:01:07.260
低到哪个程度

23
0:01:07.260 --> 0:01:10.360
那接着呢我们将会在第二节课去讲讲

24
0:01:10.360 --> 0:01:11.560
QAT

25
0:01:11.560 --> 0:01:13.360
Portalization Area Training

26
0:01:13.360 --> 0:01:14.660
杆子量化训练

27
0:01:14.660 --> 0:01:17.660
我们一般都叫QAT杆子量化

28
0:01:17.660 --> 0:01:18.960
为啥叫杆子呢

29
0:01:18.960 --> 0:01:21.560
是因为我们在训练的过程当中

30
0:01:21.560 --> 0:01:24.760
去感知到我们需要量化的程度

31
0:01:24.760 --> 0:01:26.960
那接着呢在最后一节课

32
0:01:27.060 --> 0:01:29.860
我们会去讲讲训练后量化

33
0:01:29.860 --> 0:01:32.060
PTQ Post Training

34
0:01:32.060 --> 0:01:33.360
Quantization

35
0:01:33.360 --> 0:01:36.360
训练完之后再做量化

36
0:01:36.360 --> 0:01:39.760
那在第三节课我们还有一个很重要的内容要补充的

37
0:01:39.760 --> 0:01:42.260
就是量化的部署

38
0:01:42.260 --> 0:01:45.460
不管我们前面的算法多复杂多炫酷也好

39
0:01:45.460 --> 0:01:47.460
最终我们都离不开一个话题

40
0:01:47.460 --> 0:01:51.760
怎幺把我们的网络模型经过量化后的部署起来

41
0:01:51.760 --> 0:01:54.960
所以这个话题这个内容也是非常重要

42
0:01:55.060 --> 0:01:56.660
而我搜了很多文章

43
0:01:56.660 --> 0:01:59.760
确实很少会把这些所有的内容串起来

44
0:02:01.460 --> 0:02:04.660
现在呢我们开始正式的内容

45
0:02:04.660 --> 0:02:08.260
首先第一个就是量化的基础哦

46
0:02:08.260 --> 0:02:09.360
上面这段废话呢

47
0:02:09.360 --> 0:02:11.460
我就不跟大家一起去读了

48
0:02:11.460 --> 0:02:12.260
我们的数字呢

49
0:02:12.260 --> 0:02:15.260
在计算机里面其实有很多种表示方式

50
0:02:15.260 --> 0:02:15.860
那这里面呢

51
0:02:15.860 --> 0:02:18.060
一般我们都会有Fp32

52
0:02:18.060 --> 0:02:19.860
还有Fp16

53
0:02:19.860 --> 0:02:20.760
Int32

54
0:02:20.760 --> 0:02:21.360
Int16

55
0:02:21.360 --> 0:02:22.560
还有Int8

56
0:02:22.560 --> 0:02:24.460
一般呢都会有这些指令

57
0:02:24.460 --> 0:02:26.760
那我们可以看到像Fp32呢

58
0:02:26.760 --> 0:02:30.560
是我们一般用来去做训练的一个精度

59
0:02:31.460 --> 0:02:32.060
很多时候呢

60
0:02:32.060 --> 0:02:33.860
我们会开启混合精度

61
0:02:33.860 --> 0:02:35.060
那所谓的混合精度呢

62
0:02:35.060 --> 0:02:39.660
一般是把Fp32跟Fp16混合到一起去训练的

63
0:02:39.660 --> 0:02:40.560
因为Fp16呢

64
0:02:40.560 --> 0:02:42.760
它这样的内存位置或者占用的地址空间呢

65
0:02:42.760 --> 0:02:44.260
确实少了很多

66
0:02:45.860 --> 0:02:48.260
而我们一般值的模型量化呢

67
0:02:48.260 --> 0:02:50.760
除了把Fp32降成Fp16

68
0:02:50.760 --> 0:02:53.060
更多的是降到Int16

69
0:02:53.060 --> 0:02:53.660
Int8

70
0:02:53.660 --> 0:02:58.460
甚至Int4更低比特的一种表示

71
0:02:58.460 --> 0:02:59.060
那这种呢

72
0:02:59.060 --> 0:03:02.060
叫做模型量化的技术

73
0:03:02.060 --> 0:03:08.060
把Fp32 32位的比特转换成为更低比特的方式或者技术

74
0:03:08.060 --> 0:03:10.360
我们叫做模型量化

75
0:03:10.360 --> 0:03:10.960
接下来呢

76
0:03:10.960 --> 0:03:13.860
我们看一下神经网络有什幺特点

77
0:03:13.860 --> 0:03:14.760
那右边这个图呢

78
0:03:14.760 --> 0:03:16.460
我们已经看了好多遍了

79
0:03:16.460 --> 0:03:16.860
其实呢

80
0:03:16.860 --> 0:03:18.460
我们神经网络一般来说呢

81
0:03:18.460 --> 0:03:21.560
我们训练的时候数据的参数量会很大

82
0:03:21.560 --> 0:03:21.960
第二个

83
0:03:21.960 --> 0:03:24.160
我们的计算量也会很大

84
0:03:24.160 --> 0:03:27.760
第三个就是占用很大的内存空间

85
0:03:27.760 --> 0:03:30.160
而占用这幺多内存空间都这幺大

86
0:03:30.160 --> 0:03:33.860
为的就是得到一个高精度的模型

87
0:03:33.860 --> 0:03:34.460
但是呢

88
0:03:34.460 --> 0:03:36.560
我们要把神经网络部署起来

89
0:03:36.560 --> 0:03:38.260
所以我们希望这个网络模型呢

90
0:03:38.260 --> 0:03:39.360
越小越好

91
0:03:39.360 --> 0:03:39.860
于是呢

92
0:03:39.860 --> 0:03:43.060
研究者就发现了网络模型的量化的技术

93
0:03:43.060 --> 0:03:46.860
那我们现在首先先不要去谈谈模型量化到底做了什幺

94
0:03:46.860 --> 0:03:50.360
我们去看看模型量化的一些好处

95
0:03:50.460 --> 0:03:52.260
那这个呢是我总结了几个点

96
0:03:52.260 --> 0:03:55.360
第一个点就是保持相同的精度

97
0:03:55.360 --> 0:03:56.960
我们的神经网络训练的时候呢

98
0:03:56.960 --> 0:03:57.860
一般来说啊

99
0:03:57.860 --> 0:04:00.060
对噪声不是说非常的敏感

100
0:04:00.060 --> 0:04:02.860
因为里面有非常大量的用于的参数

101
0:04:02.860 --> 0:04:04.660
而且都是可学习的参数

102
0:04:04.660 --> 0:04:05.060
第二个呢

103
0:04:05.060 --> 0:04:07.160
就是加速运算了

104
0:04:07.160 --> 0:04:11.260
因为大家都知道像传统的卷迹神经网络里面的卷迹的操作

105
0:04:11.260 --> 0:04:14.460
一般都是用IP32的浮点数去运算的

106
0:04:14.460 --> 0:04:15.960
越低比特的运算呢

107
0:04:15.960 --> 0:04:18.160
它确实能够减少运算的功耗

108
0:04:18.160 --> 0:04:20.660
而且性能更高

109
0:04:20.660 --> 0:04:21.360
那第三点呢

110
0:04:21.360 --> 0:04:23.260
就是节省内存

111
0:04:23.260 --> 0:04:26.460
我从IP32存储到INT8、INT4

112
0:04:26.460 --> 0:04:27.360
越低的精度

113
0:04:27.360 --> 0:04:29.360
我所占用的地址空间和内存空间呢

114
0:04:29.360 --> 0:04:31.760
肯定是越小的

115
0:04:31.760 --> 0:04:32.260
下面呢

116
0:04:32.260 --> 0:04:36.460
我们看一下模型量化的五个特点

117
0:04:36.460 --> 0:04:36.960
第一个呢

118
0:04:36.960 --> 0:04:40.960
就是我们对神经网络模型的参数进行压缩

119
0:04:40.960 --> 0:04:42.360
把以前200兆的模型呢

120
0:04:42.360 --> 0:04:43.960
可能压缩到只有50兆

121
0:04:43.960 --> 0:04:46.460
而且我们可以提升运算的速度

122
0:04:46.460 --> 0:04:47.960
另外还可以降低我们的内存

123
0:04:47.960 --> 0:04:48.960
降低我们的功耗

124
0:04:48.960 --> 0:04:50.460
提升芯片的面积

125
0:04:50.460 --> 0:04:51.160
那这里面呢

126
0:04:51.160 --> 0:04:54.160
只是对我们上面进行一个简单的总结

127
0:04:54.160 --> 0:04:58.760
接下来我们看一下量化技术落地的三大挑战

128
0:04:58.760 --> 0:05:02.660
现在我们来看一下量化技术落地的三大挑战呢

129
0:05:02.660 --> 0:05:05.260
其中第一个就是精度的挑战

130
0:05:05.260 --> 0:05:05.960
首先呢

131
0:05:05.960 --> 0:05:08.860
我们现在的量化方法大部分都是线性的量化

132
0:05:08.860 --> 0:05:09.560
另外的话

133
0:05:09.560 --> 0:05:11.160
我们的模型比特越低

134
0:05:11.160 --> 0:05:12.160
精度损失越大

135
0:05:12.160 --> 0:05:12.860
我们的任务呢

136
0:05:12.860 --> 0:05:13.860
不同任务里面

137
0:05:13.860 --> 0:05:15.560
精度损失也是不一样的

138
0:05:15.560 --> 0:05:16.660
而且模型越小

139
0:05:16.660 --> 0:05:19.460
确实对我们的损失精度影响也是很大

140
0:05:19.460 --> 0:05:22.060
所以第一大的挑战就是精度

141
0:05:22.060 --> 0:05:25.260
我们还是希望能够保持相同的精度

142
0:05:25.260 --> 0:05:28.760
那第二个比较大的挑战就是硬件的支持程度

143
0:05:28.760 --> 0:05:31.160
因为不同的硬件提供的比特数

144
0:05:31.160 --> 0:05:32.660
或者比特的指令集呢

145
0:05:32.660 --> 0:05:33.660
是不同的

146
0:05:33.660 --> 0:05:35.760
需要做很多的对于kernel

147
0:05:35.760 --> 0:05:39.360
或者根据指令集进行一些优化

148
0:05:39.360 --> 0:05:43.260
那最后一点就是软件算法是否真的能够加速

149
0:05:43.260 --> 0:05:45.260
这是一个悖论哦

150
0:05:45.260 --> 0:05:47.460
我们虽然网络模型经过压缩了

151
0:05:47.460 --> 0:05:49.760
但是要是硬件指令不支持的情况

152
0:05:49.760 --> 0:05:50.960
我在运算的时候呢

153
0:05:50.960 --> 0:05:53.360
还是要反量化量化回FP30哦

154
0:05:53.360 --> 0:05:53.960
这个时候呢

155
0:05:53.960 --> 0:05:56.460
我们就需要插入很多cast算子

156
0:05:56.460 --> 0:05:58.860
那这会影响我们kernel的执行的性能

157
0:05:58.860 --> 0:06:03.160
所以说软件算法是否能够真正得到一个加速呢

158
0:06:03.160 --> 0:06:04.160
这是一个疑问哦

159
0:06:06.860 --> 0:06:07.460
下面呢

160
0:06:07.460 --> 0:06:09.360
我想提出几个问题

161
0:06:09.360 --> 0:06:12.060
引起大家的一个一起去思考

162
0:06:12.060 --> 0:06:13.760
大家也可以停下来几分钟

163
0:06:13.860 --> 0:06:15.360
去思考一下这些问题

164
0:06:15.360 --> 0:06:16.060
第一个就是

165
0:06:16.060 --> 0:06:20.760
为什幺模型量化技术能够对实际的部署起到真正的作用

166
0:06:20.760 --> 0:06:22.060
虽然我们有很多挑战

167
0:06:22.060 --> 0:06:27.060
但确实模型量化能够真正的加速我们的部署的速度

168
0:06:27.060 --> 0:06:27.660
那第二个呢

169
0:06:27.660 --> 0:06:31.660
就是为什幺需要对网络模型进行量化压缩呢

170
0:06:31.660 --> 0:06:33.760
量化压缩到底有什幺好处

171
0:06:33.760 --> 0:06:35.060
那第三点就是

172
0:06:35.060 --> 0:06:40.460
为什幺我们不直接训练的时候就训练低比特精度的模型呢

173
0:06:40.460 --> 0:06:41.060
哎

174
0:06:41.060 --> 0:06:42.260
这个很有意思

175
0:06:42.260 --> 0:06:43.860
对于大模型也是

176
0:06:43.860 --> 0:06:45.160
我大模型训练的时候呢

177
0:06:45.160 --> 0:06:47.860
动作就百亿级别千亿级别万亿级别的模型

178
0:06:47.860 --> 0:06:49.260
模型参数量这幺大

179
0:06:49.260 --> 0:06:52.160
为什幺你不直接训练一个小模型呢

180
0:06:52.160 --> 0:06:54.160
最后一点就是

181
0:06:54.160 --> 0:06:55.460
在什幺情况下

182
0:06:55.460 --> 0:07:00.760
我们不应该还是应该去使用我们的模型量化技术呢

183
0:07:00.760 --> 0:07:04.060
那这四个点我希望大家停下来去思考一下

184
0:07:13.160 --> 0:07:13.660
现在呢

185
0:07:13.660 --> 0:07:17.660
我要给大家去汇报一下量化原理了

186
0:07:17.660 --> 0:07:18.360
首先呢

187
0:07:18.360 --> 0:07:20.260
量化的方法方法呢

188
0:07:20.260 --> 0:07:23.760
现在来看的一般分为三大种

189
0:07:23.760 --> 0:07:27.660
那第一种就是量化训练QAT

190
0:07:27.660 --> 0:07:28.860
它最主要的方式呢

191
0:07:28.860 --> 0:07:33.060
就是让我们在网络模型训练的时候去感知量化运算

192
0:07:33.060 --> 0:07:36.260
到底对我们的网络模型精度带来什幺样的影响

193
0:07:36.260 --> 0:07:36.960
然后呢

194
0:07:36.960 --> 0:07:41.160
通过Fine Tuning去降低我们整个量化的误差

195
0:07:41.160 --> 0:07:41.660
接着呢

196
0:07:41.660 --> 0:07:43.560
我们有PTQ

197
0:07:43.560 --> 0:07:46.160
那PTQ就是我们的离线量化

198
0:07:46.160 --> 0:07:49.160
离线量化又分为动态和静态

199
0:07:49.160 --> 0:07:50.060
那一般来说呢

200
0:07:50.060 --> 0:07:52.160
我们会用静态会比较多

201
0:07:52.160 --> 0:07:52.860
动态呢

202
0:07:52.860 --> 0:07:53.960
其实比较简单

203
0:07:53.960 --> 0:07:56.560
就是我们直接把已经训练好的网络模型呢

204
0:07:56.560 --> 0:08:02.860
全种直接从FV3的时候转成或者应试成int8和flop16

205
0:08:02.860 --> 0:08:03.660
这种类型呢

206
0:08:03.660 --> 0:08:04.960
进行一个推理

207
0:08:04.960 --> 0:08:06.260
而静态量化呢

208
0:08:06.260 --> 0:08:09.660
可能会使用少量或者没有标签的数据

209
0:08:09.660 --> 0:08:11.060
进行一个校准

210
0:08:11.460 --> 0:08:13.760
那校准的方法有非常的多

211
0:08:13.760 --> 0:08:14.360
那现在呢

212
0:08:14.360 --> 0:08:17.660
用的比较多的就是KL散度

213
0:08:17.660 --> 0:08:19.460
我们往下看一个图

214
0:08:19.460 --> 0:08:22.260
更好的去理解这三种具体的算法

215
0:08:22.260 --> 0:08:24.060
首先就是感知量化QAT

216
0:08:24.060 --> 0:08:26.460
我们有一个训练好的网络模型

217
0:08:26.460 --> 0:08:27.560
对这个网络模型呢

218
0:08:27.560 --> 0:08:30.560
进行一个转换超越一些伪量化的算子

219
0:08:30.560 --> 0:08:31.060
接着呢

220
0:08:31.060 --> 0:08:32.560
得到一个新的网络模型

221
0:08:32.560 --> 0:08:35.060
对新的网络模型进行Fine Tuning

222
0:08:35.060 --> 0:08:35.760
最后呢

223
0:08:35.760 --> 0:08:40.360
得到真正量化后或者感知量化后的一个网络模型

224
0:08:40.360 --> 0:08:40.960
接着呢

225
0:08:40.960 --> 0:08:42.260
去给部署端的

226
0:08:44.160 --> 0:08:45.360
静态离线量化呢

227
0:08:45.360 --> 0:08:47.860
首先我们有一个训练好的网络模型

228
0:08:47.860 --> 0:08:50.460
我们有一堆训练的数据

229
0:08:50.460 --> 0:08:51.860
这个是Training Data

230
0:08:51.860 --> 0:08:52.560
改错了

231
0:08:52.560 --> 0:08:55.460
有一堆训练的数据或者等待训练的数据

232
0:08:55.460 --> 0:08:56.060
然后呢

233
0:08:56.060 --> 0:08:58.760
去给我们的网络模型进行一个校准

234
0:08:58.760 --> 0:08:59.960
那这个校准的方法呢

235
0:08:59.960 --> 0:09:02.760
有可能会用KL散度或者其他的方式

236
0:09:02.760 --> 0:09:03.460
最后呢

237
0:09:03.460 --> 0:09:07.560
就是PTQ的Dynamic动态离线量化

238
0:09:07.560 --> 0:09:09.560
我们输一个神级网络模型

239
0:09:09.560 --> 0:09:09.860
然后呢

240
0:09:09.860 --> 0:09:11.660
对这个神级网络模型进行转换

241
0:09:11.660 --> 0:09:14.560
最后得到我们转换后或者量化后的网络模型

242
0:09:14.560 --> 0:09:16.160
主要是由这三种方法

243
0:09:17.460 --> 0:09:17.960
那现在呢

244
0:09:17.960 --> 0:09:20.760
我们看一下这三种方法有什幺区别

245
0:09:20.760 --> 0:09:23.060
做一个简单的比较了

246
0:09:23.060 --> 0:09:24.160
量化训练呢

247
0:09:24.160 --> 0:09:27.960
确实我们的进度损失是比较少的

248
0:09:27.960 --> 0:09:31.160
但是它的缺点就是需要使用大量带标签的数据

249
0:09:31.160 --> 0:09:33.460
进行Fine Tuning或者一个重训的

250
0:09:33.460 --> 0:09:36.760
那第二种方式就是静态离线量化PTQ

251
0:09:36.760 --> 0:09:38.160
它的好处就是

252
0:09:38.260 --> 0:09:40.460
进度损失也是比较小的

253
0:09:40.460 --> 0:09:41.660
但是不能说没有哦

254
0:09:41.660 --> 0:09:44.560
因为它缺乏了训练或者Fine Tuning的步骤

255
0:09:44.560 --> 0:09:48.860
它需要有一些少量无标签的数据进行校准的

256
0:09:48.860 --> 0:09:52.560
最后一种就是动态离线量化就是PTQ了

257
0:09:52.560 --> 0:09:54.360
它这种方式进度的损失

258
0:09:54.360 --> 0:09:56.760
一般来说我们不可控

259
0:09:56.760 --> 0:09:58.060
不过好处呢

260
0:09:58.060 --> 0:10:00.560
就是它没有任何使用的约束

261
0:10:00.560 --> 0:10:01.960
你想咋用就咋用

262
0:10:01.960 --> 0:10:02.360
这个呢

263
0:10:02.360 --> 0:10:03.460
也是ZOMBIE呢

264
0:10:03.460 --> 0:10:05.360
在MASKBALL里面做的其中一个项目

265
0:10:05.360 --> 0:10:07.360
做得非常的深入了

266
0:10:07.360 --> 0:10:09.660
这可能导致我引用的额外的知识呢

267
0:10:09.660 --> 0:10:10.460
稍微有点多了

268
0:10:11.860 --> 0:10:12.360
现在呢

269
0:10:12.360 --> 0:10:15.660
我们正式的来到量化的一个原理

270
0:10:17.660 --> 0:10:18.160
首先呢

271
0:10:18.160 --> 0:10:20.060
我们来理解一下网络模型的优化

272
0:10:20.060 --> 0:10:22.960
它主要是在我们定点跟浮点数之间呢

273
0:10:22.960 --> 0:10:26.960
创建了一个简单的数据或者数学的硬式关系

274
0:10:26.960 --> 0:10:27.960
那这个硬式关系呢

275
0:10:27.960 --> 0:10:29.960
主要是线性的硬式关系

276
0:10:29.960 --> 0:10:30.960
下面这个图呢

277
0:10:30.960 --> 0:10:32.860
上面是指我们的浮点的

278
0:10:32.860 --> 0:10:35.360
或者我们的权重的参数的数值

279
0:10:35.360 --> 0:10:37.060
它当然有最小和最大值

280
0:10:37.060 --> 0:10:38.260
当然也有个0值

281
0:10:38.260 --> 0:10:39.760
我们希望把这一堆数据

282
0:10:39.760 --> 0:10:41.560
硬式到一个具体的范围

283
0:10:41.560 --> 0:10:43.760
-127到127之间

284
0:10:43.760 --> 0:10:44.560
那这时候呢

285
0:10:44.560 --> 0:10:45.760
-127到127

286
0:10:45.760 --> 0:10:48.760
这就是我们INT8所表示的范围

287
0:10:48.760 --> 0:10:50.360
那另外我们还有一种方式

288
0:10:50.360 --> 0:10:51.160
就是

289
0:10:51.760 --> 0:10:53.160
截断的方式

290
0:10:53.160 --> 0:10:56.060
我们去设置一个最小值和最大值

291
0:10:56.060 --> 0:10:58.560
把最小值和最大值范围的参数呢

292
0:10:58.560 --> 0:11:01.760
去硬式到-127到127之间

293
0:11:01.760 --> 0:11:02.760
而不在这个范围内

294
0:11:02.760 --> 0:11:03.460
你的数据呢

295
0:11:03.460 --> 0:11:05.560
直接把它丢弃掉

296
0:11:05.660 --> 0:11:07.260
这个呢就是简单的量化方法

297
0:11:07.260 --> 0:11:09.260
我们再往下看

298
0:11:09.260 --> 0:11:10.860
其实我们量化的类型啊

299
0:11:10.860 --> 0:11:13.960
可以分为对称量化和非对称量化

300
0:11:13.960 --> 0:11:15.560
所谓的对称很简单

301
0:11:15.560 --> 0:11:17.960
就是我们以0作为中心轴

302
0:11:17.960 --> 0:11:19.760
做-127到127

303
0:11:19.760 --> 0:11:21.260
就是两边的对称

304
0:11:21.260 --> 0:11:23.760
我们可能会用INT来表示

305
0:11:23.760 --> 0:11:25.560
另外一种非对称的量化呢

306
0:11:25.560 --> 0:11:27.060
可能我们就没有中心轴了

307
0:11:27.060 --> 0:11:28.860
我们以0作为开始

308
0:11:28.860 --> 0:11:30.560
以255作为结束

309
0:11:30.560 --> 0:11:31.260
那这种呢

310
0:11:31.260 --> 0:11:33.760
表示方式直接使用了uint

311
0:11:33.860 --> 0:11:35.760
去进行一个表示的

312
0:11:37.760 --> 0:11:38.460
哎

313
0:11:38.460 --> 0:11:40.560
下面我们才来真正的来到我们的

314
0:11:40.560 --> 0:11:41.660
量化的原理

315
0:11:41.660 --> 0:11:43.260
真正看一下公式呢

316
0:11:43.260 --> 0:11:45.060
要搞清楚量化的原理

317
0:11:45.060 --> 0:11:47.860
肯定要搞清楚我们的数据硬式的关系

318
0:11:47.860 --> 0:11:51.160
就是浮点跟定点的数据的转换公式

319
0:11:51.160 --> 0:11:51.960
那现在呢

320
0:11:51.960 --> 0:11:54.460
我们的r表示我们的数的浮点的数据

321
0:11:54.460 --> 0:11:56.460
fp3到fp16都行

322
0:11:56.460 --> 0:11:57.260
而q呢

323
0:11:57.260 --> 0:11:59.760
表示我们量化后的定点的一个数据

324
0:11:59.760 --> 0:12:01.560
我们的int的数据

325
0:12:01.560 --> 0:12:02.260
而z呢

326
0:12:02.260 --> 0:12:05.960
就是我们的偏移值要做对称还是不对称

327
0:12:05.960 --> 0:12:08.260
而s就是我们的缩放因子

328
0:12:08.260 --> 0:12:11.060
这里面的很重要的就是找到缩放因子

329
0:12:11.060 --> 0:12:12.860
还有我们的zero point

330
0:12:12.860 --> 0:12:16.160
零点表示就能够求得到我们的q

331
0:12:17.160 --> 0:12:18.260
上面这条公式呢

332
0:12:18.260 --> 0:12:19.360
我们叫做量化

333
0:12:19.360 --> 0:12:20.460
下面这条公式呢

334
0:12:20.460 --> 0:12:22.460
我们叫做反量化

335
0:12:22.460 --> 0:12:25.760
那我们的终极目标还是求得到我们的x和z

336
0:12:25.760 --> 0:12:28.160
我们看看这两个是怎幺去求的

337
0:12:28.160 --> 0:12:29.360
求解x和z啊

338
0:12:29.360 --> 0:12:31.160
其实有很多种方法

339
0:12:31.160 --> 0:12:31.660
这里面呢

340
0:12:31.660 --> 0:12:34.560
我们列举了一种线性的量化的求解方法

341
0:12:34.560 --> 0:12:37.560
就用min max进行一个求解的

342
0:12:37.560 --> 0:12:39.460
我们的x其实比较好求

343
0:12:39.460 --> 0:12:42.660
我们把r max减去r min除以q max减去q min

344
0:12:42.660 --> 0:12:45.360
那一般来说q max q min我们是知道的

345
0:12:45.360 --> 0:12:46.660
-127到117

346
0:12:46.660 --> 0:12:48.460
255到0之间

347
0:12:48.460 --> 0:12:49.760
所以我们通过这种公式呢

348
0:12:49.760 --> 0:12:52.060
就可以求得到我们的一个x

349
0:12:52.060 --> 0:12:54.860
而z呢也是比较好求的

350
0:12:54.860 --> 0:12:55.560
下面呢

351
0:12:55.560 --> 0:12:58.260
我们来看一下一个具体的例子啊

352
0:12:58.260 --> 0:13:00.860
刚才其实讲的有点抽象

353
0:13:00.960 --> 0:13:02.860
s就是我们的scale offset

354
0:13:02.860 --> 0:13:04.160
就是我们的刚才的z

355
0:13:04.160 --> 0:13:06.160
假设输入的是一个uint的数据

356
0:13:06.160 --> 0:13:07.460
我们为了得到float

357
0:13:07.460 --> 0:13:08.960
可以通过下面这条公式

358
0:13:08.960 --> 0:13:10.260
当然了这是反量化

359
0:13:10.260 --> 0:13:10.960
量化的时候呢

360
0:13:10.960 --> 0:13:12.460
我们输入我们的float

361
0:13:12.460 --> 0:13:16.160
然后除以我们的scale做一个45路

362
0:13:16.160 --> 0:13:17.060
减去我们的offset

363
0:13:17.060 --> 0:13:19.160
就得到了我们的uint

364
0:13:19.160 --> 0:13:20.660
那同样的scale和offset呢

365
0:13:20.660 --> 0:13:22.860
也是通过这个公式上面讲的

366
0:13:22.860 --> 0:13:24.160
去进行计算的

367
0:13:24.160 --> 0:13:26.560
那看这条公式或者这几条公式呢

368
0:13:26.560 --> 0:13:29.060
确实比刚才上面直接用q啊z啊

369
0:13:29.060 --> 0:13:30.660
比较明确

370
0:13:30.660 --> 0:13:32.660
那今天的内容呢就这幺多了

371
0:13:32.660 --> 0:13:34.160
可能稍微有点长了点

372
0:13:34.160 --> 0:13:36.760
欢迎大家去阅读更多新的论文

373
0:13:36.760 --> 0:13:37.360
好了

374
0:13:37.360 --> 0:13:38.260
谢谢大家

375
0:13:38.260 --> 0:13:39.360
拜了个拜

376
0:13:39.360 --> 0:13:41.060
卷的不行了卷的不行了

377
0:13:41.060 --> 0:13:42.860
记得一键三连加关注哦

378
0:13:42.860 --> 0:13:44.460
所有的内容都会开源在

379
0:13:44.460 --> 0:13:46.460
下面这条链接里面

380
0:13:46.460 --> 0:13:47.660
拜了个拜

