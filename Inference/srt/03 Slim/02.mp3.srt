0
0:00:00.000 --> 0:00:08.600
嗨大家好我是宗米卷王来了卷王来了

1
0:00:08.600 --> 0:00:11.520
那我们今天呢给大家带来一个新的内容

2
0:00:11.520 --> 0:00:15.320
就是推理引擎或者推理系统里面的模型压缩

3
0:00:15.320 --> 0:00:19.160
也可以叫模型小型化或者模型轻量化这个工作

4
0:00:19.160 --> 0:00:24.960
那今天的我们主要是给大家去带来一个新的内容模型的量化

5
0:00:26.160 --> 0:00:28.880
回到我们今天课程的主角

6
0:00:28.880 --> 0:00:33.640
我们今天的课程呢主要是去讲讲低比特量化

7
0:00:33.640 --> 0:00:39.880
那在低比特量化里面呢宗米呢可能会分开三节课给大家介绍的

8
0:00:41.160 --> 0:00:45.040
因为内容有点多所以我把低比特量化呢采访成三节课

9
0:00:45.040 --> 0:00:51.920
每节课呢尽量控制在十分钟以内让大家听得比较愉快心情比较愉悦

10
0:00:51.920 --> 0:00:56.280
那第一个呢就是最基本的量化基础啊basic concept

11
0:00:56.280 --> 0:00:58.680
第二个呢就是量化的原理

12
0:00:58.720 --> 0:01:07.000
去看看具体的量化的算法是怎么实现的到底低比特指的是啥低到哪个程度

13
0:01:07.360 --> 0:01:11.640
那接着呢我们将会在第二节课去讲讲QAT

14
0:01:11.640 --> 0:01:14.760
Portability Area Training感知量化训练

15
0:01:14.760 --> 0:01:19.000
我们一般都叫QAT感知量化为啥叫感知呢

16
0:01:19.000 --> 0:01:24.800
是因为我们在训练的过程当中去感知到我们需要量化的程度

17
0:01:24.840 --> 0:01:36.000
那接着呢在最后一节课我们会去讲讲训练后量化PQQ Post Training Quantilization训练完之后再做量化

18
0:01:36.400 --> 0:01:41.440
那在第三节课我们还有一个很重要的内容要补充的就是量化的部署

19
0:01:42.360 --> 0:01:47.480
不管忘了前面的算法呀多复杂多炫酷也好最终我们都离不开一个话题

20
0:01:47.480 --> 0:01:51.720
怎么把我们的网络模型经过量化后的部署起来

21
0:01:51.800 --> 0:01:55.000
所以这个话题这个内容也是非常重要

22
0:01:55.000 --> 0:01:59.920
而我搜了很多文章确实很少会把这所有的内容串起来

23
0:02:01.440 --> 0:02:03.920
现在呢我们开始正式的内容

24
0:02:04.600 --> 0:02:07.680
首先第一个就是量化的基础哦

25
0:02:08.240 --> 0:02:11.000
上面这段废话呢我就不跟大家一起去读了

26
0:02:11.480 --> 0:02:15.200
我们的数字呢在计算机里面其实有很多种表示方式

27
0:02:15.200 --> 0:02:19.840
那这里面呢一般我们都会有FP三十二还有FP十六

28
0:02:19.840 --> 0:02:24.400
英特三十二英特十六还有英特八一般呢都会有这些指令

29
0:02:24.400 --> 0:02:30.520
那我们可以看到像FP三十二呢是我们一般用来去做训练的一个精度

30
0:02:31.440 --> 0:02:33.840
很多时候呢我们会开启混合精度

31
0:02:33.840 --> 0:02:39.600
那所谓的混合精度呢一般是把FP三十二跟FP十六混合到一起去训练的

32
0:02:39.600 --> 0:02:44.320
因为FP十六呢它这样的类似位置或者占用的地址空间呢确实少了很多

33
0:02:45.840 --> 0:02:48.240
而我们一般指的模型量化呢

34
0:02:48.280 --> 0:02:58.320
除了把FP三十二降成FP十六更多的是降到英特十六英特八甚至英特四更低比特的一种表示

35
0:02:58.560 --> 0:03:01.440
那这种呢叫做模型量化的技术

36
0:03:02.120 --> 0:03:09.680
把FP三十二三十二位的比特转换成为更低比特的方式或者技术我们叫做模型量化

37
0:03:10.440 --> 0:03:13.680
接下来呢我们看一下神经网络有什么特点

38
0:03:13.920 --> 0:03:16.440
那右边这个图呢我们已经看了好多遍了

39
0:03:16.440 --> 0:03:21.520
其实呢我们神经网络一般来说呢我们训练的时候数据的参数量会很大

40
0:03:21.520 --> 0:03:24.160
第二个我们的计算量也会很大

41
0:03:24.160 --> 0:03:27.600
第三个就是占用很大的内存空间

42
0:03:27.760 --> 0:03:33.240
而占用这么多内存空间都这么大为的就是得到一个高精度的模型

43
0:03:33.880 --> 0:03:39.200
但是呢我们要把神经网络部署起来所以我们希望这个网络模型呢越小越好

44
0:03:39.400 --> 0:03:43.120
于是呢研究者就发现了网络模型的量化的技术

45
0:03:43.120 --> 0:03:46.920
那我们现在首先先不要去谈谈模型量化到底做了什么

46
0:03:46.920 --> 0:03:50.320
我们去看看模型量化的一些好处

47
0:03:50.320 --> 0:03:55.160
那这个呢是我总结了几个点第一个点就是保持相同的精度

48
0:03:55.480 --> 0:04:00.120
我们的神经网络训练的时候呢一般来说啊对噪声不是说非常的敏感

49
0:04:00.120 --> 0:04:04.640
因为里面有非常大量勇于的参数而且都是可学习的参数

50
0:04:04.640 --> 0:04:06.920
第二个呢就是加速运算啊

51
0:04:07.280 --> 0:04:11.360
因为大家都知道像传统的卷机神经网络里面的卷机的操作

52
0:04:11.360 --> 0:04:14.520
一般都是用IP三十二的浮点数去运算的

53
0:04:14.520 --> 0:04:20.080
越低比特的运算呢它确实能够减少运算的功耗而且性能更高

54
0:04:20.640 --> 0:04:23.240
那第三点呢就是节省内存

55
0:04:23.240 --> 0:04:26.240
我从IP三十二存储到英特巴英特斯

56
0:04:26.520 --> 0:04:30.880
越低的精度我所占用的地址空间和内存空间呢肯定是越小的

57
0:04:31.800 --> 0:04:35.760
下面呢我们看一下模型量化的五个特点

58
0:04:36.440 --> 0:04:41.000
第一个呢就是我们对神经网络模型的参数进行压缩

59
0:04:41.000 --> 0:04:44.000
把以前两百兆的模型呢可能压缩到只有五十兆

60
0:04:44.000 --> 0:04:46.520
而且我们可以提升运算的速度

61
0:04:46.520 --> 0:04:50.480
另外还可以降低我们的内存降低我们的功耗提升芯片的面积

62
0:04:50.480 --> 0:04:54.200
那这里面呢只是对我们上面进行一个简单的总结

63
0:04:54.200 --> 0:04:57.920
接下来我们看一下量化技术落地的三个挑战

64
0:04:58.800 --> 0:05:05.320
现在我们来看一下量化技术落地的三大挑战呢其中第一个就是精度的挑战

65
0:05:05.320 --> 0:05:08.920
首先呢我们现在的量化方法大部分都是线性的量化

66
0:05:08.960 --> 0:05:15.640
另外的话我们的模型比特越低精度损失越大我们的任务呢不同任务里面精度损失也是不一样的

67
0:05:15.640 --> 0:05:19.560
而且模型越小确实对我们的损失精度影响也是很大

68
0:05:19.560 --> 0:05:25.320
说第一大的挑战就是精度我们还是希望能够保持相同的精度

69
0:05:25.320 --> 0:05:28.840
那第二个比较大的挑战就是硬件的知识程度

70
0:05:28.840 --> 0:05:33.720
因为不同的硬件提供的比特数或者比特的指令集呢是不同的

71
0:05:33.720 --> 0:05:39.400
需要做很多的对于克努或者根据指令集进行一些优化

72
0:05:39.400 --> 0:05:45.320
那最后一点就是软件算法是否真的能够加速这是一个悖论哦

73
0:05:45.320 --> 0:05:56.480
我们虽然网络模型经过压缩了但是要是硬件指令不知识的情况我在运算的时候呢还是要反量化量化回FP三十二这个时候呢我们就需要插入很多开始的算子

74
0:05:56.480 --> 0:06:03.200
那这会影响我们克努的执行的性能所以说软件算法是否能够真正得到一个加速呢

75
0:06:03.240 --> 0:06:04.280
这是一个疑问哦

76
0:06:06.960 --> 0:06:15.440
下面呢我想提出几个问题引起大家的一个一起去思考大家也可以停下来几分钟去思考一下这些问题

77
0:06:15.440 --> 0:06:27.160
第一个就是为什么模型量化技术能够对实际的部署起到真正的作用虽然我们有很多挑战但确实模型量化能够真正的加速我们的部署的速度

78
0:06:27.160 --> 0:06:33.160
那第二个呢就是为什么需要对网络模型进行量化压缩呢量化压缩到底有什么好处

79
0:06:33.720 --> 0:06:43.240
那第三点就是为什么我们不直接训练的时候就训练低比特精度的模型呢哎这个很有意思对于大模型也是

80
0:06:43.960 --> 0:06:51.160
我大模型训练的时候呢动着就百亿级别千亿级别万亿级别的模型模型差出量这么大为什么你不直接训练一个小模型呢

81
0:06:52.280 --> 0:07:00.280
最后一点就是在什么情况下我们不应该还是应该去使用我们的模型量化技术呢

82
0:07:00.840 --> 0:07:04.040
那这四个点我希望大家停下来去思考一下

83
0:07:13.280 --> 0:07:23.440
现在呢我要给大家去汇报一下量化原理了首先呢量化的方法方法呢现在来看呢一般分为三大种

84
0:07:23.440 --> 0:07:36.160
那第一种就是量化训练QAT它最主要的方式呢就是让我们在网络模型训练的时候去感知量化运算到底对我们的网络模型精度带来什么样的影响

85
0:07:36.160 --> 0:07:53.400
然后呢通过反许内去降低我们整个量化的误差接着呢我们有ptq那ptq啊就是我们的离线量化离线量化又分为动态和静态那一般来说呢我们会用静态会比较多动态呢其实比较简单

86
0:07:53.400 --> 0:08:17.080
就是我们直接把已经训练好的网络模型呢全种直接从fp三时候转成或者映射成int八和float十六这种类型呢进行一个推理而静态量化呢可能会使用少量或者没有标签的数据进行一个校准那校准的方法有非常的多那现在呢用的比较多的就是KO散度

87
0:08:17.080 --> 0:08:17.600
我们往下看一个图更好的去理解这三种具体的算法首先就是感知量化qat我们有一个训练好的网络模型对这个网络模型呢进行一个转换超越一些伪量化的算子接着呢得到一个新的网络模型对新的网络模型进行发推内最后呢得到真正量化后或者感知量化后的一个网络模型接着呢去给部署端的静态离线量化呢首先我们有一个训练好的网络

88
0:08:47.080 --> 0:09:17.080
模型我们有一堆训练的数据哦这个是春令地塔改错咯有一堆训练的数据或者等待训练的数据然后呢去给我们的网络模型进行一个校准那这个校准的方法呢有可能会用KO散度或者其他的方式最后呢就是ptq的dynamic动态离线量化我们输一个神经网络模型然后呢对这个神经网络模型进行转换最后得到我们转换后或者量化后的网络模型主要是由这三种方法

89
0:09:17.080 --> 0:09:47.080
那现在呢我们看一下这三种方法有什么区别啊做一个简单的比较了呃量化训练呢确实我们的精度损失是比较少的但是它的缺点就是需要使用大量代标签的数据进行发修炼或者一个重训的那第二种方式就是静态离线量化ptq它的好处就是精度损失也是比较小的但是不能说没有哦因为它缺乏了训练或者发修炼的步骤它需要有一些少量无标签的数据

90
0:09:47.080 --> 0:10:17.080
进行校准的最后一种就是动态离线量化就是ptq了它这种方式精度的损失一般来说我们不可控不过好处呢就是它没有任何使用的约束你想咋用就咋用这个呢也是宗敏呢在曼斯博里面做的其中一个项目做得非常的深入了这可能导致我引用的额外的知识呢稍微有点多了现在呢我们正式的来到量化的一个原理

91
0:10:17.080 --> 0:10:47.080
首先呢我们来理解一下网络模型的优化它主要是在我们定点跟浮点数之间呢建立了一个简单的数据或者数学的映射关系那这个映射关系呢主要是线性的映射关系下面这个图呢上面是指我们的浮点的或者我们的权重的参数的数值它当然有最小和最大值当然也有个零子我们希望把这一堆数据映射到一个具体的范围负一二七到一二七之间那这时候呢负一二七到一二七

92
0:10:47.080 --> 0:11:17.080
它所表示的范围那另外我们还有一种方式就是截断的方式我们去设置一个最小值和最大值把最小值和最大值范围的参数呢去映射到负一二七到一二七之间而不在这个范围内你的数据呢直接把它丢弃掉这个呢就是简单的量化方法我们再往下看其实我们量化的类型啊可以分为对称量化和非对称量化所谓的对称很简单就是我们以零作为对称

93
0:11:17.080 --> 0:11:47.080
中心轴做负一二七到一二七就是两边的对称我们可能会用印来表示另外一种非对称的量化呢可能我们就没有中心轴了我们以零作为开始以二五五作为结束那这种呢表示方式直接使用了用印去进行一个表示的下面我们才能真正的来到我们的量化的原理真正看一下公式呢要搞清楚量化的原理肯定要搞清楚我们的数据映射

94
0:11:47.080 --> 0:11:51.080
的关系就是浮点跟定点的数据的转换公式

95
0:11:51.080 --> 0:11:56.360
那现在呢我们的阿表示我们的数的浮点的数据二批三十二二批十六都行

96
0:11:56.360 --> 0:12:01.480
而Q呢表示我们量化后的定点的一个数据我们的印的数据

97
0:12:01.480 --> 0:12:05.880
而Z呢就是我们的偏移值要做对称还是不对称

98
0:12:05.880 --> 0:12:11.000
而S就是我们的缩放因子这里面呢很重要的就是找到缩放因子

99
0:12:11.000 --> 0:12:16.360
还有我们的零点表示就能够求得到我们的Q

100
0:12:17.080 --> 0:12:22.440
上面这条公式呢我们叫做量化下面这条公式呢我们叫做反量化

101
0:12:22.440 --> 0:12:25.720
那我们的终极目标还是求得到我们的X和Z

102
0:12:25.720 --> 0:12:28.120
我们看看这两个是怎么去求的

103
0:12:28.120 --> 0:12:34.600
求解X和Z呀其实有很多种方法这里面呢我们列举了一种线性的量化的求解方法

104
0:12:34.600 --> 0:12:39.480
就用minmax进行一个求解的我们的X其实比较好求

105
0:12:39.480 --> 0:12:42.760
我们把rmax减去rmin除以qmax减去qmin

106
0:12:42.760 --> 0:12:48.520
那一般来说qmaxqmin我们是知道的负一二七到一七二五五到零之间

107
0:12:48.520 --> 0:12:52.040
所以我们通过这种公式呢就可以求得到我们的一个X

108
0:12:52.040 --> 0:12:58.280
而Z呢也是比较好求的下面呢我们来看一下一个具体的例子啊

109
0:12:58.280 --> 0:13:00.200
刚才其实讲得有点抽象

110
0:13:00.200 --> 0:13:06.200
S就是我们的scale offset就是我们的刚才的Z假设输入的是一个优印的数据

111
0:13:06.200 --> 0:13:08.840
我们为了得到float可以通过下面这条公式

112
0:13:08.840 --> 0:13:12.280
但要了这是反量化量化的时候呢我们输入我们的float

113
0:13:12.280 --> 0:13:19.000
然后除以我们scale做一个四十五入减去我们的offset就得到了我们的优印

114
0:13:19.000 --> 0:13:24.040
那同样的scale和offset呢也是通过这个公式上面讲的去进行计算的

115
0:13:24.040 --> 0:13:30.360
那看这条公式或者这几条公式呢确实比刚才上面直接用q和z啊比较明确

116
0:13:30.360 --> 0:13:36.680
那今天的内容呢就这么多了可能稍微有点长了点欢迎大家去阅读更多新的论文
