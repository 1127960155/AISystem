0
0:00:00.000 --> 0:00:04.560


1
0:00:05.900 --> 0:00:07.060
嘿大家好

2
0:00:07.060 --> 0:00:08.100
我是ZOMI

3
0:00:09.100 --> 0:00:19.660
如果大家觉得我的视频里面有讲解得不明白的地方或者讲解得比较寒酷的地方呢也非常欢迎各位好哥哥和小姐姐呢给我留言和评论录

4
0:00:20.260 --> 0:00:23.660
宗敏呢会根据相对的内容呢进行一个更新

5
0:00:24.560 --> 0:00:25.940
废话就不多说啦

6
0:00:25.940 --> 0:00:28.560
我们来到推进引擎模型压缩里面的

7
0:00:28.560 --> 0:00:31.360
模型减资披露

8
0:00:33.000 --> 0:00:35.400
减资这个工作呢还是很有意思的

9
0:00:35.400 --> 0:00:39.520
我们看一下这个内容里面我要给大家汇报哪些点

10
0:00:39.520 --> 0:00:43.560
那首先呢我们看一下减资跟量化具体有什么区别

11
0:00:43.560 --> 0:00:47.400
接着呢我们去了解一下减资算法的分类

12
0:00:47.400 --> 0:00:50.560
因为减资确实有很多种不同的减法

13
0:00:51.000 --> 0:00:54.400
在第三个内容我们就去看看减资的流程

14
0:00:54.400 --> 0:00:58.520
根据不同的减资算法呢去提出我们对应的减资的流程

15
0:00:58.520 --> 0:01:04.280
最后呢以一个最开始或者现在已经最成熟的减资算法作为例子

16
0:01:04.280 --> 0:01:10.080
就是LLM减资这个算法呢去展开或者作为我们最后的结束

17
0:01:12.880 --> 0:01:16.520
现在我们正式的来到第一个内容减资和量化有什么区别

18
0:01:16.520 --> 0:01:22.280
因为我们在上一节内容里面其实充分地去给大家汇报了量化所相关的知识点

19
0:01:22.280 --> 0:01:24.480
接着呢我们去看一下减资

20
0:01:24.720 --> 0:01:29.080
下面呢主要是看一下减资和量化具体或者本质的区别在哪

21
0:01:29.080 --> 0:01:32.440
首先呢我们看一个模型压缩的概念

22
0:01:32.440 --> 0:01:36.040
模型压缩实际上呢主要是对三个部分进行优化

23
0:01:36.920 --> 0:01:39.840
第一个点就是减少密集的内存访问量

24
0:01:39.840 --> 0:01:44.840
也就是减少我们跟内存的访问的次数访问得越少越好

25
0:01:44.840 --> 0:01:47.320
因为访问内存确实太耗时间了

26
0:01:48.040 --> 0:01:51.520
第二点呢就是提高获取模型参数的时间

27
0:01:51.520 --> 0:01:54.440
这个点说白了就是我们的模型的参数

28
0:01:54.440 --> 0:01:57.680
越少越好我们的模型越小越好

29
0:01:57.680 --> 0:02:00.480
第三点就是加速模型推理的时间

30
0:02:00.480 --> 0:02:05.000
在推理的情况下呢真正的能够把我们的实验提上去

31
0:02:06.000 --> 0:02:11.640
不管是减资不管是减资还是量化其实它属于模型压缩的一部分

32
0:02:11.640 --> 0:02:16.280
而我们所做的工作呢都是围绕着下面三个点进行优化的

33
0:02:19.480 --> 0:02:23.040
具体量化和减资的区别呢我们看一下下面这个图呢

34
0:02:23.040 --> 0:02:29.760
左边的这个是我们原始的网络模型的数据存的或者训练的时候呢是IP三二时候进行存储的

35
0:02:29.760 --> 0:02:35.600
但是呢我们经过量化之后呢我们的存储的数据呢就变成八比特或者英特巴

36
0:02:35.600 --> 0:02:46.800
在实际网络模型量化推理的时候呢如果硬件呢支持八比特运算的指令集呢那这个时候呢八比特或者英特巴的这个数据呢就可以真正的在我们的硬件上面部署起来

37
0:02:47.920 --> 0:02:51.000
原来的网络模型有多少参数量

38
0:02:51.000 --> 0:03:17.360
量化之后它仍然有多少参数量下面呢我们看一下模型的减资模型减资呢左边这个呢就是原始训练过后或者培训练过后的一个网络模型的一些参数量经过减资之后呢我们就会把一些不重要的或者勇于的非关键的权重或者数据呢把它减掉剩下一些比较重要的数据那这个呢就是减资的原理可以看到呢减资跟量化是完全不一样的

39
0:03:18.360 --> 0:03:25.040
通过这两个图呢我们可以看到虽然减资和量化的目标是一样的但是它们的优化手段是不一样的哦

40
0:03:27.680 --> 0:03:47.320
宗敏呢就发现谷歌写了一篇比较有意思的文章土碰老土碰确实谷歌在AI这方面的研究啊非常的深厚这篇文章呢确实我觉得可以作为一个抛类或者减资的一个白皮书去看也非常欢迎大家去阅读一下这篇论文那这篇论文呢我总结了几个观点第一个就是在内存占用上呢

41
0:03:47.360 --> 0:04:12.440
相同的情况下实际上啊一些又大又稀疏的网络模型会比一些小的密集的网络模型能够实现更好的精度所以说我们一般呢不希望它训练一个小模型而是训练一个大模型接着呢执行一个减资确实会比直接训练一个小模型呢更加有效这也是对应于上一个视频所分享里面提出的一个疑问

42
0:04:13.440 --> 0:04:21.920
第二个点呢就是经过减资之后的一个稀疏模型呢确实要优于同体积就相同大小的一个非稀疏的网络模型

43
0:04:21.920 --> 0:04:30.120
那第二点呢就充分的说明其实我们的网络模型有很多的参数其实是不一定是需要的它可能没有用

44
0:04:31.320 --> 0:04:39.600
第三点就是在资源受限的情况下减资呢属于一种比较高效或者比较有效的模型的压缩的策略和方法

45
0:04:39.600 --> 0:04:49.480
我更多呢是关注于一二三条也是这篇文章啊做了大量的销容实验之后得到的一个结论也非常欢迎大家去看看

46
0:04:51.280 --> 0:05:05.560
下面我们来看一个第二个比较重要的内容减资算法的分类下面呢我总结了或者我不必了我看了很多的减资的算法了这里面呢其实不要求大家也看那么多

47
0:05:05.560 --> 0:05:12.600
看了这么多算法之后呢我总结了几个方向就是减资算法的分类

48
0:05:12.600 --> 0:05:19.720
那减资算法呢主要有两个类别一个是左边的非结构化的减资unstructured parallel

49
0:05:19.720 --> 0:05:24.120
第二个呢就是结构化的减资structured parallel

50
0:05:24.120 --> 0:05:34.120
非结构化的减资呢就像左边的这个图主要是对一些独立的权重或者神经元再或者一些神经元的链接进行减资就是随机的减

51
0:05:34.120 --> 0:05:40.720
这也是为啥我说可能我在一六年一七年一八年的时候接触减资算法他那时候还没有那么成熟

52
0:05:40.720 --> 0:05:52.560
那第二个呢就是结构化的减资右边的这三个更多的是结构化的减资结构化的减资就会有规律有顺序地对我们的神经网络或者我们的计算图进行减资

53
0:05:52.560 --> 0:06:02.320
几个比较经典的就是对fitter进行减资对channel进行减资对layer进行减资减资的维度减资的方式不太一样

54
0:06:04.920 --> 0:06:20.920
而两种的减资方式呢也有它的利弊我们现在来看一下非结构化的减资它的一个好处就是减资算法特别的简单模型的压缩比例确实可以压得非常的高那它的缺点也是非常明显的

55
0:06:20.920 --> 0:06:32.520
嗯第一个比较明显的问题呢就是精度不可控精度不可控其实一般我们来说不可接受真的是不可接受的我花了这么多力气去训练一个好模型

56
0:06:32.520 --> 0:06:54.520
就是为了提高精度嘛你给我推理的时候精度不可控你玩呢玩呀一般来说呀这个非结构化的减资呢我们很少的去用而且呢非结构化的减资呢减资后基本上我们的权重呢会极度的稀疏化没有专用的硬件呢去实现是很难够真正的去做一个训练推理的加速

57
0:06:54.520 --> 0:06:59.520
那第二个就是结构化的减资结构化的减资的好处

58
0:06:59.520 --> 0:07:28.520
主要是大部分算法在channel和layer层面去做一个减资保留了原始剪辑的一个结构化不需要专用的硬件去实现而且减资算法呀比较有规律比较好学习那坏处就是减资的算法相对来说比较复杂需要我们真正的去投进去去了解下面这个图呢采自于这篇文章它里面呢就做了很多的消融实验可以看到大部分的这个是没有减资的

59
0:07:28.520 --> 0:07:44.520
而朴冷之前全做参数的一个统计我们可以看到其实大部分的数据都集中在零零的数据有非常的多经过结构化的减资之后呢我们看到了整个数据的分布啊确实更加的服从于高斯分布

60
0:07:44.520 --> 0:07:57.520
这也是我们希望看到的就不需要我们的网络模型不要那么多勇于的参数一大堆勇于的参数一大堆零存来干嘛还不省点内存空间

61
0:07:57.520 --> 0:08:27.520
现在呢来到我们第二个内容了就是减资的流程看一下减资有几个流程减资一般是怎么做的首先我们可以看到减资啊一般对减资来说有三种常见的算法第一个呢就是训练一个模型先训练一个模型然后呢对这个模型进行减资最后呢对减资后的模型呢进行微调第二种就是在模型训练的过程当中进行减资啊就边训边减啦

62
0:08:27.520 --> 0:08:57.520
减完之后呢对模型再进行一个微调为啥都会有微调呢我们下面会讲到啊大家不要急第三种就是直接进行减资然后呢从头开始训练的第三种方式呢其实用的比较少可以看到刚才的刚才我给大家介绍的减资的三种常见的算法呢有比较规律的三个内容第一个就是训练第二个就是减资第三个就是翻修内

63
0:08:57.520 --> 0:09:27.520
微调三个框框三个作用啊每个框框都有不同的作用哦这里面呢我对减资呢就做了一个总结就是减资的主要的单元有三个一个是训练减资微调它各自起到什么作用我们现在来看一看训练的作用呢主要是得到最佳的网络模型的性能用它作为基准就benchmark我们在做一个翻修内或者修内的工作之后呢需要回顾一下跟我们训练的时候精度是不是相当

64
0:09:27.520 --> 0:09:57.520
相同的有没有破坏或极度地降低了我们模型训练的精度那第二个工作呢就是真正的per net减资我们会根据不同的算法呢对我们刚才训练的网络模型进行减资调整一下我们网络模型的通道数啊权中数啊等其他的参数最后一个单元呢就是微调我们需要在原始的数据集呢之上进行微调那这个微调的模型呢就是经过减资后的模型啦因为经过减资

65
0:09:57.520 --> 0:10:27.520
网络模型的结构变化呢于是呢我们希望通过微调来去恢复弥补减资后所丢失的精度和性能第一种模式也是最简单的就是上面这个流程的完全分解首先我们有一个已经训练好的网络模型接着呢对这个网络模型进行各种各样的减资虽然说是各种各样实际上只有结构化和非结构化两种然后呢对减资后的模型啊进行

66
0:10:27.520 --> 0:10:57.520
微调恢复一定的精度这种方式呢就是最原始最难易的模型减资的流程接着我们看一下另外两种也是用的比较多的首先呢我们还是一样拿到一个已经训练好的网络模型接着会有一个子的网络模型进行采样可能我们会有很多个子模型于是呢对这些子模型进行评估看哪个模型的精度性能比较好最后呢我们选择其中一个对它进行微调恢复网络模型的精度

67
0:10:57.520 --> 0:11:27.520
最后一种就是基于纳思自动搜索的方式那实际上呢我们在经过啊真正的工程验证当中呢最后一种更多的是在学术的前年但是在工业界确实用的很少因为基于纳思的搜索太消耗我们的资源了就没钱你训不起来没钱你捡不起来没钱你也不敢捡那看一下我们这面首先还是训练一个网络模型然后呢基于大规模的一个搜索算法进行减资有可能呢基于纳思

68
0:11:27.520 --> 0:11:57.520
的搜索方法呢就是减完之后我就不需要微调了直接输出了解完对应的减资算法现在呢我们实际的打开一个减资算法来去了解一下它具体怎么实现就是L-e-normalization首先呢我们要讲一个具体的概念就是L-e-normal是based channel poolingchannel pooling就是我们的结构化减资专门针对channel进行减资的

69
0:11:57.520 --> 0:12:27.520
刚才说了我们根据channel来减但是我们减的标准我们减的法则用什么来约束呢就是L-e-normal通过计算L-e-normal来评价卷积和到底重要不重要如果不重要就把它减掉如果L-e-normal的值呢比较低那就证明这个卷积和这个channel呢不重要于是呢就把它减掉基于这个算法原理呢我们看一下具体的算法步骤首先我们需要对每一个卷积和

70
0:12:27.520 --> 0:12:57.520
是每一个计算它的权重的绝对值那就是这条公式把每一个卷积和进行计算然后呢根据卷积和刚才算得到的xj进行排序排完序之后我们就知道哪个重要哪个不重要第三步就是设定一个预值对小于这个预值的卷积和还有它对应的feature map进行减资减掉它第四步呢在工程上面很重要下一个卷积层

71
0:12:57.520 --> 0:13:27.520
刚才减掉的feature map相关的卷积和也需要把它减掉这种有点类似于连带责任关系最后一步就是对第二层和第二家一层的新的权重重新被创建那剩下的权重呢会复制到新的模型当中重新的执行这些步骤那最后呢我们看一下L-e-norm这篇文章的实际的效果作者呢做了大量的比对实验

72
0:13:28.520 --> 0:13:30.120
就是我们减资的比例

73
0:13:30.520 --> 0:13:36.120
在减资的吸收率到百分之六四的前提下呢基本上呢也能够保持比较好的精度

74
0:13:36.120 --> 0:13:39.720
那这个呢就是L-e-norm的一个具体的实验的结果

75
0:13:41.720 --> 0:13:47.920
综理在第一次接触减资算法的时候呢去看一下当时候我们的标杆应该是一半年的时候

76
0:13:47.920 --> 0:13:49.520
拍照时没有完全起来

77
0:13:49.520 --> 0:13:54.120
探测后呢就集成了L-e-norm这一个减资的算法和公式

78
0:13:54.520 --> 0:13:59.320
所以L-e-norm呢是一个非常经典的减资算法也非常欢迎大家去学习一下

79
0:14:02.520 --> 0:14:10.320
好啦最后就是参考文献相关的论文也欢迎大家去阅读一下相关的算法


