0
0:00:00.000 --> 0:00:03.000
嗨!大家好,我是ZOMI

1
0:00:03.000 --> 0:00:07.000
如果大家觉得我的视频里面有讲解的不明白的地方

2
0:00:07.000 --> 0:00:09.000
或者讲解的比较含糊的地方呢

3
0:00:09.000 --> 0:00:12.000
也非常欢迎各位好哥哥和小姐姐呢

4
0:00:12.000 --> 0:00:15.000
给我留言和评论弹幕

5
0:00:15.000 --> 0:00:19.000
ZOMI呢会根据相对的内容来进行一个更新

6
0:00:19.000 --> 0:00:21.000
废话就不多说了

7
0:00:21.000 --> 0:00:24.000
我们来到推进引擎模型压缩里面的

8
0:00:24.000 --> 0:00:27.000
模型简单的解说

9
0:00:27.000 --> 0:00:29.000
简单的解说

10
0:00:29.000 --> 0:00:31.000
简单的解说

11
0:00:31.000 --> 0:00:33.000
简单的解说

12
0:00:33.000 --> 0:00:35.000
简单的解说

13
0:00:35.000 --> 0:00:37.000
简单的解说

14
0:00:37.000 --> 0:00:39.000
简单的解说

15
0:00:39.000 --> 0:00:41.000
简单的解说

16
0:00:41.000 --> 0:00:43.000
简单的解说

17
0:00:43.000 --> 0:00:45.000
简单的解说

18
0:00:45.000 --> 0:00:47.000
简单的解说

19
0:00:47.000 --> 0:00:49.000
简单的解说

20
0:00:49.000 --> 0:00:51.000
简单的解说

21
0:00:51.000 --> 0:00:53.000
简单的解说

22
0:00:53.000 --> 0:00:55.000
简单的解说

23
0:00:55.000 --> 0:00:57.000
简单的解说

24
0:00:57.000 --> 0:00:59.000
简单的解说

25
0:00:59.000 --> 0:01:01.000
简单的解说

26
0:01:01.000 --> 0:01:03.000
简单的解说

27
0:01:03.000 --> 0:01:05.000
简单的解说

28
0:01:05.000 --> 0:01:07.000
简单的解说

29
0:01:07.000 --> 0:01:09.000
简单的解说

30
0:01:09.000 --> 0:01:11.000
简单的解说

31
0:01:11.000 --> 0:01:13.000
简单的解说

32
0:01:13.000 --> 0:01:15.000
简单的解说

33
0:01:15.000 --> 0:01:17.000
简单的解说

34
0:01:17.000 --> 0:01:19.000
简单的解说

35
0:01:19.000 --> 0:01:21.000
简单的解说

36
0:01:21.000 --> 0:01:23.000
简单的解说

37
0:01:23.000 --> 0:01:25.000
简单的解说

38
0:01:25.000 --> 0:01:27.000
简单的解说

39
0:01:27.000 --> 0:01:29.000
简单的解说

40
0:01:29.000 --> 0:01:31.000
简单的解说

41
0:01:31.000 --> 0:01:33.000
简单的解说

42
0:01:33.000 --> 0:01:35.000
简单的解说

43
0:01:35.000 --> 0:01:37.000
简单的解说

44
0:01:37.000 --> 0:01:39.000
简单的解说

45
0:01:39.000 --> 0:01:41.000
简单的解说

46
0:01:41.000 --> 0:01:43.000
简单的解说

47
0:01:43.000 --> 0:01:45.000
简单的解说

48
0:01:45.000 --> 0:01:47.000
简单的解说

49
0:01:47.000 --> 0:01:49.000
简单的解说

50
0:01:49.000 --> 0:01:51.000
简单的解说

51
0:01:51.000 --> 0:01:53.000
简单的解说

52
0:01:53.000 --> 0:01:55.000
简单的解说

53
0:01:55.000 --> 0:01:57.000
简单的解说

54
0:01:57.000 --> 0:01:59.000
简单的解说

55
0:01:59.000 --> 0:02:01.000
简单的解说

56
0:02:01.000 --> 0:02:03.000
简单的解说

57
0:02:03.000 --> 0:02:05.000
简单的解说

58
0:02:05.000 --> 0:02:07.000
简单的解说

59
0:02:07.000 --> 0:02:09.000
简单的解说

60
0:02:09.000 --> 0:02:11.000
简单的解说

61
0:02:11.000 --> 0:02:13.000
简单的解说

62
0:02:13.000 --> 0:02:15.000
简单的解说

63
0:02:15.000 --> 0:02:17.000
简单的解说

64
0:02:17.000 --> 0:02:19.000
简单的解说

65
0:02:19.000 --> 0:02:21.000
具体量化和减字的区别

66
0:02:21.000 --> 0:02:23.000
我们看一下下面这个图

67
0:02:23.000 --> 0:02:25.000
左边的这个是我们原始的网络模型的数据

68
0:02:25.000 --> 0:02:27.000
左边的这个是我们原始的网络模型的数据

69
0:02:27.000 --> 0:02:29.000
存的或者训练的时候是IP32进行存储的

70
0:02:29.000 --> 0:02:31.000
但是我们经过量化之后

71
0:02:31.000 --> 0:02:33.000
我们的存储的数据

72
0:02:33.000 --> 0:02:35.000
就变成8bit或者int8

73
0:02:35.000 --> 0:02:37.000
在实际网络模型量化

74
0:02:37.000 --> 0:02:39.000
推理的时候

75
0:02:39.000 --> 0:02:41.000
如果硬件支持8bit运算的指令集

76
0:02:41.000 --> 0:02:43.000
那这个时候8bit或者int8的数据

77
0:02:43.000 --> 0:02:45.000
就可以真正的在我们的

78
0:02:45.000 --> 0:02:47.000
硬件上面部署起来

79
0:02:47.000 --> 0:02:49.000
原来的网络模型

80
0:02:49.000 --> 0:02:51.000
有多少参数量

81
0:02:51.000 --> 0:02:53.000
量化之后它仍然有多少参数量

82
0:02:53.000 --> 0:02:55.000
下面我们看一下

83
0:02:55.000 --> 0:02:57.000
模型的减字

84
0:02:57.000 --> 0:02:59.000
左边这个就是原始训练过后

85
0:02:59.000 --> 0:03:01.000
或者发击点过后的一个网络模型的

86
0:03:01.000 --> 0:03:03.000
一些参数量

87
0:03:03.000 --> 0:03:05.000
经过减字之后我们就会把一些

88
0:03:05.000 --> 0:03:07.000
不重要的或者勇于的

89
0:03:07.000 --> 0:03:09.000
非关键的权重或者数据

90
0:03:09.000 --> 0:03:11.000
把它减掉剩下一些比较重要的

91
0:03:11.000 --> 0:03:13.000
数据

92
0:03:13.000 --> 0:03:15.000
这个就是减字的原理

93
0:03:15.000 --> 0:03:17.000
减字和量化是完全不一样的

94
0:03:17.000 --> 0:03:19.000
通过这两个图

95
0:03:19.000 --> 0:03:21.000
我们可以看到虽然减字和量化

96
0:03:21.000 --> 0:03:23.000
的目标是一样的

97
0:03:23.000 --> 0:03:25.000
但是它们的优化手段是不一样的

98
0:03:27.000 --> 0:03:29.000
中米就发现谷歌

99
0:03:29.000 --> 0:03:31.000
写了一篇比较有意思的文章

100
0:03:31.000 --> 0:03:33.000
to pwn or not to pwn

101
0:03:33.000 --> 0:03:35.000
确实谷歌在AIC方面的研究非常的深厚

102
0:03:35.000 --> 0:03:37.000
这篇文章确实我觉得

103
0:03:37.000 --> 0:03:39.000
可以作为一个pwning或者减字的

104
0:03:39.000 --> 0:03:41.000
一个白皮书去看

105
0:03:41.000 --> 0:03:43.000
也非常欢迎大家去阅读一下这篇论文

106
0:03:43.000 --> 0:03:45.000
这篇论文我总结了几个观点

107
0:03:45.000 --> 0:03:47.000
第一个就是在内存占用相同的情况下

108
0:03:47.000 --> 0:03:49.000
实际上一些又大又稀疏的网络模型

109
0:03:49.000 --> 0:03:51.000
会比一些小的密集的网络模型能够实现更好的精度

110
0:03:51.000 --> 0:03:53.000
所以说我们一般不希望它训练一个小模型

111
0:03:53.000 --> 0:03:55.000
而是训练一个大模型

112
0:03:55.000 --> 0:03:57.000
接着执行一个减字

113
0:03:57.000 --> 0:03:59.000
确实会比直接训练一个小模型更加有效

114
0:03:59.000 --> 0:04:01.000
这也是对应于上一个视频所分享

115
0:04:01.000 --> 0:04:03.000
里面提出的一个疑问

116
0:04:03.000 --> 0:04:05.000
那幺在这篇论文中

117
0:04:05.000 --> 0:04:07.000
我们可以看到

118
0:04:07.000 --> 0:04:09.000
中米在减字和量化方面

119
0:04:09.000 --> 0:04:11.000
是非常有利的

120
0:04:11.000 --> 0:04:13.000
这也是对应于上一个视频所分享

121
0:04:13.000 --> 0:04:15.000
里面提出的一个疑问

122
0:04:15.000 --> 0:04:17.000
第二个就是经过减字之后的一个稀疏模型

123
0:04:17.000 --> 0:04:19.000
确实要由于同体积

124
0:04:19.000 --> 0:04:21.000
就是相同大小的一个

125
0:04:21.000 --> 0:04:23.000
非稀疏的网络模型

126
0:04:23.000 --> 0:04:25.000
第二点就充分的说明

127
0:04:25.000 --> 0:04:27.000
其实我们的网络模型有很多的参数

128
0:04:27.000 --> 0:04:29.000
其实是不一定是需要的

129
0:04:29.000 --> 0:04:31.000
它可能没有用

130
0:04:31.000 --> 0:04:33.000
第三点就是在资源受限的情况下

131
0:04:33.000 --> 0:04:35.000
减字属于一种比较高效

132
0:04:35.000 --> 0:04:37.000
或者比较有效的

133
0:04:37.000 --> 0:04:39.000
模型的压缩的策略和方法

134
0:04:39.000 --> 0:04:41.000
我更多是关注于

135
0:04:41.000 --> 0:04:43.000
一二三条

136
0:04:43.000 --> 0:04:45.000
也是这篇文章

137
0:04:45.000 --> 0:04:47.000
做了大量的消融实验之后

138
0:04:47.000 --> 0:04:49.000
得到的一个结论

139
0:04:49.000 --> 0:04:51.000
也非常欢迎大家去看看

140
0:04:51.000 --> 0:04:53.000
下面我们来看一个第二个

141
0:04:53.000 --> 0:04:55.000
比较重要的内容

142
0:04:55.000 --> 0:04:57.000
减字算法的分类

143
0:04:57.000 --> 0:04:59.000
下面呢

144
0:04:59.000 --> 0:05:01.000
我总结了

145
0:05:01.000 --> 0:05:03.000
或者我避避了

146
0:05:03.000 --> 0:05:05.000
我看了很多的减字的算法

147
0:05:05.000 --> 0:05:07.000
这里面呢

148
0:05:07.000 --> 0:05:09.000
我总结了几个方向

149
0:05:09.000 --> 0:05:11.000
就是减字算法的分类

150
0:05:11.000 --> 0:05:13.000
那减字算法呢

151
0:05:13.000 --> 0:05:15.000
主要有两大类别

152
0:05:15.000 --> 0:05:17.000
一个是左边的

153
0:05:17.000 --> 0:05:19.000
非结构化的减字

154
0:05:19.000 --> 0:05:21.000
第二个呢

155
0:05:21.000 --> 0:05:23.000
就是结构化的减字

156
0:05:23.000 --> 0:05:25.000
非结构化的减字呢

157
0:05:25.000 --> 0:05:27.000
就像左边的这个图

158
0:05:27.000 --> 0:05:29.000
主要是对一些独立的权重

159
0:05:29.000 --> 0:05:31.000
或者神经元

160
0:05:31.000 --> 0:05:33.000
或者一些神经元的链接

161
0:05:33.000 --> 0:05:35.000
进行减字

162
0:05:35.000 --> 0:05:37.000
比如说我在16年

163
0:05:37.000 --> 0:05:39.000
17年18年的时候

164
0:05:39.000 --> 0:05:41.000
接触减字算法

165
0:05:41.000 --> 0:05:43.000
那时候还没有那幺成熟

166
0:05:43.000 --> 0:05:45.000
第二个呢

167
0:05:45.000 --> 0:05:47.000
就是结构化的减字

168
0:05:47.000 --> 0:05:49.000
右边的这三个

169
0:05:49.000 --> 0:05:51.000
更多的是结构化的减字

170
0:05:51.000 --> 0:05:53.000
结构化的减字就会有规律

171
0:05:53.000 --> 0:05:55.000
有顺序的

172
0:05:55.000 --> 0:05:57.000
对我们的神经网络

173
0:05:57.000 --> 0:05:59.000
或者我们的计算图

174
0:05:59.000 --> 0:06:01.000
进行减字

175
0:06:01.000 --> 0:06:03.000
几个比较经典的

176
0:06:03.000 --> 0:06:05.000
而两种的减字方式呢

177
0:06:05.000 --> 0:06:07.000
也有它的利弊

178
0:06:07.000 --> 0:06:09.000
我们现在来看一下

179
0:06:09.000 --> 0:06:11.000
非结构化的减字

180
0:06:11.000 --> 0:06:13.000
它的一个好处

181
0:06:13.000 --> 0:06:15.000
就是减字算法特别的简单

182
0:06:15.000 --> 0:06:17.000
模型的压缩比例

183
0:06:17.000 --> 0:06:19.000
确实可以压得非常的高

184
0:06:19.000 --> 0:06:21.000
那它的缺点也是非常明显的

185
0:06:21.000 --> 0:06:23.000
第一个比较明显的问题呢

186
0:06:23.000 --> 0:06:25.000
就是精度不可控

187
0:06:25.000 --> 0:06:27.000
精度不可控其实是

188
0:06:27.000 --> 0:06:29.000
一般我们来说不可接受

189
0:06:29.000 --> 0:06:31.000
真的是不可接受的

190
0:06:31.000 --> 0:06:33.000
我们要用机器去训练一个好模型

191
0:06:33.000 --> 0:06:35.000
就是为了提高精度嘛

192
0:06:35.000 --> 0:06:37.000
你给我推理的时候精度不可控

193
0:06:37.000 --> 0:06:39.000
你完呢

194
0:06:39.000 --> 0:06:41.000
一般来说呀

195
0:06:41.000 --> 0:06:43.000
这个非结构化的减字呢

196
0:06:43.000 --> 0:06:45.000
我们很少的去用

197
0:06:45.000 --> 0:06:47.000
而且呢非结构化的减字呢

198
0:06:47.000 --> 0:06:49.000
减字后基本上我们的权重呢

199
0:06:49.000 --> 0:06:51.000
会极度的稀疏化

200
0:06:51.000 --> 0:06:53.000
没有专用的硬件去实现

201
0:06:53.000 --> 0:06:55.000
是很难够真正的去做一个

202
0:06:55.000 --> 0:06:57.000
训练推理的加速

203
0:06:57.000 --> 0:06:59.000
那第二个就是结构化的减字

204
0:06:59.000 --> 0:07:01.000
坏处主要是大部分算法

205
0:07:01.000 --> 0:07:03.000
在channel和layer层面去做一个

206
0:07:03.000 --> 0:07:05.000
减字保留了原始卷机的

207
0:07:05.000 --> 0:07:07.000
一个结构化不需要专用的

208
0:07:07.000 --> 0:07:09.000
硬件去实现而且减字算法呀

209
0:07:09.000 --> 0:07:11.000
比较有规律比较好学习

210
0:07:11.000 --> 0:07:13.000
那坏处就是减字的

211
0:07:13.000 --> 0:07:15.000
算法相对来说比较复杂

212
0:07:15.000 --> 0:07:17.000
需要我们真正的去

213
0:07:17.000 --> 0:07:19.000
投进去去了解

214
0:07:19.000 --> 0:07:21.000
下面这个图呢

215
0:07:21.000 --> 0:07:23.000
采用于这篇文章

216
0:07:23.000 --> 0:07:25.000
它里面呢就做了很多的

217
0:07:25.000 --> 0:07:27.000
骁勇实验可以看到大部分呢

218
0:07:27.000 --> 0:07:29.000
是没有减字的

219
0:07:29.000 --> 0:07:31.000
之前全做参数的一个统计

220
0:07:31.000 --> 0:07:33.000
我们可以看到其实大部分的数据都

221
0:07:33.000 --> 0:07:35.000
集中在0,0的数据有

222
0:07:35.000 --> 0:07:37.000
非常的多经过结构化的

223
0:07:37.000 --> 0:07:39.000
减字之后呢我们看到呢

224
0:07:39.000 --> 0:07:41.000
整个数据的分布啊

225
0:07:41.000 --> 0:07:43.000
确实更加的服从于高斯

226
0:07:43.000 --> 0:07:45.000
分布

227
0:07:45.000 --> 0:07:47.000
这也是我们希望看到的

228
0:07:47.000 --> 0:07:49.000
就不需要我们的网络模型不要那幺

229
0:07:49.000 --> 0:07:51.000
多勇于的参数一大堆

230
0:07:51.000 --> 0:07:53.000
勇于的参数一大堆0存来

231
0:07:53.000 --> 0:07:55.000
干嘛还不省点内存空间

232
0:07:57.000 --> 0:07:59.000
现在呢来到

233
0:07:59.000 --> 0:08:01.000
我们第二个内容了就是

234
0:08:01.000 --> 0:08:03.000
减字的流程看一下

235
0:08:03.000 --> 0:08:05.000
减字有几个流程减字一般

236
0:08:05.000 --> 0:08:07.000
是怎幺做的首先我们可以

237
0:08:07.000 --> 0:08:09.000
看到减字啊一般

238
0:08:09.000 --> 0:08:11.000
对减字来说我们有三种常见的算法

239
0:08:11.000 --> 0:08:13.000
第一个呢就是

240
0:08:13.000 --> 0:08:15.000
训练一个模型先训练

241
0:08:15.000 --> 0:08:17.000
一个模型然后呢对这个模型

242
0:08:17.000 --> 0:08:19.000
进行减字最后呢

243
0:08:19.000 --> 0:08:21.000
对减字后的模型呢进行

244
0:08:21.000 --> 0:08:23.000
微调第二种

245
0:08:23.000 --> 0:08:25.000
就是在模型训练的过程

246
0:08:25.000 --> 0:08:27.000
当中进行减字啊就边训

247
0:08:27.000 --> 0:08:29.000
边减了减完之后呢

248
0:08:29.000 --> 0:08:31.000
对模型再进行一个微调

249
0:08:31.000 --> 0:08:33.000
为啥都会有微调呢我们

250
0:08:33.000 --> 0:08:35.000
下面会讲到啊大家不要急

251
0:08:35.000 --> 0:08:37.000
那第三种就是

252
0:08:37.000 --> 0:08:39.000
直接进行减字然后呢

253
0:08:39.000 --> 0:08:41.000
从头开始训练那

254
0:08:41.000 --> 0:08:43.000
第三种方式呢其实用的比较少

255
0:08:43.000 --> 0:08:45.000
可以看到刚才的

256
0:08:45.000 --> 0:08:47.000
刚才会

257
0:08:47.000 --> 0:08:49.000
给大家介绍的减字的

258
0:08:49.000 --> 0:08:51.000
三种常见的算法呢有比较

259
0:08:51.000 --> 0:08:53.000
规律的三个内容第一个就是

260
0:08:53.000 --> 0:08:55.000
训练第二个就是减字

261
0:08:55.000 --> 0:08:57.000
第三个就是Fine Tuning

262
0:08:57.000 --> 0:08:59.000
微调三个框框

263
0:08:59.000 --> 0:09:01.000
三个作用啊每个框框

264
0:09:01.000 --> 0:09:03.000
都有不同的作用哦

265
0:09:03.000 --> 0:09:05.000
这里面呢我对减字呢就做了

266
0:09:05.000 --> 0:09:07.000
一个总结就是减字的

267
0:09:07.000 --> 0:09:09.000
主要的单元有三个一个是

268
0:09:09.000 --> 0:09:11.000
训练减字微调

269
0:09:11.000 --> 0:09:13.000
它各自起到什幺作用

270
0:09:13.000 --> 0:09:15.000
我们现在来看一看

271
0:09:15.000 --> 0:09:17.000
训练的作用呢主要是

272
0:09:17.000 --> 0:09:19.000
得到最佳的网络模型的性能

273
0:09:19.000 --> 0:09:21.000
用它作为基准就

274
0:09:21.000 --> 0:09:23.000
我们在做一个

275
0:09:23.000 --> 0:09:25.000
Fine Tuning或者Pooling的工作之后呢

276
0:09:25.000 --> 0:09:27.000
需要回顾一下跟我们训练的时候精度

277
0:09:27.000 --> 0:09:29.000
是不是相同的有没有破坏

278
0:09:29.000 --> 0:09:31.000
或极度的降低了我们模型

279
0:09:31.000 --> 0:09:33.000
训练的精度那第二个工作呢

280
0:09:33.000 --> 0:09:35.000
就是真正的Pruning

281
0:09:35.000 --> 0:09:37.000
减字我们会根据不同的算法呢

282
0:09:37.000 --> 0:09:39.000
对我们刚才训练的网络模型

283
0:09:39.000 --> 0:09:41.000
进行减字

284
0:09:41.000 --> 0:09:43.000
调整一下我们网络模型的信道数啊

285
0:09:43.000 --> 0:09:45.000
全中数啊等其他的参数

286
0:09:45.000 --> 0:09:47.000
最后一个单元呢就是

287
0:09:47.000 --> 0:09:49.000
微调我们需要在原始的

288
0:09:49.000 --> 0:09:51.000
数据集呢之上进行微调

289
0:09:51.000 --> 0:09:53.000
那这个微调的模型呢

290
0:09:53.000 --> 0:09:55.000
就是经过减字后的模型啦

291
0:09:55.000 --> 0:09:57.000
因为经过减字

292
0:09:57.000 --> 0:09:59.000
所以网络模型的结构变化呢

293
0:09:59.000 --> 0:10:01.000
于是呢我们希望通过微调

294
0:10:01.000 --> 0:10:03.000
来去恢复弥补减字后

295
0:10:03.000 --> 0:10:05.000
所丢失的精度和性能

296
0:10:07.000 --> 0:10:09.000
第一种模式

297
0:10:09.000 --> 0:10:11.000
也是最简单的就是上面

298
0:10:11.000 --> 0:10:13.000
这个流程的完全分解

299
0:10:13.000 --> 0:10:15.000
首先我们有一个已经训练好的

300
0:10:15.000 --> 0:10:17.000
网络模型

301
0:10:17.000 --> 0:10:19.000
然后呢对这个网络模型进行

302
0:10:19.000 --> 0:10:21.000
各种各样的减字

303
0:10:21.000 --> 0:10:23.000
虽然说是各种各样实际上

304
0:10:23.000 --> 0:10:25.000
只有结构化和非结构化两种

305
0:10:25.000 --> 0:10:27.000
然后呢对减字后的模型啊

306
0:10:27.000 --> 0:10:29.000
进行微调恢复一定的精度

307
0:10:29.000 --> 0:10:31.000
这种方式呢

308
0:10:31.000 --> 0:10:33.000
就是最原始最naive的

309
0:10:33.000 --> 0:10:35.000
模型减字的流程

310
0:10:35.000 --> 0:10:37.000
接着我们看一下另外两种

311
0:10:37.000 --> 0:10:39.000
也是用的比较多的

312
0:10:39.000 --> 0:10:41.000
首先呢我们还是一样

313
0:10:41.000 --> 0:10:43.000
拿到一个已经训练好的网络模型

314
0:10:43.000 --> 0:10:45.000
接着会有一个子的网络模型

315
0:10:45.000 --> 0:10:47.000
采样

316
0:10:47.000 --> 0:10:49.000
可能我们会有很多个子模型

317
0:10:49.000 --> 0:10:51.000
于是呢对这些子模型进行评估

318
0:10:51.000 --> 0:10:53.000
看哪个模型的精度性能比较好

319
0:10:53.000 --> 0:10:55.000
最后呢我们选择其中一个

320
0:10:55.000 --> 0:10:57.000
对它进行微调恢复网络模型的精度

321
0:10:59.000 --> 0:11:01.000
最后一种就是基于NAS

322
0:11:01.000 --> 0:11:03.000
自动搜索的方式

323
0:11:03.000 --> 0:11:05.000
那实际上呢我们在经过

324
0:11:05.000 --> 0:11:07.000
真正的工程验证当中呢

325
0:11:07.000 --> 0:11:09.000
最后一种更多的是在学术的前年

326
0:11:09.000 --> 0:11:11.000
但是在工业界确实用的很少

327
0:11:11.000 --> 0:11:13.000
因为基于NAS的搜索

328
0:11:13.000 --> 0:11:15.000
消耗我们的资源了

329
0:11:15.000 --> 0:11:17.000
就没钱你训不起来

330
0:11:17.000 --> 0:11:19.000
没钱你捡不起来

331
0:11:19.000 --> 0:11:21.000
没钱你也不敢捡

332
0:11:21.000 --> 0:11:23.000
那看一下我们这里面

333
0:11:23.000 --> 0:11:25.000
首先还是训练一个网络模型

334
0:11:25.000 --> 0:11:27.000
然后呢基于大规模的一个搜索算法

335
0:11:27.000 --> 0:11:29.000
进行剪字

336
0:11:29.000 --> 0:11:31.000
有可能基于NAS的搜索方法呢

337
0:11:31.000 --> 0:11:33.000
就是剪完之后我就不需要微调了

338
0:11:33.000 --> 0:11:35.000
直接输出

339
0:11:35.000 --> 0:11:37.000
了解完对应的剪字算法

340
0:11:37.000 --> 0:11:39.000
现在呢我们实际的打开一个剪字算法

341
0:11:39.000 --> 0:11:41.000
来去了解一下它具体怎幺实现

342
0:11:41.000 --> 0:11:43.000
Lomalization

343
0:11:45.000 --> 0:11:47.000
首先呢我们要讲一个具体的概念

344
0:11:47.000 --> 0:11:49.000
就是Lelom是Base Channel Pulling

345
0:11:49.000 --> 0:11:51.000
Channel Pulling就是我们的结构化剪字

346
0:11:51.000 --> 0:11:53.000
专门针对Channel

347
0:11:53.000 --> 0:11:55.000
进行剪字的

348
0:11:57.000 --> 0:11:59.000
刚才说了我们根据Channel来剪

349
0:11:59.000 --> 0:12:01.000
但是我们剪的标准

350
0:12:01.000 --> 0:12:03.000
我们剪的法则

351
0:12:03.000 --> 0:12:05.000
用什幺来约束呢

352
0:12:05.000 --> 0:12:07.000
就是Lelom

353
0:12:07.000 --> 0:12:09.000
通过计算Lelom来评价

354
0:12:09.000 --> 0:12:11.000
这个卷集合到底重不重要

355
0:12:11.000 --> 0:12:13.000
如果不重要就把它剪掉

356
0:12:13.000 --> 0:12:15.000
如果Lelom的值比较低

357
0:12:15.000 --> 0:12:17.000
那就证明这个卷集合

358
0:12:17.000 --> 0:12:19.000
这个Channel不重要

359
0:12:19.000 --> 0:12:21.000
于是就把它剪掉

360
0:12:21.000 --> 0:12:23.000
基于这个算法原理呢

361
0:12:23.000 --> 0:12:25.000
我们看一下具体的算法步骤

362
0:12:25.000 --> 0:12:27.000
首先我们需要对每一个卷集合

363
0:12:27.000 --> 0:12:29.000
是每一个计算它的

364
0:12:29.000 --> 0:12:31.000
全重的绝对值

365
0:12:31.000 --> 0:12:33.000
那就是这条公式

366
0:12:33.000 --> 0:12:35.000
把每一个卷集合

367
0:12:35.000 --> 0:12:37.000
进行计算

368
0:12:37.000 --> 0:12:39.000
然后把卷集合刚才算得到的xj

369
0:12:39.000 --> 0:12:41.000
进行排序

370
0:12:41.000 --> 0:12:43.000
排完序之后我们就知道

371
0:12:43.000 --> 0:12:45.000
哪个重要哪个不重要

372
0:12:45.000 --> 0:12:47.000
第三步就是设定一个域值

373
0:12:47.000 --> 0:12:49.000
对小于这个域值的卷集合

374
0:12:49.000 --> 0:12:51.000
还有它对应的feature map

375
0:12:51.000 --> 0:12:53.000
进行剪字

376
0:12:53.000 --> 0:12:55.000
剪掉它

377
0:12:55.000 --> 0:12:57.000
第四步呢在工程上面很重要

378
0:12:57.000 --> 0:12:59.000
下一个卷集层

379
0:12:59.000 --> 0:13:01.000
与刚才剪掉的feature map相关的卷集合

380
0:13:01.000 --> 0:13:03.000
也需要把它剪掉

381
0:13:03.000 --> 0:13:05.000
这种有点类似于连带子的关系

382
0:13:05.000 --> 0:13:07.000
最后一步就是

383
0:13:07.000 --> 0:13:09.000
对第i层和第i加1层的

384
0:13:09.000 --> 0:13:11.000
新的全重重新被创建

385
0:13:11.000 --> 0:13:13.000
那剩下的全重会复制到

386
0:13:13.000 --> 0:13:15.000
新的模型当中

387
0:13:15.000 --> 0:13:17.000
重新的执行这些步骤

388
0:13:19.000 --> 0:13:21.000
那最后我们看一下

389
0:13:21.000 --> 0:13:23.000
LE-LOM这篇文章的实际的效果

390
0:13:23.000 --> 0:13:25.000
作者呢

391
0:13:25.000 --> 0:13:27.000
做了大量的比对实验

392
0:13:27.000 --> 0:13:29.000
包括我们的parallel inversion

393
0:13:29.000 --> 0:13:31.000
就是我们剪字的比例

394
0:13:31.000 --> 0:13:33.000
在剪字的吸收率到60%的前提下

395
0:13:33.000 --> 0:13:35.000
基本上也能够保持

396
0:13:35.000 --> 0:13:37.000
比较好的精度

397
0:13:37.000 --> 0:13:39.000
那这个就是LE-LOM的一个具体的实验的结果

398
0:13:41.000 --> 0:13:43.000
综理在第一次接触剪字算法的时候

399
0:13:43.000 --> 0:13:45.000
去看一下

400
0:13:45.000 --> 0:13:47.000
当时候我们的标杆

401
0:13:47.000 --> 0:13:49.000
应该是18年的时候

402
0:13:49.000 --> 0:13:51.000
PyTorch没有完全起来

403
0:13:51.000 --> 0:13:53.000
TensorFlow就集成了LE-LOM

404
0:13:53.000 --> 0:13:55.000
这一个剪字的算法和公式

405
0:13:55.000 --> 0:13:57.000
所以LE-LOM是一个非常经典的剪字算法

406
0:13:57.000 --> 0:13:59.000
也非常欢迎大家去学习一下

407
0:13:59.000 --> 0:14:01.000
好了

408
0:14:01.000 --> 0:14:03.000
最后就是参考文献相关的论文

409
0:14:03.000 --> 0:14:05.000
也欢迎大家去阅读一下相关的算法

410
0:14:05.000 --> 0:14:07.000
谢谢各位

411
0:14:07.000 --> 0:14:09.000
拜了个拜

