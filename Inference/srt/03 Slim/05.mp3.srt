1
00:00:00,000 --> 00:00:03,000
嗨!大家好,我是ZOMI

2
00:00:03,000 --> 00:00:07,000
如果大家觉得我的视频里面有讲解的不明白的地方

3
00:00:07,000 --> 00:00:09,000
或者讲解的比较含糊的地方呢

4
00:00:09,000 --> 00:00:12,000
也非常欢迎各位好哥哥和小姐姐呢

5
00:00:12,000 --> 00:00:15,000
给我留言和评论弹幕

6
00:00:15,000 --> 00:00:19,000
ZOMI呢会根据相对的内容来进行一个更新

7
00:00:19,000 --> 00:00:21,000
废话就不多说了

8
00:00:21,000 --> 00:00:24,000
我们来到推进引擎模型压缩里面的

9
00:00:24,000 --> 00:00:27,000
模型简单的解说

10
00:00:27,000 --> 00:00:29,000
简单的解说

11
00:00:29,000 --> 00:00:31,000
简单的解说

12
00:00:31,000 --> 00:00:33,000
简单的解说

13
00:00:33,000 --> 00:00:35,000
简单的解说

14
00:00:35,000 --> 00:00:37,000
简单的解说

15
00:00:37,000 --> 00:00:39,000
简单的解说

16
00:00:39,000 --> 00:00:41,000
简单的解说

17
00:00:41,000 --> 00:00:43,000
简单的解说

18
00:00:43,000 --> 00:00:45,000
简单的解说

19
00:00:45,000 --> 00:00:47,000
简单的解说

20
00:00:47,000 --> 00:00:49,000
简单的解说

21
00:00:49,000 --> 00:00:51,000
简单的解说

22
00:00:51,000 --> 00:00:53,000
简单的解说

23
00:00:53,000 --> 00:00:55,000
简单的解说

24
00:00:55,000 --> 00:00:57,000
简单的解说

25
00:00:57,000 --> 00:00:59,000
简单的解说

26
00:00:59,000 --> 00:01:01,000
简单的解说

27
00:01:01,000 --> 00:01:03,000
简单的解说

28
00:01:03,000 --> 00:01:05,000
简单的解说

29
00:01:05,000 --> 00:01:07,000
简单的解说

30
00:01:07,000 --> 00:01:09,000
简单的解说

31
00:01:09,000 --> 00:01:11,000
简单的解说

32
00:01:11,000 --> 00:01:13,000
简单的解说

33
00:01:13,000 --> 00:01:15,000
简单的解说

34
00:01:15,000 --> 00:01:17,000
简单的解说

35
00:01:17,000 --> 00:01:19,000
简单的解说

36
00:01:19,000 --> 00:01:21,000
简单的解说

37
00:01:21,000 --> 00:01:23,000
简单的解说

38
00:01:23,000 --> 00:01:25,000
简单的解说

39
00:01:25,000 --> 00:01:27,000
简单的解说

40
00:01:27,000 --> 00:01:29,000
简单的解说

41
00:01:29,000 --> 00:01:31,000
简单的解说

42
00:01:31,000 --> 00:01:33,000
简单的解说

43
00:01:33,000 --> 00:01:35,000
简单的解说

44
00:01:35,000 --> 00:01:37,000
简单的解说

45
00:01:37,000 --> 00:01:39,000
简单的解说

46
00:01:39,000 --> 00:01:41,000
简单的解说

47
00:01:41,000 --> 00:01:43,000
简单的解说

48
00:01:43,000 --> 00:01:45,000
简单的解说

49
00:01:45,000 --> 00:01:47,000
简单的解说

50
00:01:47,000 --> 00:01:49,000
简单的解说

51
00:01:49,000 --> 00:01:51,000
简单的解说

52
00:01:51,000 --> 00:01:53,000
简单的解说

53
00:01:53,000 --> 00:01:55,000
简单的解说

54
00:01:55,000 --> 00:01:57,000
简单的解说

55
00:01:57,000 --> 00:01:59,000
简单的解说

56
00:01:59,000 --> 00:02:01,000
简单的解说

57
00:02:01,000 --> 00:02:03,000
简单的解说

58
00:02:03,000 --> 00:02:05,000
简单的解说

59
00:02:05,000 --> 00:02:07,000
简单的解说

60
00:02:07,000 --> 00:02:09,000
简单的解说

61
00:02:09,000 --> 00:02:11,000
简单的解说

62
00:02:11,000 --> 00:02:13,000
简单的解说

63
00:02:13,000 --> 00:02:15,000
简单的解说

64
00:02:15,000 --> 00:02:17,000
简单的解说

65
00:02:17,000 --> 00:02:19,000
简单的解说

66
00:02:19,000 --> 00:02:21,000
具体量化和减字的区别

67
00:02:21,000 --> 00:02:23,000
我们看一下下面这个图

68
00:02:23,000 --> 00:02:25,000
左边的这个是我们原始的网络模型的数据

69
00:02:25,000 --> 00:02:27,000
左边的这个是我们原始的网络模型的数据

70
00:02:27,000 --> 00:02:29,000
存的或者训练的时候是IP32进行存储的

71
00:02:29,000 --> 00:02:31,000
但是我们经过量化之后

72
00:02:31,000 --> 00:02:33,000
我们的存储的数据

73
00:02:33,000 --> 00:02:35,000
就变成8bit或者int8

74
00:02:35,000 --> 00:02:37,000
在实际网络模型量化

75
00:02:37,000 --> 00:02:39,000
推理的时候

76
00:02:39,000 --> 00:02:41,000
如果硬件支持8bit运算的指令集

77
00:02:41,000 --> 00:02:43,000
那这个时候8bit或者int8的数据

78
00:02:43,000 --> 00:02:45,000
就可以真正的在我们的

79
00:02:45,000 --> 00:02:47,000
硬件上面部署起来

80
00:02:47,000 --> 00:02:49,000
原来的网络模型

81
00:02:49,000 --> 00:02:51,000
有多少参数量

82
00:02:51,000 --> 00:02:53,000
量化之后它仍然有多少参数量

83
00:02:53,000 --> 00:02:55,000
下面我们看一下

84
00:02:55,000 --> 00:02:57,000
模型的减字

85
00:02:57,000 --> 00:02:59,000
左边这个就是原始训练过后

86
00:02:59,000 --> 00:03:01,000
或者发击点过后的一个网络模型的

87
00:03:01,000 --> 00:03:03,000
一些参数量

88
00:03:03,000 --> 00:03:05,000
经过减字之后我们就会把一些

89
00:03:05,000 --> 00:03:07,000
不重要的或者勇于的

90
00:03:07,000 --> 00:03:09,000
非关键的权重或者数据

91
00:03:09,000 --> 00:03:11,000
把它减掉剩下一些比较重要的

92
00:03:11,000 --> 00:03:13,000
数据

93
00:03:13,000 --> 00:03:15,000
这个就是减字的原理

94
00:03:15,000 --> 00:03:17,000
减字和量化是完全不一样的

95
00:03:17,000 --> 00:03:19,000
通过这两个图

96
00:03:19,000 --> 00:03:21,000
我们可以看到虽然减字和量化

97
00:03:21,000 --> 00:03:23,000
的目标是一样的

98
00:03:23,000 --> 00:03:25,000
但是它们的优化手段是不一样的

99
00:03:27,000 --> 00:03:29,000
中米就发现谷歌

100
00:03:29,000 --> 00:03:31,000
写了一篇比较有意思的文章

101
00:03:31,000 --> 00:03:33,000
to pwn or not to pwn

102
00:03:33,000 --> 00:03:35,000
确实谷歌在AIC方面的研究非常的深厚

103
00:03:35,000 --> 00:03:37,000
这篇文章确实我觉得

104
00:03:37,000 --> 00:03:39,000
可以作为一个pwning或者减字的

105
00:03:39,000 --> 00:03:41,000
一个白皮书去看

106
00:03:41,000 --> 00:03:43,000
也非常欢迎大家去阅读一下这篇论文

107
00:03:43,000 --> 00:03:45,000
这篇论文我总结了几个观点

108
00:03:45,000 --> 00:03:47,000
第一个就是在内存占用相同的情况下

109
00:03:47,000 --> 00:03:49,000
实际上一些又大又稀疏的网络模型

110
00:03:49,000 --> 00:03:51,000
会比一些小的密集的网络模型能够实现更好的精度

111
00:03:51,000 --> 00:03:53,000
所以说我们一般不希望它训练一个小模型

112
00:03:53,000 --> 00:03:55,000
而是训练一个大模型

113
00:03:55,000 --> 00:03:57,000
接着执行一个减字

114
00:03:57,000 --> 00:03:59,000
确实会比直接训练一个小模型更加有效

115
00:03:59,000 --> 00:04:01,000
这也是对应于上一个视频所分享

116
00:04:01,000 --> 00:04:03,000
里面提出的一个疑问

117
00:04:03,000 --> 00:04:05,000
那幺在这篇论文中

118
00:04:05,000 --> 00:04:07,000
我们可以看到

119
00:04:07,000 --> 00:04:09,000
中米在减字和量化方面

120
00:04:09,000 --> 00:04:11,000
是非常有利的

121
00:04:11,000 --> 00:04:13,000
这也是对应于上一个视频所分享

122
00:04:13,000 --> 00:04:15,000
里面提出的一个疑问

123
00:04:15,000 --> 00:04:17,000
第二个就是经过减字之后的一个稀疏模型

124
00:04:17,000 --> 00:04:19,000
确实要由于同体积

125
00:04:19,000 --> 00:04:21,000
就是相同大小的一个

126
00:04:21,000 --> 00:04:23,000
非稀疏的网络模型

127
00:04:23,000 --> 00:04:25,000
第二点就充分的说明

128
00:04:25,000 --> 00:04:27,000
其实我们的网络模型有很多的参数

129
00:04:27,000 --> 00:04:29,000
其实是不一定是需要的

130
00:04:29,000 --> 00:04:31,000
它可能没有用

131
00:04:31,000 --> 00:04:33,000
第三点就是在资源受限的情况下

132
00:04:33,000 --> 00:04:35,000
减字属于一种比较高效

133
00:04:35,000 --> 00:04:37,000
或者比较有效的

134
00:04:37,000 --> 00:04:39,000
模型的压缩的策略和方法

135
00:04:39,000 --> 00:04:41,000
我更多是关注于

136
00:04:41,000 --> 00:04:43,000
一二三条

137
00:04:43,000 --> 00:04:45,000
也是这篇文章

138
00:04:45,000 --> 00:04:47,000
做了大量的消融实验之后

139
00:04:47,000 --> 00:04:49,000
得到的一个结论

140
00:04:49,000 --> 00:04:51,000
也非常欢迎大家去看看

141
00:04:51,000 --> 00:04:53,000
下面我们来看一个第二个

142
00:04:53,000 --> 00:04:55,000
比较重要的内容

143
00:04:55,000 --> 00:04:57,000
减字算法的分类

144
00:04:57,000 --> 00:04:59,000
下面呢

145
00:04:59,000 --> 00:05:01,000
我总结了

146
00:05:01,000 --> 00:05:03,000
或者我避避了

147
00:05:03,000 --> 00:05:05,000
我看了很多的减字的算法

148
00:05:05,000 --> 00:05:07,000
这里面呢

149
00:05:07,000 --> 00:05:09,000
我总结了几个方向

150
00:05:09,000 --> 00:05:11,000
就是减字算法的分类

151
00:05:11,000 --> 00:05:13,000
那减字算法呢

152
00:05:13,000 --> 00:05:15,000
主要有两大类别

153
00:05:15,000 --> 00:05:17,000
一个是左边的

154
00:05:17,000 --> 00:05:19,000
非结构化的减字

155
00:05:19,000 --> 00:05:21,000
第二个呢

156
00:05:21,000 --> 00:05:23,000
就是结构化的减字

157
00:05:23,000 --> 00:05:25,000
非结构化的减字呢

158
00:05:25,000 --> 00:05:27,000
就像左边的这个图

159
00:05:27,000 --> 00:05:29,000
主要是对一些独立的权重

160
00:05:29,000 --> 00:05:31,000
或者神经元

161
00:05:31,000 --> 00:05:33,000
或者一些神经元的链接

162
00:05:33,000 --> 00:05:35,000
进行减字

163
00:05:35,000 --> 00:05:37,000
比如说我在16年

164
00:05:37,000 --> 00:05:39,000
17年18年的时候

165
00:05:39,000 --> 00:05:41,000
接触减字算法

166
00:05:41,000 --> 00:05:43,000
那时候还没有那幺成熟

167
00:05:43,000 --> 00:05:45,000
第二个呢

168
00:05:45,000 --> 00:05:47,000
就是结构化的减字

169
00:05:47,000 --> 00:05:49,000
右边的这三个

170
00:05:49,000 --> 00:05:51,000
更多的是结构化的减字

171
00:05:51,000 --> 00:05:53,000
结构化的减字就会有规律

172
00:05:53,000 --> 00:05:55,000
有顺序的

173
00:05:55,000 --> 00:05:57,000
对我们的神经网络

174
00:05:57,000 --> 00:05:59,000
或者我们的计算图

175
00:05:59,000 --> 00:06:01,000
进行减字

176
00:06:01,000 --> 00:06:03,000
几个比较经典的

177
00:06:03,000 --> 00:06:05,000
而两种的减字方式呢

178
00:06:05,000 --> 00:06:07,000
也有它的利弊

179
00:06:07,000 --> 00:06:09,000
我们现在来看一下

180
00:06:09,000 --> 00:06:11,000
非结构化的减字

181
00:06:11,000 --> 00:06:13,000
它的一个好处

182
00:06:13,000 --> 00:06:15,000
就是减字算法特别的简单

183
00:06:15,000 --> 00:06:17,000
模型的压缩比例

184
00:06:17,000 --> 00:06:19,000
确实可以压得非常的高

185
00:06:19,000 --> 00:06:21,000
那它的缺点也是非常明显的

186
00:06:21,000 --> 00:06:23,000
第一个比较明显的问题呢

187
00:06:23,000 --> 00:06:25,000
就是精度不可控

188
00:06:25,000 --> 00:06:27,000
精度不可控其实是

189
00:06:27,000 --> 00:06:29,000
一般我们来说不可接受

190
00:06:29,000 --> 00:06:31,000
真的是不可接受的

191
00:06:31,000 --> 00:06:33,000
我们要用机器去训练一个好模型

192
00:06:33,000 --> 00:06:35,000
就是为了提高精度嘛

193
00:06:35,000 --> 00:06:37,000
你给我推理的时候精度不可控

194
00:06:37,000 --> 00:06:39,000
你完呢

195
00:06:39,000 --> 00:06:41,000
一般来说呀

196
00:06:41,000 --> 00:06:43,000
这个非结构化的减字呢

197
00:06:43,000 --> 00:06:45,000
我们很少的去用

198
00:06:45,000 --> 00:06:47,000
而且呢非结构化的减字呢

199
00:06:47,000 --> 00:06:49,000
减字后基本上我们的权重呢

200
00:06:49,000 --> 00:06:51,000
会极度的稀疏化

201
00:06:51,000 --> 00:06:53,000
没有专用的硬件去实现

202
00:06:53,000 --> 00:06:55,000
是很难够真正的去做一个

203
00:06:55,000 --> 00:06:57,000
训练推理的加速

204
00:06:57,000 --> 00:06:59,000
那第二个就是结构化的减字

205
00:06:59,000 --> 00:07:01,000
坏处主要是大部分算法

206
00:07:01,000 --> 00:07:03,000
在channel和layer层面去做一个

207
00:07:03,000 --> 00:07:05,000
减字保留了原始卷机的

208
00:07:05,000 --> 00:07:07,000
一个结构化不需要专用的

209
00:07:07,000 --> 00:07:09,000
硬件去实现而且减字算法呀

210
00:07:09,000 --> 00:07:11,000
比较有规律比较好学习

211
00:07:11,000 --> 00:07:13,000
那坏处就是减字的

212
00:07:13,000 --> 00:07:15,000
算法相对来说比较复杂

213
00:07:15,000 --> 00:07:17,000
需要我们真正的去

214
00:07:17,000 --> 00:07:19,000
投进去去了解

215
00:07:19,000 --> 00:07:21,000
下面这个图呢

216
00:07:21,000 --> 00:07:23,000
采用于这篇文章

217
00:07:23,000 --> 00:07:25,000
它里面呢就做了很多的

218
00:07:25,000 --> 00:07:27,000
骁勇实验可以看到大部分呢

219
00:07:27,000 --> 00:07:29,000
是没有减字的

220
00:07:29,000 --> 00:07:31,000
之前全做参数的一个统计

221
00:07:31,000 --> 00:07:33,000
我们可以看到其实大部分的数据都

222
00:07:33,000 --> 00:07:35,000
集中在0,0的数据有

223
00:07:35,000 --> 00:07:37,000
非常的多经过结构化的

224
00:07:37,000 --> 00:07:39,000
减字之后呢我们看到呢

225
00:07:39,000 --> 00:07:41,000
整个数据的分布啊

226
00:07:41,000 --> 00:07:43,000
确实更加的服从于高斯

227
00:07:43,000 --> 00:07:45,000
分布

228
00:07:45,000 --> 00:07:47,000
这也是我们希望看到的

229
00:07:47,000 --> 00:07:49,000
就不需要我们的网络模型不要那幺

230
00:07:49,000 --> 00:07:51,000
多勇于的参数一大堆

231
00:07:51,000 --> 00:07:53,000
勇于的参数一大堆0存来

232
00:07:53,000 --> 00:07:55,000
干嘛还不省点内存空间

233
00:07:57,000 --> 00:07:59,000
现在呢来到

234
00:07:59,000 --> 00:08:01,000
我们第二个内容了就是

235
00:08:01,000 --> 00:08:03,000
减字的流程看一下

236
00:08:03,000 --> 00:08:05,000
减字有几个流程减字一般

237
00:08:05,000 --> 00:08:07,000
是怎幺做的首先我们可以

238
00:08:07,000 --> 00:08:09,000
看到减字啊一般

239
00:08:09,000 --> 00:08:11,000
对减字来说我们有三种常见的算法

240
00:08:11,000 --> 00:08:13,000
第一个呢就是

241
00:08:13,000 --> 00:08:15,000
训练一个模型先训练

242
00:08:15,000 --> 00:08:17,000
一个模型然后呢对这个模型

243
00:08:17,000 --> 00:08:19,000
进行减字最后呢

244
00:08:19,000 --> 00:08:21,000
对减字后的模型呢进行

245
00:08:21,000 --> 00:08:23,000
微调第二种

246
00:08:23,000 --> 00:08:25,000
就是在模型训练的过程

247
00:08:25,000 --> 00:08:27,000
当中进行减字啊就边训

248
00:08:27,000 --> 00:08:29,000
边减了减完之后呢

249
00:08:29,000 --> 00:08:31,000
对模型再进行一个微调

250
00:08:31,000 --> 00:08:33,000
为啥都会有微调呢我们

251
00:08:33,000 --> 00:08:35,000
下面会讲到啊大家不要急

252
00:08:35,000 --> 00:08:37,000
那第三种就是

253
00:08:37,000 --> 00:08:39,000
直接进行减字然后呢

254
00:08:39,000 --> 00:08:41,000
从头开始训练那

255
00:08:41,000 --> 00:08:43,000
第三种方式呢其实用的比较少

256
00:08:43,000 --> 00:08:45,000
可以看到刚才的

257
00:08:45,000 --> 00:08:47,000
刚才会

258
00:08:47,000 --> 00:08:49,000
给大家介绍的减字的

259
00:08:49,000 --> 00:08:51,000
三种常见的算法呢有比较

260
00:08:51,000 --> 00:08:53,000
规律的三个内容第一个就是

261
00:08:53,000 --> 00:08:55,000
训练第二个就是减字

262
00:08:55,000 --> 00:08:57,000
第三个就是Fine Tuning

263
00:08:57,000 --> 00:08:59,000
微调三个框框

264
00:08:59,000 --> 00:09:01,000
三个作用啊每个框框

265
00:09:01,000 --> 00:09:03,000
都有不同的作用哦

266
00:09:03,000 --> 00:09:05,000
这里面呢我对减字呢就做了

267
00:09:05,000 --> 00:09:07,000
一个总结就是减字的

268
00:09:07,000 --> 00:09:09,000
主要的单元有三个一个是

269
00:09:09,000 --> 00:09:11,000
训练减字微调

270
00:09:11,000 --> 00:09:13,000
它各自起到什幺作用

271
00:09:13,000 --> 00:09:15,000
我们现在来看一看

272
00:09:15,000 --> 00:09:17,000
训练的作用呢主要是

273
00:09:17,000 --> 00:09:19,000
得到最佳的网络模型的性能

274
00:09:19,000 --> 00:09:21,000
用它作为基准就

275
00:09:21,000 --> 00:09:23,000
我们在做一个

276
00:09:23,000 --> 00:09:25,000
Fine Tuning或者Pooling的工作之后呢

277
00:09:25,000 --> 00:09:27,000
需要回顾一下跟我们训练的时候精度

278
00:09:27,000 --> 00:09:29,000
是不是相同的有没有破坏

279
00:09:29,000 --> 00:09:31,000
或极度的降低了我们模型

280
00:09:31,000 --> 00:09:33,000
训练的精度那第二个工作呢

281
00:09:33,000 --> 00:09:35,000
就是真正的Pruning

282
00:09:35,000 --> 00:09:37,000
减字我们会根据不同的算法呢

283
00:09:37,000 --> 00:09:39,000
对我们刚才训练的网络模型

284
00:09:39,000 --> 00:09:41,000
进行减字

285
00:09:41,000 --> 00:09:43,000
调整一下我们网络模型的信道数啊

286
00:09:43,000 --> 00:09:45,000
全中数啊等其他的参数

287
00:09:45,000 --> 00:09:47,000
最后一个单元呢就是

288
00:09:47,000 --> 00:09:49,000
微调我们需要在原始的

289
00:09:49,000 --> 00:09:51,000
数据集呢之上进行微调

290
00:09:51,000 --> 00:09:53,000
那这个微调的模型呢

291
00:09:53,000 --> 00:09:55,000
就是经过减字后的模型啦

292
00:09:55,000 --> 00:09:57,000
因为经过减字

293
00:09:57,000 --> 00:09:59,000
所以网络模型的结构变化呢

294
00:09:59,000 --> 00:10:01,000
于是呢我们希望通过微调

295
00:10:01,000 --> 00:10:03,000
来去恢复弥补减字后

296
00:10:03,000 --> 00:10:05,000
所丢失的精度和性能

297
00:10:07,000 --> 00:10:09,000
第一种模式

298
00:10:09,000 --> 00:10:11,000
也是最简单的就是上面

299
00:10:11,000 --> 00:10:13,000
这个流程的完全分解

300
00:10:13,000 --> 00:10:15,000
首先我们有一个已经训练好的

301
00:10:15,000 --> 00:10:17,000
网络模型

302
00:10:17,000 --> 00:10:19,000
然后呢对这个网络模型进行

303
00:10:19,000 --> 00:10:21,000
各种各样的减字

304
00:10:21,000 --> 00:10:23,000
虽然说是各种各样实际上

305
00:10:23,000 --> 00:10:25,000
只有结构化和非结构化两种

306
00:10:25,000 --> 00:10:27,000
然后呢对减字后的模型啊

307
00:10:27,000 --> 00:10:29,000
进行微调恢复一定的精度

308
00:10:29,000 --> 00:10:31,000
这种方式呢

309
00:10:31,000 --> 00:10:33,000
就是最原始最naive的

310
00:10:33,000 --> 00:10:35,000
模型减字的流程

311
00:10:35,000 --> 00:10:37,000
接着我们看一下另外两种

312
00:10:37,000 --> 00:10:39,000
也是用的比较多的

313
00:10:39,000 --> 00:10:41,000
首先呢我们还是一样

314
00:10:41,000 --> 00:10:43,000
拿到一个已经训练好的网络模型

315
00:10:43,000 --> 00:10:45,000
接着会有一个子的网络模型

316
00:10:45,000 --> 00:10:47,000
采样

317
00:10:47,000 --> 00:10:49,000
可能我们会有很多个子模型

318
00:10:49,000 --> 00:10:51,000
于是呢对这些子模型进行评估

319
00:10:51,000 --> 00:10:53,000
看哪个模型的精度性能比较好

320
00:10:53,000 --> 00:10:55,000
最后呢我们选择其中一个

321
00:10:55,000 --> 00:10:57,000
对它进行微调恢复网络模型的精度

322
00:10:59,000 --> 00:11:01,000
最后一种就是基于NAS

323
00:11:01,000 --> 00:11:03,000
自动搜索的方式

324
00:11:03,000 --> 00:11:05,000
那实际上呢我们在经过

325
00:11:05,000 --> 00:11:07,000
真正的工程验证当中呢

326
00:11:07,000 --> 00:11:09,000
最后一种更多的是在学术的前年

327
00:11:09,000 --> 00:11:11,000
但是在工业界确实用的很少

328
00:11:11,000 --> 00:11:13,000
因为基于NAS的搜索

329
00:11:13,000 --> 00:11:15,000
消耗我们的资源了

330
00:11:15,000 --> 00:11:17,000
就没钱你训不起来

331
00:11:17,000 --> 00:11:19,000
没钱你捡不起来

332
00:11:19,000 --> 00:11:21,000
没钱你也不敢捡

333
00:11:21,000 --> 00:11:23,000
那看一下我们这里面

334
00:11:23,000 --> 00:11:25,000
首先还是训练一个网络模型

335
00:11:25,000 --> 00:11:27,000
然后呢基于大规模的一个搜索算法

336
00:11:27,000 --> 00:11:29,000
进行剪字

337
00:11:29,000 --> 00:11:31,000
有可能基于NAS的搜索方法呢

338
00:11:31,000 --> 00:11:33,000
就是剪完之后我就不需要微调了

339
00:11:33,000 --> 00:11:35,000
直接输出

340
00:11:35,000 --> 00:11:37,000
了解完对应的剪字算法

341
00:11:37,000 --> 00:11:39,000
现在呢我们实际的打开一个剪字算法

342
00:11:39,000 --> 00:11:41,000
来去了解一下它具体怎幺实现

343
00:11:41,000 --> 00:11:43,000
Lomalization

344
00:11:45,000 --> 00:11:47,000
首先呢我们要讲一个具体的概念

345
00:11:47,000 --> 00:11:49,000
就是Lelom是Base Channel Pulling

346
00:11:49,000 --> 00:11:51,000
Channel Pulling就是我们的结构化剪字

347
00:11:51,000 --> 00:11:53,000
专门针对Channel

348
00:11:53,000 --> 00:11:55,000
进行剪字的

349
00:11:57,000 --> 00:11:59,000
刚才说了我们根据Channel来剪

350
00:11:59,000 --> 00:12:01,000
但是我们剪的标准

351
00:12:01,000 --> 00:12:03,000
我们剪的法则

352
00:12:03,000 --> 00:12:05,000
用什幺来约束呢

353
00:12:05,000 --> 00:12:07,000
就是Lelom

354
00:12:07,000 --> 00:12:09,000
通过计算Lelom来评价

355
00:12:09,000 --> 00:12:11,000
这个卷集合到底重不重要

356
00:12:11,000 --> 00:12:13,000
如果不重要就把它剪掉

357
00:12:13,000 --> 00:12:15,000
如果Lelom的值比较低

358
00:12:15,000 --> 00:12:17,000
那就证明这个卷集合

359
00:12:17,000 --> 00:12:19,000
这个Channel不重要

360
00:12:19,000 --> 00:12:21,000
于是就把它剪掉

361
00:12:21,000 --> 00:12:23,000
基于这个算法原理呢

362
00:12:23,000 --> 00:12:25,000
我们看一下具体的算法步骤

363
00:12:25,000 --> 00:12:27,000
首先我们需要对每一个卷集合

364
00:12:27,000 --> 00:12:29,000
是每一个计算它的

365
00:12:29,000 --> 00:12:31,000
全重的绝对值

366
00:12:31,000 --> 00:12:33,000
那就是这条公式

367
00:12:33,000 --> 00:12:35,000
把每一个卷集合

368
00:12:35,000 --> 00:12:37,000
进行计算

369
00:12:37,000 --> 00:12:39,000
然后把卷集合刚才算得到的xj

370
00:12:39,000 --> 00:12:41,000
进行排序

371
00:12:41,000 --> 00:12:43,000
排完序之后我们就知道

372
00:12:43,000 --> 00:12:45,000
哪个重要哪个不重要

373
00:12:45,000 --> 00:12:47,000
第三步就是设定一个域值

374
00:12:47,000 --> 00:12:49,000
对小于这个域值的卷集合

375
00:12:49,000 --> 00:12:51,000
还有它对应的feature map

376
00:12:51,000 --> 00:12:53,000
进行剪字

377
00:12:53,000 --> 00:12:55,000
剪掉它

378
00:12:55,000 --> 00:12:57,000
第四步呢在工程上面很重要

379
00:12:57,000 --> 00:12:59,000
下一个卷集层

380
00:12:59,000 --> 00:13:01,000
与刚才剪掉的feature map相关的卷集合

381
00:13:01,000 --> 00:13:03,000
也需要把它剪掉

382
00:13:03,000 --> 00:13:05,000
这种有点类似于连带子的关系

383
00:13:05,000 --> 00:13:07,000
最后一步就是

384
00:13:07,000 --> 00:13:09,000
对第i层和第i加1层的

385
00:13:09,000 --> 00:13:11,000
新的全重重新被创建

386
00:13:11,000 --> 00:13:13,000
那剩下的全重会复制到

387
00:13:13,000 --> 00:13:15,000
新的模型当中

388
00:13:15,000 --> 00:13:17,000
重新的执行这些步骤

389
00:13:19,000 --> 00:13:21,000
那最后我们看一下

390
00:13:21,000 --> 00:13:23,000
LE-LOM这篇文章的实际的效果

391
00:13:23,000 --> 00:13:25,000
作者呢

392
00:13:25,000 --> 00:13:27,000
做了大量的比对实验

393
00:13:27,000 --> 00:13:29,000
包括我们的parallel inversion

394
00:13:29,000 --> 00:13:31,000
就是我们剪字的比例

395
00:13:31,000 --> 00:13:33,000
在剪字的吸收率到60%的前提下

396
00:13:33,000 --> 00:13:35,000
基本上也能够保持

397
00:13:35,000 --> 00:13:37,000
比较好的精度

398
00:13:37,000 --> 00:13:39,000
那这个就是LE-LOM的一个具体的实验的结果

399
00:13:41,000 --> 00:13:43,000
综理在第一次接触剪字算法的时候

400
00:13:43,000 --> 00:13:45,000
去看一下

401
00:13:45,000 --> 00:13:47,000
当时候我们的标杆

402
00:13:47,000 --> 00:13:49,000
应该是18年的时候

403
00:13:49,000 --> 00:13:51,000
PyTorch没有完全起来

404
00:13:51,000 --> 00:13:53,000
TensorFlow就集成了LE-LOM

405
00:13:53,000 --> 00:13:55,000
这一个剪字的算法和公式

406
00:13:55,000 --> 00:13:57,000
所以LE-LOM是一个非常经典的剪字算法

407
00:13:57,000 --> 00:13:59,000
也非常欢迎大家去学习一下

408
00:13:59,000 --> 00:14:01,000
好了

409
00:14:01,000 --> 00:14:03,000
最后就是参考文献相关的论文

410
00:14:03,000 --> 00:14:05,000
也欢迎大家去阅读一下相关的算法

411
00:14:05,000 --> 00:14:07,000
谢谢各位

412
00:14:07,000 --> 00:14:09,000
拜了个拜

