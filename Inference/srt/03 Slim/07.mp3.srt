1
00:00:00,000 --> 00:00:04,480
【资讯互动】

2
00:00:04,480 --> 00:00:08,360
哈喽大家好,我们来到知识增留的下集

3
00:00:08,360 --> 00:00:10,360
这里是增米的电台

4
00:00:11,360 --> 00:00:12,240
在上一期里面呢

5
00:00:12,240 --> 00:00:13,880
我们已经充分地去了解了

6
00:00:13,880 --> 00:00:16,320
知识增留的一个算法提出的背景

7
00:00:16,320 --> 00:00:19,520
还有知识增留的知识的形态

8
00:00:19,520 --> 00:00:21,920
今天呢我给大家去汇报的内容呢

9
00:00:21,920 --> 00:00:24,960
主要是下面两个具体的知识增留的方法

10
00:00:24,960 --> 00:00:28,080
还有HITMAN经典的知识增留的论文去解读

11
00:00:29,080 --> 00:00:32,880
第一个内容是知识增留的方法

12
00:00:32,880 --> 00:00:37,280
同样出自于知识增留的综述这篇论文里面

13
00:00:37,280 --> 00:00:40,880
知识增留呢其实主要分为三个方法

14
00:00:40,880 --> 00:00:42,760
第一个呢就是

15
00:00:42,760 --> 00:00:45,760
离线增留offline的distillation

16
00:00:45,760 --> 00:00:49,720
第二种呢就是在线的增留online的distillation

17
00:00:49,720 --> 00:00:52,880
第三种呢就是自增留的方式

18
00:00:52,880 --> 00:00:54,120
一共有三种

19
00:00:54,120 --> 00:00:57,280
这里面呢则有两种颜色

20
00:00:57,280 --> 00:00:59,520
一种是红色一种是绿色

21
00:00:59,520 --> 00:01:03,120
红色呢就代表预训练的模型已经训练好的

22
00:01:03,120 --> 00:01:08,120
另外一种呢绿色就代表我们将要去训练的模型

23
00:01:09,320 --> 00:01:11,920
下面我们来打开看一看第一种方法

24
00:01:11,920 --> 00:01:14,520
就是offline的distillation

25
00:01:14,520 --> 00:01:17,120
离线的增留的方式

26
00:01:17,120 --> 00:01:20,120
其实呢现在来看大部分的算法

27
00:01:20,120 --> 00:01:22,200
都采用offline的这种方式

28
00:01:22,200 --> 00:01:23,640
就是离线增留

29
00:01:23,640 --> 00:01:25,720
而增留的过程分为两个阶段

30
00:01:25,760 --> 00:01:27,480
第一个阶段就是左边的

31
00:01:27,480 --> 00:01:31,040
增留前我们先训练一个教师的模型teachmodel

32
00:01:31,040 --> 00:01:32,560
先把它训练好

33
00:01:33,440 --> 00:01:36,400
第二步呢就是把教师网络的模型的知识

34
00:01:36,400 --> 00:01:38,360
增留给学生网络

35
00:01:38,360 --> 00:01:40,480
就进行一个知识迁移的过程

36
00:01:41,200 --> 00:01:43,160
因此offline的distillation呢

37
00:01:43,160 --> 00:01:44,880
就是离线增留的这种方式呢

38
00:01:44,880 --> 00:01:47,360
更侧重于知识的迁移部分

39
00:01:47,760 --> 00:01:49,200
怎幺去迁移

40
00:01:49,200 --> 00:01:50,720
迁移什幺知识

41
00:01:50,720 --> 00:01:52,240
这个点非常重要

42
00:01:52,880 --> 00:01:54,560
这里面呢值得注意的就是

43
00:01:54,560 --> 00:01:55,520
在步骤一

44
00:01:55,520 --> 00:01:58,080
增留前教师网络模型进行一个预训练

45
00:01:58,080 --> 00:02:00,720
就提前训练我们的教师模型

46
00:02:00,720 --> 00:02:02,000
那这个教师模型呢

47
00:02:02,000 --> 00:02:04,480
一般来说教师模型的参数量呢会比较大

48
00:02:04,480 --> 00:02:06,920
而且训练的时间呢也会比较长

49
00:02:07,720 --> 00:02:10,160
现在呀好多好多的大模型

50
00:02:10,160 --> 00:02:11,760
都是采用这种方式

51
00:02:11,760 --> 00:02:13,520
去得到一个小模型的

52
00:02:13,520 --> 00:02:16,760
例如BERT网络模型就推出了tinyBERT

53
00:02:16,760 --> 00:02:19,000
也就是这种增留的方式

54
00:02:19,000 --> 00:02:21,760
好像VIT也推出了tinyVIT

55
00:02:21,760 --> 00:02:23,080
这种增留的方式

56
00:02:23,120 --> 00:02:25,440
同样都是采用offline distillation

57
00:02:26,720 --> 00:02:29,480
不过这种方式呢也有自身的弊端

58
00:02:29,920 --> 00:02:32,920
通常这种模式呢就是学生网络模型呢

59
00:02:32,920 --> 00:02:35,920
过度的依赖于我们的教师网络模型

60
00:02:37,000 --> 00:02:40,240
下面我们看一下第二种增留的方式

61
00:02:40,240 --> 00:02:41,920
online distillation

62
00:02:43,480 --> 00:02:45,240
虽然在线增留跟离线增留啊

63
00:02:45,240 --> 00:02:47,040
都是左边一个教师模型

64
00:02:47,040 --> 00:02:48,720
右边一个学生模型

65
00:02:48,840 --> 00:02:50,800
不过有一点比较大的区别就是

66
00:02:50,800 --> 00:02:53,040
这两个模型都是绿的

67
00:02:53,080 --> 00:02:55,200
这不代表老师和学生都绿了

68
00:02:55,200 --> 00:02:57,480
而是代表老师和学生

69
00:02:57,480 --> 00:02:59,880
都是一个在线学习的过程

70
00:02:59,880 --> 00:03:01,560
就是一起去学习

71
00:03:07,120 --> 00:03:09,880
教师模型和学生的模型的参数

72
00:03:09,880 --> 00:03:11,000
同时更新

73
00:03:11,440 --> 00:03:13,000
整个知识增留的算法呢

74
00:03:13,000 --> 00:03:14,960
就变成一体一个端到端

75
00:03:14,960 --> 00:03:16,600
可训练可学习的方案

76
00:03:16,600 --> 00:03:18,920
而不是教师模型自己先学

77
00:03:18,920 --> 00:03:21,320
学习完之后再把知识增留给学生

78
00:03:21,840 --> 00:03:23,960
这种属于离线offline的方式

79
00:03:23,960 --> 00:03:26,080
教师跟学生同时学习

80
00:03:26,080 --> 00:03:27,640
属于online的方式

81
00:03:27,640 --> 00:03:28,800
那online的方式呢

82
00:03:28,800 --> 00:03:30,960
有一个比较大的缺点就是

83
00:03:31,440 --> 00:03:33,000
在一个在线的环境里面呢

84
00:03:33,000 --> 00:03:35,040
很难去获得一个参数量又大

85
00:03:35,040 --> 00:03:37,120
精度又好的教师模型

86
00:03:38,320 --> 00:03:40,680
第三个知识增留的方式呢

87
00:03:40,680 --> 00:03:42,520
叫做self distillation

88
00:03:42,520 --> 00:03:45,320
就是教师模型和学生模型呢

89
00:03:45,320 --> 00:03:46,880
其实就是一个模型

90
00:03:46,880 --> 00:03:49,360
它进行一个自学习的过程

91
00:03:49,480 --> 00:03:52,080
端到端的可训练可学习的方案

92
00:03:53,000 --> 00:03:54,280
其实这种方式呢

93
00:03:54,280 --> 00:03:56,200
也属于online distillation

94
00:03:56,200 --> 00:03:59,080
就是在线增留的其中一个特例

95
00:04:01,280 --> 00:04:02,640
现在呢我们总结一下

96
00:04:02,640 --> 00:04:05,080
刚才讲到的三种知识增留的方法

97
00:04:05,080 --> 00:04:06,440
三种知识增留的方法呢

98
00:04:06,440 --> 00:04:10,080
可以看作三种不同的学习过程

99
00:04:10,080 --> 00:04:11,720
那第一种就是刚才讲到的

100
00:04:11,720 --> 00:04:13,160
offline distillation

101
00:04:13,160 --> 00:04:15,560
最常用的离线增留方法

102
00:04:16,200 --> 00:04:18,120
主要是指一个知识渊博的老师

103
00:04:18,120 --> 00:04:19,360
他自己已经学完了

104
00:04:19,360 --> 00:04:22,720
然后直接把他学到的知识传授给学生

105
00:04:22,720 --> 00:04:23,720
那现在第二种呢

106
00:04:23,720 --> 00:04:25,480
就是online distillation

107
00:04:25,480 --> 00:04:26,800
在线的知识增留

108
00:04:26,800 --> 00:04:29,640
是指老师和学生一起学习

109
00:04:29,640 --> 00:04:31,040
一起成长

110
00:04:31,040 --> 00:04:33,080
第三种就是self distillation

111
00:04:33,080 --> 00:04:35,280
就是学生自己学习

112
00:04:35,280 --> 00:04:36,400
自己成长

113
00:04:39,520 --> 00:04:41,360
下面呢我们以一个最经典的算法

114
00:04:41,360 --> 00:04:44,440
应该是16年到17年的时候

115
00:04:44,440 --> 00:04:47,720
Hinton第一次提出知识增留这个概念

116
00:04:47,760 --> 00:04:49,000
的这篇文章

117
00:04:49,000 --> 00:04:50,080
那下面这篇文章呢

118
00:04:50,080 --> 00:04:51,360
比较粗暴

119
00:04:51,360 --> 00:04:52,520
名字呢叫做

120
00:04:52,520 --> 00:04:55,920
Distilling the Knowledge in a Nature Network

121
00:04:57,680 --> 00:04:59,680
在正式了解这篇文章之前呢

122
00:04:59,680 --> 00:05:02,200
我们要提前去看看两个概念

123
00:05:02,200 --> 00:05:03,680
一个叫做hard target

124
00:05:03,680 --> 00:05:05,960
一个叫做soft target

125
00:05:05,960 --> 00:05:08,480
在传统神经网络模型当中的训练

126
00:05:08,480 --> 00:05:10,680
其实我们首先定一个损失函数

127
00:05:10,680 --> 00:05:12,240
然后定一个优化器

128
00:05:12,240 --> 00:05:15,040
优化器在不断地去优化我们的损失函数

129
00:05:15,040 --> 00:05:16,800
目的是使得我们的预示值呢

130
00:05:16,840 --> 00:05:19,120
尽可能地接近于我们的真实值

131
00:05:19,120 --> 00:05:20,720
就是Y等于Y label

132
00:05:20,720 --> 00:05:22,560
那这个就是我们的目标

133
00:05:23,480 --> 00:05:24,920
而为了产生这个目标呢

134
00:05:24,920 --> 00:05:26,960
损失函数就是使我们神经网络的

135
00:05:26,960 --> 00:05:28,400
损失值和真实值呢

136
00:05:28,400 --> 00:05:30,400
之间尽可能地少

137
00:05:30,400 --> 00:05:32,720
我们看看下面左边的这个图

138
00:05:32,720 --> 00:05:34,480
假设我现在有四个数字

139
00:05:34,480 --> 00:05:36,320
0 1 3 4 5 6 7 8 9 10

140
00:05:36,880 --> 00:05:39,240
我输进去一张图片是2

141
00:05:39,240 --> 00:05:42,440
我们希望预测是2的概率越高越好

142
00:05:42,960 --> 00:05:43,640
这种方式呢

143
00:05:43,640 --> 00:05:45,120
在数学或者统计方面呢

144
00:05:45,160 --> 00:05:47,480
就是对我们的GNU真实的数据

145
00:05:47,480 --> 00:05:49,360
起了极大的施压值

146
00:05:49,360 --> 00:05:51,640
我们看一下右边的这个图

147
00:05:52,280 --> 00:05:53,520
在知识增充里面呢

148
00:05:53,520 --> 00:05:55,440
我们就像右边的这个图所示

149
00:05:55,440 --> 00:05:57,320
希望能够学习到更多的

150
00:05:57,320 --> 00:05:59,240
其他额外相关的知识

151
00:05:59,240 --> 00:06:01,360
就是我假设输进去的一个数字

152
00:06:01,360 --> 00:06:04,040
手写数字可能长得像3

153
00:06:04,040 --> 00:06:04,560
这个时候呢

154
00:06:04,560 --> 00:06:05,800
我们希望神经网络呢

155
00:06:05,800 --> 00:06:07,960
学到更多的勇于的信息

156
00:06:07,960 --> 00:06:09,720
当然了他告诉我这个肯定是2

157
00:06:09,720 --> 00:06:10,760
那肯定是最好的

158
00:06:10,760 --> 00:06:13,320
把手写数字的字体长得有点像3

159
00:06:13,360 --> 00:06:15,880
这个勇于的信息也告诉出来

160
00:06:15,880 --> 00:06:17,360
那这个肯定是最好的

161
00:06:17,360 --> 00:06:19,120
因此呢我们看一下左边的这个图

162
00:06:19,120 --> 00:06:20,560
跟右边的这个图所示

163
00:06:20,560 --> 00:06:22,520
左边的这个呢我们叫做Hard Target

164
00:06:22,520 --> 00:06:25,120
右边的这个呢我们叫做Soft Target

165
00:06:25,120 --> 00:06:27,720
带有一些其他勇于的信息

166
00:06:28,240 --> 00:06:30,120
下面我们再了解另外一个概念

167
00:06:30,120 --> 00:06:33,040
叫做Soft Maxed with Temperature

168
00:06:33,600 --> 00:06:35,120
在Soft Maxed函数里面呢

169
00:06:35,120 --> 00:06:37,400
增加了一个温度系数

170
00:06:38,280 --> 00:06:39,840
下面我们看一下这条公式的

171
00:06:39,840 --> 00:06:41,560
几个数字的含义

172
00:06:41,560 --> 00:06:44,640
Q1呢就是指第i个类别的输出的概率

173
00:06:44,640 --> 00:06:48,640
而Zi呢就是指第i个类别输出的Logist

174
00:06:49,280 --> 00:06:52,440
下面我们需要对每个类别输出的Logist

175
00:06:52,440 --> 00:06:54,560
进行一个指数的求和

176
00:06:54,560 --> 00:06:57,120
那就得到了我们Soft Maxed的函数了

177
00:06:57,760 --> 00:07:00,520
下面呢我们进行了一个修改

178
00:07:00,520 --> 00:07:02,280
把刚才的Soft Target

179
00:07:02,280 --> 00:07:04,040
就是Soft Label的信息

180
00:07:04,280 --> 00:07:06,080
给到我们的Soft Maxed函数

181
00:07:06,080 --> 00:07:09,480
这里面呢就增加了一个温度的系数T

182
00:07:09,480 --> 00:07:10,560
Zi除以T

183
00:07:10,960 --> 00:07:12,280
Zj除以T

184
00:07:12,800 --> 00:07:14,800
当温度等于1的时候

185
00:07:14,800 --> 00:07:15,960
其实大家看到没有

186
00:07:15,960 --> 00:07:18,400
其实等于我们标准的Soft Maxed的函数

187
00:07:18,760 --> 00:07:19,840
T的数字越高呢

188
00:07:19,840 --> 00:07:22,480
Soft Maxed输出的概率的分布呢

189
00:07:22,480 --> 00:07:23,520
就越平滑

190
00:07:23,840 --> 00:07:25,640
分布的信息桑呢也就越大

191
00:07:25,640 --> 00:07:27,360
所以副标签所携带的

192
00:07:27,360 --> 00:07:29,120
一些额外的冗余的信息呢

193
00:07:29,120 --> 00:07:30,640
也会相对的放大

194
00:07:31,160 --> 00:07:33,120
这个时候呢我们网络模型的训练呢

195
00:07:33,120 --> 00:07:34,680
就更关注于我们一些

196
00:07:34,680 --> 00:07:36,360
副标签的冗余的信息

197
00:07:36,360 --> 00:07:38,080
就是我们刚才讲到的这个图

198
00:07:38,400 --> 00:07:40,240
Soft Target里面的一些

199
00:07:40,240 --> 00:07:41,960
冗余的信息额外的信息

200
00:07:41,960 --> 00:07:43,320
也把它记录下来

201
00:07:43,320 --> 00:07:45,360
就是通过简单的设置一个

202
00:07:45,360 --> 00:07:47,560
温度系数来控制

203
00:07:49,560 --> 00:07:52,400
那下面我们再看一个比较明确的图

204
00:07:53,520 --> 00:07:54,800
随着T的增加呢

205
00:07:54,800 --> 00:07:56,280
T从小到大

206
00:07:56,280 --> 00:07:58,520
我们可以看到Soft Maxed输出的分布呢

207
00:07:58,520 --> 00:07:59,560
就会越平滑

208
00:07:59,760 --> 00:08:01,400
信息桑呢也就会越大

209
00:08:01,400 --> 00:08:03,240
信息的差异呢也就会越少

210
00:08:03,640 --> 00:08:05,800
当然了怎幺找到一个合理的T

211
00:08:05,800 --> 00:08:07,840
得到我们中间这种图呢

212
00:08:08,080 --> 00:08:10,320
是很关键的一步

213
00:08:11,080 --> 00:08:12,880
下面我们就来探讨一下

214
00:08:12,880 --> 00:08:15,120
如何选择这个T

215
00:08:17,120 --> 00:08:19,160
实际上呢选择温度T呢

216
00:08:19,160 --> 00:08:20,920
主要是下面一种情况

217
00:08:21,280 --> 00:08:23,040
假设想从副标签里面呢

218
00:08:23,040 --> 00:08:24,920
学习到更多的有用的知识

219
00:08:24,920 --> 00:08:25,640
有用的信息

220
00:08:25,640 --> 00:08:26,920
或者一些额外的参数

221
00:08:26,920 --> 00:08:28,520
我们的温度T呢

222
00:08:28,520 --> 00:08:31,400
就是适当的去调高一点点

223
00:08:31,920 --> 00:08:34,040
但是呢当我们想减少副标签

224
00:08:34,040 --> 00:08:35,440
对我们整个神经网络

225
00:08:35,440 --> 00:08:37,280
或者对我们的预测值的干扰的时候呢

226
00:08:37,280 --> 00:08:38,400
我们的温度T呢

227
00:08:38,400 --> 00:08:41,600
就适当的往低去调整就好了

228
00:08:42,480 --> 00:08:44,760
当然了T的大小应该是指为多少

229
00:08:44,760 --> 00:08:46,760
我们需要根据我们实际的情况

230
00:08:46,760 --> 00:08:49,080
实际的任务进行设定的

231
00:08:49,320 --> 00:08:50,800
在分类任务和检测任务

232
00:08:50,800 --> 00:08:53,000
我们的T的选择也是不同的

233
00:08:56,080 --> 00:08:57,560
接下来呢就正式的回到

234
00:08:57,560 --> 00:08:59,160
知识增留这个算法里面

235
00:09:00,000 --> 00:09:00,960
首先去了解一下

236
00:09:00,960 --> 00:09:02,640
知识增留算法的训练流程呢

237
00:09:02,640 --> 00:09:04,080
跟传统的训练流程的

238
00:09:04,080 --> 00:09:06,000
一个不一样的区别

239
00:09:07,400 --> 00:09:09,760
第一个呢就是传统的训练流程

240
00:09:09,760 --> 00:09:11,600
传统训练流程我们刚才讲到了

241
00:09:11,600 --> 00:09:13,320
就是我们最大的目标呢

242
00:09:13,320 --> 00:09:15,280
就是训练我们的hard target

243
00:09:15,880 --> 00:09:17,520
对光truth真实的样本呢

244
00:09:17,520 --> 00:09:20,120
求极大的释然softmax的值

245
00:09:20,480 --> 00:09:23,520
但是呢在知识增留的训练过程当中呢

246
00:09:23,520 --> 00:09:25,720
我们更多的是希望学习到

247
00:09:25,720 --> 00:09:27,480
很多soft target

248
00:09:28,040 --> 00:09:30,640
利用教师模型的分类的概率呢

249
00:09:30,640 --> 00:09:31,840
作为soft target

250
00:09:32,280 --> 00:09:33,760
就像周米给大家去汇报

251
00:09:33,760 --> 00:09:35,480
AI系统的这个相关的知识呢

252
00:09:35,520 --> 00:09:37,600
我一般来说都不会照着字来念

253
00:09:37,960 --> 00:09:39,720
而是插入了很多我自己的理解

254
00:09:39,720 --> 00:09:41,560
或者额外的知识一样

255
00:09:45,400 --> 00:09:48,040
毫不意外的就是这篇文章的知识增留呢

256
00:09:48,040 --> 00:09:50,800
采用了一个offline的离线的增留方式

257
00:09:50,800 --> 00:09:52,800
然后呢结构上面呢就采用了

258
00:09:52,800 --> 00:09:55,680
经典的师生的网络模型

259
00:09:55,920 --> 00:09:57,600
teacher呢就是知识的输出子

260
00:09:57,600 --> 00:10:00,080
student呢就是知识的接受指握

261
00:10:01,960 --> 00:10:04,640
整个知识增留呢分为两个阶段

262
00:10:04,760 --> 00:10:06,040
注意是两个

263
00:10:06,040 --> 00:10:09,120
第一个就是训练我们的教师模型

264
00:10:09,800 --> 00:10:13,120
第二个就是学生模型进行增留

265
00:10:13,120 --> 00:10:14,040
就是学习

266
00:10:14,840 --> 00:10:19,200
下面呢我就分开两个给大家进行一个汇报

267
00:10:21,760 --> 00:10:25,960
现在呢更多的是一个字面和概念的意义的了解

268
00:10:25,960 --> 00:10:27,320
后面我们会展开一个图

269
00:10:27,320 --> 00:10:28,600
让大家看得更清楚

270
00:10:28,840 --> 00:10:30,520
首先我们训练teacher model的时候呢

271
00:10:30,520 --> 00:10:32,120
teacher model我们叫做NetT

272
00:10:32,400 --> 00:10:33,720
特点就是这个网络模型呢

273
00:10:33,720 --> 00:10:35,200
相对来说比较复杂

274
00:10:35,520 --> 00:10:37,200
像transformer之类的大模型呢

275
00:10:37,200 --> 00:10:39,520
确实更加适合教师模型

276
00:10:39,520 --> 00:10:42,600
那唯一的要求就是对于输入的x呢

277
00:10:42,600 --> 00:10:44,440
它都能输出一个y

278
00:10:44,440 --> 00:10:46,760
那这个y呢经过softmax的映射呢

279
00:10:46,760 --> 00:10:49,880
能够输出对应概率的一个预测的

280
00:10:49,880 --> 00:10:51,000
概率的类别值

281
00:10:51,600 --> 00:10:54,880
接着第二步就是学生模型进行增留

282
00:10:54,880 --> 00:10:56,760
那学生模型进行增留了说白了

283
00:10:56,760 --> 00:10:59,080
就是我们需要训练一个student model

284
00:10:59,080 --> 00:11:00,720
我们叫做NetX

285
00:11:01,040 --> 00:11:02,800
它的特点就是参数量呢

286
00:11:02,800 --> 00:11:06,080
相对来说比我们的teacher model要少

287
00:11:06,080 --> 00:11:08,880
我们的模型结构呢也相对来说简单

288
00:11:08,880 --> 00:11:12,040
同样的有个要求就是对于输入的x呢

289
00:11:12,040 --> 00:11:13,520
我们都能够输出y

290
00:11:13,520 --> 00:11:16,440
而y呢经过softmax的映射后呢

291
00:11:16,440 --> 00:11:19,600
能够与NetT分别对应起来

292
00:11:19,600 --> 00:11:20,880
用大白话来说呢

293
00:11:20,880 --> 00:11:23,840
就是teacher model我要预测1000个分类

294
00:11:23,840 --> 00:11:26,760
我的student model也需要预测1000个分类

295
00:11:26,760 --> 00:11:28,440
而不是变成500个分类

296
00:11:28,440 --> 00:11:30,200
那这是没办法做映射了

297
00:11:32,920 --> 00:11:33,920
在预训练阶段呢

298
00:11:33,920 --> 00:11:36,560
会训练一个性能比较好的teacher model

299
00:11:36,560 --> 00:11:38,520
所以呢我们会把teacher model的信息呢

300
00:11:38,520 --> 00:11:41,080
给到我们的student model去学习

301
00:11:41,080 --> 00:11:43,880
下面我们来看一下真正的一个算法流程

302
00:11:43,880 --> 00:11:45,960
首先第一步我需要训练一个teacher model

303
00:11:45,960 --> 00:11:47,400
就上面的这个teacher model

304
00:11:47,400 --> 00:11:49,120
我先把它训练出来

305
00:11:49,960 --> 00:11:52,480
接着呢我利用一个高温的t呢

306
00:11:52,480 --> 00:11:54,960
就是这个t的值呢适得比较大

307
00:11:54,960 --> 00:11:56,560
产生一个soft target

308
00:11:56,560 --> 00:11:59,520
把很多勇于的信息呢保留起来

309
00:12:00,320 --> 00:12:01,880
接着呢就是第二步了

310
00:12:01,880 --> 00:12:05,640
第二步就是我们的student model的学习和征留的过程

311
00:12:05,640 --> 00:12:08,360
那有两个点特别是需要注意的

312
00:12:08,360 --> 00:12:10,120
就是在我们的第三步

313
00:12:10,120 --> 00:12:11,240
第三步的时候呢

314
00:12:11,240 --> 00:12:12,120
这里面呢说白了

315
00:12:12,120 --> 00:12:13,400
就使用一个soft target

316
00:12:13,400 --> 00:12:14,520
还有hard target

317
00:12:14,520 --> 00:12:17,720
同时训练student model

318
00:12:17,720 --> 00:12:18,800
那可以看到呢

319
00:12:18,800 --> 00:12:20,520
我们上面有一部分

320
00:12:20,520 --> 00:12:21,400
就这里面呢

321
00:12:21,400 --> 00:12:24,200
有两个损失函数拼在一起

322
00:12:24,200 --> 00:12:26,160
作为一个总的损失函数

323
00:12:26,160 --> 00:12:27,120
这个损失函数呢

324
00:12:27,120 --> 00:12:29,320
就是我们的t呢会比较高

325
00:12:29,320 --> 00:12:30,800
跟上面的可以保持一致

326
00:12:30,840 --> 00:12:32,800
然后呢训练一个soft target

327
00:12:32,800 --> 00:12:36,040
通过distillation loss呢去进行一个学习

328
00:12:36,800 --> 00:12:39,440
那第二个呢就是我们的hard target

329
00:12:39,440 --> 00:12:42,360
通过一个通用的或者普通的一个损失函数

330
00:12:42,360 --> 00:12:44,480
去训练学生网络模型

331
00:12:44,480 --> 00:12:46,720
最后一步就是推理了

332
00:12:46,720 --> 00:12:47,400
推理的时候呢

333
00:12:47,400 --> 00:12:49,320
我们把t视之为1

334
00:12:49,320 --> 00:12:52,000
进行一个学生模型的在线推理

335
00:12:52,000 --> 00:12:54,240
所以student model的训练或者征留的过程呢

336
00:12:54,240 --> 00:12:55,720
会相对来说复杂一点

337
00:12:55,720 --> 00:12:56,960
它有两个损失

338
00:12:56,960 --> 00:12:58,320
一个是distillation loss

339
00:12:58,320 --> 00:13:00,560
一个是student loss

340
00:13:00,560 --> 00:13:04,560
下面呢我们来看一下具体的一个loss的情况

341
00:13:06,080 --> 00:13:08,880
这个l呢就是对应的总的损失函数

342
00:13:08,880 --> 00:13:11,160
这里面刚才讲到了有两个损失函数

343
00:13:11,160 --> 00:13:12,040
一个损失函数呢

344
00:13:12,040 --> 00:13:14,200
就是针对一个soft label的损失函数

345
00:13:14,200 --> 00:13:15,040
一个损失函数呢

346
00:13:15,040 --> 00:13:17,840
就是对应hard label的一个损失函数

347
00:13:17,840 --> 00:13:19,400
而soft label的损失函数呢

348
00:13:19,400 --> 00:13:21,000
就是来自于我们的teacher model

349
00:13:21,000 --> 00:13:21,600
hard label呢

350
00:13:21,600 --> 00:13:24,320
就是来自于我们真实的数据标签

351
00:13:24,320 --> 00:13:25,520
通过两种方式

352
00:13:25,520 --> 00:13:27,440
或者通过两个损失函数

353
00:13:27,440 --> 00:13:28,880
把它合在一起

354
00:13:28,920 --> 00:13:30,640
变成一个总的损失函数

355
00:13:30,640 --> 00:13:31,880
来去学习

356
00:13:32,600 --> 00:13:36,240
所以来说我们只需要看懂了这个图的流程

357
00:13:36,240 --> 00:13:39,560
基本上你就明白这个算法是怎幺去实现了

358
00:13:40,760 --> 00:13:42,800
好了今天的内容呢就到此为止

359
00:13:42,800 --> 00:13:43,920
如果大家有兴趣的话

360
00:13:43,920 --> 00:13:47,160
也可以看一下相关的文献和内容

361
00:13:47,160 --> 00:13:47,840
谢谢各位

362
00:13:47,840 --> 00:13:49,000
拜了个拜

363
00:13:49,000 --> 00:13:50,680
卷的不行了卷的不行了

364
00:13:50,680 --> 00:13:52,520
记得一键三连加关注哦

365
00:13:52,520 --> 00:13:55,600
所有的内容都会开源在下面这条链接里面

366
00:13:56,120 --> 00:13:56,920
拜了个拜

