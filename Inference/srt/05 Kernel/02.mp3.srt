0
0:00:00.000 --> 0:00:07.880
Hello大家好,我是好久没有更新的周米,

1
0:00:07.880 --> 0:00:11.400
最近确实实在是太忙了,忙着shareGPT这个项目,

2
0:00:11.400 --> 0:00:15.960
这个项目就导致我经常加班,晚上回来就有了惰性。

3
0:00:15.960 --> 0:00:20.240
今天我们还是来到了推定型的kernel优化,

4
0:00:20.240 --> 0:00:24.880
在整个kernel优化里面,我们来到了第一个比较简单的内容,

5
0:00:24.880 --> 0:00:27.640
就是我们看看卷机优化的整体的原理,

6
0:00:27.640 --> 0:00:29.440
卷机需要做哪些优化。

7
0:00:29.440 --> 0:00:31.520
下面我们看一下整个kernel优化,

8
0:00:31.520 --> 0:00:32.920
或者我们的卷机优化里面,

9
0:00:32.920 --> 0:00:35.720
我们还在第一个阶段,算法的优化,

10
0:00:35.720 --> 0:00:39.920
而这里面的算法主要还是讲卷机的优化。

11
0:00:39.920 --> 0:00:42.160
接着我们来再深入一下,

12
0:00:42.160 --> 0:00:46.720
kernel优化主要是围绕着kernel层去实现的,

13
0:00:46.720 --> 0:00:49.560
而我们之前其实已经给大家重复讲过了,

14
0:00:49.560 --> 0:00:52.240
我们对一个简单的卷机算子,

15
0:00:52.240 --> 0:00:56.240
在CPU里面可能会使用NEO指令集去实现,

16
0:00:56.360 --> 0:01:00.040
也可能会使用X86的AVX指令集去实现。

17
0:01:00.040 --> 0:01:04.360
在一个推进引擎里面的实现的方式就有非常多种,

18
0:01:04.360 --> 0:01:08.240
因为我们的推进引擎要支持CPU,也要支持GPU,

19
0:01:08.240 --> 0:01:10.840
而CPU就有两种不同的实现,

20
0:01:10.840 --> 0:01:14.640
可能在GPU上面我们就有更多种不同的实现,

21
0:01:14.640 --> 0:01:16.200
可能我们会用CUDA实现,

22
0:01:16.200 --> 0:01:18.040
用OpenCL,用Win卡,

23
0:01:18.040 --> 0:01:21.360
可能还会用OpenGL,还有Meta来去实现。

24
0:01:21.360 --> 0:01:22.600
最终实现完的,

25
0:01:22.600 --> 0:01:26.360
我们会把它封装成一个高性能的算子库,

26
0:01:26.360 --> 0:01:28.600
也可能直接提供kernel层。

27
0:01:28.600 --> 0:01:30.680
废话和前期知识有点多,

28
0:01:30.680 --> 0:01:32.320
我们再往下看一下,

29
0:01:32.320 --> 0:01:36.000
在整个kernel优化里面我们会涉及到哪些内容。

30
0:01:36.000 --> 0:01:39.080
首先我们会去看一下什幺为之卷机,

31
0:01:39.080 --> 0:01:42.000
那如果大家都懂的这个概念可以跳过这一节,

32
0:01:42.000 --> 0:01:43.400
来去看下一节。

33
0:01:43.400 --> 0:01:44.800
下一节我们就会讲讲

34
0:01:44.800 --> 0:01:49.280
Coffee里面用的最多的image to clump这种优化的算法,

35
0:01:49.280 --> 0:01:51.480
它是一种卷机的具体的实现方法,

36
0:01:51.480 --> 0:01:56.960
把卷机的操作变成GEMM这种运算的方式。

37
0:01:56.960 --> 0:01:59.200
然后我们会有一个空间组合的优化,

38
0:01:59.200 --> 0:02:00.880
空间组合优化其实比较简单,

39
0:02:00.880 --> 0:02:03.760
跟我们编译里面的其实是比较相似的,

40
0:02:03.760 --> 0:02:07.560
也是结合了image to clump的思想来去实现的。

41
0:02:07.560 --> 0:02:10.520
在后面的两节里面就有点意思了,

42
0:02:10.520 --> 0:02:14.320
里面我们会讲到Window Grid这种优化的算法。

43
0:02:14.320 --> 0:02:18.840
Window Grid这个优化算法是在1980几年的时候已经提出来了,

44
0:02:18.840 --> 0:02:23.200
但是当时候因为算力的问题没有很好的得到一个重用。

45
0:02:23.200 --> 0:02:26.680
现在应该是在2011年之后,

46
0:02:26.680 --> 0:02:29.200
Window Grid开始慢慢的起来了。

47
0:02:29.200 --> 0:02:32.720
确实它在做一些小卷机盒的计算的时候,

48
0:02:32.720 --> 0:02:34.520
效果性还是很好的。

49
0:02:34.520 --> 0:02:36.760
然后再到QNN Pack,

50
0:02:36.760 --> 0:02:39.640
间接卷机优化这种方式,

51
0:02:39.640 --> 0:02:43.200
去看看我们卷机怎幺做不同的优化。

52
0:02:43.200 --> 0:02:44.240
现在时不一时,

53
0:02:44.240 --> 0:02:45.680
我们来到了第一个内容,

54
0:02:45.680 --> 0:02:48.200
就是卷机的基础概念。

55
0:02:48.240 --> 0:02:50.720
我们来了解一下什幺是卷机。

56
0:02:50.720 --> 0:02:53.640
现在我们看一下什幺为之卷机。

57
0:02:53.640 --> 0:02:57.760
实际上卷机是神经网络里面的内核计算单元之一。

58
0:02:57.760 --> 0:02:59.040
为什幺叫做之一呢?

59
0:02:59.040 --> 0:03:02.560
因为在神经网络里面的最内核的几种计算单元,

60
0:03:02.560 --> 0:03:07.040
有卷机CNN,也有LSTM,还有Transformer。

61
0:03:07.040 --> 0:03:10.720
最后一个就是最常用的MatMul,矩阵相乘。

62
0:03:10.720 --> 0:03:15.080
实际上卷机在前几年应该是非常非常的火,

63
0:03:15.080 --> 0:03:17.480
基本上所有的CPU算法都离不开它。

64
0:03:17.520 --> 0:03:20.360
包括我们常用的Westnet,MobileNet,Efficient,

65
0:03:20.360 --> 0:03:24.560
这些都是使用了或者大量的使用了卷机的计算。

66
0:03:24.560 --> 0:03:30.000
但是实际上卷机的变种是非常非常的丰富和多样的。

67
0:03:30.000 --> 0:03:31.880
我们除了通用的卷机计算之外,

68
0:03:31.880 --> 0:03:34.600
我们还会在卷机里面加上BIOS,

69
0:03:34.600 --> 0:03:37.400
我们可能会对卷机进行空洞卷机,

70
0:03:37.400 --> 0:03:39.840
可能还会进行一个Device的卷机。

71
0:03:39.840 --> 0:03:42.760
所以我们会说卷机的变种非常丰富,

72
0:03:42.760 --> 0:03:45.560
而且它的计算还是非常的复杂。

73
0:03:45.600 --> 0:03:49.120
我们的神经网络或者我们的在一个网络模型当中,

74
0:03:49.120 --> 0:03:52.800
大部分时间都消耗在我们的卷机计算,

75
0:03:52.800 --> 0:03:57.880
所以如何对我们的卷机进行优化就变得非常重要。

76
0:03:57.880 --> 0:04:02.320
这也是我们这个系列或者在坑洞优化里面重点去讲解的。

77
0:04:02.320 --> 0:04:05.160
当然Transform也是一种很好的例子,

78
0:04:05.160 --> 0:04:07.000
但是Transform相关的优化,

79
0:04:07.000 --> 0:04:09.960
说实话其实并不是说非常的多,

80
0:04:09.960 --> 0:04:11.000
没有像卷机那样,

81
0:04:11.000 --> 0:04:14.800
而且卷机在端测推进器里面是非常的成熟,

82
0:04:14.800 --> 0:04:18.640
而Transform在端测推进器里面应该现在来看,

83
0:04:18.640 --> 0:04:21.240
用的不是说非常多。

84
0:04:21.240 --> 0:04:24.120
我们在前面其实已经介绍过Transform这个结构,

85
0:04:24.120 --> 0:04:28.680
在端测推进器里面现在还在慢慢的引入阶段,

86
0:04:28.680 --> 0:04:30.520
还没有等到大规模成熟,

87
0:04:30.520 --> 0:04:32.760
而且随着时间技术的发展,

88
0:04:32.760 --> 0:04:37.440
研究员就提出了多种的关于卷机的优化方法,

89
0:04:37.440 --> 0:04:39.240
包括InvisiClone还有Windows GUI的。

90
0:04:39.240 --> 0:04:43.080
下面我们来看一下什幺为之卷机。

91
0:04:43.280 --> 0:04:46.720
下面是卷机的一些我在网上找到的

92
0:04:46.720 --> 0:04:48.120
为几百颗的相关的概念,

93
0:04:48.120 --> 0:04:50.080
说实话我看的不是很懂,

94
0:04:50.080 --> 0:04:52.880
如果大家想深入了解什幺为之卷机,

95
0:04:52.880 --> 0:04:54.520
卷机跟复理业之间的关系,

96
0:04:54.520 --> 0:04:56.200
还有卷机的具体的原理,

97
0:04:56.200 --> 0:04:58.760
大家可以看一下网上相关的介绍,

98
0:04:58.760 --> 0:05:00.080
这也是非常的多,

99
0:05:00.080 --> 0:05:01.880
我们简单的去给大家念一念,

100
0:05:01.880 --> 0:05:04.360
卷机主要是通过两个函数,

101
0:05:04.360 --> 0:05:06.120
一个F,一个是G,

102
0:05:06.120 --> 0:05:10.880
通过两个函数生成第三个函数的一种具体的数学运算,

103
0:05:10.880 --> 0:05:14.080
我们可以把它变成等于HX,

104
0:05:14.080 --> 0:05:16.720
它的本质是一种特殊的积分变换,

105
0:05:16.720 --> 0:05:20.240
所以我们可以看到在这里面有个积分的符号,

106
0:05:20.240 --> 0:05:22.680
这里面就表示了表征函数,

107
0:05:22.680 --> 0:05:24.360
F是一个表征函数,

108
0:05:24.360 --> 0:05:25.840
G也是一个表征函数,

109
0:05:25.840 --> 0:05:29.280
经过翻转和平均重叠的部分的函数的乘积,

110
0:05:29.280 --> 0:05:32.320
里面的一个长度的积分,

111
0:05:32.320 --> 0:05:35.360
那幺可以看到有两个框框,

112
0:05:35.360 --> 0:05:36.840
第一个是蓝色的框框,

113
0:05:36.840 --> 0:05:40.360
第二个就是这里面移动的黄色的框框,

114
0:05:40.360 --> 0:05:42.560
蓝色的框框是我们的F涛,

115
0:05:42.560 --> 0:05:45.680
而红色的框框是我们的G,T-涛,

116
0:05:45.680 --> 0:05:50.680
而我们的三角形经过的三角形就是F跟G两个表征函数,

117
0:05:50.680 --> 0:05:52.560
而得到的一个积分的概念,

118
0:05:52.560 --> 0:05:55.480
说实话积分的概念我不明白为什幺要这幺做,

119
0:05:55.480 --> 0:05:57.960
现在我们看一下积分的公式,

120
0:05:57.960 --> 0:06:01.000
我们可以看到积分公式里面有几个比较重要的元素,

121
0:06:01.000 --> 0:06:02.200
第一个就是T,

122
0:06:02.200 --> 0:06:03.360
第二个就是涛,

123
0:06:03.360 --> 0:06:04.880
第三个就是T-涛,

124
0:06:04.880 --> 0:06:07.640
我们把它组成一个新的公式,

125
0:06:07.640 --> 0:06:10.320
现在我们可以看到新的公式就是T等于涛,

126
0:06:10.320 --> 0:06:12.320
加上T-涛,

127
0:06:12.320 --> 0:06:15.560
我们把它再去分裂成两边两条公式,

128
0:06:15.560 --> 0:06:17.640
第一条就是X等于涛,

129
0:06:17.640 --> 0:06:20.560
第二个就是Y等于T-涛,

130
0:06:20.560 --> 0:06:23.400
然后我们把这两个公式再组合起来,

131
0:06:23.400 --> 0:06:25.320
变成X加Y等于N,

132
0:06:25.320 --> 0:06:28.800
这时候可以看到这是一条线性的公式,

133
0:06:28.800 --> 0:06:30.800
我们可以往下再看一看,

134
0:06:30.800 --> 0:06:33.760
这条线性的公式把它在二维空间里面画出来了,

135
0:06:33.760 --> 0:06:36.000
就是像右边的所示,

136
0:06:36.000 --> 0:06:39.840
我们把N不断的去放成不同的一种数值,

137
0:06:39.840 --> 0:06:44.960
然后可以看到这条公式类似于在我们的平面当中不断的去划过,

138
0:06:44.960 --> 0:06:47.120
如果编辑刚才上面这条直线,

139
0:06:47.120 --> 0:06:48.960
就好像我们的毛巾,

140
0:06:48.960 --> 0:06:50.560
把毛巾卷起来,

141
0:06:50.560 --> 0:06:52.680
然后变成一个新的概念,

142
0:06:52.680 --> 0:06:54.240
或者一种新的形态一样,

143
0:06:54.240 --> 0:06:57.080
这种就是在网上比较通俗,

144
0:06:57.080 --> 0:06:59.320
或者比较容易理解的一种概念,

145
0:06:59.320 --> 0:07:01.360
现在讲不清明白没关系,

146
0:07:01.360 --> 0:07:02.720
大家可以不用看这个视频,

147
0:07:02.720 --> 0:07:05.200
这个视频其实确实没什幺太多的内容,

148
0:07:05.200 --> 0:07:09.120
我们可以看到现在很多时候我们会把积分,

149
0:07:09.120 --> 0:07:13.360
积分是对连续的数据或者无穷的数据进行一个计算的,

150
0:07:13.360 --> 0:07:15.440
这个就是信号处理当中卷积,

151
0:07:15.440 --> 0:07:19.640
把我们的连续变成一个离散的形式的表示,

152
0:07:19.640 --> 0:07:21.520
现在我们把这条公式,

153
0:07:21.520 --> 0:07:23.480
再拓展到二维的空间,

154
0:07:23.480 --> 0:07:26.000
就得到了我们神经网络里面的卷积,

155
0:07:26.000 --> 0:07:28.280
我们可以看到这里面的公式非常多,

156
0:07:28.280 --> 0:07:30.720
关于里面的参数量的非常多,

157
0:07:30.720 --> 0:07:32.040
我们逐个来打开一下,

158
0:07:32.040 --> 0:07:35.040
首先S就是我们的卷积核,

159
0:07:35.040 --> 0:07:37.040
或者我们的卷积最后的输出,

160
0:07:37.080 --> 0:07:39.360
而I就是我们的卷积的数,

161
0:07:39.360 --> 0:07:41.800
假设你当它作为一张图片就行了,

162
0:07:41.800 --> 0:07:44.240
而K就是我们的卷积核,

163
0:07:44.240 --> 0:07:46.840
右边就是具体的计算公式,

164
0:07:46.840 --> 0:07:48.640
我们再往下看一看,

165
0:07:48.640 --> 0:07:52.960
这里面有一个非常好的一个可视化的网站,

166
0:07:52.960 --> 0:07:54.720
我们打开去看看,

167
0:07:56.360 --> 0:08:00.320
这个就是GitHub里面一个非常好的一个可视化的方式,

168
0:08:00.320 --> 0:08:04.360
我们可以看到卷积形式方法非常多,

169
0:08:04.360 --> 0:08:05.960
我们有标准的卷积,

170
0:08:05.960 --> 0:08:07.120
有反卷积,

171
0:08:07.120 --> 0:08:08.560
有分组卷积,

172
0:08:08.560 --> 0:08:09.760
有可分离卷积,

173
0:08:09.760 --> 0:08:12.560
还有分组卷积的方式非常多,

174
0:08:12.560 --> 0:08:16.600
下面蓝色的比较深颜色的就是我们的卷积核,

175
0:08:16.600 --> 0:08:20.480
而下面底的蓝色就是我们的输入的图片,

176
0:08:20.480 --> 0:08:23.480
卷积核跟图片进行划床之后,

177
0:08:23.480 --> 0:08:25.520
就得到一个新的输出的结果,

178
0:08:25.520 --> 0:08:26.720
而新的输出的结果,

179
0:08:26.720 --> 0:08:31.280
这就是上面我们绿色的对应于我们公式里面的S,

180
0:08:31.280 --> 0:08:33.480
这个就是卷积的具体的方式,

181
0:08:33.520 --> 0:08:38.200
我们其实卷积更多的一开始没有应用到神经网络里面,

182
0:08:38.200 --> 0:08:41.480
在图像处理的时候用的特别的多,

183
0:08:41.480 --> 0:08:45.360
下面这个就是我们整体的卷积的一个计算,

184
0:08:45.360 --> 0:08:47.960
这个就是我们的卷积的计算核,

185
0:08:47.960 --> 0:08:49.920
这个就是我们的图片,

186
0:08:49.920 --> 0:08:52.720
图片跟卷积核进行一个擦场,

187
0:08:52.720 --> 0:08:56.120
就得到我们的最终的结果-3,

188
0:08:56.120 --> 0:08:58.520
具体的公式就比较简单,

189
0:08:58.520 --> 0:09:03.440
这里面的卷积和每一个元素可以看到-101,

190
0:09:03.440 --> 0:09:09.160
-101,然后跟上面的每一个元素进行相乘,

191
0:09:09.160 --> 0:09:12.560
-1乘以3,0乘以0,1乘以1,

192
0:09:12.560 --> 0:09:15.000
然后再把它进行求和,

193
0:09:15.000 --> 0:09:17.000
跟我们刚才的公式是一模一样的,

194
0:09:17.000 --> 0:09:20.200
最终得到我们-3这个结果,

195
0:09:20.200 --> 0:09:22.240
这个就是卷积的计算,

196
0:09:22.240 --> 0:09:25.240
既然提到卷积在图像处理里面用的非常多,

197
0:09:25.240 --> 0:09:26.880
我们现在打开Photoshop,

198
0:09:26.880 --> 0:09:30.600
看一下卷积在Photoshop里面最常用的一些功能,

199
0:09:31.120 --> 0:09:33.560
假设我们现在有这幺一张水彩画,

200
0:09:33.560 --> 0:09:35.160
然后点击滤镜,

201
0:09:35.160 --> 0:09:37.480
滤镜里面的模糊,

202
0:09:37.480 --> 0:09:39.520
对,模糊里面用的非常多,

203
0:09:39.520 --> 0:09:41.080
首先就是高斯模糊,

204
0:09:41.080 --> 0:09:45.440
高斯模糊就是我们的卷积和是符合高斯的分布,

205
0:09:45.440 --> 0:09:49.680
而这里面平均的半径就是我们的卷积和的大小,

206
0:09:49.680 --> 0:09:51.880
可以看到通过这里面的空字拖动,

207
0:09:51.880 --> 0:09:54.200
我们可以控制我们的卷积和的大小,

208
0:09:54.200 --> 0:09:56.560
卷积和的大小作用于怎幺图片,

209
0:09:56.560 --> 0:09:59.480
就使得我们的图片越来越模糊,

210
0:09:59.480 --> 0:10:01.520
像里面的很多滤镜的方式,

211
0:10:01.520 --> 0:10:03.920
就使用了不同的卷积的组成方式,

212
0:10:03.920 --> 0:10:06.840
去对我们的图片产生作用的,

213
0:10:06.840 --> 0:10:09.640
好不容易终于讲完了自己不熟悉的,

214
0:10:09.640 --> 0:10:12.280
或者不是说非常理解的概念给大家,

215
0:10:12.280 --> 0:10:14.040
说实话实在惭愧,

216
0:10:14.040 --> 0:10:16.680
下面又回到了我最熟悉的概念,

217
0:10:16.680 --> 0:10:18.240
还是系统的优化,

218
0:10:18.240 --> 0:10:19.840
还有算法的优化,

219
0:10:19.840 --> 0:10:22.120
也没有办法给大家讲得很透彻,

220
0:10:22.120 --> 0:10:23.680
所以希望大家能够谅解,

221
0:10:23.680 --> 0:10:26.160
简单的听一听或者不要听就行了,

222
0:10:26.280 --> 0:10:29.720
下面我们看一下卷积对于 Tensor 的一个优化,

223
0:10:29.720 --> 0:10:31.240
首先可以看到了张量,

224
0:10:31.240 --> 0:10:32.640
这个 Tensor 就我们的张量,

225
0:10:32.640 --> 0:10:33.880
张量在内存里面,

226
0:10:33.880 --> 0:10:39.080
一般我们现在假设以 nhwc 这种方式作为一个布局,

227
0:10:39.080 --> 0:10:42.120
可以看到下面这一扎公式,

228
0:10:42.120 --> 0:10:45.720
就是卷积对于张量的一种运算,

229
0:10:45.720 --> 0:10:48.960
我们可以看到这边有非常多的 for,

230
0:10:48.960 --> 0:10:52.320
我们逐个的看看这些 for 有什幺作用,

231
0:10:52.320 --> 0:10:55.480
首先我们有外层三层的 for,

232
0:10:55.520 --> 0:10:58.120
里面又有三层的 for,

233
0:10:58.120 --> 0:11:01.960
外面的三层的 for 就是对于我们 nhw 里面的 h,

234
0:11:01.960 --> 0:11:04.800
我们对于 h 的这个信道进行变逆,

235
0:11:04.800 --> 0:11:06.840
然后对于 w 这个信道进行变逆,

236
0:11:06.840 --> 0:11:10.160
最后对于我们的 c 的信道进行变逆,

237
0:11:10.160 --> 0:11:12.920
最后才拿到我们第一个数据 c,

238
0:11:12.920 --> 0:11:14.400
0h 0w 0c,

239
0:11:14.400 --> 0:11:15.840
然后把它复一个值,

240
0:11:15.840 --> 0:11:18.440
接着我们把这个值,

241
0:11:18.440 --> 0:11:19.960
因为我们拿到内存的地址,

242
0:11:19.960 --> 0:11:21.760
内存的地址里面存的是什幺字,

243
0:11:21.760 --> 0:11:22.720
我们是不确定的,

244
0:11:22.720 --> 0:11:24.640
所以我们首先对它进行复则,

245
0:11:24.640 --> 0:11:27.280
然后有三个内存的 for,

246
0:11:27.280 --> 0:11:30.840
三个内存的 for 就是我们的卷积核了,

247
0:11:30.840 --> 0:11:33.040
这个卷积核就是 kernel 的 h,

248
0:11:33.040 --> 0:11:34.160
kernel 的 w,

249
0:11:34.160 --> 0:11:36.360
还有 input channel,

250
0:11:36.360 --> 0:11:39.040
通过这种方式对它进行一个组织,

251
0:11:39.040 --> 0:11:44.000
然后就是我们刚才的那条求核公式里面的进行一个计算,

252
0:11:44.000 --> 0:11:50.680
具体就是这里面卷积核跟这个原始的图片进行加权求核,

253
0:11:50.680 --> 0:11:52.200
加权求核之后,

254
0:11:52.240 --> 0:11:56.360
就给我们的 c0h 0w 0c进行计算,

255
0:11:56.360 --> 0:12:01.080
可以看到一个简单的卷积的数学计算是非常的复杂,

256
0:12:01.080 --> 0:12:03.040
有非常多的嵌套的循环,

257
0:12:03.040 --> 0:12:06.960
为了去让我们的整个嵌套的循环没有那幺深,

258
0:12:06.960 --> 0:12:10.080
我们会做很多的循环的优化,

259
0:12:10.080 --> 0:12:11.960
循环的展开分块重排,

260
0:12:11.960 --> 0:12:13.440
融合拆分,

261
0:12:13.440 --> 0:12:15.280
那这些所有的概念,

262
0:12:15.280 --> 0:12:19.920
我们在之前 AI 编译器里面其实是讲过的,

263
0:12:19.960 --> 0:12:21.960
在我们的 AI 编译器后端优化,

264
0:12:21.960 --> 0:12:23.080
就算子的优化,

265
0:12:23.080 --> 0:12:26.680
循环的优化里面给大家详细的去介绍过,

266
0:12:26.680 --> 0:12:28.720
但是这里面是推进引擎的概念,

267
0:12:28.920 --> 0:12:32.520
所以我们不需要通过编译器或者不需要引入编译器的概念,

268
0:12:32.520 --> 0:12:34.560
直接用员工的方式,

269
0:12:34.560 --> 0:12:37.960
用手排的方式对循环进行展开,

270
0:12:37.960 --> 0:12:40.520
这也是科脑优化工程师所做的工作,

271
0:12:40.520 --> 0:12:42.480
另外我们还有指令的优化,

272
0:12:42.480 --> 0:12:44.480
对我们的数据进行矢量化,

273
0:12:44.480 --> 0:12:47.120
对我们的数据进行张量化,

274
0:12:47.120 --> 0:12:49.200
这些概念也是比较好理解的,

275
0:12:49.240 --> 0:12:52.960
我们现在假设 C 只有四个信道,

276
0:12:52.960 --> 0:12:57.040
这个时候我们就可以完全把它进行一个矢量化的操作,

277
0:12:57.040 --> 0:13:01.680
不用每次都执行四次相关的累加操作,

278
0:13:01.680 --> 0:13:04.320
下面我们最后还有存储的优化,

279
0:13:04.320 --> 0:13:07.120
存储优化主要是包括我们的仿存的延迟,

280
0:13:07.120 --> 0:13:09.000
还有存储的分配,

281
0:13:09.000 --> 0:13:11.520
可以往上面这一头公司里面看到了,

282
0:13:11.520 --> 0:13:15.440
我们这里面其实有大量的访问内存的方式,

283
0:13:15.440 --> 0:13:18.120
怎幺对它进行优化是一个很大的概念,

284
0:13:18.120 --> 0:13:22.280
这些我们写 kernel 的工程师非常之在行的,

285
0:13:22.280 --> 0:13:23.800
那更多相关的操作,

286
0:13:23.800 --> 0:13:26.760
更多相关的原理也可以去到我之前讲到的

287
0:13:26.760 --> 0:13:29.520
AAB 硬件后端优化里面的相关的内容,

288
0:13:29.520 --> 0:13:33.120
这里面就不会介绍太多相关的原理知识,

289
0:13:33.120 --> 0:13:35.440
今天的内容确实太不专业了,

290
0:13:35.440 --> 0:13:36.240
就到这里为止,

291
0:13:36.240 --> 0:13:36.840
谢谢各位,

292
0:13:36.840 --> 0:13:37.600
拜了个拜!

293
0:13:38.520 --> 0:13:39.320
卷的不行了,

294
0:13:39.320 --> 0:13:40.160
卷的不行了,

295
0:13:40.160 --> 0:13:42.000
记得一键三连加关注哦,

296
0:13:42.000 --> 0:13:45.240
所有的内容都会开源在下面这条链接里面,

297
0:13:45.240 --> 0:13:46.520
拜拜!

