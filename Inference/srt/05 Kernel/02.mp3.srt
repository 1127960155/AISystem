0
0:00:00.000 --> 0:00:04.680
啪巴巴巴巴巴

1
0:00:05.120 --> 0:00:11.360
哈喽大家好我是好久没有更新的周米最近确实实在事太忙了忙着去 Warren

2
0:00:11.360 --> 0:00:15.980
那这个项目呢就导致我经常加班晚上回来就有了多幸

3
0:00:16.140 --> 0:00:20.220
今天呢我们还是来到了推定型的克诺优化

4
0:00:20.480 --> 0:00:29.340
在整个克诺优化里面呢我们来到了第一个比较简单的内容就是我们看看卷机优化的整体的原理卷机需要做哪些优化

5
0:00:29.340 --> 0:00:40.060
下面我们看一下整个克诺优化或者我们的卷机优化里面呢我们还在第一个阶段算法的优化而这里面的算法呢主要还是讲卷机的优化

6
0:00:40.060 --> 0:00:46.780
接着呢我们再深入一下克诺优化主要是围绕着克诺层去实现的

7
0:00:46.780 --> 0:00:56.300
而我们之前其实已经给大家重复讲过了我们对一个简单的卷机算子在 CPU里面呢可能会使用 NEO指令集去实现

8
0:00:56.300 --> 0:01:00.140
也可能会使用差不多的 AVX 的指令集去实现

9
0:01:00.140 --> 0:01:04.700
在一个推定引擎里面的实现的方式就有非常多种

10
0:01:04.700 --> 0:01:14.700
因为我们的推定引擎要支持 CPU呢也要支持 GPU而 CPU呢就有两种不同的实现可能在 GPU上面呢我们就有更多种不同的实现

11
0:01:14.700 --> 0:01:21.460
可能我们会用酷党来实现呢用OpenCL来用温卡来可能还会用OpenGL来还有Meta来去实现

12
0:01:21.500 --> 0:01:43.460
最终实现完的我们会把它封装成一个高性能的算子库也可能直接提供克诺层废话和前期知识呢有点多我们在网下看一下在整个克诺优化里面呢我们会涉及到哪些内容首先呢我们会去看一下什么为之卷机那如果大家都懂得这个概念可以跳过这一节来去看看下一节

13
0:01:43.460 --> 0:02:07.660
下节呢我们就会讲讲可非里面用的最多的Image to Clam这种优化的算法它是一种卷机的具体的实现方法把卷机这个操作呢变成GMM这种运算的方式然后呢我们会有一个空间组合的优化空间组合优化其实比较简单跟我们编译里面的其实是比较相似的也是结合了Image to Clam的思想来去实现的

14
0:02:07.660 --> 0:02:37.660
在后面的两节里面呢就有点意思里面我们会讲到Window Grid这种优化的算法Window Grid这个优化算法呢是在一九八几年的时候呢已经提出来了但是当时候呢因为算力的问题没有很好的得到一个重用那现在呢应该是在二零一一年之后呢Window Grid开始慢慢地起来了确实它在做一些小卷机核的计算的时候呢效果性呢还是很好的然后呢再到QNN PAC间接

15
0:02:37.660 --> 0:03:07.660
卷机优化这种方式去看看我们卷机怎么做不同的优化现在实不宜迟我们来到了第一个内容就是卷机的基础概念我们来了解一下什么是卷机现在呢我们看一下什么为之卷机实际上呢卷机是神经网络里面的核心计算单元之一啊为什么叫做之一呢因为在神经网络里面的最核心的这种计算单元呢有卷机CNN也有LSTM还有Transformer

16
0:03:07.660 --> 0:03:37.660
这个就是最常用的MathML矩阵相乘那实际上呢卷机在前几年了应该是非常非常的火基本上所有的CPU算法都离不开它包括我们常用的Westnet的MobileNet的Efficient这些都是使用了或者大量地使用了卷机的计算但是呢实际上卷机的变种是非常非常的丰富和多样的我们除了通用的卷机计算之外呢我们还会在卷机里面加上BIOS我们可能会对卷机来进行空洞卷机

17
0:03:37.660 --> 0:04:07.660
我们可能还会进行一个调外思的卷机所以我们会说卷机的变种非常丰富而且它的计算呢还是非常的复杂我们的神经网络或者我们的在一个网络模型当中呢大部分时间都消耗在我们的卷机计算所以呢如何对我们的卷机进行优化就变得非常非常重要这也是我们这个系列或者在科诺优化里面重点去讲解的当然全数码也是一种很好的一种例子但是呢全数码相关的优化说实话

18
0:04:07.660 --> 0:04:37.660
其实并不是说非常的多没有像卷机那样而且卷机呢在端测推定引擎里面是非常的成熟而全数码在端测推定引擎里面呢应该现在来看呢用的不是说非常非常多我们在前面其实已经介绍过全数码这个结构呢在端测推定引擎里面呢现在还在慢慢的引入阶段还没有等到大规模成熟而且随着时间技术的发展呢研究人员呢就提出了多种的关于卷机的优化方法

19
0:04:37.660 --> 0:05:07.660
或英秘书克隆呢还有威诺贵德呢下面呢我们来看一下什么为之卷机下面呢是卷机的一些我在网上找到维基百科的相关的概念说实话我看得不是很懂如果大家想深入了解什么为之卷机卷机跟物理业之间的关系还有卷机的具体的原理大家可以看一下网上相关的介绍这也是非常的多那我们简单地去给大家练一练啊卷机呢主要是通过两个函数一个F呢一个是G通过两个函数呢

20
0:05:07.660 --> 0:05:37.660
变成第三个函数的一种具体的数学运算我们可以把它呢变成等于HX它的本质呢是一种特殊的积分变换所以我们可以看到呢在这里面有个积分的符号这里面呢就表示了表征函数啊F是一个表征函数G也是一个表征函数经过翻转和平均重叠的部分的函数的成绩里面的一个长度的积分那么可以看到有两个框框第一个是蓝色的框框第二个呢

21
0:05:37.660 --> 0:06:07.660
因为这里面移动的黄色的框框蓝色的框框呢是我们的F涛而红色的框框呢是我们的GT减涛而我们的三角形呢经过的三角形就是F跟G两个表征函数而得到的一个积分的概念说实话积分的概念我不明白为什么要这么做现在我们看一下积分的这个公式我们可以看到积分公式里面呢有几个比较重要的元素第一个呢就是T第二个呢就是涛第三个呢就是T减涛我们把它组成一个新的公式

22
0:06:07.660 --> 0:06:37.660
现在我们可以看到新的公式呢就是T等于涛加上T减去涛我们把它再去分列成两边两条公式那第一条呢就是X等于涛第二个呢就是Y等于T减涛然后呢我们把这两个公式呢再组合起来变成X加Y等于N那这时候呢就可以看到这是一条线性的公式那我们可以往下再看一看这条线性的公式呢把它在二维空间里面画出来了就是向右边的所示网把N不断地去放

23
0:06:37.660 --> 0:07:07.660
成不同的一种数值然后呢可以看到这条公式呢类似于在我们的平面当中呢不断地去滑过如果边界刚才上面这条直线呢就好像我们的毛巾把毛巾卷起来然后变成一个新的概念或一种新的形态一样这种呢就是在网上比较通俗或者比较容易理解的一种概念那现在呢讲不清明白没关系大家可以不用看这个视频这个视频其实确实没什么太多的内容我们可以看到现在呢很多时候

24
0:07:07.660 --> 0:07:37.660
我们会把积分积分是对连续的数据或者无穷的数据进行一个计算的这个就是信号处理当中卷积呢把我们的连续变成一个离散的形式的表示而现在呢我们把这条公式呢再拓展到二维的空间就得到了我们神经网络里面的卷积我们可以看到这里面的公式呢非常多关里面的函数参数量的非常多我们足够来打开一下首先呢S呢就是我们的卷积和或我们的卷积最后的输出

25
0:07:37.660 --> 0:08:07.660
A呢就是我们的卷积的数假设你当它做做一张图片就行了而K呢就是我们的卷积和那右边呢就是具体的计算公式我们再往下看一看这里面呢有一个非常好的一个可视化的网站那我们打开去看看这个呢就是GitHub里面一个非常好的一个可视化的方式我们可以看到卷积啊形式方法非常多我们有标准的卷积有反卷积

26
0:08:07.660 --> 0:08:37.660
分组卷积有可分离卷积还有分组卷积卷积的方式非常多下面蓝色的比较深颜色的就是我们的卷积和而下面底的蓝色呢就是我们的数的图片那卷积和跟图片呢在进行划床之后呢就得到一个新的输出的结果而新的输出的结果呢就上面我们绿色的对应于我们公式里面的S这个就是卷积的具体的方式让我们其实卷积呢更多的一开始啊没有应用到神经网络

27
0:08:37.660 --> 0:09:07.660
里面在图像处理的时候呢用得特别的多下面这个呢就是我们整体的卷积的一个计算这个就是我们的卷积的计算和这个呢就是我们的图片图片跟卷积和进行一个擦场就得到我们的最终的结果负三那具体的公式呢就比较简单这里面的卷积和每一个元素可以看到负一零一负一零一然后跟上面的每一个元素

28
0:09:07.660 --> 0:09:37.660
进行相乘负一乘以三零乘以零一乘以一然后再把它进行求和跟我们刚才的公式是一模一样的最终得到我们负三这个结果这个就是卷积的计算既然提到卷积在图像处理里面用得非常多我们现在打开photo shop看一下卷积在photo shop里面的最常用的一些功能假设呢我们现在有这么一张水彩画然后点击滤镜滤镜里面的模糊

29
0:09:37.660 --> 0:10:07.660
模糊里面用得非常的多首先呢就是高斯模糊高斯模糊就是我们的卷积和呢是符合高斯的分布而这里面平均的半径呢就是我们的卷积和的大小可以看到通过这里面的空字偷通了我们可以控制我们的卷积和的大小卷积和的大小作用于怎么图片呢就使得我们的图片越来越模糊像里面的很多滤镜的方式呢就使用了不同的卷积的组成方式去对我们的图片产生作用的好不容易

30
0:10:07.660 --> 0:10:37.660
终于讲完了自己不熟悉的或者不是说非常理解的概念给大家说实话实在惭愧下面呢又回到了我最熟悉的概念呢还是系统的优化还有算法的优化也没有办法给大家讲得很透彻所以希望大家能够谅解简单的听一听或者不要听就行了下面我们看一下卷积对于tensor的一个优化首先可以看到了张亮啊这个tensor就我们的张亮嘛张亮在内存里面呢一般我们现在假设以nhwc这种方式呢

31
0:10:37.660 --> 0:10:42.060
作为一个布局那可以看到下面这一咋公司呢

32
0:10:42.060 --> 0:10:45.660
就是卷积对于张亮的一种运算

33
0:10:45.660 --> 0:10:48.860
那我们可以看到这边有非常非常多的负

34
0:10:48.860 --> 0:10:52.260
我们逐个地看看这些负有什么作用

35
0:10:52.260 --> 0:10:58.060
首先呢我们有外层三层的负里面呢又有三层的负

36
0:10:58.060 --> 0:11:04.740
外面的三层的负呢就是对于我们nhw里面的h我们对于h的这个通道呢进行便利

37
0:11:04.740 --> 0:11:06.860
然后呢对于w这个通道进行便利

38
0:11:06.860 --> 0:11:10.260
最后呢对于我们的c的通道进行便利

39
0:11:10.260 --> 0:11:15.860
最后呢才拿到我们第一个数据c零h零w零c然后把它附一个值

40
0:11:15.860 --> 0:11:22.660
接着呢我们把这个值呢因为我们拿到内存的地址嘛内存的地址里面存的是什么字我们是不确定的

41
0:11:22.660 --> 0:11:24.660
所以我们首先对它进行负责

42
0:11:24.660 --> 0:11:30.860
然后呢有三个类层的负三个类层的负呢就是我们的卷积核啦

43
0:11:30.860 --> 0:11:36.460
那这个卷积核呢就是科诺的h科诺的w还有input的channel

44
0:11:36.460 --> 0:11:39.060
通过这种方式呢对它进行一个组织

45
0:11:39.060 --> 0:11:44.060
然后呢就是我们刚才的那条求核公式里面的进行一个计算

46
0:11:44.060 --> 0:12:03.060
那具体呢就是字里面卷积核跟这个原始的图片进行加权求核加权求核之后呢就给我们的c零h零w零c进行计算可以看到一个简单的卷积的数学计算是非常的复杂又非常多的嵌套的循环

47
0:12:03.060 --> 0:12:15.660
为了去让我们的整个嵌套的循环没有那么深我们会做很多的循环的优化循环的展开分块重排融合拆分那这些所有的概念呢

48
0:12:15.660 --> 0:12:28.860
我们在之前AI编译器里面呢其实是讲过的在我们的AI编译器后端优化就算子优化了循环优化里面给大家详细的去介绍过但是呢这里面是推定引擎的概念嘛

49
0:12:28.860 --> 0:12:58.860
所以我们不需要通过编译器或者不需要引入编译器的概念直接用人工的方式用手排的方式呢对循环进行展开这也是科技优化工程师所做的工作另外我们还有指令的优化对我们的数据呢进行向量化对我们的数据呢进行占量化这些概念也是比较好理解的我们现在呢假设c呢是一个只有四个通道那这个时候呢我们就可以完全把它进行一个向量化的操作不用每次都执行

50
0:12:58.860 --> 0:13:28.860
四次相关的雷家操作那下面呢我们最后还有存储的优化存储优化主要是包括我们的访存的言辞还有存储的分配可以往上面这一坨公司里面看到了我们这里面呢其实有大量的访问内存的方式怎么对它进行优化是个很大的概念这也是我们写科诺的工程师非常之在行的那更多相关的操作更多相关的原理呢也可以去到我之前讲到的AI编译器后端优化里面的

51
0:13:28.860 --> 0:13:46.900
相关的应用这里面呢就不会介绍太多相关的原理知识今天的内容呢确实太不专业了就到这里为止谢谢各位摆了个掰卷的不行了卷的不行了记得一键三连加关注哦所有的内容都会开源在下面这条链接里面摆了个掰

