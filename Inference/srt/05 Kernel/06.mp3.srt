0
0:00:00.000 --> 0:00:04.960
啪啪啪啪啪啪啪啪啪

1
0:00:04.960 --> 0:00:11.040
哈喽大家好我是那个不能再吃了不能再吃了再吃就吃不下甜品的宗米了

2
0:00:11.040 --> 0:00:19.120
来在这集里面我们终于脱离了柯冷油画里面的卷迹油画那节确实剪的挺痛苦的里面涂的画的我好心酸哦

3
0:00:19.120 --> 0:00:28.600
今天我们来到一个新的内容就是内存的布局啊内存布局这个内容呢确实非常重要不管柯冷油画也好我们只是对数据进行重排

4
0:00:28.600 --> 0:00:32.600
我们来到一个新的内容内存的布局

5
0:00:32.600 --> 0:00:40.500
其实在上一节内容里面呢我们讲到的算法的优化不管是哪种算法的优化其实都是离不开内存布局的加词

6
0:00:40.500 --> 0:00:45.300
那在这一节里面呢我们主要是看一下内存的布局有哪些不一样啊

7
0:00:45.300 --> 0:00:49.600
会去讲具体内存布局怎么去配合我们的科伦优化的

8
0:00:49.600 --> 0:00:58.000
下面呢我们看一下整个的科伦优化呢还是在这一个内容里面而内存的布局呢也是在这一节内容里面

9
0:00:58.000 --> 0:01:02.400
下面我们看一下第一个概念就是内存memory

10
0:01:02.400 --> 0:01:04.600
memory有什么不一样

11
0:01:04.600 --> 0:01:10.300
首先呢左边的这个图呢就是CPU的一个具体的或者一个通用的架构

12
0:01:10.300 --> 0:01:14.700
右边的这个图呢就是GPU的一个常用的架构

13
0:01:14.700 --> 0:01:18.200
可以呢我们简单的看一下上面两句话

14
0:01:18.200 --> 0:01:25.400
像RAM这种的储存是比较大的存储空间但是呢读取或者存储的速度呢是比较慢的

15
0:01:25.400 --> 0:01:32.300
那像CPU或者MPU里面的cache就上面的在我们CPU里面的cache呢存储的速度呢就要快很多

16
0:01:32.300 --> 0:01:35.700
但是整体的规模呢就比较少了而且造价呢比较贵

17
0:01:35.700 --> 0:01:41.800
因此呢我们可以适当的去使用CPU或者MPU里面的cache是非常的重要的

18
0:01:41.800 --> 0:01:45.700
每一次我们从RAM里面或者组存里面呢去获取数据呢

19
0:01:45.700 --> 0:01:55.300
CPU就会将这些数据啊就把memory global memory这些数据呢真正的搬运到CPU里面的cache就离我们的计算单元更近的cache

20
0:01:55.400 --> 0:02:02.500
里面呢以便的更好地利用我们的仿存的局部性的提升整体的命中率包括我们的GPU也是一样的

21
0:02:02.500 --> 0:02:13.100
我们从后测的一些global memory里面把一些数据呢读到L二的cache里面通过这种方式呢一级一级地递增提供整体的命中率

22
0:02:13.100 --> 0:02:21.100
那可以看到了这里面的GPU会通过PC一桥或者NVLink呢跟GPUGPU之间的互联互通

23
0:02:21.100 --> 0:02:28.600
而global memory呢其实就是一个事情我们这里面呢就有了一个义构的架构一个是CPU一个是GPU

24
0:02:28.600 --> 0:02:40.600
CPU里面呢有少量的核所以它叫做simd而GPU里面呢有大量的线层处sm所以它呢大部分的使用的是一个simt的架构

25
0:02:40.600 --> 0:02:49.400
那下面呢我们可以看到不管是哪种方式啊都有多级的还存如何更好地利用多级的还存是一个很重要的概念

26
0:02:49.400 --> 0:02:57.600
那下面呢我们回顾一下在之前呢我们其实已经简单地讲过了我们对内存呢确实需要进行一个对齐的

27
0:02:57.600 --> 0:03:02.900
实际上呢内存的对齐呢是以字节为单位的就是我们的最小的存储单位

28
0:03:02.900 --> 0:03:09.200
一个变量的内存地址呢刚好等于它的长度的整倍数那这种呢我们叫做自然对齐

29
0:03:09.200 --> 0:03:18.300
像在三十二位的CPU里面呢一个uint三十二的数据啊它的内存地址假设是零二十零零四

30
0:03:18.300 --> 0:03:26.000
那这种呢就属于自然对齐可以看到四根IP三十二的这个数据和这个字节单位呢是相同的

31
0:03:26.000 --> 0:03:36.200
而内存空间呢按字节进行划分对齐的好处呢就是我们可以从内存里面的任意一个地址上面进行读取都是没有问题的

32
0:03:36.200 --> 0:03:43.700
那下面呢我们了解完整体的内存在硬件上面是怎么实现的会怎么去布局的还有它的一个对齐的方式

33
0:03:43.700 --> 0:03:48.000
下面我们看一下具体的内容就我们探索内存的布局

34
0:03:48.000 --> 0:03:54.300
因为在神经网络里面大量的流动的数据呢都是以tensor作为基本单位的

35
0:03:54.300 --> 0:04:03.100
而tensor这个概念其实我们在前面几节里面已经给大家都已经讲烂了这里面呢我就简单地翻译翻了

36
0:04:03.100 --> 0:04:10.700
tensor呢是神经网络里面的一个基本的数据结构里面呢就包括里面的shape啦当然还有datatype

37
0:04:10.700 --> 0:04:16.100
shape和datatype都是很重要的shape呢就决定了我到底是一个什么方式组成

38
0:04:16.100 --> 0:04:23.200
而datatype呢就决定了我们每一个元素到底属于什么一个形状最后呢我们就按内存的地址进行排布了

39
0:04:23.200 --> 0:04:27.700
而真正的索引的时候呢会根据tensor的shape进行一个索引

40
0:04:27.700 --> 0:04:35.400
而索引每次索引多少内容呢或多少一个字节呢就是按datatype来进行决定的所以这两个数值非常重要

41
0:04:35.400 --> 0:04:46.000
那在章量里面呢我们可以看到一维的章量呢却只有一条数据二维的章量呢就是类似于一张黑白的图片三维的章量呢则是彩色的图片四维的章量呢就是很多个三维的章量

42
0:04:46.000 --> 0:04:50.900
三维的章量堆叠起来前面呢就多了一个n呢就是我们的batchsize大小

43
0:04:50.900 --> 0:04:56.300
但呢点语里面我们可能还会有五维的章量那当然六维的章量其实我现在还没想到太多啊

44
0:04:56.300 --> 0:05:06.100
而之后除非我们自己硬件进行一个切分在达芬奇或者在生成硬件里面呢做了一个切分就变成把五维变成六维进行方便硬件进行计算嘛

45
0:05:06.100 --> 0:05:14.700
但实际上你怎么切还是内存数据的排布不一样而图像一般呢我们图像章量化表示呢就是nchw

46
0:05:14.700 --> 0:05:21.200
那chw呢就是我们的一张彩色图片再加一个n呢就是我们的一个batchsize就很多张图片

47
0:05:21.200 --> 0:05:27.200
那自然语言处理呢可能nlp领域呢会比较特别我们会有sequenced就一句话

48
0:05:27.200 --> 0:05:38.100
然后一句话里面呢又有多少个word多少个token或多少个单词那一共有多少段这句话呢就是nnsw是自然语言处理的方式

49
0:05:38.100 --> 0:05:44.800
当然还有试书的章量呢有点云和图啊gnn这种方式的表达所以章量的表示呢还是非常的复杂的

50
0:05:47.300 --> 0:05:56.400
在探测的值呢存在内存里面呢是很有意思的它有两种方式第一种呢是以rolemajor的方式第二种呢是以clonemajor的方式

51
0:05:56.400 --> 0:06:02.800
这是你到底是以行作为主要的索引顺序呢还是以列作为主要的索引顺序呢

52
0:06:02.800 --> 0:06:13.800
哎这里面的类似的图呢就画错了按行呢就是按z的方式进行取的可以看到都是z而按列呢就是以n的方式进行取的n就是这种方式进行取

53
0:06:13.800 --> 0:06:30.500
所以说呢探测呀根据排列的顺序的区别呢分为行组存和列组存两种风格取数据的时候呢是按z形式来取还是按n形式来取这个就是我们最通常的nz和nd的方式

54
0:06:32.800 --> 0:07:02.800
下面呢我们看一下nchw这种数据的存储方式首先呢我们会往w先取数据就一二三先取然后再取h的数据就四五六了然后再取第三个维度c的维度c的维度就是六七八九后面绿色的这个模块最后呢再取n就不断的累加这种方式呢就是nhw的数据存储的方式那这种方式呢其实很好的一个计算呢就是max poolingmax pooling呢我们首先要需要对这么简单的一个通道呢进行

55
0:07:02.800 --> 0:07:11.200
一个取最大值像achw这种计算呢适合gpu进行运算了因为gpu的内存带宽大而且并行能力强嘛

56
0:07:12.200 --> 0:07:33.600
下面我们看一下另外一种数据的存储方式就是nhwc啊这种方式却讲烂了而nhwc呢就是先取c通道了所以我们会取一七十三先取一七十三然后再取二八十四二八十四这种方式就先取c再取w再取h然后再取n就内存的读取的方式不太一样了

57
0:07:34.600 --> 0:07:43.600
像这种方式呢更适合卷集一乘一这种计算操作更适合一些多核的cpu进行一个运算的cpu的内存带宽比较小嘛

58
0:07:45.600 --> 0:07:59.600
那下面我们看一下不同的框架的默认的选择方式因为不同的AI框架可能会有不同的方式那现在以npu与gpu为基础的像派套选或者max pool呢默认的就是nhw的这种数据的存储格式

59
0:07:59.600 --> 0:08:29.600
那tenso four呢其实一开始是默认使用nhwc的这种数据的存储方式的而面向移动端的特别是我们的推定型推定型的更多的也是采用nhwc的为什么推定型大部分采用这种格式呢因为很多推定型的都会跑在手机上面手机上面的大部分都是以arm作为或者是arm cpu作为一个主要的计算单元和计算单位所以一般默认都会使用nhwc而以gpu为主呢就会

60
0:08:29.600 --> 0:08:59.600
使用大量的nhw它的并行能力特别好嘛我们希望尽可能的去利用它的并行的操作的能力那这个时候呢不同的AI框架或者你面向不同的场景你会对数据的默认的内存排布呢会有不同的需求那下面呢我们可以看一下它里面最大的区别就是nhw呢主要是对每个通道单独做运算的时候会特别特别的快因为你数据都已经排好了我单独做运算就特别快嘛像nhwc这种方式呢就特别适合

61
0:08:59.600 --> 0:09:29.600
与那些不同通道对于同一项数做运算扭卷机这种方式确实也是特别好所以说不同的数据的排布方式对我们的科诺斯的优化是非常的讲究下面我们看一下一个新的或者特别有意思的数据的存储格式nhwx多了个s那这个s呢就其实我们往后看了n呢就是我们的backspace的大小了这里面的c

62
0:09:29.600 --> 0:09:59.600
呢就不是完完全全真正的c而是圈入数除以x除以多少个然后hw都不用说了而x呢你可以默认是四啊三十二或六十四位都可以就很有意思那这种方式呢其实在达芬奇架构里面呢我们叫做nc1hwc零那可能在那个ml里面呢它叫做nchwc一那这种方式不管怎么样啊我们的x基本上就从c里面切分出来那具体

63
0:09:59.600 --> 0:10:29.600
数据类型排布是怎么样的我们直接看下面这个图假设呢这个图还是那个刚才模拟的图我们数据呢还是按x进行排布那这个时候就很有意思了我们会一二三四假设x是四的时候我们就会把四个数先取出来一七十三还有这个x然后呢再取二八十四x这个数通过这种方式呢每次只读取我们

64
0:10:29.600 --> 0:10:59.600
整个章量里面的一部分进行处理非常适合我们的内存的排布因为我们之前讲到了内存的cash呢分l一l二l零是吧所以l零呢最接近我们的cpu而l零呢确实是最小的数量最小的每次我们在分片的时候呢就取够的数据取得刚刚好然后计算完之后下一次再取下一批这个时候呢就很好的充分地利用了我们内存空间的方案了下面呢

65
0:10:59.600 --> 0:11:29.600
具体看一下nchwx呢更好其实更好地适配于simt这种架构例如nchw四呢可以针对ARM的int八的数据类型充分地利用了CUDA里面的DP四a这个模块和这个指令呢进行一个计算的进行一个加速的另外的话nchw三是说nchw例子四呢分别可以对int八和int四的数据呢更好地利用里面的tensor core去进行一个计算的所以说啊像这种方式

66
0:11:29.600 --> 0:11:59.600
呢最重要的就是对cache更加友好减少cache的missing提高命中率这个时候是非常的重要仿存实在确实比我们的计算要慢因为计算的时候L又算得非常的快但是呢我们很多时候是数据搬运来不及数据的通讯特别慢数据的cachemissing阻碍了我们整个运算的效率这对我们整个工程实现的确实是非常的核心

67
0:11:59.600 --> 0:12:29.600
那就到这里为止在我们的内存布局之里面呢我们今天了解了三个重要的概念那第一个呢就是CPU和GPU的内存的布局它的cache到底是怎么样的一个排布方式接着呢我们了解了一下张亮tensor的nchw和nhwc具体的排布方式和它适应的场景最后呢我们引额一个nchwx这种方式去看一下不同的为什么会有这么一个x出来而这个nchwc呢就是对应于华为

68
0:12:29.600 --> 0:12:50.850
正常打盘器架构里面的一种数据的格式而nhwc呢就对应于像阿里MM或者华为玛斯莫赖这种数据的存储的方式好了今天内容到这里为止谢谢各位拜了个拜卷得不行了卷得不行了记得一键三连加关注哦所有的内容都会开源在下面这条链接里面拜了个拜

