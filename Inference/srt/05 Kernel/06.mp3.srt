1
00:00:00,000 --> 00:00:08,040
Hello 大家好 我是那个不能再吃了不能再吃了

2
00:00:08,040 --> 00:00:10,840
再吃就吃不下甜品的粽米了

3
00:00:10,840 --> 00:00:14,960
在这一节里面我们终于脱离了科诺油画里面的卷积油画

4
00:00:14,960 --> 00:00:16,440
那节确实剪的挺痛苦的

5
00:00:16,440 --> 00:00:19,040
里面涂的画都我好心酸哦

6
00:00:19,040 --> 00:00:20,560
今天我们来到一个新的内容

7
00:00:20,560 --> 00:00:22,200
就是内存的布局

8
00:00:22,200 --> 00:00:24,920
内存布局这个内容确实非常重要

9
00:00:24,920 --> 00:00:26,280
不管科诺油画也好

10
00:00:26,280 --> 00:00:28,440
我们只是对数据进行重排

11
00:00:29,440 --> 00:00:31,040
我们来到一个新的内容

12
00:00:31,040 --> 00:00:32,600
内存的布局

13
00:00:32,600 --> 00:00:34,120
其实在上一节内容里面

14
00:00:34,120 --> 00:00:35,840
我们讲到的算法的优化

15
00:00:35,840 --> 00:00:37,240
不管是哪种算法的优化

16
00:00:37,240 --> 00:00:40,520
其实都是离不开内存布局的加持

17
00:00:40,520 --> 00:00:42,160
那在这一节里面

18
00:00:42,160 --> 00:00:45,280
我们主要是看一下内存的布局有哪些不一样

19
00:00:45,280 --> 00:00:49,720
我们会去讲具体内存布局怎幺去配合我们的科诺油画的

20
00:00:49,720 --> 00:00:52,320
下面我们看一下整个的科诺油画

21
00:00:52,320 --> 00:00:54,480
还是在这一个内容里面

22
00:00:54,480 --> 00:00:58,120
而内存的布局也是在这一节内容里面

23
00:00:58,120 --> 00:01:00,120
下面我们看一下第一个概念

24
00:01:00,120 --> 00:01:02,480
就是内存 memory

25
00:01:02,480 --> 00:01:04,200
memory 有什幺不一样

26
00:01:04,200 --> 00:01:08,600
首先左边的这个图就是 CPU 的一个具体的

27
00:01:08,600 --> 00:01:10,400
或者一个通用的架构

28
00:01:10,400 --> 00:01:14,680
右边的这个图就是 GPU 的一个常用的架构

29
00:01:14,680 --> 00:01:17,920
可以我们简单的看一下上面两句话

30
00:01:17,920 --> 00:01:21,520
像 RAM 这种的组存是比较大的存储空间

31
00:01:21,520 --> 00:01:25,440
但是读取或者存储的速度是比较慢的

32
00:01:25,480 --> 00:01:27,680
那像 CPU 或者 MPU 里面的 cache

33
00:01:27,680 --> 00:01:30,240
就上面的在我们 CPU 里面的 cache

34
00:01:30,240 --> 00:01:32,360
存储的速度就要快很多

35
00:01:32,360 --> 00:01:34,360
但是整体的规模就比较少了

36
00:01:34,360 --> 00:01:35,760
而且造价比较贵

37
00:01:35,760 --> 00:01:38,280
因此我们可以适当的去使用

38
00:01:38,280 --> 00:01:41,840
CPU 或者 MPU 里面的 cache 是非常的重要的

39
00:01:41,840 --> 00:01:45,720
每一次我们从 RAM 里面或者组存里面去获取数据

40
00:01:45,720 --> 00:01:48,400
CPU 就会将这些数据就把 memory

41
00:01:48,400 --> 00:01:49,920
global memory 这些数据

42
00:01:49,920 --> 00:01:52,360
真正的搬运到 CPU 里面的 cache

43
00:01:52,400 --> 00:01:56,120
就离我们的计算单元更近的 cache 里面

44
00:01:56,120 --> 00:01:58,920
以便更好地利用我们的仿存的局部性

45
00:01:58,920 --> 00:02:00,600
提升整体的命中率

46
00:02:00,600 --> 00:02:02,600
包括我们的 GPU 也是一样的

47
00:02:02,600 --> 00:02:05,280
我们从 host 的一些 global memory 里面

48
00:02:05,280 --> 00:02:08,000
把一些数据读到 L2 的 cache 里面

49
00:02:08,000 --> 00:02:10,960
通过这种方式一级一级地递增

50
00:02:10,960 --> 00:02:13,200
提供整体的命中率

51
00:02:13,200 --> 00:02:14,960
那可以看到了这里面

52
00:02:14,960 --> 00:02:18,040
GPU 会通过 PCIe 桥或者 NVLink

53
00:02:18,040 --> 00:02:21,200
跟 GPU 之间互粘互通

54
00:02:21,240 --> 00:02:23,960
而 global memory 其实就是一个事情

55
00:02:23,960 --> 00:02:26,360
我们这里面就有了一个异构的架构

56
00:02:26,360 --> 00:02:28,720
一个是 CPU 一个是 GPU

57
00:02:28,720 --> 00:02:31,400
CPU 里面有少量的核

58
00:02:31,400 --> 00:02:32,960
所以它叫做 SIMD

59
00:02:32,960 --> 00:02:36,400
而 GPU 里面有大量的线程处 SM

60
00:02:36,400 --> 00:02:40,720
所以它大部分使用的是一个 SIMT 的架构

61
00:02:40,720 --> 00:02:42,120
那下面我们可以看到

62
00:02:42,120 --> 00:02:45,320
不管是哪种方式都有多级的缓存

63
00:02:45,320 --> 00:02:47,600
如何更好的利用多级的缓存

64
00:02:47,600 --> 00:02:49,480
是一个很重要的概念

65
00:02:49,520 --> 00:02:51,320
那下面我们回顾一下

66
00:02:51,320 --> 00:02:53,840
在之前我们其实已经简单的讲过了

67
00:02:53,840 --> 00:02:57,720
我们对内存确实需要进行一个对齐的

68
00:02:57,720 --> 00:03:00,840
实际上内存的对齐是以字节为单位的

69
00:03:00,840 --> 00:03:02,960
就是我们的最小的存储单位

70
00:03:02,960 --> 00:03:04,520
一个变量的内存地址

71
00:03:04,520 --> 00:03:06,760
刚好等于它的长度的整倍数

72
00:03:06,760 --> 00:03:09,320
那这种我们叫做自然对齐

73
00:03:09,320 --> 00:03:11,880
像在 32 位的 CPU 里面

74
00:03:11,880 --> 00:03:15,440
一个 UINT32 的数据

75
00:03:15,440 --> 00:03:18,400
它的内存地址假设是 0x2004

76
00:03:18,440 --> 00:03:20,480
那这种就属于自然对齐

77
00:03:20,480 --> 00:03:22,680
可以看到 4 跟 32 的这个数据

78
00:03:22,680 --> 00:03:26,040
和这个字节单位是相同的

79
00:03:26,040 --> 00:03:29,160
而内存空间按字节进行划分

80
00:03:29,160 --> 00:03:32,080
对齐的好处就是我们可以从内存里面的

81
00:03:32,080 --> 00:03:34,640
任意一个地址上面进行读取

82
00:03:34,640 --> 00:03:36,240
都是没有问题的

83
00:03:36,240 --> 00:03:38,800
那下面我们了解完整体的内存

84
00:03:38,800 --> 00:03:40,600
在硬件上面是怎幺实现的

85
00:03:40,600 --> 00:03:41,680
或者怎幺去布局的

86
00:03:41,680 --> 00:03:43,760
还有它的一个对齐的方式

87
00:03:43,760 --> 00:03:45,600
下面我们看一下具体的内容

88
00:03:45,600 --> 00:03:48,040
就我们 Tensor 内存的布局

89
00:03:48,040 --> 00:03:49,360
因为在神经网络里面

90
00:03:49,360 --> 00:03:51,080
大量的流动的数据

91
00:03:51,080 --> 00:03:54,320
都是以 Tensor 作为基本单位的

92
00:03:54,320 --> 00:03:55,320
而 Tensor 这个概念

93
00:03:55,320 --> 00:03:56,920
其实我们在前面几页里面

94
00:03:56,920 --> 00:03:58,880
已经给大家都已经讲烂了

95
00:03:58,880 --> 00:04:02,040
这里面我就简单的翻一翻了

96
00:04:03,040 --> 00:04:05,640
Tensor 是神经网络里面的

97
00:04:05,640 --> 00:04:07,000
一个基本的数据结构

98
00:04:07,000 --> 00:04:09,120
里面就包括里面的 Shape 了

99
00:04:09,120 --> 00:04:10,600
当然还有 Data Type

100
00:04:10,600 --> 00:04:12,800
Shape 和 Data Type 都是很重要的

101
00:04:12,800 --> 00:04:13,920
Shape 就决定了

102
00:04:13,920 --> 00:04:16,120
我到底是一个什幺方式组成

103
00:04:16,120 --> 00:04:17,520
而 Data Type 就决定了

104
00:04:17,520 --> 00:04:18,480
我们每一个元素

105
00:04:18,480 --> 00:04:20,240
到底属于什幺一个形状

106
00:04:20,240 --> 00:04:23,240
最后我们就按内存的地址进行排布了

107
00:04:23,240 --> 00:04:24,760
而真正的索引的时候

108
00:04:24,760 --> 00:04:26,400
会根据 Tensor 的 Shape

109
00:04:26,400 --> 00:04:27,720
进行一个索引

110
00:04:27,720 --> 00:04:29,800
而索引每次索引多少内容

111
00:04:29,800 --> 00:04:31,200
或者多少一个字节

112
00:04:31,200 --> 00:04:33,600
就是按 Data Type 来进行决定的

113
00:04:33,600 --> 00:04:35,440
所以这两个数值非常重要

114
00:04:35,440 --> 00:04:36,480
那在张量里面

115
00:04:36,480 --> 00:04:37,320
我们可以看到

116
00:04:37,320 --> 00:04:38,200
一维的张量

117
00:04:38,200 --> 00:04:39,200
确实就一条数据

118
00:04:39,200 --> 00:04:39,920
二维的张量

119
00:04:39,920 --> 00:04:41,800
就是类似于一张黑白的图片

120
00:04:41,800 --> 00:04:42,640
三维的张量

121
00:04:42,640 --> 00:04:44,120
就是彩色的图片

122
00:04:44,120 --> 00:04:45,120
四维的张量

123
00:04:45,120 --> 00:04:46,560
就是很多个三维的张量

124
00:04:46,600 --> 00:04:49,120
堆叠起来前面就多了一个 n

125
00:04:49,120 --> 00:04:50,800
就是我们的 Batch Size 大小

126
00:04:50,800 --> 00:04:51,200
当然了

127
00:04:51,200 --> 00:04:51,800
点语里面

128
00:04:51,800 --> 00:04:53,440
我们可能还会有五维的张量

129
00:04:53,440 --> 00:04:54,480
那当然六维的张量

130
00:04:54,480 --> 00:04:56,200
其实我现在还没想到太多

131
00:04:56,200 --> 00:04:59,160
之后除非我们自己硬件进行一个切分

132
00:04:59,160 --> 00:05:01,520
在达芬奇或者在生腾硬件里面

133
00:05:01,520 --> 00:05:02,360
做了一个切分

134
00:05:02,360 --> 00:05:04,040
就变成把五维变成六维

135
00:05:04,040 --> 00:05:06,000
方便硬件进行计算

136
00:05:06,000 --> 00:05:07,120
但实际上你怎幺切

137
00:05:07,120 --> 00:05:09,120
还是内存数据的排布不一样

138
00:05:09,120 --> 00:05:10,480
而图像一般

139
00:05:10,480 --> 00:05:12,240
我们图像张量化表示

140
00:05:12,240 --> 00:05:14,600
就是 nchw

141
00:05:14,640 --> 00:05:17,160
那 chw 就是我们的一张彩色图片

142
00:05:17,160 --> 00:05:18,160
再加一个 n

143
00:05:18,160 --> 00:05:19,760
就是我们的一个 Batch Size

144
00:05:19,760 --> 00:05:21,160
就很多张图片

145
00:05:21,160 --> 00:05:22,240
那自然语言处理

146
00:05:22,240 --> 00:05:24,680
可能 NLP 领域会比较特别

147
00:05:24,680 --> 00:05:26,080
我们会有 Sequenced

148
00:05:26,080 --> 00:05:27,120
就一句话

149
00:05:27,120 --> 00:05:28,000
然后一句话里面

150
00:05:28,000 --> 00:05:29,440
又有多少个 Rost

151
00:05:29,440 --> 00:05:30,320
多少个 Token

152
00:05:30,320 --> 00:05:32,320
或者多少个单词

153
00:05:32,320 --> 00:05:34,400
那一共有多少段

154
00:05:34,400 --> 00:05:35,040
这句话

155
00:05:35,040 --> 00:05:36,640
就是 nnsw

156
00:05:36,640 --> 00:05:38,080
是自然语言处理的方式

157
00:05:38,080 --> 00:05:39,680
当然还有 C 书的张量

158
00:05:39,680 --> 00:05:40,840
有点语和图

159
00:05:40,840 --> 00:05:42,400
GNN 这种方式的表达

160
00:05:42,400 --> 00:05:43,400
所以张量的表示

161
00:05:43,400 --> 00:05:44,960
还是非常的复杂的

162
00:05:47,240 --> 00:05:48,240
在 Tensor 的值

163
00:05:48,400 --> 00:05:49,320
存在内存里面

164
00:05:49,480 --> 00:05:50,640
是很有意思的

165
00:05:50,640 --> 00:05:51,880
它有两种方式

166
00:05:51,880 --> 00:05:54,040
第一种是以 row major 的方式

167
00:05:54,040 --> 00:05:56,320
第二种是以 column major 的方式

168
00:05:56,320 --> 00:05:58,280
就是你到底是以行

169
00:05:58,280 --> 00:05:59,960
作为主要的缩影顺序

170
00:06:00,240 --> 00:06:02,480
还是以列作为主要的缩影顺序

171
00:06:03,000 --> 00:06:05,040
这里面的绿色的图就画错了

172
00:06:05,040 --> 00:06:07,760
按行就是按 J 的方式进行取的

173
00:06:07,760 --> 00:06:09,080
可以看到都是 J

174
00:06:09,080 --> 00:06:11,680
而按列就是以 N 的方式进行取的

175
00:06:11,720 --> 00:06:13,680
N 就是这种方式进行取

176
00:06:13,680 --> 00:06:15,080
所以说 Tensor

177
00:06:15,360 --> 00:06:17,160
根据排列的顺序的区别

178
00:06:17,360 --> 00:06:20,520
分为行 组存和列组存

179
00:06:20,520 --> 00:06:21,800
两种风格

180
00:06:21,800 --> 00:06:22,720
取数据的时候

181
00:06:22,960 --> 00:06:25,160
是按 J 形式来取

182
00:06:25,160 --> 00:06:27,040
还是按 N 形式来取

183
00:06:27,040 --> 00:06:28,640
这个就是我们最通常的

184
00:06:28,640 --> 00:06:30,520
NJ 和 ND 的方式

185
00:06:32,960 --> 00:06:34,280
下面我们看一下

186
00:06:34,280 --> 00:06:37,040
NCHW 这种数据的存储方式

187
00:06:37,040 --> 00:06:40,200
首先我们会往 W 先取数据

188
00:06:40,200 --> 00:06:41,240
就 1 2 3 先取

189
00:06:41,360 --> 00:06:42,640
然后再取 H 的数据

190
00:06:42,640 --> 00:06:43,520
就 4 5 6 了

191
00:06:43,520 --> 00:06:45,040
然后再取第三个维度

192
00:06:45,040 --> 00:06:46,480
C 的维度

193
00:06:46,480 --> 00:06:47,360
就是 6 7 8 9

194
00:06:47,360 --> 00:06:48,760
后面绿色的模块

195
00:06:48,760 --> 00:06:51,360
最后再取 N 就不断的累加

196
00:06:51,360 --> 00:06:52,160
这种方式就是

197
00:06:52,160 --> 00:06:55,080
NHW 的数据存储的方式

198
00:06:55,080 --> 00:06:58,040
这种方式其实很好的一个计算

199
00:06:58,160 --> 00:06:59,640
就是 Max Pooling

200
00:06:59,640 --> 00:07:02,000
我们首先要需要对这幺简单的一个信道

201
00:07:02,320 --> 00:07:04,600
进行一个取最大值

202
00:07:04,600 --> 00:07:06,320
像 NCHW 这种计算

203
00:07:06,560 --> 00:07:08,160
是和 GPU 进行运算了

204
00:07:08,160 --> 00:07:09,640
因为 GPU 的内存带宽大

205
00:07:09,640 --> 00:07:10,920
而且并行能力强

206
00:07:12,240 --> 00:07:14,280
下面我们看一下

207
00:07:14,280 --> 00:07:15,600
另外一种数据的存储方式

208
00:07:15,600 --> 00:07:16,760
就是 NHWC

209
00:07:16,880 --> 00:07:18,600
这种方式确实讲烂了

210
00:07:18,600 --> 00:07:21,160
NHWC 就是先取 C 信道了

211
00:07:21,160 --> 00:07:22,680
所以我们会取 173

212
00:07:22,680 --> 00:07:23,880
先取 173

213
00:07:23,880 --> 00:07:27,160
然后再取 2814

214
00:07:27,160 --> 00:07:28,880
这种方式就先取 C

215
00:07:28,880 --> 00:07:29,760
再取 W

216
00:07:29,760 --> 00:07:30,560
再取 H

217
00:07:30,560 --> 00:07:31,680
然后再取 N

218
00:07:31,680 --> 00:07:34,240
就内存的读取的方式不太一样了

219
00:07:34,800 --> 00:07:37,320
像这种方式更适合卷积一乘一

220
00:07:37,320 --> 00:07:38,760
这种计算操作

221
00:07:38,760 --> 00:07:40,480
更适合一些多核的 CPU

222
00:07:40,480 --> 00:07:42,160
进行一个运算的

223
00:07:42,160 --> 00:07:43,760
CPU 的内存带宽比较小

224
00:07:46,040 --> 00:07:46,840
下面我们看一下

225
00:07:46,840 --> 00:07:48,840
不同的框架的默认的选择方式

226
00:07:48,840 --> 00:07:50,000
因为不同的 AI 框架

227
00:07:50,000 --> 00:07:51,640
可能会有不同的方式

228
00:07:51,640 --> 00:07:53,840
那现在以 NPU 与 GPU 为基础的

229
00:07:53,840 --> 00:07:55,240
像 PyTorch 或者 Minespot

230
00:07:55,600 --> 00:07:59,880
默认就是 NCHW 的这种数据的存储格式

231
00:07:59,880 --> 00:08:02,440
那 Tensorflow 其实一开始是默认使用

232
00:08:02,440 --> 00:08:05,480
NHWC 的这种数据的存储方式的

233
00:08:05,480 --> 00:08:07,040
而面向移动端

234
00:08:07,280 --> 00:08:09,160
特别是我们的推定型

235
00:08:09,400 --> 00:08:12,800
推定型更多的也是采用 NHWC 的

236
00:08:12,800 --> 00:08:15,000
为什幺推定型大部分采用这种格式呢

237
00:08:15,000 --> 00:08:18,240
因为很多推定型都会跑在手机上面

238
00:08:18,240 --> 00:08:21,080
手机上面大部分都是以 ARM 作为

239
00:08:21,080 --> 00:08:24,720
或者是 ARM CPU 作为一个主要的计算单元

240
00:08:24,720 --> 00:08:25,600
或计算单位

241
00:08:25,600 --> 00:08:28,080
所以一般默认都会使用 NHWC

242
00:08:28,080 --> 00:08:29,160
而以 GPU 为主

243
00:08:29,160 --> 00:08:31,440
就会使用大量的 NCHW

244
00:08:31,440 --> 00:08:32,760
它的并行能力特别好

245
00:08:32,760 --> 00:08:34,720
我们希望尽可能的去利用它的

246
00:08:34,720 --> 00:08:36,640
并行的操作的能力

247
00:08:36,640 --> 00:08:37,440
那这个时候

248
00:08:37,440 --> 00:08:38,640
不同的 AI 框架

249
00:08:38,640 --> 00:08:40,240
或者你面向不同的场景

250
00:08:40,240 --> 00:08:42,600
你会对数据的默认的内存排布

251
00:08:42,600 --> 00:08:44,080
会有不同的需求

252
00:08:44,080 --> 00:08:45,320
那下面我们可以看一下

253
00:08:45,320 --> 00:08:46,960
它里面最大的区别

254
00:08:46,960 --> 00:08:50,360
就是 NCHW 主要是对每个信道

255
00:08:50,360 --> 00:08:52,160
单独做运算的时候

256
00:08:52,160 --> 00:08:53,600
会特别的快

257
00:08:53,600 --> 00:08:55,080
因为你数据都已经排好了

258
00:08:55,080 --> 00:08:56,440
我单独做运算就特别快

259
00:08:56,840 --> 00:08:58,880
像 NHWC 这种方式

260
00:08:59,000 --> 00:09:01,280
就特别适合于那些不同信道

261
00:09:01,280 --> 00:09:03,320
对于同一像素做运算

262
00:09:03,320 --> 00:09:04,680
你有卷集这种方式

263
00:09:04,680 --> 00:09:05,920
确实也是特别好

264
00:09:05,960 --> 00:09:09,000
所以说不同的数据的排布方式

265
00:09:09,000 --> 00:09:10,520
对于我们的 Kernels 的优化

266
00:09:10,520 --> 00:09:12,880
是非常的讲究

267
00:09:15,680 --> 00:09:17,120
下面我们看一下一个新的

268
00:09:17,120 --> 00:09:19,920
或者特别有意思的数据的存储格式

269
00:09:19,920 --> 00:09:23,400
NCHW X 多了个 S

270
00:09:23,400 --> 00:09:26,360
那这个 S 就其实我们往后看

271
00:09:26,360 --> 00:09:28,720
N 就是我们的 batch size 的大小了

272
00:09:28,720 --> 00:09:31,840
这里面的 C 就不是完完全全真正的 C

273
00:09:31,840 --> 00:09:35,640
而是 channel 数除以 X 除以多少个

274
00:09:35,680 --> 00:09:37,560
然后 HW 都不用说了

275
00:09:37,560 --> 00:09:39,560
而 X 你可以默认是 4

276
00:09:39,560 --> 00:09:42,240
32 或 64 位都可以

277
00:09:42,240 --> 00:09:43,400
就很有意思

278
00:09:43,400 --> 00:09:45,920
那这种方式其实在达芬奇架构里面

279
00:09:45,920 --> 00:09:48,880
我们叫做 NCHWC0

280
00:09:48,880 --> 00:09:50,960
那可能在 M&N 里面

281
00:09:50,960 --> 00:09:54,320
它叫做 NCHWC1

282
00:09:54,320 --> 00:09:55,880
那这种方式不管怎幺样

283
00:09:55,880 --> 00:09:58,800
我们的 X 基本上就从 C 里面切换出来

284
00:09:58,800 --> 00:10:01,080
那具体在数据内存排布是怎幺样的

285
00:10:01,080 --> 00:10:02,960
我们直接看下面的这个图

286
00:10:03,000 --> 00:10:06,320
假设这个图还是刚才模拟的图

287
00:10:06,320 --> 00:10:10,840
我们数据还是按 X 进行排布

288
00:10:10,840 --> 00:10:12,320
那这个时候就很有意思了

289
00:10:12,320 --> 00:10:14,880
我们会 1 2 3 4

290
00:10:14,880 --> 00:10:16,600
假设 X 是 4 的时候

291
00:10:16,600 --> 00:10:19,600
我们就会把四个数先取出来

292
00:10:19,600 --> 00:10:22,600
1 7 13 还有这个 X

293
00:10:22,600 --> 00:10:26,920
然后再取 2 8 14 X 这个数

294
00:10:26,920 --> 00:10:28,000
通过这种方式

295
00:10:28,000 --> 00:10:33,240
每次只读取我们整个章量里面的一部分进行处理

296
00:10:33,240 --> 00:10:35,920
非常适合我们的内存的排布

297
00:10:35,920 --> 00:10:37,040
因为我们之前讲到了

298
00:10:37,040 --> 00:10:40,400
内存的 Cache 分 L1 L2 L0 是吧

299
00:10:40,400 --> 00:10:42,520
所以 L0 最接近我们的 CPU

300
00:10:42,520 --> 00:10:45,240
而 L0 确实是数量最小的

301
00:10:45,240 --> 00:10:46,640
每次我们在分片的时候

302
00:10:46,640 --> 00:10:48,240
就取够的数据

303
00:10:48,240 --> 00:10:49,280
取得刚刚好

304
00:10:49,280 --> 00:10:50,440
然后计算完之后

305
00:10:50,440 --> 00:10:52,200
下一次再取下一批

306
00:10:52,200 --> 00:10:53,800
这个时候就很好的

307
00:10:53,800 --> 00:10:57,160
充分的利用了我们内存空间的方案了

308
00:10:59,000 --> 00:11:00,160
下面就具体看一下

309
00:11:00,160 --> 00:11:04,360
NCW-X 其实更好地适配于 SIMT 这种架构

310
00:11:04,360 --> 00:11:08,960
例如 NCHW-4 可以针对 ARM 的 INT8 的数据类型

311
00:11:08,960 --> 00:11:12,960
充分的利用了 CUDA 里面的 DP4A 这个模块

312
00:11:12,960 --> 00:11:15,320
或者这个指令进行一个计算的

313
00:11:15,320 --> 00:11:16,600
进行一个加速的

314
00:11:16,600 --> 00:11:17,520
另外的话

315
00:11:17,520 --> 00:11:21,000
NCHW-32 NCHW-4

316
00:11:21,000 --> 00:11:23,800
分别可以对 INT8 和 INT4 的数据

317
00:11:23,800 --> 00:11:28,200
更好的利用里面的 Tensor Core 去进行一个计算的

318
00:11:28,200 --> 00:11:30,080
所以说像这种方式

319
00:11:30,080 --> 00:11:32,400
最重要的就是对 Cache 更加友好

320
00:11:32,400 --> 00:11:34,000
减少 Cache missing

321
00:11:34,000 --> 00:11:35,200
提高命中率

322
00:11:35,200 --> 00:11:37,400
这个时候是非常的重要

323
00:11:37,400 --> 00:11:41,000
仿存实在确实比我们的计算要慢

324
00:11:41,000 --> 00:11:42,200
因为计算的时候

325
00:11:42,200 --> 00:11:43,880
AOU 算的非常的快

326
00:11:43,880 --> 00:11:47,000
但是我们很多时候是数据搬运来不及

327
00:11:47,000 --> 00:11:48,600
数据的通讯特别慢

328
00:11:48,600 --> 00:11:50,400
数据的 Cache missing

329
00:11:50,400 --> 00:11:53,000
阻碍了我们整个运算的效率

330
00:11:53,000 --> 00:11:54,600
这对我们的整个工程实现

331
00:11:54,600 --> 00:11:57,000
确实是非常的恶心

332
00:11:59,000 --> 00:12:00,600
今天的内容就到这里为止

333
00:12:00,600 --> 00:12:02,600
在我们的内存布局这里面

334
00:12:02,600 --> 00:12:05,000
我们今天了解了三个重要的概念

335
00:12:05,000 --> 00:12:08,600
第一个就是 CPU 和 GPU 的内存的布局

336
00:12:08,600 --> 00:12:10,800
它的 Cache 到底是怎幺样的一个排布方式

337
00:12:10,800 --> 00:12:12,400
接着我们了解了一下

338
00:12:12,400 --> 00:12:15,400
张亮 Tensor 的 NCHW 和 NCHW-C

339
00:12:15,400 --> 00:12:17,800
具体的排布方式和它适应的场景

340
00:12:17,800 --> 00:12:22,600
最后我们引用了一个 NCHW-X 这种方式

341
00:12:22,600 --> 00:12:25,600
去看一下不同的为什幺会有这幺一个 X 出来

342
00:12:25,600 --> 00:12:27,600
而这个 NCHW-C

343
00:12:27,600 --> 00:12:29,200
就是对应于华为

344
00:12:29,200 --> 00:12:31,800
升腾达芬奇加工里面的一种数据的格式

345
00:12:31,800 --> 00:12:33,400
而 NCHW-4

346
00:12:33,400 --> 00:12:35,000
就对应于像阿里MMM

347
00:12:35,000 --> 00:12:36,400
或者华为Marsmallite

348
00:12:36,400 --> 00:12:39,000
这种数据的存储的方式

349
00:12:39,000 --> 00:12:40,600
好了 今天的内容到这里为止

350
00:12:40,600 --> 00:12:42,400
谢谢各位 拜拜

351
00:12:42,400 --> 00:12:44,000
卷的不行了 卷的不行了

352
00:12:44,000 --> 00:12:45,800
记得一键三连加关注哦

353
00:12:45,800 --> 00:12:47,400
所有的内容都会开源在

354
00:12:47,400 --> 00:12:49,400
下面这条链接里面

355
00:12:49,400 --> 00:12:50,600
拜拜

