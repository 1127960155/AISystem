0
0:00:04.000 --> 0:00:08.040
Hello 大家好 我是那个不能再吃了不能再吃了

1
0:00:08.040 --> 0:00:10.840
再吃就吃不下甜品的ZOMI了

2
0:00:10.840 --> 0:00:14.960
在这一节里面我们终于脱离了科诺油画里面的卷积油画

3
0:00:14.960 --> 0:00:16.440
那节确实剪的挺痛苦的

4
0:00:16.440 --> 0:00:19.040
里面涂的画都我好心酸哦

5
0:00:19.040 --> 0:00:20.560
今天我们来到一个新的内容

6
0:00:20.560 --> 0:00:22.200
就是内存的布局

7
0:00:22.200 --> 0:00:24.920
内存布局这个内容确实非常重要

8
0:00:24.920 --> 0:00:26.280
不管科诺油画也好

9
0:00:26.280 --> 0:00:28.440
我们只是对数据进行重排

10
0:00:29.440 --> 0:00:31.040
我们来到一个新的内容

11
0:00:31.040 --> 0:00:32.600
内存的布局

12
0:00:32.600 --> 0:00:34.120
其实在上一节内容里面

13
0:00:34.120 --> 0:00:35.840
我们讲到的算法的优化

14
0:00:35.840 --> 0:00:37.240
不管是哪种算法的优化

15
0:00:37.240 --> 0:00:40.520
其实都是离不开内存布局的加持

16
0:00:40.520 --> 0:00:42.160
那在这一节里面

17
0:00:42.160 --> 0:00:45.280
我们主要是看一下内存的布局有哪些不一样

18
0:00:45.280 --> 0:00:49.720
我们会去讲具体内存布局怎幺去配合我们的科诺油画的

19
0:00:49.720 --> 0:00:52.320
下面我们看一下整个的科诺油画

20
0:00:52.320 --> 0:00:54.480
还是在这一个内容里面

21
0:00:54.480 --> 0:00:58.120
而内存的布局也是在这一节内容里面

22
0:00:58.120 --> 0:01:00.120
下面我们看一下第一个概念

23
0:01:00.120 --> 0:01:02.480
就是内存 memory

24
0:01:02.480 --> 0:01:04.200
memory 有什幺不一样

25
0:01:04.200 --> 0:01:08.600
首先左边的这个图就是 CPU 的一个具体的

26
0:01:08.600 --> 0:01:10.400
或者一个通用的架构

27
0:01:10.400 --> 0:01:14.680
右边的这个图就是 GPU 的一个常用的架构

28
0:01:14.680 --> 0:01:17.920
可以我们简单的看一下上面两句话

29
0:01:17.920 --> 0:01:21.520
像 RAM 这种的组存是比较大的存储空间

30
0:01:21.520 --> 0:01:25.440
但是读取或者存储的速度是比较慢的

31
0:01:25.480 --> 0:01:27.680
那像 CPU 或者 MPU 里面的 cache

32
0:01:27.680 --> 0:01:30.240
就上面的在我们 CPU 里面的 cache

33
0:01:30.240 --> 0:01:32.360
存储的速度就要快很多

34
0:01:32.360 --> 0:01:34.360
但是整体的规模就比较少了

35
0:01:34.360 --> 0:01:35.760
而且造价比较贵

36
0:01:35.760 --> 0:01:38.280
因此我们可以适当的去使用

37
0:01:38.280 --> 0:01:41.840
CPU 或者 MPU 里面的 cache 是非常的重要的

38
0:01:41.840 --> 0:01:45.720
每一次我们从 RAM 里面或者组存里面去获取数据

39
0:01:45.720 --> 0:01:48.400
CPU 就会将这些数据就把 memory

40
0:01:48.400 --> 0:01:49.920
global memory 这些数据

41
0:01:49.920 --> 0:01:52.360
真正的搬运到 CPU 里面的 cache

42
0:01:52.400 --> 0:01:56.120
就离我们的计算单元更近的 cache 里面

43
0:01:56.120 --> 0:01:58.920
以便更好地利用我们的仿存的局部性

44
0:01:58.920 --> 0:02:00.600
提升整体的命中率

45
0:02:00.600 --> 0:02:02.600
包括我们的 GPU 也是一样的

46
0:02:02.600 --> 0:02:05.280
我们从 host 的一些 global memory 里面

47
0:02:05.280 --> 0:02:08.000
把一些数据读到 L2 的 cache 里面

48
0:02:08.000 --> 0:02:10.960
通过这种方式一级一级地递增

49
0:02:10.960 --> 0:02:13.200
提供整体的命中率

50
0:02:13.200 --> 0:02:14.960
那可以看到了这里面

51
0:02:14.960 --> 0:02:18.040
GPU 会通过 PCIe 桥或者 NVLink

52
0:02:18.040 --> 0:02:21.200
跟 GPU 之间互粘互通

53
0:02:21.240 --> 0:02:23.960
而 global memory 其实就是一个事情

54
0:02:23.960 --> 0:02:26.360
我们这里面就有了一个异构的架构

55
0:02:26.360 --> 0:02:28.720
一个是 CPU 一个是 GPU

56
0:02:28.720 --> 0:02:31.400
CPU 里面有少量的核

57
0:02:31.400 --> 0:02:32.960
所以它叫做 SIMD

58
0:02:32.960 --> 0:02:36.400
而 GPU 里面有大量的线程处 SM

59
0:02:36.400 --> 0:02:40.720
所以它大部分使用的是一个 SIMT 的架构

60
0:02:40.720 --> 0:02:42.120
那下面我们可以看到

61
0:02:42.120 --> 0:02:45.320
不管是哪种方式都有多级的缓存

62
0:02:45.320 --> 0:02:47.600
如何更好的利用多级的缓存

63
0:02:47.600 --> 0:02:49.480
是一个很重要的概念

64
0:02:49.520 --> 0:02:51.320
那下面我们回顾一下

65
0:02:51.320 --> 0:02:53.840
在之前我们其实已经简单的讲过了

66
0:02:53.840 --> 0:02:57.720
我们对内存确实需要进行一个对齐的

67
0:02:57.720 --> 0:03:00.840
实际上内存的对齐是以字节为单位的

68
0:03:00.840 --> 0:03:02.960
就是我们的最小的存储单位

69
0:03:02.960 --> 0:03:04.520
一个变量的内存地址

70
0:03:04.520 --> 0:03:06.760
刚好等于它的长度的整倍数

71
0:03:06.760 --> 0:03:09.320
那这种我们叫做自然对齐

72
0:03:09.320 --> 0:03:11.880
像在 32 位的 CPU 里面

73
0:03:11.880 --> 0:03:15.440
一个 UINT32 的数据

74
0:03:15.440 --> 0:03:18.400
它的内存地址假设是 0x2004

75
0:03:18.440 --> 0:03:20.480
那这种就属于自然对齐

76
0:03:20.480 --> 0:03:22.680
可以看到 4 跟 32 的这个数据

77
0:03:22.680 --> 0:03:26.040
和这个字节单位是相同的

78
0:03:26.040 --> 0:03:29.160
而内存空间按字节进行划分

79
0:03:29.160 --> 0:03:32.080
对齐的好处就是我们可以从内存里面的

80
0:03:32.080 --> 0:03:34.640
任意一个地址上面进行读取

81
0:03:34.640 --> 0:03:36.240
都是没有问题的

82
0:03:36.240 --> 0:03:38.800
那下面我们了解完整体的内存

83
0:03:38.800 --> 0:03:40.600
在硬件上面是怎幺实现的

84
0:03:40.600 --> 0:03:41.680
或者怎幺去布局的

85
0:03:41.680 --> 0:03:43.760
还有它的一个对齐的方式

86
0:03:43.760 --> 0:03:45.600
下面我们看一下具体的内容

87
0:03:45.600 --> 0:03:48.040
就我们 Tensor 内存的布局

88
0:03:48.040 --> 0:03:49.360
因为在神经网络里面

89
0:03:49.360 --> 0:03:51.080
大量的流动的数据

90
0:03:51.080 --> 0:03:54.320
都是以 Tensor 作为基本单位的

91
0:03:54.320 --> 0:03:55.320
而 Tensor 这个概念

92
0:03:55.320 --> 0:03:56.920
其实我们在前面几页里面

93
0:03:56.920 --> 0:03:58.880
已经给大家都已经讲烂了

94
0:03:58.880 --> 0:04:02.040
这里面我就简单的翻一翻了

95
0:04:03.040 --> 0:04:05.640
Tensor 是神经网络里面的

96
0:04:05.640 --> 0:04:07.000
一个基本的数据结构

97
0:04:07.000 --> 0:04:09.120
里面就包括里面的 Shape 了

98
0:04:09.120 --> 0:04:10.600
当然还有 Data Type

99
0:04:10.600 --> 0:04:12.800
Shape 和 Data Type 都是很重要的

100
0:04:12.800 --> 0:04:13.920
Shape 就决定了

101
0:04:13.920 --> 0:04:16.120
我到底是一个什幺方式组成

102
0:04:16.120 --> 0:04:17.520
而 Data Type 就决定了

103
0:04:17.520 --> 0:04:18.480
我们每一个元素

104
0:04:18.480 --> 0:04:20.240
到底属于什幺一个形状

105
0:04:20.240 --> 0:04:23.240
最后我们就按内存的地址进行排布了

106
0:04:23.240 --> 0:04:24.760
而真正的索引的时候

107
0:04:24.760 --> 0:04:26.400
会根据 Tensor 的 Shape

108
0:04:26.400 --> 0:04:27.720
进行一个索引

109
0:04:27.720 --> 0:04:29.800
而索引每次索引多少内容

110
0:04:29.800 --> 0:04:31.200
或者多少一个字节

111
0:04:31.200 --> 0:04:33.600
就是按 Data Type 来进行决定的

112
0:04:33.600 --> 0:04:35.440
所以这两个数值非常重要

113
0:04:35.440 --> 0:04:36.480
那在张量里面

114
0:04:36.480 --> 0:04:37.320
我们可以看到

115
0:04:37.320 --> 0:04:38.200
一维的张量

116
0:04:38.200 --> 0:04:39.200
确实就一条数据

117
0:04:39.200 --> 0:04:39.920
二维的张量

118
0:04:39.920 --> 0:04:41.800
就是类似于一张黑白的图片

119
0:04:41.800 --> 0:04:42.640
三维的张量

120
0:04:42.640 --> 0:04:44.120
就是彩色的图片

121
0:04:44.120 --> 0:04:45.120
四维的张量

122
0:04:45.120 --> 0:04:46.560
就是很多个三维的张量

123
0:04:46.600 --> 0:04:49.120
堆叠起来前面就多了一个 n

124
0:04:49.120 --> 0:04:50.800
就是我们的 Batch Size 大小

125
0:04:50.800 --> 0:04:51.200
当然了

126
0:04:51.200 --> 0:04:51.800
点语里面

127
0:04:51.800 --> 0:04:53.440
我们可能还会有五维的张量

128
0:04:53.440 --> 0:04:54.480
那当然六维的张量

129
0:04:54.480 --> 0:04:56.200
其实我现在还没想到太多

130
0:04:56.200 --> 0:04:59.160
之后除非我们自己硬件进行一个切分

131
0:04:59.160 --> 0:05:01.520
在达芬奇或者在生腾硬件里面

132
0:05:01.520 --> 0:05:02.360
做了一个切分

133
0:05:02.360 --> 0:05:04.040
就变成把五维变成六维

134
0:05:04.040 --> 0:05:06.000
方便硬件进行计算

135
0:05:06.000 --> 0:05:07.120
但实际上你怎幺切

136
0:05:07.120 --> 0:05:09.120
还是内存数据的排布不一样

137
0:05:09.120 --> 0:05:10.480
而图像一般

138
0:05:10.480 --> 0:05:12.240
我们图像张量化表示

139
0:05:12.240 --> 0:05:14.600
就是 nchw

140
0:05:14.640 --> 0:05:17.160
那 chw 就是我们的一张彩色图片

141
0:05:17.160 --> 0:05:18.160
再加一个 n

142
0:05:18.160 --> 0:05:19.760
就是我们的一个 Batch Size

143
0:05:19.760 --> 0:05:21.160
就很多张图片

144
0:05:21.160 --> 0:05:22.240
那自然语言处理

145
0:05:22.240 --> 0:05:24.680
可能 NLP 领域会比较特别

146
0:05:24.680 --> 0:05:26.080
我们会有 Sequenced

147
0:05:26.080 --> 0:05:27.120
就一句话

148
0:05:27.120 --> 0:05:28.000
然后一句话里面

149
0:05:28.000 --> 0:05:29.440
又有多少个 Rost

150
0:05:29.440 --> 0:05:30.320
多少个 Token

151
0:05:30.320 --> 0:05:32.320
或者多少个单词

152
0:05:32.320 --> 0:05:34.400
那一共有多少段

153
0:05:34.400 --> 0:05:35.040
这句话

154
0:05:35.040 --> 0:05:36.640
就是 nnsw

155
0:05:36.640 --> 0:05:38.080
是自然语言处理的方式

156
0:05:38.080 --> 0:05:39.680
当然还有 C 书的张量

157
0:05:39.680 --> 0:05:40.840
有点语和图

158
0:05:40.840 --> 0:05:42.400
GNN 这种方式的表达

159
0:05:42.400 --> 0:05:43.400
所以张量的表示

160
0:05:43.400 --> 0:05:44.960
还是非常的复杂的

161
0:05:47.240 --> 0:05:48.240
在 Tensor 的值

162
0:05:48.400 --> 0:05:49.320
存在内存里面

163
0:05:49.480 --> 0:05:50.640
是很有意思的

164
0:05:50.640 --> 0:05:51.880
它有两种方式

165
0:05:51.880 --> 0:05:54.040
第一种是以 row major 的方式

166
0:05:54.040 --> 0:05:56.320
第二种是以 column major 的方式

167
0:05:56.320 --> 0:05:58.280
就是你到底是以行

168
0:05:58.280 --> 0:05:59.960
作为主要的缩影顺序

169
0:06:00.240 --> 0:06:02.480
还是以列作为主要的缩影顺序

170
0:06:03.000 --> 0:06:05.040
这里面的绿色的图就画错了

171
0:06:05.040 --> 0:06:07.760
按行就是按 J 的方式进行取的

172
0:06:07.760 --> 0:06:09.080
可以看到都是 J

173
0:06:09.080 --> 0:06:11.680
而按列就是以 N 的方式进行取的

174
0:06:11.720 --> 0:06:13.680
N 就是这种方式进行取

175
0:06:13.680 --> 0:06:15.080
所以说 Tensor

176
0:06:15.360 --> 0:06:17.160
根据排列的顺序的区别

177
0:06:17.360 --> 0:06:20.520
分为行 组存和列组存

178
0:06:20.520 --> 0:06:21.800
两种风格

179
0:06:21.800 --> 0:06:22.720
取数据的时候

180
0:06:22.960 --> 0:06:25.160
是按 J 形式来取

181
0:06:25.160 --> 0:06:27.040
还是按 N 形式来取

182
0:06:27.040 --> 0:06:28.640
这个就是我们最通常的

183
0:06:28.640 --> 0:06:30.520
NJ 和 ND 的方式

184
0:06:32.960 --> 0:06:34.280
下面我们看一下

185
0:06:34.280 --> 0:06:37.040
NCHW 这种数据的存储方式

186
0:06:37.040 --> 0:06:40.200
首先我们会往 W 先取数据

187
0:06:40.200 --> 0:06:41.240
就 1 2 3 先取

188
0:06:41.360 --> 0:06:42.640
然后再取 H 的数据

189
0:06:42.640 --> 0:06:43.520
就 4 5 6 了

190
0:06:43.520 --> 0:06:45.040
然后再取第三个维度

191
0:06:45.040 --> 0:06:46.480
C 的维度

192
0:06:46.480 --> 0:06:47.360
就是 6 7 8 9

193
0:06:47.360 --> 0:06:48.760
后面绿色的模块

194
0:06:48.760 --> 0:06:51.360
最后再取 N 就不断的累加

195
0:06:51.360 --> 0:06:52.160
这种方式就是

196
0:06:52.160 --> 0:06:55.080
NHW 的数据存储的方式

197
0:06:55.080 --> 0:06:58.040
这种方式其实很好的一个计算

198
0:06:58.160 --> 0:06:59.640
就是 Max Pooling

199
0:06:59.640 --> 0:07:02.000
我们首先要需要对这幺简单的一个信道

200
0:07:02.320 --> 0:07:04.600
进行一个取最大值

201
0:07:04.600 --> 0:07:06.320
像 NCHW 这种计算

202
0:07:06.560 --> 0:07:08.160
是和 GPU 进行运算了

203
0:07:08.160 --> 0:07:09.640
因为 GPU 的内存带宽大

204
0:07:09.640 --> 0:07:10.920
而且并行能力强

205
0:07:12.240 --> 0:07:14.280
下面我们看一下

206
0:07:14.280 --> 0:07:15.600
另外一种数据的存储方式

207
0:07:15.600 --> 0:07:16.760
就是 NHWC

208
0:07:16.880 --> 0:07:18.600
这种方式确实讲烂了

209
0:07:18.600 --> 0:07:21.160
NHWC 就是先取 C 信道了

210
0:07:21.160 --> 0:07:22.680
所以我们会取 173

211
0:07:22.680 --> 0:07:23.880
先取 173

212
0:07:23.880 --> 0:07:27.160
然后再取 2814

213
0:07:27.160 --> 0:07:28.880
这种方式就先取 C

214
0:07:28.880 --> 0:07:29.760
再取 W

215
0:07:29.760 --> 0:07:30.560
再取 H

216
0:07:30.560 --> 0:07:31.680
然后再取 N

217
0:07:31.680 --> 0:07:34.240
就内存的读取的方式不太一样了

218
0:07:34.800 --> 0:07:37.320
像这种方式更适合卷积一乘一

219
0:07:37.320 --> 0:07:38.760
这种计算操作

220
0:07:38.760 --> 0:07:40.480
更适合一些多核的 CPU

221
0:07:40.480 --> 0:07:42.160
进行一个运算的

222
0:07:42.160 --> 0:07:43.760
CPU 的内存带宽比较小

223
0:07:46.040 --> 0:07:46.840
下面我们看一下

224
0:07:46.840 --> 0:07:48.840
不同的框架的默认的选择方式

225
0:07:48.840 --> 0:07:50.000
因为不同的 AI 框架

226
0:07:50.000 --> 0:07:51.640
可能会有不同的方式

227
0:07:51.640 --> 0:07:53.840
那现在以 NPU 与 GPU 为基础的

228
0:07:53.840 --> 0:07:55.240
像 PyTorch 或者 Minespot

229
0:07:55.600 --> 0:07:59.880
默认就是 NCHW 的这种数据的存储格式

230
0:07:59.880 --> 0:08:02.440
那 Tensorflow 其实一开始是默认使用

231
0:08:02.440 --> 0:08:05.480
NHWC 的这种数据的存储方式的

232
0:08:05.480 --> 0:08:07.040
而面向移动端

233
0:08:07.280 --> 0:08:09.160
特别是我们的推定型

234
0:08:09.400 --> 0:08:12.800
推定型更多的也是采用 NHWC 的

235
0:08:12.800 --> 0:08:15.000
为什幺推定型大部分采用这种格式呢

236
0:08:15.000 --> 0:08:18.240
因为很多推定型都会跑在手机上面

237
0:08:18.240 --> 0:08:21.080
手机上面大部分都是以 ARM 作为

238
0:08:21.080 --> 0:08:24.720
或者是 ARM CPU 作为一个主要的计算单元

239
0:08:24.720 --> 0:08:25.600
或计算单位

240
0:08:25.600 --> 0:08:28.080
所以一般默认都会使用 NHWC

241
0:08:28.080 --> 0:08:29.160
而以 GPU 为主

242
0:08:29.160 --> 0:08:31.440
就会使用大量的 NCHW

243
0:08:31.440 --> 0:08:32.760
它的并行能力特别好

244
0:08:32.760 --> 0:08:34.720
我们希望尽可能的去利用它的

245
0:08:34.720 --> 0:08:36.640
并行的操作的能力

246
0:08:36.640 --> 0:08:37.440
那这个时候

247
0:08:37.440 --> 0:08:38.640
不同的 AI 框架

248
0:08:38.640 --> 0:08:40.240
或者你面向不同的场景

249
0:08:40.240 --> 0:08:42.600
你会对数据的默认的内存排布

250
0:08:42.600 --> 0:08:44.080
会有不同的需求

251
0:08:44.080 --> 0:08:45.320
那下面我们可以看一下

252
0:08:45.320 --> 0:08:46.960
它里面最大的区别

253
0:08:46.960 --> 0:08:50.360
就是 NCHW 主要是对每个信道

254
0:08:50.360 --> 0:08:52.160
单独做运算的时候

255
0:08:52.160 --> 0:08:53.600
会特别的快

256
0:08:53.600 --> 0:08:55.080
因为你数据都已经排好了

257
0:08:55.080 --> 0:08:56.440
我单独做运算就特别快

258
0:08:56.840 --> 0:08:58.880
像 NHWC 这种方式

259
0:08:59.000 --> 0:09:01.280
就特别适合于那些不同信道

260
0:09:01.280 --> 0:09:03.320
对于同一像素做运算

261
0:09:03.320 --> 0:09:04.680
你有卷集这种方式

262
0:09:04.680 --> 0:09:05.920
确实也是特别好

263
0:09:05.960 --> 0:09:09.000
所以说不同的数据的排布方式

264
0:09:09.000 --> 0:09:10.520
对于我们的 Kernels 的优化

265
0:09:10.520 --> 0:09:12.880
是非常的讲究

266
0:09:15.680 --> 0:09:17.120
下面我们看一下一个新的

267
0:09:17.120 --> 0:09:19.920
或者特别有意思的数据的存储格式

268
0:09:19.920 --> 0:09:23.400
NCHW X 多了个 S

269
0:09:23.400 --> 0:09:26.360
那这个 S 就其实我们往后看

270
0:09:26.360 --> 0:09:28.720
N 就是我们的 batch size 的大小了

271
0:09:28.720 --> 0:09:31.840
这里面的 C 就不是完完全全真正的 C

272
0:09:31.840 --> 0:09:35.640
而是 channel 数除以 X 除以多少个

273
0:09:35.680 --> 0:09:37.560
然后 HW 都不用说了

274
0:09:37.560 --> 0:09:39.560
而 X 你可以默认是 4

275
0:09:39.560 --> 0:09:42.240
32 或 64 位都可以

276
0:09:42.240 --> 0:09:43.400
就很有意思

277
0:09:43.400 --> 0:09:45.920
那这种方式其实在达芬奇架构里面

278
0:09:45.920 --> 0:09:48.880
我们叫做 NCHWC0

279
0:09:48.880 --> 0:09:50.960
那可能在 M&N 里面

280
0:09:50.960 --> 0:09:54.320
它叫做 NCHWC1

281
0:09:54.320 --> 0:09:55.880
那这种方式不管怎幺样

282
0:09:55.880 --> 0:09:58.800
我们的 X 基本上就从 C 里面切换出来

283
0:09:58.800 --> 0:10:01.080
那具体在数据内存排布是怎幺样的

284
0:10:01.080 --> 0:10:02.960
我们直接看下面的这个图

285
0:10:03.000 --> 0:10:06.320
假设这个图还是刚才模拟的图

286
0:10:06.320 --> 0:10:10.840
我们数据还是按 X 进行排布

287
0:10:10.840 --> 0:10:12.320
那这个时候就很有意思了

288
0:10:12.320 --> 0:10:14.880
我们会 1 2 3 4

289
0:10:14.880 --> 0:10:16.600
假设 X 是 4 的时候

290
0:10:16.600 --> 0:10:19.600
我们就会把四个数先取出来

291
0:10:19.600 --> 0:10:22.600
1 7 13 还有这个 X

292
0:10:22.600 --> 0:10:26.920
然后再取 2 8 14 X 这个数

293
0:10:26.920 --> 0:10:28.000
通过这种方式

294
0:10:28.000 --> 0:10:33.240
每次只读取我们整个章量里面的一部分进行处理

295
0:10:33.240 --> 0:10:35.920
非常适合我们的内存的排布

296
0:10:35.920 --> 0:10:37.040
因为我们之前讲到了

297
0:10:37.040 --> 0:10:40.400
内存的 Cache 分 L1 L2 L0 是吧

298
0:10:40.400 --> 0:10:42.520
所以 L0 最接近我们的 CPU

299
0:10:42.520 --> 0:10:45.240
而 L0 确实是数量最小的

300
0:10:45.240 --> 0:10:46.640
每次我们在分片的时候

301
0:10:46.640 --> 0:10:48.240
就取够的数据

302
0:10:48.240 --> 0:10:49.280
取得刚刚好

303
0:10:49.280 --> 0:10:50.440
然后计算完之后

304
0:10:50.440 --> 0:10:52.200
下一次再取下一批

305
0:10:52.200 --> 0:10:53.800
这个时候就很好的

306
0:10:53.800 --> 0:10:57.160
充分的利用了我们内存空间的方案了

307
0:10:59.000 --> 0:11:00.160
下面就具体看一下

308
0:11:00.160 --> 0:11:04.360
NCW-X 其实更好地适配于 SIMT 这种架构

309
0:11:04.360 --> 0:11:08.960
例如 NCHW-4 可以针对 ARM 的 INT8 的数据类型

310
0:11:08.960 --> 0:11:12.960
充分的利用了 CUDA 里面的 DP4A 这个模块

311
0:11:12.960 --> 0:11:15.320
或者这个指令进行一个计算的

312
0:11:15.320 --> 0:11:16.600
进行一个加速的

313
0:11:16.600 --> 0:11:17.520
另外的话

314
0:11:17.520 --> 0:11:21.000
NCHW-32 NCHW-4

315
0:11:21.000 --> 0:11:23.800
分别可以对 INT8 和 INT4 的数据

316
0:11:23.800 --> 0:11:28.200
更好的利用里面的 Tensor Core 去进行一个计算的

317
0:11:28.200 --> 0:11:30.080
所以说像这种方式

318
0:11:30.080 --> 0:11:32.400
最重要的就是对 Cache 更加友好

319
0:11:32.400 --> 0:11:34.000
减少 Cache missing

320
0:11:34.000 --> 0:11:35.200
提高命中率

321
0:11:35.200 --> 0:11:37.400
这个时候是非常的重要

322
0:11:37.400 --> 0:11:41.000
仿存实在确实比我们的计算要慢

323
0:11:41.000 --> 0:11:42.200
因为计算的时候

324
0:11:42.200 --> 0:11:43.880
AOU 算的非常的快

325
0:11:43.880 --> 0:11:47.000
但是我们很多时候是数据搬运来不及

326
0:11:47.000 --> 0:11:48.600
数据的通讯特别慢

327
0:11:48.600 --> 0:11:50.400
数据的 Cache missing

328
0:11:50.400 --> 0:11:53.000
阻碍了我们整个运算的效率

329
0:11:53.000 --> 0:11:54.600
这对我们的整个工程实现

330
0:11:54.600 --> 0:11:57.000
确实是非常的恶心

331
0:11:59.000 --> 0:12:00.600
今天的内容就到这里为止

332
0:12:00.600 --> 0:12:02.600
在我们的内存布局这里面

333
0:12:02.600 --> 0:12:05.000
我们今天了解了三个重要的概念

334
0:12:05.000 --> 0:12:08.600
第一个就是 CPU 和 GPU 的内存的布局

335
0:12:08.600 --> 0:12:10.800
它的 Cache 到底是怎幺样的一个排布方式

336
0:12:10.800 --> 0:12:12.400
接着我们了解了一下

337
0:12:12.400 --> 0:12:15.400
张亮 Tensor 的 NCHW 和 NCHW-C

338
0:12:15.400 --> 0:12:17.800
具体的排布方式和它适应的场景

339
0:12:17.800 --> 0:12:22.600
最后我们引用了一个 NCHW-X 这种方式

340
0:12:22.600 --> 0:12:25.600
去看一下不同的为什幺会有这幺一个 X 出来

341
0:12:25.600 --> 0:12:27.600
而这个 NCHW-C

342
0:12:27.600 --> 0:12:29.200
就是对应于华为

343
0:12:29.200 --> 0:12:31.800
升腾达芬奇加工里面的一种数据的格式

344
0:12:31.800 --> 0:12:33.400
而 NCHW-4

345
0:12:33.400 --> 0:12:35.000
就对应于像阿里 MNN

346
0:12:35.000 --> 0:12:36.400
或者华为 MindSpore Lite

347
0:12:36.400 --> 0:12:39.000
这种数据的存储的方式

