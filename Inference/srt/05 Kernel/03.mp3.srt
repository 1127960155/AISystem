0
0:00:00.000 --> 0:00:06.800
Hello 大家好,宗明又回来了

1
0:00:06.800 --> 0:00:09.640
今天我们还是在推理引擎的最后一个内容

2
0:00:09.640 --> 0:00:10.800
Kernels 优化

3
0:00:10.800 --> 0:00:13.200
今天主要是给大家去分享一下

4
0:00:13.200 --> 0:00:16.800
Image2Clone 这种卷机的优化方式

5
0:00:16.800 --> 0:00:20.000
回到我们整个 Kernel 优化的课程系列

6
0:00:20.000 --> 0:00:23.200
我们会讲算法的优化内存的布局

7
0:00:23.200 --> 0:00:25.600
还有编译和调度优化

8
0:00:25.600 --> 0:00:28.000
在整体里面算法的优化

9
0:00:28.000 --> 0:00:30.600
我们将会深入的去讲解一下

10
0:00:30.600 --> 0:00:33.800
而算法的优化更多的是集中在 Kernel 层

11
0:00:33.800 --> 0:00:36.000
这里面很多不同的算法

12
0:00:36.000 --> 0:00:38.400
或者我们理解到的具体执行的算法

13
0:00:38.400 --> 0:00:40.800
都承载在我们的 Kernel 层

14
0:00:40.800 --> 0:00:42.400
在这一节课程里面

15
0:00:42.400 --> 0:00:44.800
我们将会聚焦两个内容

16
0:00:44.800 --> 0:00:47.000
第一个内容就是 Image2Clone

17
0:00:47.000 --> 0:00:50.800
第二个就是 Spatial Pack Optimizer

18
0:00:50.800 --> 0:00:53.000
空间组合优化两个方式

19
0:00:53.000 --> 0:00:55.400
现在我们来到第一个内容

20
0:00:55.600 --> 0:00:58.800
就是 Image2Clone 整体的算法原理

21
0:01:00.200 --> 0:01:03.200
在 Image2Clone 的算法原理

22
0:01:03.200 --> 0:01:05.200
其实 IMG2Clone

23
0:01:05.200 --> 0:01:07.400
我们简单叫做 IM2Clone

24
0:01:07.400 --> 0:01:10.200
然后其实这种方式是

25
0:01:10.200 --> 0:01:13.400
Cafe 早期的 AI 框架里面去采用的

26
0:01:13.400 --> 0:01:15.200
为什幺说是早期

27
0:01:15.200 --> 0:01:18.600
是因为一开始卷机的计算方式

28
0:01:18.600 --> 0:01:19.600
确实太复杂了

29
0:01:19.600 --> 0:01:21.000
而且不容易优化

30
0:01:21.000 --> 0:01:24.200
于是 Cafe 一开始就采用了 Image2Clone

31
0:01:24.200 --> 0:01:26.600
这种方式把我们高维的脏量

32
0:01:26.600 --> 0:01:29.400
转换成为低维的矩阵的相乘

33
0:01:29.400 --> 0:01:32.400
我们简单的看看下面的示例图

34
0:01:32.400 --> 0:01:35.000
我们知道在整个神经网络里面

35
0:01:35.000 --> 0:01:37.000
基本上很多计算

36
0:01:37.000 --> 0:01:40.200
都是用高维的脏量去进行计算的

37
0:01:40.200 --> 0:01:41.800
而高维的脏量的表示

38
0:01:41.800 --> 0:01:45.200
一般使用 NHWC 这种表示方式

39
0:01:45.200 --> 0:01:47.200
然后我们会把高维的脏量

40
0:01:47.200 --> 0:01:49.400
这种 NHWC 转换成为

41
0:01:49.400 --> 0:01:51.600
通过 IM2Clone 这种方式

42
0:01:51.600 --> 0:01:54.600
转换成为 GMM 或者 Bloss MTK 这种库

43
0:01:54.600 --> 0:01:57.200
可以进行直接矩阵层的操作

44
0:01:57.200 --> 0:01:59.800
接着我们进行一个逆变的操作

45
0:01:59.800 --> 0:02:02.000
Clone to Image 这种方式

46
0:02:02.000 --> 0:02:05.200
把我们的脏量恢复成为 NHWC 的格式

47
0:02:05.200 --> 0:02:07.400
给下一个算子去进行计算

48
0:02:07.400 --> 0:02:10.000
而这里面我们就把卷机的方式

49
0:02:10.000 --> 0:02:12.000
就变成 GMM 的方式了

50
0:02:12.000 --> 0:02:15.800
下面我们看一个比较具体的例子

51
0:02:15.800 --> 0:02:17.200
在具体例子进入之前

52
0:02:17.200 --> 0:02:19.800
我们看看 InvisClone 整体的算法过程

53
0:02:19.800 --> 0:02:22.000
虽然我们只看算法过程没什幺意思

54
0:02:22.000 --> 0:02:25.200
但是有助于我们后面的了解

55
0:02:25.200 --> 0:02:27.400
首先这里面分开两个步骤

56
0:02:27.400 --> 0:02:30.400
第一个步骤就是 IM2Clone

57
0:02:30.400 --> 0:02:33.000
就把我们的输入的数据

58
0:02:33.000 --> 0:02:34.800
还有我们的权重的数据

59
0:02:34.800 --> 0:02:36.000
转换排布

60
0:02:36.000 --> 0:02:40.000
转换成为 GMM 可以进行计算的排布

61
0:02:40.000 --> 0:02:41.800
所以这里面分开两个步骤

62
0:02:41.800 --> 0:02:44.400
第一个步骤就是我们的第一行

63
0:02:44.400 --> 0:02:46.200
第一句话 IM2Clone

64
0:02:46.200 --> 0:02:48.400
第二个步骤就是 GMM

65
0:02:48.400 --> 0:02:50.600
第二行第二句话的意思

66
0:02:50.600 --> 0:02:52.400
下面我们具体来看看

67
0:02:52.400 --> 0:02:55.600
一般的图像是怎幺操作的

68
0:02:55.600 --> 0:02:57.800
可以看到图像的数的维度

69
0:02:57.800 --> 0:02:59.000
一般都是三维

70
0:02:59.000 --> 0:03:01.000
H W 3

71
0:03:01.000 --> 0:03:03.800
H 就是长、宽乘以三个信道

72
0:03:03.800 --> 0:03:05.400
是我们一般的图片

73
0:03:05.400 --> 0:03:08.000
而卷机盒可能会有四维

74
0:03:08.000 --> 0:03:10.000
N C K H K W

75
0:03:10.000 --> 0:03:12.600
K H K W 代表我们的卷机盒

76
0:03:12.600 --> 0:03:14.200
Kernel 的长和宽

77
0:03:14.200 --> 0:03:16.800
而 C 是代表单一个卷机盒的

78
0:03:17.800 --> 0:03:20.800
N 代表是有多少这样的一个小组

79
0:03:20.800 --> 0:03:22.800
最后的输出的维度就是

80
0:03:22.800 --> 0:03:26.400
N H W 我们最右边的图所示

81
0:03:26.400 --> 0:03:30.200
当然这是最简单的图片的卷机的操作

82
0:03:30.200 --> 0:03:31.800
现在我们来看一下

83
0:03:31.800 --> 0:03:33.200
在我们的神经网络里面

84
0:03:33.200 --> 0:03:36.600
数据的排布一般都是高维的、四维的

85
0:03:36.600 --> 0:03:39.800
甚至到了点匀它可能是五维的

86
0:03:39.800 --> 0:03:41.600
我们默认的以四维为例子

87
0:03:41.600 --> 0:03:45.000
N H W C 输入的就不是图片了

88
0:03:45.000 --> 0:03:46.200
而是 Feature Map

89
0:03:46.200 --> 0:03:48.200
或者我们的一个 Tensor

90
0:03:48.200 --> 0:03:50.000
Tensor 的维度就是 N

91
0:03:50.000 --> 0:03:52.200
I H I W I C

92
0:03:52.200 --> 0:03:55.200
I 就是 Input Height Input Width

93
0:03:55.200 --> 0:03:57.600
输入的 Feature Map 长和宽

94
0:03:57.600 --> 0:04:00.800
而 I C 就是 Input Channels 的大小

95
0:04:00.800 --> 0:04:02.800
一共有 N 组这样的维度

96
0:04:02.800 --> 0:04:04.800
而卷机盒的维度

97
0:04:04.800 --> 0:04:07.200
可能我们在前面加了个 K W

98
0:04:07.200 --> 0:04:09.600
K H Kernels Width Kernels Height

99
0:04:09.600 --> 0:04:13.800
然后 I C 另外有一个就是 Featured N

100
0:04:13.800 --> 0:04:15.600
这个是相同的

101
0:04:15.600 --> 0:04:18.000
而输出的维度其实也有所不同

102
0:04:18.000 --> 0:04:21.000
大家要注意的就是前面的下标

103
0:04:21.000 --> 0:04:23.800
哪个是 I 哪个是 O 哪个是 N

104
0:04:23.800 --> 0:04:26.400
输出就是 O H O W 乘以 O C

105
0:04:26.400 --> 0:04:29.800
当然了同样有 N 组这样的一些张量

106
0:04:29.800 --> 0:04:30.600
或者 Feature Map

107
0:04:30.600 --> 0:04:33.400
下面我们看一下更加具体的例子

108
0:04:33.400 --> 0:04:34.800
可以看到 InVisual Comp

109
0:04:34.800 --> 0:04:39.400
最重要的是改变了数据的排布的方式

110
0:04:39.400 --> 0:04:41.200
既然改变了数据的排布方式

111
0:04:41.200 --> 0:04:44.400
就可以方便我们把刚才全重的数据

112
0:04:44.400 --> 0:04:46.200
变成一个二维的矩阵

113
0:04:46.200 --> 0:04:50.000
把我们 Feature Map 变成了一个二维的矩阵

114
0:04:50.000 --> 0:04:52.800
通过行跟列相乘

115
0:04:52.800 --> 0:04:56.000
得到我们最终的输出的结果

116
0:04:56.000 --> 0:04:58.800
就完成了整个卷机的计算

117
0:04:58.800 --> 0:04:59.600
很有意思

118
0:04:59.600 --> 0:05:02.400
更多的是数学上面的变换

119
0:05:02.400 --> 0:05:06.400
那下面我们看看怎幺进行算法的重排

120
0:05:06.400 --> 0:05:09.800
或者怎幺对我们的内存的数据进行重排

121
0:05:09.800 --> 0:05:12.600
可以看到灰色的这个小框框

122
0:05:12.800 --> 0:05:17.200
是我们卷机盒或者滑动窗口的大小

123
0:05:17.200 --> 0:05:20.600
那滑动窗口是 1 2 3 4 5 6 这幺排的数据

124
0:05:20.600 --> 0:05:23.400
但实际上我们的数据在滑动窗口里面

125
0:05:23.400 --> 0:05:25.600
是 1 2 3 7 8 9

126
0:05:25.600 --> 0:05:28.000
那可能下面还有更大的数了

127
0:05:28.000 --> 0:05:31.000
那这个时候我们就把一个滑动窗口的数据

128
0:05:31.000 --> 0:05:33.000
对它进行展开

129
0:05:33.000 --> 0:05:35.000
同样对第二个滑动窗口

130
0:05:35.000 --> 0:05:37.000
是第二个圈楼的滑动窗口

131
0:05:37.000 --> 0:05:39.200
对它进行展开

132
0:05:39.200 --> 0:05:41.600
不断的展开成为一行

133
0:05:41.600 --> 0:05:46.400
那这一行对应的就是 kw 乘以 kh 乘以 ic

134
0:05:46.400 --> 0:05:49.600
是完完全全跟我们的卷机盒展开的方式

135
0:05:49.600 --> 0:05:52.200
是对应起来就方便我们相乘

136
0:05:52.200 --> 0:05:55.000
而我们一共有多少行呢

137
0:05:55.000 --> 0:05:57.600
多少就是 oh 乘以 ow 行了

138
0:05:57.600 --> 0:06:00.400
这个就是对应到我们的输出

139
0:06:00.400 --> 0:06:04.600
下面我们看一下怎幺对权重的数据进行重排

140
0:06:04.600 --> 0:06:06.200
权重的数据进行重排

141
0:06:06.200 --> 0:06:07.600
其实很好理解

142
0:06:07.600 --> 0:06:09.600
而且很有意思一点就是

143
0:06:09.600 --> 0:06:13.800
我们权重的数据重排不是在 kernel 执行的时候进行重排的

144
0:06:13.800 --> 0:06:16.200
在我们的推定引擎架构里面

145
0:06:16.200 --> 0:06:20.600
一般对我们的 InVisual Clamp 这种转换的方式的数据重排

146
0:06:20.600 --> 0:06:25.400
会在我们的模型转换或者图优化的过程当中

147
0:06:25.400 --> 0:06:29.000
特别是布局优化或者内存优化的时候

148
0:06:29.000 --> 0:06:31.200
进行转换重排的

149
0:06:31.200 --> 0:06:34.200
但有可能我们在 OneTie 的预编译阶段

150
0:06:34.200 --> 0:06:35.800
或者 OneTie 的预执行阶段

151
0:06:35.800 --> 0:06:38.000
进行重排也是有可能的

152
0:06:38.000 --> 0:06:41.200
具体就取决于我们的推定引擎的架构是怎幺设计的

153
0:06:41.200 --> 0:06:43.600
这个模块应该装载在哪里

154
0:06:43.600 --> 0:06:48.800
现在又回到我们刚才的 image2clamp 的这种算法过程里面

155
0:06:48.800 --> 0:06:51.200
对权重的数据进行重排

156
0:06:51.200 --> 0:06:54.200
权重数据进行重排是比较简单的

157
0:06:54.200 --> 0:06:57.200
我们可以看到左边的紫色的这小框框

158
0:06:57.200 --> 0:06:59.400
就是我们的一个卷积核

159
0:06:59.400 --> 0:07:02.200
那卷积核我们 1 2 3 4 5 6 7 8 9

160
0:07:02.200 --> 0:07:05.200
直接把它展开成为一行

161
0:07:05.200 --> 0:07:06.800
可以看到直接展开成一行

162
0:07:07.000 --> 0:07:09.400
然后对第二个 channel 进行展开

163
0:07:09.400 --> 0:07:10.800
第三个 channel 进行展开

164
0:07:10.800 --> 0:07:12.800
每个 fit 对它进行展开

165
0:07:12.800 --> 0:07:17.000
所以一共有 n 行 n 乘以 kw 乘以 kh 乘以 ic

166
0:07:17.000 --> 0:07:19.400
就组成了一个大的矩阵

167
0:07:19.400 --> 0:07:22.800
把我们高维的四维的张量的数据

168
0:07:22.800 --> 0:07:24.600
变成一个二维的矩阵

169
0:07:24.600 --> 0:07:26.200
既然变成一个二维矩阵

170
0:07:26.200 --> 0:07:29.600
就非常好的利用我们 GMM 的特性

171
0:07:29.600 --> 0:07:32.200
对它进行一个计算

172
0:07:32.200 --> 0:07:35.200
现在我们整体的看看一个过程

173
0:07:35.200 --> 0:07:36.600
这里面分开两步

174
0:07:36.600 --> 0:07:38.600
上面的是原来高维的数据

175
0:07:38.600 --> 0:07:39.600
高维的张量

176
0:07:39.600 --> 0:07:41.400
进行一个卷积的过程

177
0:07:41.400 --> 0:07:42.800
这个就是卷积核

178
0:07:42.800 --> 0:07:44.200
这个就是我们的 feature map

179
0:07:44.200 --> 0:07:47.000
最终得到我们的输出的结果

180
0:07:47.000 --> 0:07:51.000
然后我们把它进行 image to column 的方式

181
0:07:51.000 --> 0:07:52.000
把我们的权重

182
0:07:52.000 --> 0:07:54.400
把我们的卷积核进行转换

183
0:07:54.400 --> 0:07:57.600
成为一个单独的二维的矩阵

184
0:07:57.600 --> 0:08:00.400
接着我们把 feature map 同样进行展开

185
0:08:00.400 --> 0:08:02.400
变成一个单独的矩阵

186
0:08:02.400 --> 0:08:05.600
接着两个矩阵相乘得到我们的输出的矩阵

187
0:08:05.600 --> 0:08:06.600
输出的矩阵

188
0:08:06.600 --> 0:08:08.200
最后的箭头是向上的

189
0:08:08.200 --> 0:08:09.400
大家值得注意的就是

190
0:08:09.400 --> 0:08:11.200
我们需要通过 column to image

191
0:08:11.200 --> 0:08:13.400
把它逆变回来

192
0:08:13.400 --> 0:08:15.400
接下来我们整体的看看

193
0:08:15.400 --> 0:08:17.400
Image to column 的算法流程

194
0:08:17.400 --> 0:08:19.000
算法流程分为四步

195
0:08:19.000 --> 0:08:22.400
第一步就是对我们的输入的数据

196
0:08:22.400 --> 0:08:24.400
进行展开

197
0:08:24.400 --> 0:08:28.200
展开成为一个独特的独立的权重

198
0:08:28.200 --> 0:08:30.600
第二个就是对我们的权重的数据

199
0:08:30.600 --> 0:08:32.400
进行展开

200
0:08:32.400 --> 0:08:34.400
同样就是就展开重排

201
0:08:34.400 --> 0:08:37.800
接着对上面一二步得到的两个矩阵

202
0:08:37.800 --> 0:08:39.200
进行相乘

203
0:08:39.200 --> 0:08:41.200
最终得到我们的输出的矩阵

204
0:08:41.200 --> 0:08:42.400
输出的矩阵

205
0:08:42.400 --> 0:08:45.200
同样需要进行一个逆变的过程

206
0:08:45.200 --> 0:08:47.800
所以整体来说分开四个步骤

207
0:08:47.800 --> 0:08:50.800
四个步骤的执行方式和执行的模块

208
0:08:50.800 --> 0:08:52.000
都是不一样的

209
0:08:52.000 --> 0:08:55.200
可能 1 3 4

210
0:08:55.200 --> 0:08:58.000
是进行到我们的具体的 kernel 层里面

211
0:08:58.000 --> 0:08:59.200
但是第二个

212
0:08:59.200 --> 0:09:01.000
可能会在我们的预编一阶段

213
0:09:01.000 --> 0:09:02.200
或者在我们的

214
0:09:02.400 --> 0:09:04.800
离线转换优化模块里面去实现

215
0:09:04.800 --> 0:09:06.600
那下面我们来总结一下

216
0:09:06.600 --> 0:09:09.400
Amy to Clumb的计算的方式

217
0:09:09.400 --> 0:09:10.800
可以看到 Amy to Clumb

218
0:09:10.800 --> 0:09:14.000
就是把我们传统的卷积大量的计算

219
0:09:14.000 --> 0:09:17.000
使用 GMM 这种经过优化的库

220
0:09:17.000 --> 0:09:19.400
来进行一个加速的

221
0:09:19.400 --> 0:09:21.800
但是有个问题就是使用 Amy to Clumb

222
0:09:21.800 --> 0:09:23.600
我们可以看到我们需要对数据

223
0:09:23.600 --> 0:09:25.000
进行重排

224
0:09:25.000 --> 0:09:26.800
把三维或者高维的数据

225
0:09:26.800 --> 0:09:29.200
展开成为二维的矩阵

226
0:09:29.200 --> 0:09:30.400
那这个时候

227
0:09:30.400 --> 0:09:32.800
我们就需要对数据的数据

228
0:09:32.800 --> 0:09:34.600
拷贝多份

229
0:09:34.600 --> 0:09:37.800
而且就对我们的内存有开销了

230
0:09:37.800 --> 0:09:40.200
第二个就是转换之后

231
0:09:40.200 --> 0:09:42.000
我们就有很多不同

232
0:09:42.000 --> 0:09:45.000
已经实现好的高速的优化库

233
0:09:45.000 --> 0:09:49.400
Plus、NPT、LumPy 这种去实现

234
0:09:49.400 --> 0:09:50.600
然后值得一提的

235
0:09:50.600 --> 0:09:52.600
就是我们刚才也给大家介绍过

236
0:09:52.600 --> 0:09:55.200
就是我们首先会在全众

237
0:09:55.200 --> 0:09:58.000
会预先的去把它转换成为 Amy to Clumb

238
0:09:58.000 --> 0:09:59.800
而不是在真正全众来的时候

239
0:09:59.800 --> 0:10:00.800
或者计算的时候

240
0:10:00.800 --> 0:10:02.000
我才做转换

241
0:10:02.000 --> 0:10:03.600
我们在训练的阶段

242
0:10:03.600 --> 0:10:07.000
其实已经获得到我们的全众的数据了

243
0:10:07.000 --> 0:10:08.200
而 Input 的数据

244
0:10:08.200 --> 0:10:10.800
确实只有在数据真正来的时候

245
0:10:10.800 --> 0:10:12.000
数据流来的时候

246
0:10:12.000 --> 0:10:12.800
我们才能感知

247
0:10:12.800 --> 0:10:17.400
那这时候确实没办法进行提前的重排

248
0:10:17.400 --> 0:10:20.200
下面我们来到第二个内容

249
0:10:20.200 --> 0:10:22.800
也是这一节小课里面的

250
0:10:22.800 --> 0:10:23.600
最后一个内容

251
0:10:23.600 --> 0:10:26.000
空间的组合优化

252
0:10:26.000 --> 0:10:28.200
像 Amy to Clumb 这种方式

253
0:10:28.200 --> 0:10:29.200
到空间组合优化

254
0:10:29.200 --> 0:10:30.800
Amy to Clumb 可以理解为一个

255
0:10:30.800 --> 0:10:34.000
比较朴素的卷积的优化的手段

256
0:10:34.000 --> 0:10:35.200
把卷积的方式

257
0:10:35.200 --> 0:10:37.800
变换成为一个矩阵层的方式

258
0:10:37.800 --> 0:10:39.200
而空间组合优化

259
0:10:39.200 --> 0:10:41.400
更多的是基于一个分字的思想

260
0:10:41.400 --> 0:10:42.200
既然分字

261
0:10:42.200 --> 0:10:45.200
我们就可以知道里面很重要的一个词

262
0:10:45.200 --> 0:10:46.800
就是空间

263
0:10:46.800 --> 0:10:47.600
空间这两个字

264
0:10:47.600 --> 0:10:49.600
就帮我们的卷积的计算

265
0:10:49.600 --> 0:10:50.800
利用空间的特征

266
0:10:50.800 --> 0:10:52.800
划分成为若干份了

267
0:10:52.800 --> 0:10:54.400
然后进行分别处理

268
0:10:54.400 --> 0:10:55.400
那我们可以看到

269
0:10:55.400 --> 0:10:58.200
下面就是空间组合优化的一种方式

270
0:10:58.200 --> 0:10:58.600
当然了

271
0:10:58.600 --> 0:11:01.400
这里面不是用 GMM 来去示例

272
0:11:01.400 --> 0:11:03.600
也不是用 Amy to Clumb 来示例

273
0:11:03.600 --> 0:11:06.200
而是直接用传统的卷积的方式

274
0:11:06.200 --> 0:11:07.600
进行示例

275
0:11:07.600 --> 0:11:09.600
这里面我们对输入的数据

276
0:11:09.600 --> 0:11:12.400
划分成为四个模块

277
0:11:12.400 --> 0:11:13.200
四个部分

278
0:11:13.200 --> 0:11:16.000
也就是 IH 还有 IW

279
0:11:16.000 --> 0:11:18.200
进行两两切分

280
0:11:18.200 --> 0:11:20.000
划分成为四块

281
0:11:20.000 --> 0:11:20.800
这第一块

282
0:11:20.800 --> 0:11:21.800
第二块

283
0:11:21.800 --> 0:11:23.000
第三块

284
0:11:23.000 --> 0:11:24.000
第四块

285
0:11:24.000 --> 0:11:25.400
分别对我们的权重

286
0:11:25.400 --> 0:11:26.400
进行卷积

287
0:11:26.400 --> 0:11:27.400
得到我们的输出

288
0:11:27.400 --> 0:11:29.200
最后把输出拼接到一起

289
0:11:29.200 --> 0:11:30.200
这种就是最简单

290
0:11:30.200 --> 0:11:32.600
最朴素的空间优化的方法了

291
0:11:32.600 --> 0:11:34.000
现在我们看一下

292
0:11:34.000 --> 0:11:37.000
整个空间优化的原理

293
0:11:37.000 --> 0:11:38.400
原理还是非常简单的

294
0:11:38.400 --> 0:11:39.800
主要是下面这几个图

295
0:11:39.800 --> 0:11:42.400
划这几个图确实花了不少时间

296
0:11:42.400 --> 0:11:43.600
像空间组合优化

297
0:11:43.600 --> 0:11:45.600
其实是非常好理解的原理

298
0:11:45.600 --> 0:11:47.200
首先我们会把输入的

299
0:11:47.200 --> 0:11:49.000
一个很大的 Feature Map

300
0:11:49.000 --> 0:11:51.200
把它分成 Unpack 层

301
0:11:51.200 --> 0:11:53.400
多个不同的 Feature Map

302
0:11:53.400 --> 0:11:54.800
根据不同的小的 Feature Map

303
0:11:54.800 --> 0:11:56.200
跟我们的权重

304
0:11:56.200 --> 0:11:58.600
跟我们的数据进行卷积

305
0:11:58.600 --> 0:12:00.800
得到多个数据的输出

306
0:12:00.800 --> 0:12:02.000
但是有一点值得注意的

307
0:12:02.000 --> 0:12:04.400
就是这里面的小框框的窗口

308
0:12:04.400 --> 0:12:06.000
或者跨分的一个小模块

309
0:12:06.000 --> 0:12:07.800
必须要跟我们的卷积和的

310
0:12:07.800 --> 0:12:10.400
Windows 的大小相匹配

311
0:12:10.400 --> 0:12:12.400
而且跟我们的 Stride 相匹配

312
0:12:12.400 --> 0:12:14.200
这个时候就可以很好的

313
0:12:14.200 --> 0:12:16.400
利用我们的计算机的存储结构

314
0:12:16.400 --> 0:12:18.600
获得我们的性能的提升

315
0:12:18.600 --> 0:12:21.200
为什幺说是计算机的存储结构了

316
0:12:21.200 --> 0:12:22.800
因为我们知道计算机里面

317
0:12:22.800 --> 0:12:24.600
或者我们的 CPU GPU 里面

318
0:12:24.800 --> 0:12:27.000
还有 L0 L1 L2

319
0:12:27.000 --> 0:12:29.000
不同的 Cache

320
0:12:29.000 --> 0:12:31.200
有不同的性能的提升

321
0:12:31.200 --> 0:12:33.000
下面我们看一下

322
0:12:33.000 --> 0:12:35.800
空间左右化一些注意的点

323
0:12:35.800 --> 0:12:38.000
其实我们在上文里面

324
0:12:38.000 --> 0:12:39.400
讲了不管是 InVisual Comp

325
0:12:39.400 --> 0:12:41.400
还是空间左右化

326
0:12:41.400 --> 0:12:44.800
我们都忽略了 Padding 的操作

327
0:12:44.800 --> 0:12:46.200
有了 Padding 这个操作

328
0:12:46.200 --> 0:12:47.800
其实还是需要注意

329
0:12:47.800 --> 0:12:49.400
Padding 为 Value 的时候

330
0:12:49.400 --> 0:12:51.200
可以基本上忽略了

331
0:12:51.200 --> 0:12:53.600
但是 Padding 等于 Thin 的时候

332
0:12:53.600 --> 0:12:55.200
需要利用边界补0

333
0:12:55.200 --> 0:12:56.600
不在边界补0的时候

334
0:12:56.600 --> 0:12:59.400
需要利用 0g 的张量的值

335
0:12:59.400 --> 0:13:00.600
所以一般来说

336
0:13:00.600 --> 0:13:02.400
我们都会多出了

337
0:13:02.400 --> 0:13:05.800
那幺一小个模块进行重叠计算的

338
0:13:05.800 --> 0:13:07.600
这也是在我们真正计算的时候

339
0:13:07.600 --> 0:13:10.200
需要注意的一个内容

340
0:13:10.200 --> 0:13:11.400
那我们现在来看看

341
0:13:11.400 --> 0:13:15.600
空间组合优化的 Coins 它的问题

342
0:13:15.600 --> 0:13:18.600
在真正的空间组合优化这种方式

343
0:13:18.600 --> 0:13:20.800
确实用的非常多

344
0:13:20.800 --> 0:13:22.800
它是我们 Kernel 实现的一个

345
0:13:22.800 --> 0:13:24.200
很重要的 Trick

346
0:13:24.200 --> 0:13:26.400
假设我们现在把一些张量

347
0:13:26.400 --> 0:13:29.000
分成边长为 4 或者 8

348
0:13:29.000 --> 0:13:30.400
为什幺会为 4 和 8

349
0:13:30.400 --> 0:13:32.600
是因为方便我们的 AI 编译器

350
0:13:32.600 --> 0:13:34.200
或者我们的传统编译器

351
0:13:34.200 --> 0:13:37.200
进行矢量化的操作

352
0:13:37.200 --> 0:13:39.000
矢量化的转变

353
0:13:39.000 --> 0:13:41.000
不过值得注意的就是

354
0:13:41.000 --> 0:13:45.600
这个模块并不是分的越小越好的

355
0:13:45.600 --> 0:13:46.600
当然了越小

356
0:13:46.600 --> 0:13:47.800
它有个好处就是

357
0:13:47.800 --> 0:13:51.200
充分的利用了计算机体系里面的

358
0:13:51.200 --> 0:13:52.600
多级的 Cache

359
0:13:52.600 --> 0:13:54.400
但是模块越小

360
0:13:54.400 --> 0:13:56.200
局部性也就越高

361
0:13:56.200 --> 0:13:57.200
负面的作用就是

362
0:13:57.200 --> 0:14:00.800
消耗更多的额外的内存

363
0:14:00.800 --> 0:14:03.000
这也是它带来的好处和问题

364
0:14:03.000 --> 0:14:05.000
所以我们在 Kernel 实现的时候

365
0:14:05.000 --> 0:14:06.600
需要把握一个度

366
0:14:06.600 --> 0:14:08.800
寻找一个合适的划分的方式

367
0:14:08.800 --> 0:14:11.000
或者合适的划分的尺寸

368
0:14:11.000 --> 0:14:12.200
是一个不容易的事情

369
0:14:12.200 --> 0:14:14.400
我们需要经过大量的时间的优化

370
0:14:14.400 --> 0:14:15.600
当然了这里我们也可以通过

371
0:14:15.600 --> 0:14:18.400
AI 编译器去自动的寻优

372
0:14:18.600 --> 0:14:21.200
但是在推理场景

373
0:14:21.200 --> 0:14:23.000
一般寻优完一次之后

374
0:14:23.000 --> 0:14:25.200
我们基本上就很少去改动了

375
0:14:25.200 --> 0:14:29.000
那最后我们来到回头

376
0:14:29.000 --> 0:14:31.200
看看整个推理引擎架构里面

377
0:14:31.200 --> 0:14:34.000
我们主要讲的是在 Kernel 优化

378
0:14:34.000 --> 0:14:35.000
而 Kernel 优化

379
0:14:35.000 --> 0:14:36.600
其实这里面有很多种

380
0:14:36.600 --> 0:14:38.200
第一种像左边的这个

381
0:14:38.200 --> 0:14:40.000
就是针对 x86 的

382
0:14:40.000 --> 0:14:41.600
而这种中间的这个

383
0:14:41.600 --> 0:14:43.000
就针对 GPU 的

384
0:14:43.000 --> 0:14:44.600
不管是 PC 的 GPU

385
0:14:44.600 --> 0:14:47.200
还是手机的 GPU 都会有

386
0:14:47.200 --> 0:14:50.200
而另外一种是针对我们的编译器的

387
0:14:50.200 --> 0:14:51.800
就通过编译器来实现的

388
0:14:51.800 --> 0:14:54.400
所以说可能我们的 Kernel 层

389
0:14:54.400 --> 0:14:56.800
在整个推理引擎的代码里面

390
0:14:56.800 --> 0:14:58.800
占了绝大部分

391
0:14:58.800 --> 0:15:00.400
大部分都是 Kernel 层

392
0:15:00.400 --> 0:15:02.000
一个 CUDA 的算子

393
0:15:02.000 --> 0:15:05.400
我们可能就有很多种不同的实现

394
0:15:05.400 --> 0:15:07.200
例如我们会实现

395
0:15:07.200 --> 0:15:08.800
Image Clump 的这种方式

396
0:15:08.800 --> 0:15:10.400
我们对 3x3 的卷机

397
0:15:10.400 --> 0:15:12.000
可能会使用 Window Grid

398
0:15:12.000 --> 0:15:14.800
另外我们会使用 TNM Pack 的方式

399
0:15:14.800 --> 0:15:16.000
针对普通的卷机

400
0:15:16.000 --> 0:15:17.600
我们有普通卷机的实现方式

401
0:15:17.600 --> 0:15:19.400
所以说一个卷机的操作

402
0:15:19.400 --> 0:15:22.000
我们就可能有七八种 Kernel 了

403
0:15:22.000 --> 0:15:23.200
针对 x86 里面

404
0:15:23.200 --> 0:15:25.600
可能同样的一个卷机操作

405
0:15:25.600 --> 0:15:27.200
又有七八种 Kernel 了

406
0:15:27.200 --> 0:15:28.200
那我们一个推理引擎

407
0:15:28.200 --> 0:15:30.200
要同时支持 CPU

408
0:15:30.200 --> 0:15:32.400
也要同时具体 GPU

409
0:15:32.400 --> 0:15:34.000
可能一个 3x3 的卷机

410
0:15:34.000 --> 0:15:34.800
Window Grid

411
0:15:34.800 --> 0:15:37.000
就有两个 Kernel 的实现方式了

412
0:15:37.000 --> 0:15:38.600
所以说为什幺 Kernel 层

413
0:15:38.600 --> 0:15:40.200
是非常的厚重

414
0:15:40.200 --> 0:15:41.600
也是这个原因

415
0:15:41.600 --> 0:15:42.200
好了

416
0:15:42.200 --> 0:15:44.800
今天的内容就到这里为止

417
0:15:44.800 --> 0:15:45.600
谢谢各位

418
0:15:45.600 --> 0:15:46.600
拜了个拜

