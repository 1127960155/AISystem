0
0:00:00.000 --> 0:00:30.000
哈喽大家好欢乐的时光过得特别快又是时候说拜拜欢乐的时光过得特别快又是时候说拜拜啦我是综米今天呢我们来到推定型的模型优化计算机优化里面的最后一个内容啦所以我的今天的内容会排得特别的密然后呢我也会讲得特别的快了一点点首先呢我们今天主要是去讲讲我们

1
0:00:30.000 --> 0:00:32.920
一些优化还有雷尔跟梅梅瑞的一些优化

2
0:00:32.920 --> 0:00:35.560
那其实呢这些很多的相关的知识呢我们

3
0:00:35.560 --> 0:00:37.360
呃简单的理解一下就好了

4
0:00:37.360 --> 0:00:41.240
更多的是希望各个同事呢能够在工作台当中跟我一起去发现

5
0:00:41.240 --> 0:00:44.120
更多不同的可能性或者更多优化的方式

6
0:00:44.120 --> 0:00:49.560
那可以看到在整个工作流程当中呢我们现在来到了第二个和第三个步骤里面

7
0:00:49.560 --> 0:00:53.400
当然它没有一个明确的界限秩分的基本上都可以通用的

8
0:00:53.400 --> 0:00:55.200
而帕斯的管理也是非常重要

9
0:00:55.200 --> 0:00:59.960
在这里面呢我推荐大家去看看AI兵器里面的前端优化里面相关的

10
0:00:59.960 --> 0:01:05.120
内容虽然大家可以看到啊前端优化AI兵器啊看了呀确实太少了

11
0:01:05.120 --> 0:01:10.080
你甚至就我自己不断的点来点去换了不同电脑可能在有时候出差的时候在北京点了

12
0:01:10.080 --> 0:01:14.280
有时候才深圳点来确实点来点去啊就特别多了已经点的变成

13
0:01:14.280 --> 0:01:17.600
不可能绝对不可能

14
0:01:17.600 --> 0:01:20.960
嗯那现在呢我们来到计算图详解的第二个内容

15
0:01:20.960 --> 0:01:28.560
那第二个内容就是其他图的优化可以看到其他图的优化呢很多就是针对不同的AI框架它可能没有直接的实现

16
0:01:29.120 --> 0:01:35.880
而是通过一些特殊的一些组合更多的是跟硬件或特殊的网络模型或特殊的领域相关的

17
0:01:35.880 --> 0:01:44.120
我们看几个就假设是雷亚浪的融合呢还有披比路的替换呢麦特茂的转变呢还有比纳瑞跟艾里姆万斯呢

18
0:01:44.120 --> 0:01:52.760
嗯最后还有比丢森还有哥波普林啊这些确实特别多我们看看具体的图这里面呢就一个一个的去给大家展开啦

19
0:01:52.760 --> 0:01:57.640
实际上啊像我们有些算子假设比路加茂的然后加艾里姆万斯森

20
0:01:57.640 --> 0:02:02.840
然后加比路这种方式如果我们遇到这种图啊直接变成一个披比路就行了

21
0:02:02.840 --> 0:02:12.640
当然了如果我们的引擎里面推进引擎里面的没有披比路这个算子我们也可以把它拆分成这堆算子的一个具体的实现就可以代替掉披比路了

22
0:02:12.640 --> 0:02:22.440
虽说这里面的没有一个统一的界限更多的是根据我们具体底层有什么算子我们就提供什么优化的帕斯而不是优化的帕斯呢我随便写

23
0:02:22.440 --> 0:02:31.720
你写推进引擎的人写克诺的人你就随便自己写我们是有一个非常良好的合作关系才能够把整个推进引擎呢做到极致性能的优化

24
0:02:31.720 --> 0:02:41.240
那下面呢像我们可以看到麦茂呢它确实有两个transport但实际上呢有些推进引擎里面呢它就已经把输的数据呢自动的做了个transport

25
0:02:41.240 --> 0:02:52.400
那这个时候呢我们可以减少一个数据的搬运的过程当中但我们还有很多像这种兵威茂的呢还有兵力爱的呢就可以换成艾里姆万森这种特殊的

26
0:02:52.400 --> 0:03:04.840
方式下面呢我们来看一个比较特殊终于觉得最近呢也是比较有意思或者应该是去年年底吧今年年初去年年底啊现在已经是二三年了

27
0:03:06.640 --> 0:03:20.720
有时候我在想啊有没有可能你在看这个视频的时候已经到二四二五年的时候然后你发现嘿我怎么还在看二二年的视频或者二三年的视频啊嘿嘿确实这个文章呢是二三年发的它叫做flash attention

28
0:03:20.720 --> 0:03:50.720
它里面呢就对而摊选呢做了一个特殊的优化下面我们看一下具体的这个图文章里面铺出来的图我们简单的去解读一下像现在呢大家用的非常多的而摊选或者全琢磨的一些网络模型的层但是呢摊选确实很少在推进引擎里面去应用确实呢像而摊选成或者全琢磨成了它没有跑得像卷机层这么快也没有经过那么多的特殊的优化那卷机的特殊的优化呢我们会将会在下一个内容里面

29
0:03:50.720 --> 0:03:54.240
科诺的执行或者具体的文摊里面去给大家介绍的

30
0:03:54.240 --> 0:03:58.320
那这里面呢我们看一下flash attention里面具体做了哪些工作

31
0:03:59.120 --> 0:04:06.400
其实我们知道呢在而摊选或者全琢磨这个里面呢大部分都是算QKB

32
0:04:06.400 --> 0:04:12.960
通过QKB这三个矩阵不断的相乘呢就得到我们的全琢磨或者Multi-Attention这个层

33
0:04:12.960 --> 0:04:19.360
接着呢我们下一个网络模型呢就是算softmax那softmax在这里面呢就简称SM

34
0:04:19.440 --> 0:04:22.240
不要误解不要误解这里不是

35
0:04:34.960 --> 0:04:42.480
这里面呢就有一个SM去算QKB可以看到假设呢我们AI引擎呢会跑在具体的一些芯片里面

36
0:04:42.480 --> 0:04:47.040
而具体的一些加速芯片的大部分都不会有太多的一些SM

37
0:04:47.040 --> 0:04:53.760
而SM确实里面的容量非常有限于是呢我们就会对我们的矩阵呢分块来进行计算

38
0:04:53.760 --> 0:04:59.520
那这个loop呢也大家可以去看一下AI编译器里面的有一节内容就是科诺的优化

39
0:04:59.520 --> 0:05:05.120
或者那个后端的优化里面就会讲为什么要做loop啦然后怎么对这些进行一个切片呢

40
0:05:05.120 --> 0:05:12.960
那这边我们回到flash attention这个内容里面看到了我们的K呢假设我们就会把一些取出一小块进行计算

41
0:05:12.960 --> 0:05:16.480
那像这种Q呢我们也会取出一小块进行计算

42
0:05:16.480 --> 0:05:22.880
那计算完之后呢我们QKB要相乘嘛相乘完之后再给softmax进行一个执行运算的

43
0:05:22.880 --> 0:05:27.680
但是softmax里面呢就会把数据摊平摊成一条进行一个计算

44
0:05:27.680 --> 0:05:33.840
因为softmax呢是接收一个vector的进行计算的如果是这样的话呢就算得非常慢了

45
0:05:33.840 --> 0:05:45.760
于是呢作者呢就在flash attention里面呢就提到了我通过滚动的方式呢去计算我的softmax去计算这个SM使得我的速度呢就进一步去提升

46
0:05:45.760 --> 0:05:58.160
算完一块QKB再给到softmax的结果进行重排通过这种新颖的计算方式呢使得我们的attention在GPT-二里面呢有了接近七倍的提升了

47
0:05:58.160 --> 0:06:13.920
哇这个可不得了啊大家知道训练一个GPT-三的时间呢要八十多天啊八十多天一百二十八个GPU有多少人有多少人有资源去算这个的基本上呢如果我不做一些大魔性的项目呢却拿不到这么多资源的

48
0:06:13.920 --> 0:06:26.000
所以说一般来说训练这个模型二块全存网的模型特别特别慢但是有了flash attention之后呢我们就可以把真正的attention的推理毛贴attention的推理呢变成现实

49
0:06:26.000 --> 0:06:42.800
如果我讲得不清楚呢也非常欢迎大家去翻一翻这篇论文那这篇论文里面有非常多的公式里面给到的一个附录也是非常多下面呢我们来到计算图优化的第三个部分计算图优化详节

50
0:06:42.800 --> 0:06:53.520
那在这里面呢我还是非常推荐大家去看一看这个内容为什么呢因为在第三个部分的更多的是对layout跟memory的一些优化

51
0:06:53.520 --> 0:07:10.960
那可以看到我们layout的优化呢就是我们的数据布局的优化在数据布局里面的确实讲了非常的多从NCHW呢到NHWC呢再到华为自己推出的NCHWC0这种方式确实很特别可以看到不同的算子的层或者不同的客户呢

52
0:07:10.960 --> 0:07:17.760
我们需要做一个客户费塔的一个转换那针对网络模型上一层跟下一层的算子的相同可能不需要转换

53
0:07:17.760 --> 0:07:28.800
但上一层输入呢跟下一层输入不同的时候我们就需要进行一个插入具体的算子这也是在我们图优化里面去做的如果是相同的时候呢我们就要删掉一些算子

54
0:07:28.800 --> 0:07:33.320
所以说这里面的研究呢要根据我们的计算图来进行优化

55
0:07:33.320 --> 0:07:38.000
那第二个内容呢就是内存分配的算法

56
0:07:38.000 --> 0:07:47.560
确实内存分配呢要在图有图的概念呢进行一个预分配而分配的方式呢有两个一个是in-place operation

57
0:07:47.560 --> 0:07:57.200
就像我们下面右下角这个图假设呢我计算完这个算子之后呢这块内存黄色橙色的这块内存呢已经不需要了

58
0:07:57.200 --> 0:08:06.000
而且下一个操作呢也是animized的就跟它的内存大小是一样的就我直接复盖掉原来的内存进行一个原地的替换

59
0:08:06.000 --> 0:08:13.880
然后就不用开辟新的空间了这也需要根据我们的图来进行优化的第二种就是memory sharing

60
0:08:13.880 --> 0:08:24.640
memory sharing里面呢就很特别的就是你不能原地的复盖但是呢我可以如果你这个数据呢就算完这个softmax的之后呢我这个数据暂时已经不用了

61
0:08:24.640 --> 0:08:33.040
我算下一个的时候呢确实为了节省我的计算的空间或者节省我们的内存那这个时候呢我们就可以共享我们的一些内存空间

62
0:08:33.040 --> 0:08:50.920
那算这个算子的时候呢就复盖掉原来的一些就可以了然后对它进行更新然后从这里面呢去取从红色的这个内存块里面去预取那这种呢就是两个数据大小相同而且前一个数据呢参与计算后面的数据呢不需要那我们就后面的就可以复盖掉了

63
0:08:50.920 --> 0:09:00.400
所以内存优化呢有这几种方式那更详细的我们还是在内存分配这个内容里面给大家进行详细的介绍这里面简单的回顾一下

64
0:09:00.720 --> 0:09:19.440
今天的内容呢就到这里为止我们在推定引擎架构里面回顾一下整个推定引擎架构主要分开两部分第一部分就是上面的我们的埃阿这层使黄色的以上这一部分呢确实会埃埃金黄色以上的

65
0:09:19.440 --> 0:09:22.240
哇

66
0:09:22.240 --> 0:09:25.200
艾奥丁

67
0:09:25.200 --> 0:09:30.360
就是我们的一个蓝色的这个模块就是我们的离线转换模块

68
0:09:30.360 --> 0:09:40.520
那我们离线转换模块呢有离线的优化压缩了还有离线的转换图优化首先呢先把我们从不同埃埃框下训练得到的网络模型呢转换成为自己的埃阿

69
0:09:40.520 --> 0:09:53.600
然后基于这个埃阿呢可以做很多不同的计算图的优化接下来我们在下一个环节里面将会去给大家讲讲执行的优化论谈的优化和可能的优化那下一期再见摆了个拜

70
0:09:53.600 --> 0:10:00.320
卷得不行了卷得不行了记得一键三连加关注哦所有的内容都会开源在下面这条链接里面

71
0:10:00.320 --> 0:10:01.680
摆了个拜

