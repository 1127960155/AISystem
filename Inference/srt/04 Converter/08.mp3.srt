1
00:00:00,000 --> 00:00:02,000
快乐的时光过得特别快

2
00:00:02,000 --> 00:00:04,000
又是时候说拜拜

3
00:00:04,000 --> 00:00:06,000
欢乐的时光过得特别快

4
00:00:06,000 --> 00:00:08,000
又是时候说拜拜

5
00:00:08,000 --> 00:00:09,000
我是ZOMI

6
00:00:09,000 --> 00:00:12,000
今天我们来到推定型的模型优化

7
00:00:12,000 --> 00:00:15,000
计算机优化里面的最后一个内容了

8
00:00:15,000 --> 00:00:18,000
所以我今天的内容会排得特别的密

9
00:00:18,000 --> 00:00:21,000
然后我也会讲得特别的快了一点点

10
00:00:21,000 --> 00:00:23,000
首先我们今天主要是去讲讲

11
00:00:23,000 --> 00:00:24,000
我们X10的内容

12
00:00:24,000 --> 00:00:26,000
因为我们今天要讲的内容

13
00:00:26,000 --> 00:00:28,000
其实是一个很简单的内容

14
00:00:28,000 --> 00:00:29,000
我们今天主要是去讲讲

15
00:00:29,000 --> 00:00:30,000
我们X10的一些优化

16
00:00:30,000 --> 00:00:33,000
还有Layer跟Memory的一些优化

17
00:00:33,000 --> 00:00:35,000
其实这些很多的相关的知识

18
00:00:35,000 --> 00:00:37,000
我们简单的理解一下就好了

19
00:00:37,000 --> 00:00:39,000
更多的是希望各个同事

20
00:00:39,000 --> 00:00:40,000
能够在工作档当中

21
00:00:40,000 --> 00:00:41,000
跟我一起去发现

22
00:00:41,000 --> 00:00:42,000
更多不同的可能性

23
00:00:42,000 --> 00:00:44,000
或者更多优化的方式

24
00:00:44,000 --> 00:00:46,000
可以看到在整个工作流程当中

25
00:00:46,000 --> 00:00:49,000
我们现在来到了第二个和第三个步骤里面

26
00:00:49,000 --> 00:00:51,000
当然他们有一个明确的界限之分

27
00:00:51,000 --> 00:00:53,000
基本上都可以通用的

28
00:00:53,000 --> 00:00:55,000
而Path的管理也是非常重要

29
00:00:55,000 --> 00:00:57,000
在这里面我推荐大家去看看

30
00:00:57,000 --> 00:01:00,000
AI编辑里面的前端优化里面相关的内容

31
00:01:00,000 --> 00:01:01,000
虽然大家可以看到

32
00:01:01,000 --> 00:01:05,000
前端优化AI编辑看了确实太少了

33
00:01:05,000 --> 00:01:07,000
你甚至就我自己不断的点来点去

34
00:01:07,000 --> 00:01:08,000
换了不同电脑

35
00:01:08,000 --> 00:01:09,000
可能在有时候出差的时候

36
00:01:09,000 --> 00:01:10,000
在北京点了

37
00:01:10,000 --> 00:01:11,000
有时候在深圳点了

38
00:01:11,000 --> 00:01:12,000
确实点来点去

39
00:01:12,000 --> 00:01:14,000
特别多了已经点的变成

40
00:01:14,000 --> 00:01:15,000
不可能

41
00:01:16,000 --> 00:01:17,000
绝对不可能

42
00:01:17,000 --> 00:01:21,000
现在我们来到计算图详解的第二个内容

43
00:01:21,000 --> 00:01:23,000
第二个内容就是其他图的优化

44
00:01:23,000 --> 00:01:25,000
可以看到其他图的优化了很多

45
00:01:25,000 --> 00:01:27,000
就是针对不同的AI框架

46
00:01:27,000 --> 00:01:28,000
他可能没有直接的实现

47
00:01:29,000 --> 00:01:31,000
而是通过一些特殊的一些组合

48
00:01:31,000 --> 00:01:33,000
更多的是跟硬件或者特殊的网络模型

49
00:01:33,000 --> 00:01:35,000
或者特殊的领域相关的

50
00:01:35,000 --> 00:01:36,000
我们看几个

51
00:01:36,000 --> 00:01:38,000
就是假设是Layer Lump的融合

52
00:01:38,000 --> 00:01:40,000
还有Pivot Loop的替换

53
00:01:40,000 --> 00:01:41,000
Metamount的转变

54
00:01:41,000 --> 00:01:43,000
还有Binary跟Element Wise

55
00:01:43,000 --> 00:01:45,000
最后还有Videosum

56
00:01:45,000 --> 00:01:46,000
还有Global Pooling

57
00:01:46,000 --> 00:01:47,000
所以确实特别多

58
00:01:47,000 --> 00:01:49,000
我们看看具体的图

59
00:01:49,000 --> 00:01:52,000
这里面就一个一个的去给大家展开了

60
00:01:52,000 --> 00:01:55,000
实际上像我们有些算子

61
00:01:55,000 --> 00:01:56,000
假设VLOOP加Mod

62
00:01:56,000 --> 00:01:57,000
然后加Element Wise Sum

63
00:01:57,000 --> 00:01:58,000
然后加VLOOP这种方式

64
00:01:58,000 --> 00:02:00,000
如果我们遇到这种图

65
00:02:00,000 --> 00:02:02,000
直接变成一个Pivot Loop就行了

66
00:02:02,000 --> 00:02:03,000
当然了

67
00:02:03,000 --> 00:02:04,000
如果我们的引擎里面

68
00:02:04,000 --> 00:02:06,000
推进引擎里面的没有Pivot Loop这个算子

69
00:02:06,000 --> 00:02:08,000
我们也可以把它拆分成

70
00:02:08,000 --> 00:02:10,000
这堆算子的一个具体的实现

71
00:02:10,000 --> 00:02:12,000
就可以代替掉Pivot Loop了

72
00:02:12,000 --> 00:02:14,000
所以说这里面没有一个统一的界限

73
00:02:14,000 --> 00:02:18,000
更多的是根据我们具体底层有什么算子

74
00:02:18,000 --> 00:02:20,000
我们就提供什么优化的Path

75
00:02:20,000 --> 00:02:21,000
而不是优化的Path

76
00:02:21,000 --> 00:02:22,000
我随便写

77
00:02:22,000 --> 00:02:23,000
你写推进引擎的人

78
00:02:23,000 --> 00:02:24,000
写Kernel的人

79
00:02:24,000 --> 00:02:25,000
你就随便自己写

80
00:02:25,000 --> 00:02:28,000
我们是有一个非常良好的合作关系

81
00:02:28,000 --> 00:02:29,000
才能够把整个推进引擎

82
00:02:29,000 --> 00:02:31,000
做到极致性能的优化

83
00:02:31,000 --> 00:02:34,000
下面像我们可以看到MapMod

84
00:02:34,000 --> 00:02:35,000
它确实有两个Transport

85
00:02:35,000 --> 00:02:38,000
但实际上有些推进引擎里面

86
00:02:38,000 --> 00:02:39,000
它就已经把数的数据

87
00:02:39,000 --> 00:02:41,000
自动的做了一个Transport

88
00:02:41,000 --> 00:02:43,000
这个时候我们可以减少一个数据的

89
00:02:43,000 --> 00:02:45,000
搬运的过程当中

90
00:02:45,000 --> 00:02:47,000
但我们还有很多像这种BinwayMod

91
00:02:47,000 --> 00:02:48,000
还有BinwayAdd

92
00:02:48,000 --> 00:02:51,000
就可以换成EliminizeSum

93
00:02:51,000 --> 00:02:53,000
这种特殊的方式

94
00:02:54,000 --> 00:02:57,000
下面我们来看一个比较特殊

95
00:02:57,000 --> 00:02:59,000
总比觉得最近也是比较有意思

96
00:02:59,000 --> 00:03:01,000
或者应该是去年年底吧

97
00:03:01,000 --> 00:03:02,000
今年年初

98
00:03:02,000 --> 00:03:03,000
去年年底

99
00:03:03,000 --> 00:03:05,000
现在已经是23年了

100
00:03:06,000 --> 00:03:07,000
有时候我在想

101
00:03:07,000 --> 00:03:09,000
有没有可能你在看这个视频的时候

102
00:03:09,000 --> 00:03:10,000
已经到24 25年的时候

103
00:03:10,000 --> 00:03:11,000
然后你发现

104
00:03:11,000 --> 00:03:13,000
我怎么还在看22年的视频

105
00:03:13,000 --> 00:03:14,000
或者23年的视频

106
00:03:15,000 --> 00:03:18,000
确实这个文章是23年发的

107
00:03:18,000 --> 00:03:20,000
它叫做FlashAttention

108
00:03:20,000 --> 00:03:22,000
它里面就对Attention

109
00:03:22,000 --> 00:03:23,000
做了一个特殊的优化

110
00:03:23,000 --> 00:03:26,000
下面我们看一下具体的图

111
00:03:26,000 --> 00:03:27,000
文章里面铺出来的图

112
00:03:27,000 --> 00:03:29,000
我们简单的去解读一下

113
00:03:29,000 --> 00:03:32,000
像现在大家用的非常多的

114
00:03:32,000 --> 00:03:33,000
Attention或者Transformer的

115
00:03:33,000 --> 00:03:35,000
一些网络模型的层

116
00:03:35,000 --> 00:03:37,000
但是Tension确实很少

117
00:03:37,000 --> 00:03:39,000
在推进引擎里面去应用

118
00:03:39,000 --> 00:03:41,000
确实像Attention成或者Transformer成了

119
00:03:41,000 --> 00:03:43,000
它没有跑得像卷机层

120
00:03:43,000 --> 00:03:44,000
这么快也没有经过

121
00:03:44,000 --> 00:03:46,000
那么多的特殊的优化

122
00:03:46,000 --> 00:03:48,000
那卷机的特殊的优化

123
00:03:48,000 --> 00:03:50,000
我们将会在下一个内容里面

124
00:03:50,000 --> 00:03:51,000
Kernel的运行

125
00:03:51,000 --> 00:03:53,000
或者具体的文态里面

126
00:03:53,000 --> 00:03:54,000
去给大家介绍的

127
00:03:54,000 --> 00:03:55,000
这里面我们看一下

128
00:03:55,000 --> 00:03:56,000
FlashAttention里面

129
00:03:56,000 --> 00:03:58,000
具体做了哪些工作

130
00:03:59,000 --> 00:04:00,000
其实我们知道

131
00:04:00,000 --> 00:04:02,000
在Attention或者Transformer里面

132
00:04:02,000 --> 00:04:06,000
大部分都是算QKB

133
00:04:06,000 --> 00:04:09,000
通过QKB这三个矩阵不断的相乘

134
00:04:09,000 --> 00:04:11,000
就得到我们的Transformer

135
00:04:11,000 --> 00:04:12,000
或者Multi-Attention这个层

136
00:04:12,000 --> 00:04:15,000
接着我们下一个网络模型

137
00:04:15,000 --> 00:04:16,000
就是算SoftMesh

138
00:04:16,000 --> 00:04:19,000
那SoftMesh在这里面就简称SM

139
00:04:19,000 --> 00:04:21,000
不要误解

140
00:04:21,000 --> 00:04:22,000
这里不是

141
00:04:35,000 --> 00:04:38,000
这里面就有一个SM去算QKB

142
00:04:38,000 --> 00:04:40,000
可以看到假设我们AI引擎

143
00:04:40,000 --> 00:04:42,000
会跑在具体的一些芯片里面

144
00:04:42,000 --> 00:04:44,000
具体的一些加速芯片

145
00:04:44,000 --> 00:04:47,000
大部分都不会有太多的一些SM

146
00:04:47,000 --> 00:04:49,000
SM确实里面的容量非常有限

147
00:04:49,000 --> 00:04:51,000
于是我们就会对我们的矩阵

148
00:04:51,000 --> 00:04:53,000
分块来进行计算

149
00:04:53,000 --> 00:04:54,000
那这个Loop

150
00:04:54,000 --> 00:04:56,000
大家可以去看一下

151
00:04:56,000 --> 00:04:58,000
AI编译器里面的有一节内容

152
00:04:58,000 --> 00:04:59,000
就是Kernel的优化

153
00:04:59,000 --> 00:05:01,000
或者后端的优化里面就会讲

154
00:05:01,000 --> 00:05:02,000
为什么要装Loop

155
00:05:02,000 --> 00:05:05,000
然后怎么对这些进行一个切片

156
00:05:05,000 --> 00:05:06,000
这里面我们回到

157
00:05:06,000 --> 00:05:08,000
FlashAttention的一个内容里面

158
00:05:08,000 --> 00:05:10,000
看到我们的K

159
00:05:10,000 --> 00:05:11,000
假设我们就会把一些

160
00:05:11,000 --> 00:05:13,000
取出一小块进行计算

161
00:05:13,000 --> 00:05:14,000
那像这种Q

162
00:05:14,000 --> 00:05:16,000
我们也可以取出一小块进行计算

163
00:05:16,000 --> 00:05:18,000
那计算完之后

164
00:05:18,000 --> 00:05:19,000
我们QKB要相乘

165
00:05:19,000 --> 00:05:20,000
相乘完之后

166
00:05:20,000 --> 00:05:22,000
再给SoftMesh进行一个运行运算的

167
00:05:22,000 --> 00:05:24,000
但是SoftMesh里面

168
00:05:24,000 --> 00:05:25,000
就会把数据摊平

169
00:05:25,000 --> 00:05:27,000
摊成一条进行一个计算

170
00:05:27,000 --> 00:05:29,000
因为SoftMesh是接受一个

171
00:05:29,000 --> 00:05:31,000
vector进行计算的

172
00:05:31,000 --> 00:05:32,000
如果是这样的话

173
00:05:32,000 --> 00:05:34,000
就算得非常慢了

174
00:05:34,000 --> 00:05:36,000
于是作者就在FlashAttention里面

175
00:05:36,000 --> 00:05:37,000
就提到了

176
00:05:37,000 --> 00:05:39,000
我通过滚动的方式

177
00:05:39,000 --> 00:05:41,000
去计算我的SoftMesh

178
00:05:41,000 --> 00:05:42,000
去计算这个SM

179
00:05:42,000 --> 00:05:45,000
使得我的速度就进一步去提升

180
00:05:45,000 --> 00:05:47,000
算完一块QKB

181
00:05:47,000 --> 00:05:49,000
再给到SoftMesh的结果进行重排

182
00:05:49,000 --> 00:05:52,000
通过这种新颖的计算方式

183
00:05:52,000 --> 00:05:53,000
使得我们的Attention

184
00:05:53,000 --> 00:05:55,000
在GPT-2里面

185
00:05:55,000 --> 00:05:58,000
有了接近7倍的提升了

186
00:05:58,000 --> 00:05:59,000
这个可不得了

187
00:05:59,000 --> 00:06:00,000
大家要知道

188
00:06:00,000 --> 00:06:02,000
训练一个GPT-3的时间

189
00:06:02,000 --> 00:06:04,000
要80多天

190
00:06:04,000 --> 00:06:06,000
80多天128个GPU

191
00:06:06,000 --> 00:06:08,000
有多少人有多少人

192
00:06:08,000 --> 00:06:10,000
有资源去算这个的

193
00:06:10,000 --> 00:06:11,000
基本上如果我不做一些

194
00:06:11,000 --> 00:06:12,000
大模型的项目

195
00:06:12,000 --> 00:06:14,000
却拿不到这么多资源的

196
00:06:14,000 --> 00:06:15,000
所以说一般来说

197
00:06:15,000 --> 00:06:16,000
训练这个模型

198
00:06:16,000 --> 00:06:17,000
或者全磁网的模型

199
00:06:17,000 --> 00:06:18,000
特别特别慢

200
00:06:18,000 --> 00:06:19,000
但是有了FlashAttention之后

201
00:06:19,000 --> 00:06:20,000
我们就可以把

202
00:06:20,000 --> 00:06:22,000
真正的Attention的推理

203
00:06:22,000 --> 00:06:23,000
MultiAttention的推理

204
00:06:23,000 --> 00:06:25,000
变成现实

205
00:06:26,000 --> 00:06:27,000
如果我讲得不清楚

206
00:06:27,000 --> 00:06:28,000
也非常欢迎大家

207
00:06:28,000 --> 00:06:30,000
去翻一翻这篇论文

208
00:06:30,000 --> 00:06:31,000
这篇论文里面

209
00:06:31,000 --> 00:06:32,000
有非常多的公式

210
00:06:32,000 --> 00:06:33,000
里面给到的一个附录

211
00:06:33,000 --> 00:06:35,000
也是非常多

212
00:06:35,000 --> 00:06:36,000
下面我们来到

213
00:06:36,000 --> 00:06:39,000
计算图优化的第三个部分

214
00:06:39,000 --> 00:06:42,000
计算图优化详解

215
00:06:42,000 --> 00:06:43,000
那在这里面

216
00:06:43,000 --> 00:06:45,000
我还是非常推荐

217
00:06:45,000 --> 00:06:47,000
大家去看一看这个内容

218
00:06:47,000 --> 00:06:48,000
为什么呢

219
00:06:48,000 --> 00:06:49,000
因为在第三个部分

220
00:06:49,000 --> 00:06:51,000
更多的是对Layout跟Memory

221
00:06:51,000 --> 00:06:53,000
的一些优化

222
00:06:53,000 --> 00:06:54,000
可以看到

223
00:06:54,000 --> 00:06:55,000
我们Layout的优化

224
00:06:55,000 --> 00:06:57,000
就是我们的数据布局的优化

225
00:06:57,000 --> 00:06:58,000
在数据布局里面

226
00:06:58,000 --> 00:07:00,000
确实讲了非常多

227
00:07:00,000 --> 00:07:02,000
从NZHW到NHWC

228
00:07:02,000 --> 00:07:04,000
再到华为自己推出的

229
00:07:04,000 --> 00:07:06,000
NCHWC0

230
00:07:06,000 --> 00:07:08,000
这种方式确实很特别

231
00:07:08,000 --> 00:07:09,000
可以看到

232
00:07:09,000 --> 00:07:10,000
不同的算子的层

233
00:07:10,000 --> 00:07:11,000
或者不同的Customer

234
00:07:11,000 --> 00:07:12,000
我们需要做一个

235
00:07:12,000 --> 00:07:13,000
Customer Data的转换

236
00:07:13,000 --> 00:07:14,000
针对网络模型

237
00:07:14,000 --> 00:07:16,000
上一层跟下层的算子的相同

238
00:07:16,000 --> 00:07:17,000
可能不需要转换

239
00:07:17,000 --> 00:07:18,000
但上一层输入

240
00:07:18,000 --> 00:07:20,000
跟下层输入不同的时候

241
00:07:20,000 --> 00:07:21,000
我们就需要进行一个

242
00:07:21,000 --> 00:07:23,000
插入具体的算子

243
00:07:23,000 --> 00:07:25,000
这也是在我们图优化里面去做的

244
00:07:25,000 --> 00:07:26,000
如果是相同的时候

245
00:07:26,000 --> 00:07:28,000
我们就要删掉一些算子

246
00:07:28,000 --> 00:07:30,000
所以说这里面的研究

247
00:07:30,000 --> 00:07:31,000
要根据我们的计算图

248
00:07:31,000 --> 00:07:33,000
来进行优化

249
00:07:33,000 --> 00:07:34,000
第二个内容

250
00:07:34,000 --> 00:07:37,000
就是内存分配的算法

251
00:07:37,000 --> 00:07:39,000
确实内存分配

252
00:07:39,000 --> 00:07:40,000
要在图

253
00:07:40,000 --> 00:07:42,000
有图的概念进行一个预分配

254
00:07:42,000 --> 00:07:44,000
分配的方式有两个

255
00:07:44,000 --> 00:07:47,000
一个是Impress Operation

256
00:07:47,000 --> 00:07:50,000
就像我们下面右下角这个图

257
00:07:50,000 --> 00:07:53,000
假设我计算完这个算子之后

258
00:07:53,000 --> 00:07:54,000
这块内存

259
00:07:54,000 --> 00:07:56,000
黄色橙色的这块内存

260
00:07:56,000 --> 00:07:57,000
已经不需要了

261
00:07:57,000 --> 00:07:58,000
而且下一个操作

262
00:07:58,000 --> 00:07:59,000
也是Element Wise的

263
00:07:59,000 --> 00:08:01,000
就跟它的内存大小是一样的

264
00:08:01,000 --> 00:08:04,000
就我直接覆盖掉原来的内存

265
00:08:04,000 --> 00:08:06,000
进行一个原地的替换

266
00:08:06,000 --> 00:08:08,000
然后就不用打开新的空间了

267
00:08:08,000 --> 00:08:09,000
这需要根据我们的图

268
00:08:09,000 --> 00:08:10,000
来进行优化的

269
00:08:10,000 --> 00:08:13,000
第二种就是Memory Sharing

270
00:08:13,000 --> 00:08:16,000
Memory Sharing里面就很特别的

271
00:08:16,000 --> 00:08:18,000
就是你不能原地的覆盖

272
00:08:18,000 --> 00:08:19,000
但是我可以

273
00:08:19,000 --> 00:08:20,000
如果你这个数据

274
00:08:20,000 --> 00:08:22,000
就算完这个softmesh之后

275
00:08:22,000 --> 00:08:24,000
我这个数据暂时已经不用了

276
00:08:24,000 --> 00:08:25,000
我算下一个的时候

277
00:08:25,000 --> 00:08:27,000
确实为了节省我的计算的空间

278
00:08:27,000 --> 00:08:29,000
或者节省我们的内存

279
00:08:29,000 --> 00:08:30,000
这个时候我们就可以

280
00:08:30,000 --> 00:08:33,000
共享我们的一些内存空间

281
00:08:33,000 --> 00:08:34,000
在算这个算子的时候

282
00:08:34,000 --> 00:08:36,000
就覆盖掉原来的一些就可以了

283
00:08:36,000 --> 00:08:38,000
然后对它进行更新

284
00:08:38,000 --> 00:08:39,000
然后从这里面去取

285
00:08:39,000 --> 00:08:42,000
从红色的这个内存块里面去预取

286
00:08:42,000 --> 00:08:45,000
这种就是两个数据大小相同

287
00:08:45,000 --> 00:08:46,000
而且前一个数据

288
00:08:46,000 --> 00:08:48,000
参与计算后面的数据不需要

289
00:08:48,000 --> 00:08:51,000
那我们就后面的就可以覆盖掉了

290
00:08:51,000 --> 00:08:53,000
所以内存优化有这几种方式

291
00:08:53,000 --> 00:08:54,000
更详细的

292
00:08:54,000 --> 00:08:55,000
我们还是在内存分配

293
00:08:55,000 --> 00:08:56,000
这个内容里面

294
00:08:56,000 --> 00:08:58,000
给大家已经详细的介绍了

295
00:08:58,000 --> 00:09:00,000
这里面简单的回顾一下

296
00:09:02,000 --> 00:09:05,000
今天的内容就到这里为止

297
00:09:05,000 --> 00:09:07,000
我们在推进引擎架构里面回顾一下

298
00:09:07,000 --> 00:09:10,000
整个推进引擎架构主要分开两部分

299
00:09:10,000 --> 00:09:12,000
第一部分就是上面的

300
00:09:12,000 --> 00:09:15,000
我们的IR这层始皇室的以上

301
00:09:15,000 --> 00:09:17,000
这一部分确实换了

302
00:09:17,000 --> 00:09:19,000
金黄色以上的

303
00:09:26,000 --> 00:09:28,000
就是我们的一个蓝色的模块

304
00:09:28,000 --> 00:09:30,000
就是我们的脱机转换模块

305
00:09:30,000 --> 00:09:31,000
那我们脱机转换模块了

306
00:09:31,000 --> 00:09:33,000
有脱机的优化压缩了

307
00:09:33,000 --> 00:09:35,000
还有脱机的转换图优化

308
00:09:35,000 --> 00:09:37,000
首先先把我们从不同AI框架训练

309
00:09:37,000 --> 00:09:39,000
得到的网络模型

310
00:09:39,000 --> 00:09:40,000
转换成为自己的IR

311
00:09:40,000 --> 00:09:41,000
然后基于这个IR

312
00:09:41,000 --> 00:09:44,000
可以做很多不同的计算图的优化

313
00:09:44,000 --> 00:09:46,000
接下来我们在下一个环节里面

314
00:09:46,000 --> 00:09:47,000
将会去给大家讲讲

315
00:09:47,000 --> 00:09:48,000
运行的优化

316
00:09:48,000 --> 00:09:49,000
Runtime的优化

317
00:09:49,000 --> 00:09:50,000
还有kernel的优化

318
00:09:50,000 --> 00:09:51,000
那下一期再见

319
00:09:51,000 --> 00:09:52,000
拜拜

320
00:09:53,000 --> 00:09:55,000
卷的不行了

321
00:09:55,000 --> 00:09:57,000
记得一键三连加关注哦

322
00:09:57,000 --> 00:09:59,000
所有的内容都会开源在

323
00:09:59,000 --> 00:10:00,000
下面这条链接里面

324
00:10:00,000 --> 00:10:02,000
掰了个掰

