0
0:00:00.000 --> 0:00:02.000
快乐的时光过得特别快

1
0:00:02.000 --> 0:00:04.000
又是时候说拜拜

2
0:00:04.000 --> 0:00:06.000
欢乐的时光过得特别快

3
0:00:06.000 --> 0:00:08.000
又是时候说拜拜

4
0:00:08.000 --> 0:00:09.000
我是ZOMI

5
0:00:09.000 --> 0:00:12.000
今天我们来到推定型的模型优化

6
0:00:12.000 --> 0:00:15.000
计算机优化里面的最后一个内容了

7
0:00:15.000 --> 0:00:18.000
所以我今天的内容会排得特别的密

8
0:00:18.000 --> 0:00:21.000
然后我也会讲得特别的快了一点点

9
0:00:21.000 --> 0:00:23.000
首先我们今天主要是去讲讲

10
0:00:23.000 --> 0:00:24.000
我们X10的内容

11
0:00:24.000 --> 0:00:26.000
因为我们今天要讲的内容

12
0:00:26.000 --> 0:00:28.000
其实是一个很简单的内容

13
0:00:28.000 --> 0:00:29.000
我们今天主要是去讲讲

14
0:00:29.000 --> 0:00:30.000
我们X10的一些优化

15
0:00:30.000 --> 0:00:33.000
还有Layer跟Memory的一些优化

16
0:00:33.000 --> 0:00:35.000
其实这些很多的相关的知识

17
0:00:35.000 --> 0:00:37.000
我们简单的理解一下就好了

18
0:00:37.000 --> 0:00:39.000
更多的是希望各个同事

19
0:00:39.000 --> 0:00:40.000
能够在工作档当中

20
0:00:40.000 --> 0:00:41.000
跟我一起去发现

21
0:00:41.000 --> 0:00:42.000
更多不同的可能性

22
0:00:42.000 --> 0:00:44.000
或者更多优化的方式

23
0:00:44.000 --> 0:00:46.000
可以看到在整个工作流程当中

24
0:00:46.000 --> 0:00:49.000
我们现在来到了第二个和第三个步骤里面

25
0:00:49.000 --> 0:00:51.000
当然他们有一个明确的界限之分

26
0:00:51.000 --> 0:00:53.000
基本上都可以通用的

27
0:00:53.000 --> 0:00:55.000
而Path的管理也是非常重要

28
0:00:55.000 --> 0:00:57.000
在这里面我推荐大家去看看

29
0:00:57.000 --> 0:01:00.000
AI编辑里面的前端优化里面相关的内容

30
0:01:00.000 --> 0:01:01.000
虽然大家可以看到

31
0:01:01.000 --> 0:01:05.000
前端优化AI编辑看了确实太少了

32
0:01:05.000 --> 0:01:07.000
你甚至就我自己不断的点来点去

33
0:01:07.000 --> 0:01:08.000
换了不同电脑

34
0:01:08.000 --> 0:01:09.000
可能在有时候出差的时候

35
0:01:09.000 --> 0:01:10.000
在北京点了

36
0:01:10.000 --> 0:01:11.000
有时候在深圳点了

37
0:01:11.000 --> 0:01:12.000
确实点来点去

38
0:01:12.000 --> 0:01:14.000
特别多了已经点的变成

39
0:01:14.000 --> 0:01:15.000
不可能

40
0:01:16.000 --> 0:01:17.000
绝对不可能

41
0:01:17.000 --> 0:01:21.000
现在我们来到计算图详解的第二个内容

42
0:01:21.000 --> 0:01:23.000
第二个内容就是其他图的优化

43
0:01:23.000 --> 0:01:25.000
可以看到其他图的优化了很多

44
0:01:25.000 --> 0:01:27.000
就是针对不同的AI框架

45
0:01:27.000 --> 0:01:28.000
他可能没有直接的实现

46
0:01:29.000 --> 0:01:31.000
而是通过一些特殊的一些组合

47
0:01:31.000 --> 0:01:33.000
更多的是跟硬件或者特殊的网络模型

48
0:01:33.000 --> 0:01:35.000
或者特殊的领域相关的

49
0:01:35.000 --> 0:01:36.000
我们看几个

50
0:01:36.000 --> 0:01:38.000
就是假设是Layer Lump的融合

51
0:01:38.000 --> 0:01:40.000
还有Pivot Loop的替换

52
0:01:40.000 --> 0:01:41.000
Metamount的转变

53
0:01:41.000 --> 0:01:43.000
还有Binary跟Element Wise

54
0:01:43.000 --> 0:01:45.000
最后还有Videosum

55
0:01:45.000 --> 0:01:46.000
还有Global Pooling

56
0:01:46.000 --> 0:01:47.000
所以确实特别多

57
0:01:47.000 --> 0:01:49.000
我们看看具体的图

58
0:01:49.000 --> 0:01:52.000
这里面就一个一个的去给大家展开了

59
0:01:52.000 --> 0:01:55.000
实际上像我们有些算子

60
0:01:55.000 --> 0:01:56.000
假设VLOOP加Mod

61
0:01:56.000 --> 0:01:57.000
然后加Element Wise Sum

62
0:01:57.000 --> 0:01:58.000
然后加VLOOP这种方式

63
0:01:58.000 --> 0:02:00.000
如果我们遇到这种图

64
0:02:00.000 --> 0:02:02.000
直接变成一个Pivot Loop就行了

65
0:02:02.000 --> 0:02:03.000
当然了

66
0:02:03.000 --> 0:02:04.000
如果我们的引擎里面

67
0:02:04.000 --> 0:02:06.000
推进引擎里面的没有Pivot Loop这个算子

68
0:02:06.000 --> 0:02:08.000
我们也可以把它拆分成

69
0:02:08.000 --> 0:02:10.000
这堆算子的一个具体的实现

70
0:02:10.000 --> 0:02:12.000
就可以代替掉Pivot Loop了

71
0:02:12.000 --> 0:02:14.000
所以说这里面没有一个统一的界限

72
0:02:14.000 --> 0:02:18.000
更多的是根据我们具体底层有什么算子

73
0:02:18.000 --> 0:02:20.000
我们就提供什么优化的Path

74
0:02:20.000 --> 0:02:21.000
而不是优化的Path

75
0:02:21.000 --> 0:02:22.000
我随便写

76
0:02:22.000 --> 0:02:23.000
你写推进引擎的人

77
0:02:23.000 --> 0:02:24.000
写Kernel的人

78
0:02:24.000 --> 0:02:25.000
你就随便自己写

79
0:02:25.000 --> 0:02:28.000
我们是有一个非常良好的合作关系

80
0:02:28.000 --> 0:02:29.000
才能够把整个推进引擎

81
0:02:29.000 --> 0:02:31.000
做到极致性能的优化

82
0:02:31.000 --> 0:02:34.000
下面像我们可以看到MapMod

83
0:02:34.000 --> 0:02:35.000
它确实有两个Transport

84
0:02:35.000 --> 0:02:38.000
但实际上有些推进引擎里面

85
0:02:38.000 --> 0:02:39.000
它就已经把数的数据

86
0:02:39.000 --> 0:02:41.000
自动的做了一个Transport

87
0:02:41.000 --> 0:02:43.000
这个时候我们可以减少一个数据的

88
0:02:43.000 --> 0:02:45.000
搬运的过程当中

89
0:02:45.000 --> 0:02:47.000
但我们还有很多像这种BinwayMod

90
0:02:47.000 --> 0:02:48.000
还有BinwayAdd

91
0:02:48.000 --> 0:02:51.000
就可以换成EliminizeSum

92
0:02:51.000 --> 0:02:53.000
这种特殊的方式

93
0:02:54.000 --> 0:02:57.000
下面我们来看一个比较特殊

94
0:02:57.000 --> 0:02:59.000
总比觉得最近也是比较有意思

95
0:02:59.000 --> 0:03:01.000
或者应该是去年年底吧

96
0:03:01.000 --> 0:03:02.000
今年年初

97
0:03:02.000 --> 0:03:03.000
去年年底

98
0:03:03.000 --> 0:03:05.000
现在已经是23年了

99
0:03:06.000 --> 0:03:07.000
有时候我在想

100
0:03:07.000 --> 0:03:09.000
有没有可能你在看这个视频的时候

101
0:03:09.000 --> 0:03:10.000
已经到24 25年的时候

102
0:03:10.000 --> 0:03:11.000
然后你发现

103
0:03:11.000 --> 0:03:13.000
我怎么还在看22年的视频

104
0:03:13.000 --> 0:03:14.000
或者23年的视频

105
0:03:15.000 --> 0:03:18.000
确实这个文章是23年发的

106
0:03:18.000 --> 0:03:20.000
它叫做FlashAttention

107
0:03:20.000 --> 0:03:22.000
它里面就对Attention

108
0:03:22.000 --> 0:03:23.000
做了一个特殊的优化

109
0:03:23.000 --> 0:03:26.000
下面我们看一下具体的图

110
0:03:26.000 --> 0:03:27.000
文章里面铺出来的图

111
0:03:27.000 --> 0:03:29.000
我们简单的去解读一下

112
0:03:29.000 --> 0:03:32.000
像现在大家用的非常多的

113
0:03:32.000 --> 0:03:33.000
Attention或者Transformer的

114
0:03:33.000 --> 0:03:35.000
一些网络模型的层

115
0:03:35.000 --> 0:03:37.000
但是Tension确实很少

116
0:03:37.000 --> 0:03:39.000
在推进引擎里面去应用

117
0:03:39.000 --> 0:03:41.000
确实像Attention成或者Transformer成了

118
0:03:41.000 --> 0:03:43.000
它没有跑得像卷机层

119
0:03:43.000 --> 0:03:44.000
这么快也没有经过

120
0:03:44.000 --> 0:03:46.000
那么多的特殊的优化

121
0:03:46.000 --> 0:03:48.000
那卷机的特殊的优化

122
0:03:48.000 --> 0:03:50.000
我们将会在下一个内容里面

123
0:03:50.000 --> 0:03:51.000
Kernel的运行

124
0:03:51.000 --> 0:03:53.000
或者具体的文态里面

125
0:03:53.000 --> 0:03:54.000
去给大家介绍的

126
0:03:54.000 --> 0:03:55.000
这里面我们看一下

127
0:03:55.000 --> 0:03:56.000
FlashAttention里面

128
0:03:56.000 --> 0:03:58.000
具体做了哪些工作

129
0:03:59.000 --> 0:04:00.000
其实我们知道

130
0:04:00.000 --> 0:04:02.000
在Attention或者Transformer里面

131
0:04:02.000 --> 0:04:06.000
大部分都是算QKB

132
0:04:06.000 --> 0:04:09.000
通过QKB这三个矩阵不断的相乘

133
0:04:09.000 --> 0:04:11.000
就得到我们的Transformer

134
0:04:11.000 --> 0:04:12.000
或者Multi-Attention这个层

135
0:04:12.000 --> 0:04:15.000
接着我们下一个网络模型

136
0:04:15.000 --> 0:04:16.000
就是算SoftMesh

137
0:04:16.000 --> 0:04:19.000
那SoftMesh在这里面就简称SM

138
0:04:19.000 --> 0:04:21.000
不要误解

139
0:04:21.000 --> 0:04:22.000
这里不是

140
0:04:35.000 --> 0:04:38.000
这里面就有一个SM去算QKB

141
0:04:38.000 --> 0:04:40.000
可以看到假设我们AI引擎

142
0:04:40.000 --> 0:04:42.000
会跑在具体的一些芯片里面

143
0:04:42.000 --> 0:04:44.000
具体的一些加速芯片

144
0:04:44.000 --> 0:04:47.000
大部分都不会有太多的一些SM

145
0:04:47.000 --> 0:04:49.000
SM确实里面的容量非常有限

146
0:04:49.000 --> 0:04:51.000
于是我们就会对我们的矩阵

147
0:04:51.000 --> 0:04:53.000
分块来进行计算

148
0:04:53.000 --> 0:04:54.000
那这个Loop

149
0:04:54.000 --> 0:04:56.000
大家可以去看一下

150
0:04:56.000 --> 0:04:58.000
AI编译器里面的有一节内容

151
0:04:58.000 --> 0:04:59.000
就是Kernel的优化

152
0:04:59.000 --> 0:05:01.000
或者后端的优化里面就会讲

153
0:05:01.000 --> 0:05:02.000
为什么要装Loop

154
0:05:02.000 --> 0:05:05.000
然后怎么对这些进行一个切片

155
0:05:05.000 --> 0:05:06.000
这里面我们回到

156
0:05:06.000 --> 0:05:08.000
FlashAttention的一个内容里面

157
0:05:08.000 --> 0:05:10.000
看到我们的K

158
0:05:10.000 --> 0:05:11.000
假设我们就会把一些

159
0:05:11.000 --> 0:05:13.000
取出一小块进行计算

160
0:05:13.000 --> 0:05:14.000
那像这种Q

161
0:05:14.000 --> 0:05:16.000
我们也可以取出一小块进行计算

162
0:05:16.000 --> 0:05:18.000
那计算完之后

163
0:05:18.000 --> 0:05:19.000
我们QKB要相乘

164
0:05:19.000 --> 0:05:20.000
相乘完之后

165
0:05:20.000 --> 0:05:22.000
再给SoftMesh进行一个运行运算的

166
0:05:22.000 --> 0:05:24.000
但是SoftMesh里面

167
0:05:24.000 --> 0:05:25.000
就会把数据摊平

168
0:05:25.000 --> 0:05:27.000
摊成一条进行一个计算

169
0:05:27.000 --> 0:05:29.000
因为SoftMesh是接受一个

170
0:05:29.000 --> 0:05:31.000
vector进行计算的

171
0:05:31.000 --> 0:05:32.000
如果是这样的话

172
0:05:32.000 --> 0:05:34.000
就算得非常慢了

173
0:05:34.000 --> 0:05:36.000
于是作者就在FlashAttention里面

174
0:05:36.000 --> 0:05:37.000
就提到了

175
0:05:37.000 --> 0:05:39.000
我通过滚动的方式

176
0:05:39.000 --> 0:05:41.000
去计算我的SoftMesh

177
0:05:41.000 --> 0:05:42.000
去计算这个SM

178
0:05:42.000 --> 0:05:45.000
使得我的速度就进一步去提升

179
0:05:45.000 --> 0:05:47.000
算完一块QKB

180
0:05:47.000 --> 0:05:49.000
再给到SoftMesh的结果进行重排

181
0:05:49.000 --> 0:05:52.000
通过这种新颖的计算方式

182
0:05:52.000 --> 0:05:53.000
使得我们的Attention

183
0:05:53.000 --> 0:05:55.000
在GPT-2里面

184
0:05:55.000 --> 0:05:58.000
有了接近7倍的提升了

185
0:05:58.000 --> 0:05:59.000
这个可不得了

186
0:05:59.000 --> 0:06:00.000
大家要知道

187
0:06:00.000 --> 0:06:02.000
训练一个GPT-3的时间

188
0:06:02.000 --> 0:06:04.000
要80多天

189
0:06:04.000 --> 0:06:06.000
80多天128个GPU

190
0:06:06.000 --> 0:06:08.000
有多少人有多少人

191
0:06:08.000 --> 0:06:10.000
有资源去算这个的

192
0:06:10.000 --> 0:06:11.000
基本上如果我不做一些

193
0:06:11.000 --> 0:06:12.000
大模型的项目

194
0:06:12.000 --> 0:06:14.000
却拿不到这么多资源的

195
0:06:14.000 --> 0:06:15.000
所以说一般来说

196
0:06:15.000 --> 0:06:16.000
训练这个模型

197
0:06:16.000 --> 0:06:17.000
或者全磁网的模型

198
0:06:17.000 --> 0:06:18.000
特别特别慢

199
0:06:18.000 --> 0:06:19.000
但是有了FlashAttention之后

200
0:06:19.000 --> 0:06:20.000
我们就可以把

201
0:06:20.000 --> 0:06:22.000
真正的Attention的推理

202
0:06:22.000 --> 0:06:23.000
MultiAttention的推理

203
0:06:23.000 --> 0:06:25.000
变成现实

204
0:06:26.000 --> 0:06:27.000
如果我讲得不清楚

205
0:06:27.000 --> 0:06:28.000
也非常欢迎大家

206
0:06:28.000 --> 0:06:30.000
去翻一翻这篇论文

207
0:06:30.000 --> 0:06:31.000
这篇论文里面

208
0:06:31.000 --> 0:06:32.000
有非常多的公式

209
0:06:32.000 --> 0:06:33.000
里面给到的一个附录

210
0:06:33.000 --> 0:06:35.000
也是非常多

211
0:06:35.000 --> 0:06:36.000
下面我们来到

212
0:06:36.000 --> 0:06:39.000
计算图优化的第三个部分

213
0:06:39.000 --> 0:06:42.000
计算图优化详解

214
0:06:42.000 --> 0:06:43.000
那在这里面

215
0:06:43.000 --> 0:06:45.000
我还是非常推荐

216
0:06:45.000 --> 0:06:47.000
大家去看一看这个内容

217
0:06:47.000 --> 0:06:48.000
为什么呢

218
0:06:48.000 --> 0:06:49.000
因为在第三个部分

219
0:06:49.000 --> 0:06:51.000
更多的是对Layout跟Memory

220
0:06:51.000 --> 0:06:53.000
的一些优化

221
0:06:53.000 --> 0:06:54.000
可以看到

222
0:06:54.000 --> 0:06:55.000
我们Layout的优化

223
0:06:55.000 --> 0:06:57.000
就是我们的数据布局的优化

224
0:06:57.000 --> 0:06:58.000
在数据布局里面

225
0:06:58.000 --> 0:07:00.000
确实讲了非常多

226
0:07:00.000 --> 0:07:02.000
从NZHW到NHWC

227
0:07:02.000 --> 0:07:04.000
再到华为自己推出的

228
0:07:04.000 --> 0:07:06.000
NCHWC0

229
0:07:06.000 --> 0:07:08.000
这种方式确实很特别

230
0:07:08.000 --> 0:07:09.000
可以看到

231
0:07:09.000 --> 0:07:10.000
不同的算子的层

232
0:07:10.000 --> 0:07:11.000
或者不同的Customer

233
0:07:11.000 --> 0:07:12.000
我们需要做一个

234
0:07:12.000 --> 0:07:13.000
Customer Data的转换

235
0:07:13.000 --> 0:07:14.000
针对网络模型

236
0:07:14.000 --> 0:07:16.000
上一层跟下层的算子的相同

237
0:07:16.000 --> 0:07:17.000
可能不需要转换

238
0:07:17.000 --> 0:07:18.000
但上一层输入

239
0:07:18.000 --> 0:07:20.000
跟下层输入不同的时候

240
0:07:20.000 --> 0:07:21.000
我们就需要进行一个

241
0:07:21.000 --> 0:07:23.000
插入具体的算子

242
0:07:23.000 --> 0:07:25.000
这也是在我们图优化里面去做的

243
0:07:25.000 --> 0:07:26.000
如果是相同的时候

244
0:07:26.000 --> 0:07:28.000
我们就要删掉一些算子

245
0:07:28.000 --> 0:07:30.000
所以说这里面的研究

246
0:07:30.000 --> 0:07:31.000
要根据我们的计算图

247
0:07:31.000 --> 0:07:33.000
来进行优化

248
0:07:33.000 --> 0:07:34.000
第二个内容

249
0:07:34.000 --> 0:07:37.000
就是内存分配的算法

250
0:07:37.000 --> 0:07:39.000
确实内存分配

251
0:07:39.000 --> 0:07:40.000
要在图

252
0:07:40.000 --> 0:07:42.000
有图的概念进行一个预分配

253
0:07:42.000 --> 0:07:44.000
分配的方式有两个

254
0:07:44.000 --> 0:07:47.000
一个是Impress Operation

255
0:07:47.000 --> 0:07:50.000
就像我们下面右下角这个图

256
0:07:50.000 --> 0:07:53.000
假设我计算完这个算子之后

257
0:07:53.000 --> 0:07:54.000
这块内存

258
0:07:54.000 --> 0:07:56.000
黄色橙色的这块内存

259
0:07:56.000 --> 0:07:57.000
已经不需要了

260
0:07:57.000 --> 0:07:58.000
而且下一个操作

261
0:07:58.000 --> 0:07:59.000
也是Element Wise的

262
0:07:59.000 --> 0:08:01.000
就跟它的内存大小是一样的

263
0:08:01.000 --> 0:08:04.000
就我直接覆盖掉原来的内存

264
0:08:04.000 --> 0:08:06.000
进行一个原地的替换

265
0:08:06.000 --> 0:08:08.000
然后就不用打开新的空间了

266
0:08:08.000 --> 0:08:09.000
这需要根据我们的图

267
0:08:09.000 --> 0:08:10.000
来进行优化的

268
0:08:10.000 --> 0:08:13.000
第二种就是Memory Sharing

269
0:08:13.000 --> 0:08:16.000
Memory Sharing里面就很特别的

270
0:08:16.000 --> 0:08:18.000
就是你不能原地的覆盖

271
0:08:18.000 --> 0:08:19.000
但是我可以

272
0:08:19.000 --> 0:08:20.000
如果你这个数据

273
0:08:20.000 --> 0:08:22.000
就算完这个softmesh之后

274
0:08:22.000 --> 0:08:24.000
我这个数据暂时已经不用了

275
0:08:24.000 --> 0:08:25.000
我算下一个的时候

276
0:08:25.000 --> 0:08:27.000
确实为了节省我的计算的空间

277
0:08:27.000 --> 0:08:29.000
或者节省我们的内存

278
0:08:29.000 --> 0:08:30.000
这个时候我们就可以

279
0:08:30.000 --> 0:08:33.000
共享我们的一些内存空间

280
0:08:33.000 --> 0:08:34.000
在算这个算子的时候

281
0:08:34.000 --> 0:08:36.000
就覆盖掉原来的一些就可以了

282
0:08:36.000 --> 0:08:38.000
然后对它进行更新

283
0:08:38.000 --> 0:08:39.000
然后从这里面去取

284
0:08:39.000 --> 0:08:42.000
从红色的这个内存块里面去预取

285
0:08:42.000 --> 0:08:45.000
这种就是两个数据大小相同

286
0:08:45.000 --> 0:08:46.000
而且前一个数据

287
0:08:46.000 --> 0:08:48.000
参与计算后面的数据不需要

288
0:08:48.000 --> 0:08:51.000
那我们就后面的就可以覆盖掉了

289
0:08:51.000 --> 0:08:53.000
所以内存优化有这几种方式

290
0:08:53.000 --> 0:08:54.000
更详细的

291
0:08:54.000 --> 0:08:55.000
我们还是在内存分配

292
0:08:55.000 --> 0:08:56.000
这个内容里面

293
0:08:56.000 --> 0:08:58.000
给大家已经详细的介绍了

294
0:08:58.000 --> 0:09:00.000
这里面简单的回顾一下

295
0:09:02.000 --> 0:09:05.000
今天的内容就到这里为止

296
0:09:05.000 --> 0:09:07.000
我们在推进引擎架构里面回顾一下

297
0:09:07.000 --> 0:09:10.000
整个推进引擎架构主要分开两部分

298
0:09:10.000 --> 0:09:12.000
第一部分就是上面的

299
0:09:12.000 --> 0:09:15.000
我们的IR这层始皇室的以上

300
0:09:15.000 --> 0:09:17.000
这一部分确实换了

301
0:09:17.000 --> 0:09:19.000
金黄色以上的

302
0:09:26.000 --> 0:09:28.000
就是我们的一个蓝色的模块

303
0:09:28.000 --> 0:09:30.000
就是我们的脱机转换模块

304
0:09:30.000 --> 0:09:31.000
那我们脱机转换模块了

305
0:09:31.000 --> 0:09:33.000
有脱机的优化压缩了

306
0:09:33.000 --> 0:09:35.000
还有脱机的转换图优化

307
0:09:35.000 --> 0:09:37.000
首先先把我们从不同AI框架训练

308
0:09:37.000 --> 0:09:39.000
得到的网络模型

309
0:09:39.000 --> 0:09:40.000
转换成为自己的IR

310
0:09:40.000 --> 0:09:41.000
然后基于这个IR

311
0:09:41.000 --> 0:09:44.000
可以做很多不同的计算图的优化

312
0:09:44.000 --> 0:09:46.000
接下来我们在下一个环节里面

313
0:09:46.000 --> 0:09:47.000
将会去给大家讲讲

314
0:09:47.000 --> 0:09:48.000
运行的优化

315
0:09:48.000 --> 0:09:49.000
Runtime的优化

316
0:09:49.000 --> 0:09:50.000
还有kernel的优化

317
0:09:50.000 --> 0:09:51.000
那下一期再见

318
0:09:51.000 --> 0:09:52.000
拜拜

319
0:09:53.000 --> 0:09:55.000
卷的不行了

320
0:09:55.000 --> 0:09:57.000
记得一键三连加关注哦

321
0:09:57.000 --> 0:09:59.000
所有的内容都会开源在

322
0:09:59.000 --> 0:10:00.000
下面这条链接里面

323
0:10:00.000 --> 0:10:02.000
掰了个掰

