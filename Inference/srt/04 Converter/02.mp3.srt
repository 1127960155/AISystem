0
0:00:00.000 --> 0:00:07.560
Hello,大家好,我是宗米

1
0:00:07.560 --> 0:00:12.080
这里面宗米两个英文单词都是大写

2
0:00:12.080 --> 0:00:15.240
因为它是我中文名字的一个缩写

3
0:00:15.240 --> 0:00:20.600
今天我要给大家汇报的还是模型转换和优化这个内容里面

4
0:00:20.600 --> 0:00:23.480
在模型转换技术这个内容里面

5
0:00:23.480 --> 0:00:26.200
后面的更新频率应该会越来越慢

6
0:00:26.200 --> 0:00:27.200
因为很多知识

7
0:00:27.680 --> 0:00:32.320
其实在网上很难搜得到更多的是工程化的一些经验的总结

8
0:00:33.720 --> 0:00:36.880
这里面应该会分开两个视频给大家介绍的

9
0:00:36.880 --> 0:00:39.840
我们可以看一下主要有很多内容

10
0:00:40.400 --> 0:00:44.880
首先在第一个视频我们更多的是聚焦于一些工程理念和知识概念

11
0:00:44.880 --> 0:00:46.480
例如模型转换的挑战

12
0:00:46.480 --> 0:00:49.000
还有整体的架构应该长什幺样子

13
0:00:49.000 --> 0:00:54.640
接着我们去看一看模型的串行化和反串行化的操作

14
0:00:54.800 --> 0:00:58.120
串行化的工作就是把其他AI框架的网络模型

15
0:00:58.120 --> 0:01:00.480
转换成为我们推进行的模型

16
0:01:01.240 --> 0:01:04.520
反串行化就是把已经保存下来的网络模型

17
0:01:04.520 --> 0:01:08.000
加载到我们的内存当中给推进行去执行的

18
0:01:08.000 --> 0:01:11.360
接着我们会去介绍串行化和反串行化当中

19
0:01:11.360 --> 0:01:13.600
用的很多的两种格式

20
0:01:13.600 --> 0:01:15.440
一种是Portal Buffer

21
0:01:15.440 --> 0:01:17.280
一种是Flat Buffer

22
0:01:17.280 --> 0:01:18.600
讲完这两个内容之后

23
0:01:18.840 --> 0:01:20.800
我们将会在下个内容里面

24
0:01:20.800 --> 0:01:24.480
来到一些比较内核的技术或者内核的内容

25
0:01:24.520 --> 0:01:27.000
就是自定义计算图的IR

26
0:01:27.000 --> 0:01:30.840
针对推力引擎的计算图的IR应该怎幺定义

27
0:01:31.360 --> 0:01:34.040
然后第二个内容也是很重要的一块

28
0:01:34.040 --> 0:01:37.000
转换的流程和技术的细节

29
0:01:37.000 --> 0:01:40.440
流程和细节这个就是指导我们怎幺去开发

30
0:01:40.440 --> 0:01:41.680
怎幺去写代码的

31
0:01:43.520 --> 0:01:45.720
下面我们来到第一个正式的内容

32
0:01:45.720 --> 0:01:48.160
转换模块的挑战和架构

33
0:01:48.160 --> 0:01:49.440
其实挑战和架构

34
0:01:49.440 --> 0:01:53.360
我们在上一节里面已经详细的去给大家去汇报过

35
0:01:53.400 --> 0:01:55.640
今天我们简单的去罗列一下

36
0:01:56.800 --> 0:01:59.720
Converter这个模块其实有非常多的挑战

37
0:01:59.720 --> 0:02:03.640
第一个就是AI框架或者AI本身有非常多的模型

38
0:02:03.640 --> 0:02:05.720
而且有非常多的框架

39
0:02:05.720 --> 0:02:07.880
不同的框架有不同的知识格式

40
0:02:07.880 --> 0:02:10.920
而且我们需要支持非常多主流的网络模型

41
0:02:10.920 --> 0:02:12.560
AI发展的越来越多

42
0:02:12.560 --> 0:02:15.080
很多模型都是千奇百怪

43
0:02:15.080 --> 0:02:18.720
最后就是需要支持很多AI特有的一些特性

44
0:02:19.840 --> 0:02:21.560
为了应对上面的这些挑战

45
0:02:21.720 --> 0:02:24.720
所以我们设计了一个转换模块的整体的架构

46
0:02:24.720 --> 0:02:26.640
主要是由Graph Converter

47
0:02:26.640 --> 0:02:29.760
还有Graph Optimizer两个模块来去组成

48
0:02:29.760 --> 0:02:32.360
今天我们主要是围绕着Graph Converter

49
0:02:32.360 --> 0:02:35.240
就是我们的图转换的模块

50
0:02:35.240 --> 0:02:36.640
下面我们可以看一下

51
0:02:36.640 --> 0:02:40.040
主要就是聚焦于上面的这坨内容

52
0:02:40.040 --> 0:02:42.760
每一个AI框架都会有自己的一个Converter

53
0:02:42.760 --> 0:02:46.960
最终都会汇聚成我们自己推理引擎的IR

54
0:02:48.040 --> 0:02:49.720
既然IR很重要

55
0:02:49.720 --> 0:02:52.640
我们看一下整个转换模块的工作流程当中

56
0:02:52.640 --> 0:02:55.640
可以看到左边的很多的不同的框架

57
0:02:55.640 --> 0:02:58.720
最后都会汇聚成自己的一个独立的IR

58
0:02:59.160 --> 0:03:00.680
转换模块的阶段

59
0:03:00.920 --> 0:03:03.680
我们统一了整个计算图的IR

60
0:03:03.880 --> 0:03:05.560
于是在优化模块的时候

61
0:03:05.560 --> 0:03:08.120
我们就可以通过统一的自定义的IR

62
0:03:08.120 --> 0:03:11.080
完成很多不同的计算图的优化的模式

63
0:03:11.080 --> 0:03:12.280
或者优化图的Path

64
0:03:12.280 --> 0:03:15.000
这个就是为什幺我们需要自定义的IR

65
0:03:15.000 --> 0:03:17.560
为什幺需要深入的去给大家讲解

66
0:03:17.560 --> 0:03:19.520
转换模块的作用

67
0:03:20.320 --> 0:03:22.760
下面我们来看一下第二个比较重要的内容

68
0:03:22.760 --> 0:03:26.200
就是模型的串行化和反串行化

69
0:03:26.440 --> 0:03:29.360
首先我们来了解一下模型的串行化的工作

70
0:03:29.600 --> 0:03:31.480
其实串行化很简单

71
0:03:32.240 --> 0:03:34.360
我们把模型在部署的时候

72
0:03:34.360 --> 0:03:36.320
怎幺去把已经训练好的模型

73
0:03:36.320 --> 0:03:38.400
AI框架训练出来的模型

74
0:03:38.400 --> 0:03:40.480
把它存储起来

75
0:03:40.480 --> 0:03:42.280
给后续我们需要File Tuning

76
0:03:42.280 --> 0:03:44.000
或者推理的时候使用的

77
0:03:44.000 --> 0:03:47.240
而反串行化就是把我们刚才保存下来的

78
0:03:47.240 --> 0:03:49.320
网络模型的结构还有权重

79
0:03:49.800 --> 0:03:51.800
反串行到内存当中

80
0:03:51.800 --> 0:03:54.080
内存就变成一个具体的对象

81
0:03:54.080 --> 0:03:56.880
我们看一下下面的图

82
0:03:57.880 --> 0:03:59.280
在AI框架执行阶段

83
0:03:59.400 --> 0:04:01.480
我们写的很多网络模型的代码

84
0:04:01.480 --> 0:04:02.920
还有一些权重参数

85
0:04:03.040 --> 0:04:06.120
其实都变成我们内存的一个对象

86
0:04:06.120 --> 0:04:07.360
我们需要保存下来

87
0:04:07.360 --> 0:04:08.160
把我们的权重

88
0:04:08.160 --> 0:04:09.600
把我们的代码固化下来

89
0:04:09.600 --> 0:04:12.520
变成我们硬盘的一些具体的地址

90
0:04:12.880 --> 0:04:14.000
最后要加载的时候

91
0:04:14.160 --> 0:04:16.080
就变成我们需要反串行化

92
0:04:16.080 --> 0:04:17.640
回去我们的内存对象

93
0:04:17.880 --> 0:04:19.800
这个就是一般的AI框架里面

94
0:04:19.800 --> 0:04:22.080
所使用的一个流程

95
0:04:23.960 --> 0:04:25.920
而在推理引擎也是相同的

96
0:04:25.920 --> 0:04:28.640
左边就是AI框架训练的一个网络模型

97
0:04:28.640 --> 0:04:29.880
我们把它串行化

98
0:04:29.880 --> 0:04:32.000
需要用推理引擎的串行化的API

99
0:04:32.000 --> 0:04:32.960
把它固化下来

100
0:04:32.960 --> 0:04:34.760
成为我们硬盘的数据

101
0:04:35.040 --> 0:04:36.680
在真正推理引擎执行的时候

102
0:04:36.800 --> 0:04:38.080
我们需要把一些数据

103
0:04:38.080 --> 0:04:39.880
反串行化成为内存的对象

104
0:04:40.080 --> 0:04:41.280
最后再去执行

105
0:04:41.280 --> 0:04:42.880
这个就是整体的流程

106
0:04:43.440 --> 0:04:45.680
下面我们来看一下串行化的分类

107
0:04:47.080 --> 0:04:49.240
实际上串行化的格式有很多种

108
0:04:49.240 --> 0:04:49.840
有XML

109
0:04:50.040 --> 0:04:50.360
JSON

110
0:04:50.560 --> 0:04:52.560
还有Portal Buffer和Fat Buffer

111
0:04:52.560 --> 0:04:55.560
而在AI框架或者AI的领域里面

112
0:04:55.560 --> 0:04:58.200
Portal Buffer是用的最为广泛的

113
0:04:58.200 --> 0:04:59.680
我们可以看到下面这个图

114
0:04:59.680 --> 0:05:02.320
谷歌是Portal Buffer的一个发起者

115
0:05:02.320 --> 0:05:04.640
最后我们现在经常用的onix

116
0:05:04.640 --> 0:05:07.160
是Facebook和微软组成一个联盟

117
0:05:07.160 --> 0:05:09.280
一起去支持这种开放性的格式

118
0:05:09.280 --> 0:05:10.000
而另外一方面

119
0:05:10.000 --> 0:05:11.880
我们平时用的苹果很多AI功能

120
0:05:11.880 --> 0:05:14.280
包括Siri用的就是CoreML的格式

121
0:05:14.280 --> 0:05:16.400
而CoreML也是继承于Portal Buffer

122
0:05:16.400 --> 0:05:17.920
进行自己一个魔改

123
0:05:17.920 --> 0:05:19.880
或者自己的一个修改定义的

124
0:05:21.440 --> 0:05:23.200
下面我们以一个简单的例子

125
0:05:23.200 --> 0:05:25.800
去看一下Pytorch的串行化的方式

126
0:05:26.320 --> 0:05:27.440
Pytorch的内部格式

127
0:05:27.640 --> 0:05:30.440
只是存储已经训好的网络模型的状态

128
0:05:30.440 --> 0:05:31.680
所谓的这些状态

129
0:05:31.680 --> 0:05:33.960
主要是包括我们的权重了

130
0:05:33.960 --> 0:05:34.760
偏移了

131
0:05:34.760 --> 0:05:35.960
用了很多的数据

132
0:05:35.960 --> 0:05:37.120
然后我们可以看到

133
0:05:37.120 --> 0:05:38.240
我们的权重了

134
0:05:38.240 --> 0:05:39.080
偏移了

135
0:05:39.080 --> 0:05:40.760
优化器的更新的参数

136
0:05:41.080 --> 0:05:44.440
更多的是对网络模型的权重参数信息

137
0:05:44.440 --> 0:05:46.160
进行加载和保存的

138
0:05:46.440 --> 0:05:49.040
其他参数其实也有非常的多

139
0:05:49.040 --> 0:05:50.680
我们只是不一一列举了

140
0:05:50.680 --> 0:05:51.360
另外的话

141
0:05:51.360 --> 0:05:52.720
像Pytorch的内部格式

142
0:05:52.920 --> 0:05:54.800
非常类似于Python里面的

143
0:05:54.800 --> 0:05:55.800
串行化的方式

144
0:05:55.800 --> 0:05:57.840
直接用Pico来去做的

145
0:05:57.840 --> 0:06:00.800
这个就是Pytorch原生的方式

146
0:06:01.000 --> 0:06:03.360
在代码里面就直接touch.save

147
0:06:03.360 --> 0:06:05.400
然后把我们的网络模型

148
0:06:05.800 --> 0:06:06.480
告诉API

149
0:06:06.480 --> 0:06:08.720
我们要存在哪个地址就可以了

150
0:06:09.400 --> 0:06:10.480
下次加载的时候

151
0:06:10.600 --> 0:06:12.400
直接model.loadstatedist

152
0:06:12.400 --> 0:06:14.280
然后就可以进行一个推理

153
0:06:14.280 --> 0:06:16.480
所以用起来是比较简单的

154
0:06:17.000 --> 0:06:19.800
但是这种方式实在是太naive了

155
0:06:19.800 --> 0:06:21.320
就是非常的原始

156
0:06:21.600 --> 0:06:24.480
它只是保存了网络模型的对应的参数

157
0:06:24.480 --> 0:06:25.680
网络模型的结构

158
0:06:25.680 --> 0:06:27.560
网络模型的信息计算图

159
0:06:27.560 --> 0:06:29.160
这些信息它都没有保存

160
0:06:29.160 --> 0:06:31.080
而是通过代码来去承载

161
0:06:31.080 --> 0:06:32.360
那下面我们来看一下

162
0:06:32.360 --> 0:06:33.440
另外一个方面

163
0:06:33.560 --> 0:06:34.720
就是Pytorch

164
0:06:34.720 --> 0:06:36.720
另外一个串行化的方式

165
0:06:36.720 --> 0:06:37.720
onlix

166
0:06:39.400 --> 0:06:40.800
onlix

167
0:06:40.800 --> 0:06:43.520
大家都知道Pytorch要导到一些推理

168
0:06:43.520 --> 0:06:44.480
情绪计算的时候

169
0:06:44.880 --> 0:06:48.200
一般我们都会把它转成一个onlix的格式

170
0:06:48.200 --> 0:06:49.520
那Pytorch

171
0:06:49.520 --> 0:06:51.000
那Pytorch内部

172
0:06:51.200 --> 0:06:53.520
其实是支持onlix的export的

173
0:06:53.520 --> 0:06:55.120
包括我们现在在升腾

174
0:06:55.120 --> 0:06:57.040
去对接到Pytorch的框架

175
0:06:57.040 --> 0:07:00.360
也是通过onlix的一个接口去实现的

176
0:07:00.360 --> 0:07:03.040
下面确实看到代码很简单

177
0:07:03.040 --> 0:07:05.920
我们前面的都是一些加载网络模型

178
0:07:05.920 --> 0:07:08.760
最重要的就是这条torch-onlix-export

179
0:07:08.760 --> 0:07:10.920
这条语句就告诉我们

180
0:07:11.120 --> 0:07:13.600
需要把Pytorch的一个网络模型

181
0:07:13.880 --> 0:07:16.560
保存为AlexNet.onlix

182
0:07:16.720 --> 0:07:17.920
这里面的保存的信息

183
0:07:18.040 --> 0:07:20.240
就会比Pytorch原生要多很多

184
0:07:20.240 --> 0:07:22.240
除了网络模型的权重偏移

185
0:07:22.240 --> 0:07:23.520
还有优化器的参数

186
0:07:23.520 --> 0:07:25.680
它还会保存网络模型的结构

187
0:07:25.680 --> 0:07:27.360
每一层所使用的算子

188
0:07:27.360 --> 0:07:28.120
tensor的shape

189
0:07:28.120 --> 0:07:29.960
还有很多的额外的信息

190
0:07:29.960 --> 0:07:32.720
那这些就是Pytorch串行化的一个过程

191
0:07:33.040 --> 0:07:38.360
在最后一个内容里面

192
0:07:38.360 --> 0:07:39.800
也是比较长的一个内容

193
0:07:39.800 --> 0:07:42.800
我们来看一下目标文档的格式

194
0:07:42.800 --> 0:07:44.000
这里面用的更多的

195
0:07:44.000 --> 0:07:44.840
在AI领域

196
0:07:44.920 --> 0:07:47.240
更多的是Portal Buffer和Fact Buffer

197
0:07:47.600 --> 0:07:49.880
现在我们来看一下Portal Buffer

198
0:07:49.880 --> 0:07:52.080
其实Portal Buffer它aka叫Portal Buffer

199
0:07:52.480 --> 0:07:55.080
实际上它叫做Portoco Buffer

200
0:07:55.080 --> 0:07:56.280
看一下它的logo

201
0:07:56.280 --> 0:07:57.720
五颜六色的就知道

202
0:07:57.720 --> 0:08:00.040
大部分都是像谷歌的风格

203
0:08:00.320 --> 0:08:03.400
也是谷歌发起的一个开源性的项目

204
0:08:03.400 --> 0:08:06.120
因为它确实有很多特殊的优点

205
0:08:06.120 --> 0:08:07.960
比XML还有JSON要好

206
0:08:07.960 --> 0:08:10.000
所以现在很多AI框架

207
0:08:10.000 --> 0:08:10.680
Tensorflow

208
0:08:10.680 --> 0:08:11.480
MindSpore

209
0:08:11.480 --> 0:08:13.760
Pytorch都是使用Portal Buffer

210
0:08:13.760 --> 0:08:16.480
作为它一个主要的导出的格式

211
0:08:18.480 --> 0:08:19.320
现在我们看一下

212
0:08:19.320 --> 0:08:21.640
Portal Buffer的一个文档的语法

213
0:08:21.640 --> 0:08:23.040
那基本的语法的规则

214
0:08:23.240 --> 0:08:24.960
下面就是一段message

215
0:08:24.960 --> 0:08:26.160
然后就告诉我

216
0:08:26.160 --> 0:08:28.240
这段message属于哪个域

217
0:08:28.280 --> 0:08:30.840
然后在这个域里面加了个花和号

218
0:08:30.840 --> 0:08:32.120
那中间的这两行

219
0:08:32.280 --> 0:08:35.080
就是具体的字段的规则或者内容

220
0:08:35.080 --> 0:08:36.400
具体的中间两行

221
0:08:36.560 --> 0:08:37.840
就是具体的内容了

222
0:08:37.840 --> 0:08:39.840
我们看一下每一行代表什幺意思

223
0:08:40.040 --> 0:08:42.320
首先我们有一个字段的规则

224
0:08:42.320 --> 0:08:44.560
告诉它这个字段是属于哪个范围

225
0:08:44.560 --> 0:08:46.000
然后有个数据类型

226
0:08:46.000 --> 0:08:47.200
有个名称

227
0:08:47.200 --> 0:08:48.040
等于

228
0:08:48.040 --> 0:08:49.400
然后就有一个域值了

229
0:08:49.400 --> 0:08:50.360
等于什幺

230
0:08:50.560 --> 0:08:53.400
这个就是Portal Buffer的一个文档的

231
0:08:53.400 --> 0:08:54.920
最主要的语法规则

232
0:08:55.880 --> 0:08:57.160
下面就是cafe

233
0:08:57.160 --> 0:09:00.040
这个AI框架用Portal Buffer去表示的

234
0:09:00.040 --> 0:09:02.520
那这里面有一个data layer

235
0:09:02.520 --> 0:09:04.760
去声明data layer是怎幺组成的

236
0:09:04.920 --> 0:09:07.560
这种也是Portal Buffer的写的格式

237
0:09:07.560 --> 0:09:10.000
那右边就是卷积层

238
0:09:10.520 --> 0:09:14.080
通过这种方式去表示我们的卷积层

239
0:09:15.640 --> 0:09:17.920
下面我们看一下两个AI框架有什幺区别

240
0:09:18.600 --> 0:09:20.520
像cafe这种早期的AI框架

241
0:09:20.640 --> 0:09:22.000
是使用Portal Buffer的格式

242
0:09:22.120 --> 0:09:24.320
去写我们的网络模型的定义的

243
0:09:24.320 --> 0:09:26.440
而后来TensorFlow确实觉得

244
0:09:26.440 --> 0:09:28.280
大家去写这种网络模型的定义

245
0:09:28.600 --> 0:09:30.920
去写底层的这些Portal Buffer很容易出错

246
0:09:30.920 --> 0:09:33.600
那还不如通过python去封装好

247
0:09:33.600 --> 0:09:36.360
然后给到TFTensorFlow去封装好的API

248
0:09:36.360 --> 0:09:37.520
给用户去调

249
0:09:37.520 --> 0:09:40.000
然后通过简单的去调一些API

250
0:09:40.000 --> 0:09:42.360
就可以把Portal Buffer给调起来

251
0:09:42.360 --> 0:09:43.880
去写我们的网络模型

252
0:09:43.880 --> 0:09:46.160
所以说当时候TensorFlow出来

253
0:09:46.320 --> 0:09:47.720
确实大家觉得

254
0:09:48.120 --> 0:09:51.600
原来AI框架开发AI进程还能这幺玩

255
0:09:51.600 --> 0:09:52.920
在1718年的时候

256
0:09:53.080 --> 0:09:55.960
确实它已经是一个很大的一个创新

257
0:09:56.080 --> 0:09:57.880
不过后来又有了PyTorch

258
0:09:57.880 --> 0:09:59.400
这也是另外一个故事

259
0:09:59.400 --> 0:10:00.920
不在我们今天的主线

260
0:10:03.600 --> 0:10:06.080
现在我们稍微深入的去看一下

261
0:10:06.080 --> 0:10:08.560
Portal Buffer的一个编码的模式

262
0:10:09.280 --> 0:10:10.480
简单的去理解一下

263
0:10:11.720 --> 0:10:12.600
我们的计算机

264
0:10:13.160 --> 0:10:15.920
一般来说它是通过二进制进行编码

265
0:10:15.920 --> 0:10:19.360
那幺就是010101这种方式

266
0:10:20.040 --> 0:10:21.320
它的基数是2

267
0:10:21.320 --> 0:10:23.040
规则就是逢二进一

268
0:10:23.040 --> 0:10:25.680
像int这种类型是由32位去组成

269
0:10:25.800 --> 0:10:27.600
每位的数值就是2的n

270
0:10:27.600 --> 0:10:30.480
次方n就是0的31这个范围

271
0:10:30.480 --> 0:10:31.680
大家可以去翻一翻

272
0:10:31.680 --> 0:10:33.760
计算机原理去了解一下

273
0:10:33.920 --> 0:10:35.760
这里面就不详细的展开

274
0:10:35.760 --> 0:10:37.040
而Portal Buffer这种

275
0:10:37.160 --> 0:10:39.240
采用的是TLV的编码模式

276
0:10:39.440 --> 0:10:40.400
TLV说白了

277
0:10:40.400 --> 0:10:41.400
你可能听不懂

278
0:10:41.400 --> 0:10:42.440
我一开始也听不懂

279
0:10:42.440 --> 0:10:45.040
但是我们把它的详细打印出来

280
0:10:45.040 --> 0:10:46.040
它其实就是Tag

281
0:10:46.560 --> 0:10:47.040
Length

282
0:10:47.280 --> 0:10:49.560
还有Value的模式进行编码

283
0:10:50.120 --> 0:10:53.200
像Tag跟Value它其实是一对的

284
0:10:53.200 --> 0:10:54.040
对应起来的

285
0:10:54.040 --> 0:10:57.040
一个类似于我们经常字典里面的key

286
0:10:57.040 --> 0:10:58.480
一个类似于Value

287
0:10:58.480 --> 0:11:01.200
而Length就代表我们整个Value的长度

288
0:11:01.200 --> 0:11:02.120
我们写Value的时候

289
0:11:02.400 --> 0:11:04.320
有一个长度告诉我们的计算机

290
0:11:04.320 --> 0:11:05.520
我要保存多长

291
0:11:05.520 --> 0:11:07.040
方便我们的地址索引

292
0:11:07.520 --> 0:11:09.520
这个时候我们整个Portal Buffer的

293
0:11:09.520 --> 0:11:10.280
对外的对象

294
0:11:10.600 --> 0:11:12.920
就用一个Message来去描述

295
0:11:12.920 --> 0:11:14.760
这整一个数据结构

296
0:11:14.760 --> 0:11:16.840
或者我们整个的对象结构

297
0:11:17.040 --> 0:11:17.880
通过这种方式

298
0:11:18.040 --> 0:11:20.200
就比较好的进行一个编码

299
0:11:20.200 --> 0:11:22.200
我们看一下它Portal Buffer的编码模式

300
0:11:22.200 --> 0:11:26.120
我们字里面就不详细的去给大家介绍了

301
0:11:26.120 --> 0:11:28.040
因为它会有一个编码的过程

302
0:11:28.040 --> 0:11:29.960
也会有一个解码的过程

303
0:11:30.800 --> 0:11:32.320
通过TLV的编码方式

304
0:11:32.440 --> 0:11:34.840
就把我们内存的对象和内存的数据结构

305
0:11:35.200 --> 0:11:37.080
变成我们硬盘的数据

306
0:11:37.760 --> 0:11:39.880
第二个我们看一下Flat Buffer

307
0:11:39.880 --> 0:11:41.960
可能很多人听过Portal Buffer

308
0:11:41.960 --> 0:11:46.000
但是至少在ZOMI开发Twin引擎的时候

309
0:11:46.120 --> 0:11:48.120
我是真没接触过Flat Buffer

310
0:11:48.120 --> 0:11:49.640
确实用的也比较小

311
0:11:49.640 --> 0:11:52.120
像Portal Buffer用的会更多

312
0:11:52.320 --> 0:11:54.040
我们看一下Flat Buffer

313
0:11:54.040 --> 0:11:57.200
它对比Portal Buffer有一些的主要的优点

314
0:11:57.200 --> 0:11:59.280
所以我们会在Twin引擎里面

315
0:11:59.280 --> 0:12:01.280
大量的去用到Flat Buffer

316
0:12:01.280 --> 0:12:03.600
我们后面会去讲讲有哪些用到

317
0:12:05.080 --> 0:12:07.600
像Flat Buffer它有自己主要的特点

318
0:12:07.600 --> 0:12:10.520
一个就是数据的访问不需要解析

319
0:12:10.520 --> 0:12:11.520
这点很重要

320
0:12:11.520 --> 0:12:12.400
不需要解析

321
0:12:12.400 --> 0:12:14.680
我们证明了内存肯定会更高效

322
0:12:14.680 --> 0:12:15.800
速度会更快

323
0:12:15.800 --> 0:12:18.880
生成的代码量也会相对来说比较少

324
0:12:18.880 --> 0:12:21.320
所以说这是它的一个很重要的优点

325
0:12:22.240 --> 0:12:23.720
很多人就会问

326
0:12:23.720 --> 0:12:25.520
既然Flat Buffer那幺好

327
0:12:25.520 --> 0:12:28.480
为什幺你不直接用Flat Buffer去代替掉

328
0:12:28.480 --> 0:12:29.720
Portal Buffer呢

329
0:12:29.720 --> 0:12:32.280
这个就是它们之间的一个对比

330
0:12:32.280 --> 0:12:34.640
我就在这里面不详细的展开

331
0:12:36.240 --> 0:12:39.240
其实Portal Buffer支持的格式和类型会更加多

332
0:12:39.240 --> 0:12:41.720
而且它的接口也会更加多

333
0:12:41.720 --> 0:12:44.400
非常方便我们做一些常用的工作

334
0:12:44.760 --> 0:12:47.960
除了在AI框架里面用Portal Buffer去表示

335
0:12:47.960 --> 0:12:49.680
神经网络模型的Meta数据

336
0:12:49.680 --> 0:12:50.920
还有它的权重数据

337
0:12:50.960 --> 0:12:52.560
它其实还有很多作用

338
0:12:52.560 --> 0:12:54.680
特别是在一些游戏的协议的传输

339
0:12:54.920 --> 0:12:56.920
还有网络自动的传输里面

340
0:12:56.920 --> 0:12:59.080
Portal Buffer还是做得非常好的

341
0:12:59.080 --> 0:13:02.520
而且它经过编辑有利于数据的加减密

342
0:13:04.200 --> 0:13:06.280
下面我们看一下Flat Buffer

343
0:13:06.280 --> 0:13:10.120
我们刚才说到Flat Buffer其实也是谷歌去发起的

344
0:13:10.120 --> 0:13:12.320
后来很多AI推理的框架

345
0:13:12.440 --> 0:13:14.320
确实把Flat Buffer用起来

346
0:13:14.600 --> 0:13:16.920
最主要的两个有MMM

347
0:13:16.920 --> 0:13:20.320
MMM就是阿里推出的一个推理引擎

348
0:13:20.520 --> 0:13:23.680
推理引擎在ID非常多的APP里面已经用到了

349
0:13:23.680 --> 0:13:26.400
官网宣传有18个APP已经用了

350
0:13:26.400 --> 0:13:28.680
包括我们经常刷的淘宝

351
0:13:28.680 --> 0:13:32.480
里面的很多AI功能就是用了MMM做一个推理的

352
0:13:32.800 --> 0:13:35.960
像华为的Mouseboard Lite里面的Screema

353
0:13:35.960 --> 0:13:39.480
或者里面的AR也是用Flat Buffer去定义的

354
0:13:39.920 --> 0:13:42.240
好了今天的内容就到这里为止

355
0:13:42.240 --> 0:13:44.360
我们简单的总结一下

356
0:13:45.480 --> 0:13:48.080
因为转换模块会遇到很多的挑战

357
0:13:48.080 --> 0:13:51.000
于是我们设计了一个转换模块的架构

358
0:13:51.000 --> 0:13:54.080
去承载这些挑战或者去应对这些挑战的

359
0:13:55.200 --> 0:13:58.320
在推理引擎里面的转换模块很重要的一个工作

360
0:13:58.720 --> 0:14:01.720
就是把不同AI框架训练出来的网络模型

361
0:14:02.120 --> 0:14:06.080
串行化成推理引擎能够识别的网络模型

362
0:14:06.280 --> 0:14:08.240
在推理引擎真正去执行的时候

363
0:14:08.400 --> 0:14:12.520
就会把网络模型反串行化为我们的内存的对象

364
0:14:12.520 --> 0:14:14.240
然后给OneTime去执行

365
0:14:14.240 --> 0:14:17.720
这个就是模型串行化和反串行化最重要的工作

366
0:14:17.800 --> 0:14:21.000
串行化和反串行化里面用到什幺数据的格式

367
0:14:21.000 --> 0:14:23.120
或者文档的格式或者标准呢

368
0:14:24.440 --> 0:14:26.360
于是我们最后就介绍了

369
0:14:26.360 --> 0:14:30.480
Portal Buffer和Thread Buffer两种文档格式的内容

370
0:14:30.480 --> 0:14:33.800
而在推理引擎里面用的更多的是Thread Buffer

371
0:14:34.200 --> 0:14:36.080
因为它在反串行化的过程当中

372
0:14:36.080 --> 0:14:40.040
不需要解析反串行化串行化的过程会更加的快

373
0:14:40.880 --> 0:14:44.280
我们在推理引擎里面更多的会用到Thread Buffer

374
0:14:44.280 --> 0:14:45.440
而不是Portal Buffer

375
0:14:45.600 --> 0:14:47.360
我们将会在下一节内容里面

376
0:14:47.360 --> 0:14:50.360
去跟大家看看如何自定义图的IR

377
0:14:50.360 --> 0:14:53.560
还有转换的流程和具体的技术细节

378
0:14:53.560 --> 0:14:55.640
后面的这节课更吸引哦

379
0:14:55.640 --> 0:14:56.440
谢谢各位

380
0:14:56.440 --> 0:14:57.400
拜了个拜

381
0:14:58.080 --> 0:14:58.840
卷的不行了

382
0:14:58.840 --> 0:14:59.720
卷的不行了

383
0:14:59.720 --> 0:15:01.520
记得一键三连加关注哦

384
0:15:01.520 --> 0:15:04.720
所有的内容都会开源在下面这条链接里面

385
0:15:05.160 --> 0:15:06.040
拜拜

