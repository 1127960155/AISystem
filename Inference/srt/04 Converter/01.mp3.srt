0
0:00:00.000 --> 0:00:06.880
Hello大家好

1
0:00:06.880 --> 0:00:07.720
我是曾铭

2
0:00:07.720 --> 0:00:09.520
今天我们来到一个新的内容

3
0:00:09.800 --> 0:00:13.360
虽然这个内容还是在推理引擎下面

4
0:00:13.360 --> 0:00:16.720
但是我们来到模型转换和优化这个内容里面

5
0:00:16.720 --> 0:00:20.040
本来我在这个内容只是讲讲模型转换优化

6
0:00:20.040 --> 0:00:21.400
接下来我们要讲哪些内容

7
0:00:21.400 --> 0:00:24.640
后来梳理着就变成了模型转换优化的

8
0:00:24.640 --> 0:00:27.360
一个整体的架构和它的流程

9
0:00:27.880 --> 0:00:31.760
在接下来在正式进入模型转换优化的这个内容里面

10
0:00:31.760 --> 0:00:33.720
我想跟大家一起去回顾一下

11
0:00:33.720 --> 0:00:35.040
之前讲的一个内容

12
0:00:35.040 --> 0:00:37.160
之前讲的推理系统的介绍的时候

13
0:00:37.320 --> 0:00:39.680
其实更关注的是推理系统的架构

14
0:00:39.680 --> 0:00:41.520
还有推理引擎的架构

15
0:00:41.520 --> 0:00:43.840
接着我们了解了整个大的概念之后

16
0:00:44.240 --> 0:00:47.320
就开始来到了模型的小型化

17
0:00:47.320 --> 0:00:49.240
或者叫做模型轻量化

18
0:00:49.240 --> 0:00:51.840
主要是针对CNN和Transformer两个结构

19
0:00:51.840 --> 0:00:54.440
进行一个小型化或者轻量化的工作

20
0:00:54.440 --> 0:00:56.560
然后我们在离线转换模块的时候

21
0:00:56.880 --> 0:00:59.680
去讲了很多离线的优化压缩

22
0:00:59.680 --> 0:01:01.640
特别是低比德的量化模型剪子

23
0:01:01.640 --> 0:01:05.080
支持灯流这些常见的模型压缩的功能

24
0:01:06.080 --> 0:01:07.680
在第四和第五节的内容

25
0:01:07.840 --> 0:01:10.200
更多的是聚焦于推理引擎的

26
0:01:10.200 --> 0:01:12.560
真正里面的一些内核的模块

27
0:01:12.840 --> 0:01:15.640
模型转换也是作为其中一个很重要的模块

28
0:01:15.640 --> 0:01:18.680
这里面我们将会分开三个内容来给大家介绍

29
0:01:18.680 --> 0:01:22.440
第一个就是模型转换和优化的整体的架构和流程

30
0:01:22.880 --> 0:01:25.240
接着我们去看一下模型格式的转换

31
0:01:25.280 --> 0:01:28.120
最后我们看一下模型离线的优化

32
0:01:28.560 --> 0:01:30.600
经过了格式转换还有离线优化之后

33
0:01:30.720 --> 0:01:32.680
我们真正的就会进入到第五节

34
0:01:32.680 --> 0:01:34.400
One time和在线的优化

35
0:01:35.160 --> 0:01:36.560
而第四个和第五个内容

36
0:01:36.680 --> 0:01:40.600
也是推理引擎一个非常重要的组成部分

37
0:01:41.440 --> 0:01:43.600
回到推理引擎架构里面

38
0:01:43.600 --> 0:01:47.400
我们今天更多是聚焦于模型转换工具

39
0:01:47.640 --> 0:01:49.680
这一块非常重要

40
0:01:49.680 --> 0:01:52.160
没有这一块没有办法去衔接我们后面的

41
0:01:52.160 --> 0:01:53.280
One time和Kernel

42
0:01:53.800 --> 0:01:56.320
这个模块最终要有两个功能

43
0:01:56.320 --> 0:01:58.520
一个功能就是模型的格式转换

44
0:01:58.520 --> 0:02:01.160
第二个就是计算图的优化

45
0:02:01.160 --> 0:02:03.640
这里面我用了一条虚线去代表

46
0:02:03.640 --> 0:02:05.440
上面就是模型转换

47
0:02:05.440 --> 0:02:07.480
下面就是图优化

48
0:02:09.120 --> 0:02:10.480
在接下来的内容里面

49
0:02:10.480 --> 0:02:12.680
总比想跟大家一起去探讨一下

50
0:02:12.680 --> 0:02:16.120
转换模块的挑战和相关的目标

51
0:02:16.400 --> 0:02:18.440
首先我们看一下转换模块

52
0:02:18.600 --> 0:02:20.680
Converter它其实有非常大的挑战

53
0:02:20.680 --> 0:02:22.840
我们这里面总结了4条

54
0:02:23.800 --> 0:02:25.960
第一条就是AI模型本身

55
0:02:26.080 --> 0:02:29.000
其实有非常多的算子的

56
0:02:29.240 --> 0:02:31.400
推理引擎需要用有限的算子

57
0:02:31.400 --> 0:02:34.520
来实现不同AI框架所需要的算子

58
0:02:34.520 --> 0:02:37.160
因为推理引擎它需要对接很多种

59
0:02:37.160 --> 0:02:40.320
不同AI框架训练出来的网络模型

60
0:02:40.320 --> 0:02:43.360
不同的AI框架它的算子有自己的定义

61
0:02:43.360 --> 0:02:46.920
接着第二点就是有非常多的框架

62
0:02:46.920 --> 0:02:50.120
不同的框架有自己的一个模型的文档格式

63
0:02:50.520 --> 0:02:52.480
第三点就是推理引擎

64
0:02:52.600 --> 0:02:55.320
需要支持很多主流的网络模型的结构

65
0:02:55.320 --> 0:02:58.360
包括CNN GNN还有Transformer

66
0:02:58.360 --> 0:03:01.920
最后一点就是相关的DSL的一些特性

67
0:03:02.040 --> 0:03:02.920
Domain Specific

68
0:03:02.920 --> 0:03:06.760
就是我们的一个深度学习专用领域的一些特性

69
0:03:07.120 --> 0:03:09.880
需要支持动态设备的任意维度的输出

70
0:03:10.000 --> 0:03:11.600
还有单控制流的模型

71
0:03:13.040 --> 0:03:16.320
下面我们逐个的去展开看一看

72
0:03:16.520 --> 0:03:19.240
AI模型本身包含非常多的算子

73
0:03:19.400 --> 0:03:20.280
我们可以看一下

74
0:03:20.440 --> 0:03:22.280
下面这个图就是总结的一个图

75
0:03:22.280 --> 0:03:23.520
虽然咖啡现在用的很少

76
0:03:23.520 --> 0:03:25.640
但是我们看一下Tensorflow和PyTorch

77
0:03:25.640 --> 0:03:26.520
这两个框架

78
0:03:26.520 --> 0:03:27.960
实际上这两个框架Self

79
0:03:28.080 --> 0:03:31.640
就是它自身本来具有非常多的算子

80
0:03:31.640 --> 0:03:32.720
不同的AI框架

81
0:03:32.720 --> 0:03:34.360
算子的冲突度非常的高

82
0:03:34.360 --> 0:03:37.640
但是这些算子确实定义也不太一样

83
0:03:37.640 --> 0:03:38.920
例如PyTorch的Padding

84
0:03:38.920 --> 0:03:40.040
跟Tensorflow的Padding

85
0:03:40.040 --> 0:03:41.440
它们虽然都叫Padding

86
0:03:41.440 --> 0:03:45.000
但是它们Padd的一个方式和方向也是不同的

87
0:03:45.440 --> 0:03:47.080
第二个点就是推理引擎

88
0:03:47.200 --> 0:03:49.440
虽然我们接下来要实现的推理引擎

89
0:03:49.440 --> 0:03:52.160
不可能把每一个框架这幺多算子

90
0:03:52.240 --> 0:03:53.120
都实现一遍

91
0:03:53.120 --> 0:03:55.680
所以我们用有限的算子去对接

92
0:03:55.680 --> 0:03:57.480
或者实现不同AI框架

93
0:03:57.480 --> 0:03:59.240
训练出来的网络模型

94
0:04:00.840 --> 0:04:02.000
往下看一下

95
0:04:02.000 --> 0:04:05.120
其实我们历经了非常多的不同的框架

96
0:04:05.120 --> 0:04:07.520
包括Tensorflow有1.0跟2.0

97
0:04:07.520 --> 0:04:09.760
PyTorch有之前的一点多版本

98
0:04:09.760 --> 0:04:11.200
到现在的2点多版本

99
0:04:11.200 --> 0:04:12.920
所以说不同的AI框架

100
0:04:12.920 --> 0:04:14.400
训练出来的网络模型

101
0:04:14.400 --> 0:04:15.320
还有算子

102
0:04:15.320 --> 0:04:17.080
它之间是有差异的

103
0:04:17.080 --> 0:04:18.600
而且不同版本之间

104
0:04:18.600 --> 0:04:20.560
它又会增加不同的算子

105
0:04:20.680 --> 0:04:21.520
不同的AI框架

106
0:04:21.520 --> 0:04:24.000
它的模型转换格式也是不一样的

107
0:04:24.000 --> 0:04:26.160
所以说我们会遇到非常多的

108
0:04:26.160 --> 0:04:28.120
工程性的问题

109
0:04:28.960 --> 0:04:31.320
针对上面一二三四个问题

110
0:04:31.320 --> 0:04:33.960
其实推理引擎都要逐一的去解决

111
0:04:33.960 --> 0:04:36.240
包括我们在思考整个架构的时候

112
0:04:36.240 --> 0:04:37.160
面对这些问题

113
0:04:37.160 --> 0:04:39.520
我们应该怎幺去设计好我们的架构

114
0:04:39.520 --> 0:04:41.520
才能够让我们整个模块

115
0:04:41.520 --> 0:04:44.640
或者让我们整个推理引擎做得更好

116
0:04:45.760 --> 0:04:48.520
第一个点是因为我们的算子非常的多

117
0:04:48.520 --> 0:04:49.280
不同的AI框架

118
0:04:49.280 --> 0:04:51.160
有不同的算子的格式的定义

119
0:04:51.160 --> 0:04:52.680
于是这里面就要求

120
0:04:52.680 --> 0:04:55.160
推理引擎需要有自己的算子的定义

121
0:04:55.160 --> 0:04:57.680
还有对应的格式

122
0:04:57.680 --> 0:04:58.520
有了这个之后

123
0:04:58.520 --> 0:04:59.840
就可以去对接到

124
0:04:59.840 --> 0:05:02.720
不同的AI框架的算子层了

125
0:05:03.720 --> 0:05:04.600
针对第二个问题

126
0:05:04.600 --> 0:05:07.280
我们需要支持非常多不同的AI框架

127
0:05:07.280 --> 0:05:10.280
每个AI框架都有自己的文档格式定义

128
0:05:10.280 --> 0:05:13.600
于是这里面就要求我们的一个推理引擎

129
0:05:13.600 --> 0:05:16.800
需要有自己自定义的计算图的IR

130
0:05:16.840 --> 0:05:20.000
去对接到不同的AI框架里面的计算图

131
0:05:21.080 --> 0:05:24.120
第三点要支持CNI GNI Transom等

132
0:05:24.120 --> 0:05:25.640
主流的网络模型结构

133
0:05:25.640 --> 0:05:27.560
这个时候对我们的推理引擎

134
0:05:27.560 --> 0:05:29.280
就要求我们有丰富的Demo

135
0:05:29.280 --> 0:05:30.360
还有Benchmark

136
0:05:30.360 --> 0:05:31.160
有了Benchmark

137
0:05:31.160 --> 0:05:32.760
就可以提供主流模型的

138
0:05:32.760 --> 0:05:34.360
性能和功能的基准

139
0:05:34.360 --> 0:05:36.240
来保证来去看护

140
0:05:36.240 --> 0:05:39.240
我们的整个推理引擎的可用性

141
0:05:40.520 --> 0:05:42.280
最后一步是因为深度学习

142
0:05:42.280 --> 0:05:43.920
有它的特殊性

143
0:05:43.920 --> 0:05:45.600
需要支持动态的Shape

144
0:05:45.640 --> 0:05:47.160
支持N1维度的输出

145
0:05:47.520 --> 0:05:48.760
支持空自流

146
0:05:48.760 --> 0:05:50.320
于是我们要求推理引擎

147
0:05:50.320 --> 0:05:52.520
要支持非常好的可扩展性

148
0:05:52.520 --> 0:05:55.240
还有AI的比较重要的一些相关的特性

149
0:05:55.240 --> 0:05:56.040
例如动态Shape

150
0:05:57.120 --> 0:05:58.400
针对不同的任务

151
0:05:58.400 --> 0:06:00.680
在CV里面例如检测分割分类

152
0:06:00.680 --> 0:06:01.600
在NLP里面

153
0:06:01.600 --> 0:06:02.960
MOM Marks

154
0:06:02.960 --> 0:06:06.800
这些我们需要做大量的基层测试和验证

155
0:06:06.800 --> 0:06:09.240
保证我们确实能够处理很多

156
0:06:09.240 --> 0:06:12.040
不同类型的网络模型

157
0:06:12.040 --> 0:06:13.240
特别是像动态Shape

158
0:06:13.400 --> 0:06:15.000
可能在分类里面是没有的

159
0:06:15.040 --> 0:06:16.720
但是当我们遇到一些

160
0:06:17.040 --> 0:06:19.040
但是当我们遇到一些分割的场景

161
0:06:19.040 --> 0:06:20.880
可能会用到很多的动态Shape

162
0:06:22.960 --> 0:06:24.640
把其他AI框架的网络模型

163
0:06:24.800 --> 0:06:27.880
转换成为自己推理引擎的一个网络模型

164
0:06:28.000 --> 0:06:30.120
接着我们就需要对网络模型

165
0:06:30.120 --> 0:06:32.200
或者计算图进行优化

166
0:06:33.280 --> 0:06:34.120
而在优化之前

167
0:06:34.240 --> 0:06:35.480
我们需要分析一下

168
0:06:35.480 --> 0:06:37.920
到底我们需要优化哪些内容

169
0:06:37.920 --> 0:06:40.040
在计算图里面到底有哪些用于

170
0:06:40.040 --> 0:06:42.800
才能更好的执行我们的一个优化

171
0:06:42.800 --> 0:06:44.680
所以我们首先来分析一下

172
0:06:44.720 --> 0:06:45.640
或者总结一下

173
0:06:45.640 --> 0:06:47.560
到底有哪些优化的挑战

174
0:06:47.560 --> 0:06:49.480
这里面总比总结了4条

175
0:06:49.480 --> 0:06:51.280
第一条是结构的用于

176
0:06:51.280 --> 0:06:52.640
第二条是精度的用于

177
0:06:52.760 --> 0:06:54.240
第三条是算法的用于

178
0:06:54.240 --> 0:06:56.120
第四条是读写的用于

179
0:06:56.120 --> 0:06:58.800
我们下面逐条的来去看一看

180
0:06:59.800 --> 0:07:01.960
首先是结构的用于

181
0:07:01.960 --> 0:07:04.240
结构的用于其实我们在AI编译器里面

182
0:07:04.680 --> 0:07:07.360
大量的去给大家普及过了

183
0:07:07.680 --> 0:07:09.000
这里面确实有很多

184
0:07:09.000 --> 0:07:11.280
跟AI编译器相关的一些内容

185
0:07:11.280 --> 0:07:13.200
我们的深度学习网络模型里面

186
0:07:13.240 --> 0:07:14.600
有非常大量的

187
0:07:15.280 --> 0:07:17.360
没有效果或者没有用的计算节点

188
0:07:17.360 --> 0:07:19.200
还有很多重复计算的词图

189
0:07:19.200 --> 0:07:20.480
还有相同的结构

190
0:07:20.920 --> 0:07:22.760
我们都可以在保留相同

191
0:07:22.760 --> 0:07:24.600
计算图语义的情况下

192
0:07:25.120 --> 0:07:27.600
去去掉这些用于的结构

193
0:07:27.600 --> 0:07:30.600
说白了就是我怎幺改这个图都好

194
0:07:30.600 --> 0:07:32.880
我保证计算图的语义

195
0:07:33.040 --> 0:07:34.440
它的执行的方式

196
0:07:34.440 --> 0:07:37.080
跟用户的期望是相同的

197
0:07:38.000 --> 0:07:38.720
所以就引出了

198
0:07:38.720 --> 0:07:40.600
我们在计算图优化的过程当中

199
0:07:40.600 --> 0:07:42.040
需要执行一些算子的融合

200
0:07:42.360 --> 0:07:43.120
算子的替换

201
0:07:43.440 --> 0:07:46.960
常量的值节等常用的优化的功能

202
0:07:46.960 --> 0:07:49.640
去对我们的结构用于进行优化

203
0:07:51.240 --> 0:07:54.040
第二个点就是精度用于

204
0:07:54.040 --> 0:07:55.520
实际上在我们推进引擎

205
0:07:55.720 --> 0:07:58.480
大部分存的数据都是张量

206
0:07:58.680 --> 0:08:02.080
一般我们以FP32浮点数来去一个存储的

207
0:08:02.080 --> 0:08:03.960
但是在某些情况下

208
0:08:03.960 --> 0:08:04.640
特别是分类

209
0:08:04.800 --> 0:08:05.880
我们确实可以压到

210
0:08:05.880 --> 0:08:08.760
IP16和int8甚至更低比特

211
0:08:09.040 --> 0:08:11.040
数据中可能存在大量的零

212
0:08:11.040 --> 0:08:12.480
或者重复的数据

213
0:08:13.640 --> 0:08:15.120
这个时候针对精度用于

214
0:08:15.320 --> 0:08:16.840
确实我们可以做很多

215
0:08:16.840 --> 0:08:19.120
模型压缩相关的工作

216
0:08:19.720 --> 0:08:20.200
这个功能

217
0:08:20.320 --> 0:08:22.240
其实我们在上一个内容里面

218
0:08:22.240 --> 0:08:23.640
给大家详细的介绍过

219
0:08:23.640 --> 0:08:24.800
做一些低比特的量化

220
0:08:24.920 --> 0:08:26.400
减字和征流

221
0:08:28.040 --> 0:08:30.880
第三个就是算法的用于

222
0:08:31.400 --> 0:08:33.880
算法的用于听上去有点虚

223
0:08:33.880 --> 0:08:36.280
就是算子扩了颗粒层面实现的算法

224
0:08:36.280 --> 0:08:39.240
本身就存在着计算的用于

225
0:08:40.120 --> 0:08:41.120
什幺叫计算用于

226
0:08:42.000 --> 0:08:43.440
这里面的钟敏就举了一个

227
0:08:43.440 --> 0:08:44.520
比较明确的例子

228
0:08:44.520 --> 0:08:46.800
我们做一个君子模糊的滑窗

229
0:08:46.800 --> 0:08:49.040
还有拉普拉斯的一个滑窗的时候

230
0:08:49.600 --> 0:08:51.800
实际上这里面都是通过一个卷积的方式

231
0:08:51.800 --> 0:08:52.400
去实现的

232
0:08:52.400 --> 0:08:54.800
只是这个卷积核比较特殊

233
0:08:54.800 --> 0:08:56.880
君子卷积可能它的卷积核

234
0:08:56.880 --> 0:08:59.320
是通过高斯定理来去实现的

235
0:08:59.320 --> 0:09:00.520
拉普拉斯的滑窗

236
0:09:00.680 --> 0:09:02.960
就是通过拉普拉斯定理来去实现的

237
0:09:02.960 --> 0:09:04.680
他们的计算原理都是一样的

238
0:09:04.680 --> 0:09:07.880
这个时候就存在着计算的用于了

239
0:09:08.320 --> 0:09:09.880
因为存在计算的用于

240
0:09:09.880 --> 0:09:12.200
于是就要求我们的推理引擎

241
0:09:12.400 --> 0:09:13.520
需要统一算子

242
0:09:13.520 --> 0:09:15.360
还有计算图的表达

243
0:09:15.360 --> 0:09:16.920
统一了算子计算图的表达

244
0:09:17.320 --> 0:09:20.000
我们就可以针对发现的计算用于

245
0:09:20.000 --> 0:09:21.640
进行一个统一

246
0:09:21.640 --> 0:09:23.040
然后整体去提升我们

247
0:09:23.040 --> 0:09:24.520
Kernel的泛化性

248
0:09:25.760 --> 0:09:28.760
第4点就是读写的用于

249
0:09:28.760 --> 0:09:30.360
在我们的计算场景当中

250
0:09:30.520 --> 0:09:32.320
确实会有大量

251
0:09:32.320 --> 0:09:34.360
存在大量的内存访问的问题

252
0:09:34.520 --> 0:09:36.040
内存是不是连续的

253
0:09:36.160 --> 0:09:38.040
要不要进行大量的内存访问

254
0:09:38.040 --> 0:09:40.400
都会是一个很严重的挑战

255
0:09:41.280 --> 0:09:42.600
针对读写用于这个问题

256
0:09:42.920 --> 0:09:44.960
于是在我们的优化模块里面

257
0:09:45.080 --> 0:09:47.160
就需要进行一些数据的排布的优化

258
0:09:47.160 --> 0:09:49.160
还有内存分配的优化

259
0:09:50.280 --> 0:09:51.360
了解完转化模块

260
0:09:51.360 --> 0:09:53.680
优化模块遇到的一些问题和挑战

261
0:09:53.680 --> 0:09:54.720
带着这些疑问

262
0:09:54.720 --> 0:09:55.800
或者带着这些目标

263
0:09:56.000 --> 0:09:57.800
我们就需要去设计好

264
0:09:57.800 --> 0:10:00.240
推理引擎整个离线模块的架构

265
0:10:00.240 --> 0:10:02.760
还有它的工作流程一些挑战

266
0:10:03.120 --> 0:10:04.280
现在我们看一下

267
0:10:04.400 --> 0:10:07.040
转化模块的整个架构

268
0:10:07.040 --> 0:10:08.440
直接看下面这个图

269
0:10:08.440 --> 0:10:11.840
转化模块我们分为一个图的转化

270
0:10:11.840 --> 0:10:13.600
还有图的优化

271
0:10:13.600 --> 0:10:14.800
两大个内容

272
0:10:14.800 --> 0:10:17.280
图的转化首先我们会遇到非常多

273
0:10:17.280 --> 0:10:18.920
不同的AI框架

274
0:10:18.920 --> 0:10:20.560
于是针对每个AI框架

275
0:10:20.560 --> 0:10:21.960
确实它有自己的API

276
0:10:21.960 --> 0:10:23.800
所以不可能通过一个converter

277
0:10:23.800 --> 0:10:26.360
能够把它所有的AI框架都转换过来

278
0:10:26.360 --> 0:10:28.840
于是就会针对Mathsport这个AI框架

279
0:10:28.840 --> 0:10:30.880
可能有Mathsport单独的converter

280
0:10:30.880 --> 0:10:33.080
Pytorch一般都会export到onix

281
0:10:33.200 --> 0:10:36.160
针对onix我们有自己独立的converter

282
0:10:36.160 --> 0:10:37.680
通过不同的converter

283
0:10:37.680 --> 0:10:39.960
都统一转换成为自己推理引擎的

284
0:10:39.960 --> 0:10:41.360
AI中间表达

285
0:10:41.360 --> 0:10:42.960
有了这个中间表达了

286
0:10:42.960 --> 0:10:44.880
后面我们在做图优化的时候

287
0:10:44.880 --> 0:10:46.000
都是基于这个AI

288
0:10:46.000 --> 0:10:47.840
都是基于自己定义的计算图

289
0:10:47.840 --> 0:10:51.160
进行一个改写或者修改的

290
0:10:52.160 --> 0:10:54.800
AI上面就是我们的模型转换

291
0:10:54.800 --> 0:10:56.000
或者格式转换

292
0:10:56.000 --> 0:10:59.320
AI下面就是我们的图优化的模块

293
0:10:59.760 --> 0:11:00.800
在图这个模块

294
0:11:00.880 --> 0:11:02.640
我们要做很多的算子融合

295
0:11:03.120 --> 0:11:05.120
替换内存重排

296
0:11:05.120 --> 0:11:06.280
数据重排

297
0:11:06.280 --> 0:11:07.560
还有内存分配

298
0:11:07.560 --> 0:11:09.080
计算图优化的这个功能

299
0:11:09.240 --> 0:11:11.160
其实不是说非常的新

300
0:11:11.160 --> 0:11:13.120
如果大家看过了解过

301
0:11:13.120 --> 0:11:14.440
AI编译器这个系列

302
0:11:14.600 --> 0:11:16.520
可以发现这里面有很多功能

303
0:11:16.520 --> 0:11:19.000
都类似于AI编译器的前端优化

304
0:11:19.000 --> 0:11:22.360
对这里面其实有很多功能可以附现的

305
0:11:22.360 --> 0:11:24.160
而我们现在公司

306
0:11:24.160 --> 0:11:26.160
其实有一部分专家就觉得

307
0:11:26.160 --> 0:11:27.440
计算图优化这个功能

308
0:11:27.640 --> 0:11:29.480
应该通过编译器来去做

309
0:11:29.480 --> 0:11:31.720
所以希望另一个编译器的项目

310
0:11:31.760 --> 0:11:35.040
但是总比觉得其实没有必要做的这幺的重

311
0:11:35.480 --> 0:11:37.400
很多时候在推定引擎

312
0:11:37.520 --> 0:11:39.720
其实没有必要去做一个编译器

313
0:11:39.720 --> 0:11:42.040
更多的基于那个pattern去优化

314
0:11:42.040 --> 0:11:44.760
更多的基于我们的规则进行优化就好了

315
0:11:44.760 --> 0:11:46.000
没有必要做那幺重

316
0:11:46.000 --> 0:11:47.120
因为一个离线模块

317
0:11:47.240 --> 0:11:48.800
可能它不需要太大

318
0:11:48.800 --> 0:11:51.480
简单的很小的几个M可以了

319
0:11:51.480 --> 0:11:53.080
因为我们推定引擎的AI

320
0:11:53.080 --> 0:11:55.520
尽可能的设计的比较简单

321
0:11:55.520 --> 0:11:57.440
跟我们训练的框架是不一样的

322
0:11:57.880 --> 0:11:58.760
训练的框架

323
0:11:58.880 --> 0:12:00.560
我们要考虑的问题非常多

324
0:12:00.600 --> 0:12:02.960
例如有三个特别重要的特性

325
0:12:03.240 --> 0:12:04.600
第一个就是自动规分

326
0:12:04.600 --> 0:12:07.400
第二个就是分布式的并行

327
0:12:07.560 --> 0:12:10.240
第三个点就是静态图和动态图的问题

328
0:12:10.480 --> 0:12:12.160
像现在这些大部分的问题

329
0:12:12.520 --> 0:12:14.760
在推定引擎我们没有必要考虑的太多

330
0:12:14.760 --> 0:12:16.680
所以没有必要搞一个编译器出来

331
0:12:16.680 --> 0:12:19.320
更多的去做一些pattern的修改就好了

332
0:12:19.920 --> 0:12:20.760
废话就不多说

333
0:12:20.760 --> 0:12:24.400
我们回到整个转化模块的工作流程里面

334
0:12:24.600 --> 0:12:26.080
去看一下刚才的架构图

335
0:12:26.200 --> 0:12:29.080
针对我们的工作流程有什幺不一样

336
0:12:29.200 --> 0:12:30.960
左边的这个是转化模块

337
0:12:30.960 --> 0:12:33.840
我们用了蓝色底来去给大家划分

338
0:12:33.840 --> 0:12:36.320
右边的这个就是优化的模块

339
0:12:36.320 --> 0:12:38.600
用了黄色底去进行划分

340
0:12:38.600 --> 0:12:41.360
我们可以看到确实也有很多个converter

341
0:12:41.360 --> 0:12:43.160
有非常多的转换器

342
0:12:43.520 --> 0:12:44.800
通过不同的转换器

343
0:12:44.800 --> 0:12:48.040
把不同的AI框架训练出来的一个网络模型

344
0:12:48.920 --> 0:12:51.000
转换成为我们推定引擎的IR

345
0:12:51.000 --> 0:12:52.520
有了推定引擎的IR之后

346
0:12:52.600 --> 0:12:54.720
我们现在就变成一个统一的表达了

347
0:12:54.720 --> 0:12:56.600
于是在做后面的优化

348
0:12:56.720 --> 0:12:58.240
我分开了三段

349
0:12:59.080 --> 0:13:00.920
一段叫做pre-optimize

350
0:13:00.920 --> 0:13:03.040
一段叫做正式的optimize

351
0:13:03.040 --> 0:13:05.600
最后一段叫post-optimize

352
0:13:05.600 --> 0:13:07.800
三个阶段有三个不同的内容

353
0:13:08.120 --> 0:13:11.640
这个也是Zombie看到很多推定引擎的项目代码

354
0:13:11.640 --> 0:13:12.960
所总结出来的

355
0:13:13.840 --> 0:13:17.080
首先会对转化模块传过来的一个计算图

356
0:13:17.080 --> 0:13:18.960
做一些公共表达式的消除

357
0:13:18.960 --> 0:13:19.880
死代码的消除

358
0:13:19.880 --> 0:13:21.240
还有怠速简化

359
0:13:21.520 --> 0:13:24.040
常用的怠速简化消除的功能

360
0:13:25.200 --> 0:13:26.640
自行一些公共的功能之后

361
0:13:26.800 --> 0:13:28.480
就正式的对我们的计算图

362
0:13:28.480 --> 0:13:31.360
对我们的神经网络的一些知识涌进来了

363
0:13:31.560 --> 0:13:33.600
第一点就有算子融合算子替换

364
0:13:33.600 --> 0:13:34.760
还有常量折叠

365
0:13:34.760 --> 0:13:37.440
这个就是最常用的一些方式

366
0:13:37.440 --> 0:13:39.640
也是我们作为中间优化层

367
0:13:40.840 --> 0:13:42.520
来到了post-optimize这个阶段

368
0:13:42.640 --> 0:13:45.520
其实代表我们的计算图基本上能换的就换了

369
0:13:46.040 --> 0:13:48.400
更多的是对数据的格式转换

370
0:13:48.480 --> 0:13:50.880
NCHW到NHWC这种

371
0:13:50.880 --> 0:13:52.400
还有内存的布局计算

372
0:13:52.400 --> 0:13:55.120
另外会把一些重复的算子把它和平掉

373
0:13:55.760 --> 0:13:58.080
到这个阶段已经没有太多了

374
0:13:58.160 --> 0:14:01.040
更多的在前面这两个阶段已经把它干完了

375
0:14:01.040 --> 0:14:02.280
在最后一个阶段

376
0:14:02.600 --> 0:14:03.880
主要是对我们的数据

377
0:14:03.880 --> 0:14:07.160
对我们的内存进行一些管理和预管理的工作

378
0:14:09.160 --> 0:14:10.760
好了今天的内容就这幺多

379
0:14:10.760 --> 0:14:11.760
我们回过一下

380
0:14:11.760 --> 0:14:14.640
这里面跟大家一起汇报了一下

381
0:14:14.640 --> 0:14:17.440
模型转换的遇到了一些挑战和目标

382
0:14:17.680 --> 0:14:21.360
接着我们看了一下计算图优化的一些挑战和目标

383
0:14:21.360 --> 0:14:22.600
带着这些挑战和目标

384
0:14:22.720 --> 0:14:25.600
我们就去设计了推理引擎的整体的架构

385
0:14:25.640 --> 0:14:28.760
然后把架构图变成我们的工作流程图

386
0:14:28.760 --> 0:14:31.040
把每一个模块都梳理清楚

387
0:14:32.440 --> 0:14:34.080
卷的不行了

388
0:14:34.080 --> 0:14:35.920
记得一键三连加关注哦

389
0:14:35.920 --> 0:14:38.920
所有的内容都会开源在下面这条链接里面

390
0:14:39.520 --> 0:14:40.240
摆了个掰

