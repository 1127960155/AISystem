1
00:00:00,000 --> 00:00:06,880
Hello大家好

2
00:00:06,880 --> 00:00:07,720
我是曾铭

3
00:00:07,720 --> 00:00:09,520
今天我们来到一个新的内容

4
00:00:09,800 --> 00:00:13,360
虽然这个内容还是在推理引擎下面

5
00:00:13,360 --> 00:00:16,720
但是我们来到模型转换和优化这个内容里面

6
00:00:16,720 --> 00:00:20,040
本来我在这个内容只是讲讲模型转换优化

7
00:00:20,040 --> 00:00:21,400
接下来我们要讲哪些内容

8
00:00:21,400 --> 00:00:24,640
后来梳理着就变成了模型转换优化的

9
00:00:24,640 --> 00:00:27,360
一个整体的架构和它的流程

10
00:00:27,880 --> 00:00:31,760
在接下来在正式进入模型转换优化的这个内容里面

11
00:00:31,760 --> 00:00:33,720
我想跟大家一起去回顾一下

12
00:00:33,720 --> 00:00:35,040
之前讲的一个内容

13
00:00:35,040 --> 00:00:37,160
之前讲的推理系统的介绍的时候

14
00:00:37,320 --> 00:00:39,680
其实更关注的是推理系统的架构

15
00:00:39,680 --> 00:00:41,520
还有推理引擎的架构

16
00:00:41,520 --> 00:00:43,840
接着我们了解了整个大的概念之后

17
00:00:44,240 --> 00:00:47,320
就开始来到了模型的小型化

18
00:00:47,320 --> 00:00:49,240
或者叫做模型轻量化

19
00:00:49,240 --> 00:00:51,840
主要是针对CNN和Transformer两个结构

20
00:00:51,840 --> 00:00:54,440
进行一个小型化或者轻量化的工作

21
00:00:54,440 --> 00:00:56,560
然后我们在离线转换模块的时候

22
00:00:56,880 --> 00:00:59,680
去讲了很多离线的优化压缩

23
00:00:59,680 --> 00:01:01,640
特别是低比德的量化模型剪子

24
00:01:01,640 --> 00:01:05,080
支持灯流这些常见的模型压缩的功能

25
00:01:06,080 --> 00:01:07,680
在第四和第五节的内容

26
00:01:07,840 --> 00:01:10,200
更多的是聚焦于推理引擎的

27
00:01:10,200 --> 00:01:12,560
真正里面的一些内核的模块

28
00:01:12,840 --> 00:01:15,640
模型转换也是作为其中一个很重要的模块

29
00:01:15,640 --> 00:01:18,680
这里面我们将会分开三个内容来给大家介绍

30
00:01:18,680 --> 00:01:22,440
第一个就是模型转换和优化的整体的架构和流程

31
00:01:22,880 --> 00:01:25,240
接着我们去看一下模型格式的转换

32
00:01:25,280 --> 00:01:28,120
最后我们看一下模型离线的优化

33
00:01:28,560 --> 00:01:30,600
经过了格式转换还有离线优化之后

34
00:01:30,720 --> 00:01:32,680
我们真正的就会进入到第五节

35
00:01:32,680 --> 00:01:34,400
One time和在线的优化

36
00:01:35,160 --> 00:01:36,560
而第四个和第五个内容

37
00:01:36,680 --> 00:01:40,600
也是推理引擎一个非常重要的组成部分

38
00:01:41,440 --> 00:01:43,600
回到推理引擎架构里面

39
00:01:43,600 --> 00:01:47,400
我们今天更多是聚焦于模型转换工具

40
00:01:47,640 --> 00:01:49,680
这一块非常重要

41
00:01:49,680 --> 00:01:52,160
没有这一块没有办法去衔接我们后面的

42
00:01:52,160 --> 00:01:53,280
One time和Kernel

43
00:01:53,800 --> 00:01:56,320
这个模块最终要有两个功能

44
00:01:56,320 --> 00:01:58,520
一个功能就是模型的格式转换

45
00:01:58,520 --> 00:02:01,160
第二个就是计算图的优化

46
00:02:01,160 --> 00:02:03,640
这里面我用了一条虚线去代表

47
00:02:03,640 --> 00:02:05,440
上面就是模型转换

48
00:02:05,440 --> 00:02:07,480
下面就是图优化

49
00:02:09,120 --> 00:02:10,480
在接下来的内容里面

50
00:02:10,480 --> 00:02:12,680
总比想跟大家一起去探讨一下

51
00:02:12,680 --> 00:02:16,120
转换模块的挑战和相关的目标

52
00:02:16,400 --> 00:02:18,440
首先我们看一下转换模块

53
00:02:18,600 --> 00:02:20,680
Converter它其实有非常大的挑战

54
00:02:20,680 --> 00:02:22,840
我们这里面总结了4条

55
00:02:23,800 --> 00:02:25,960
第一条就是AI模型本身

56
00:02:26,080 --> 00:02:29,000
其实有非常多的算子的

57
00:02:29,240 --> 00:02:31,400
推理引擎需要用有限的算子

58
00:02:31,400 --> 00:02:34,520
来实现不同AI框架所需要的算子

59
00:02:34,520 --> 00:02:37,160
因为推理引擎它需要对接很多种

60
00:02:37,160 --> 00:02:40,320
不同AI框架训练出来的网络模型

61
00:02:40,320 --> 00:02:43,360
不同的AI框架它的算子有自己的定义

62
00:02:43,360 --> 00:02:46,920
接着第二点就是有非常多的框架

63
00:02:46,920 --> 00:02:50,120
不同的框架有自己的一个模型的文档格式

64
00:02:50,520 --> 00:02:52,480
第三点就是推理引擎

65
00:02:52,600 --> 00:02:55,320
需要支持很多主流的网络模型的结构

66
00:02:55,320 --> 00:02:58,360
包括CNN GNN还有Transformer

67
00:02:58,360 --> 00:03:01,920
最后一点就是相关的DSL的一些特性

68
00:03:02,040 --> 00:03:02,920
Domain Specific

69
00:03:02,920 --> 00:03:06,760
就是我们的一个深度学习专用领域的一些特性

70
00:03:07,120 --> 00:03:09,880
需要支持动态设备的任意维度的输出

71
00:03:10,000 --> 00:03:11,600
还有单控制流的模型

72
00:03:13,040 --> 00:03:16,320
下面我们逐个的去展开看一看

73
00:03:16,520 --> 00:03:19,240
AI模型本身包含非常多的算子

74
00:03:19,400 --> 00:03:20,280
我们可以看一下

75
00:03:20,440 --> 00:03:22,280
下面这个图就是总结的一个图

76
00:03:22,280 --> 00:03:23,520
虽然咖啡现在用的很少

77
00:03:23,520 --> 00:03:25,640
但是我们看一下Tensorflow和PyTorch

78
00:03:25,640 --> 00:03:26,520
这两个框架

79
00:03:26,520 --> 00:03:27,960
实际上这两个框架Self

80
00:03:28,080 --> 00:03:31,640
就是它自身本来具有非常多的算子

81
00:03:31,640 --> 00:03:32,720
不同的AI框架

82
00:03:32,720 --> 00:03:34,360
算子的冲突度非常的高

83
00:03:34,360 --> 00:03:37,640
但是这些算子确实定义也不太一样

84
00:03:37,640 --> 00:03:38,920
例如PyTorch的Padding

85
00:03:38,920 --> 00:03:40,040
跟Tensorflow的Padding

86
00:03:40,040 --> 00:03:41,440
它们虽然都叫Padding

87
00:03:41,440 --> 00:03:45,000
但是它们Padd的一个方式和方向也是不同的

88
00:03:45,440 --> 00:03:47,080
第二个点就是推理引擎

89
00:03:47,200 --> 00:03:49,440
虽然我们接下来要实现的推理引擎

90
00:03:49,440 --> 00:03:52,160
不可能把每一个框架这幺多算子

91
00:03:52,240 --> 00:03:53,120
都实现一遍

92
00:03:53,120 --> 00:03:55,680
所以我们用有限的算子去对接

93
00:03:55,680 --> 00:03:57,480
或者实现不同AI框架

94
00:03:57,480 --> 00:03:59,240
训练出来的网络模型

95
00:04:00,840 --> 00:04:02,000
往下看一下

96
00:04:02,000 --> 00:04:05,120
其实我们历经了非常多的不同的框架

97
00:04:05,120 --> 00:04:07,520
包括Tensorflow有1.0跟2.0

98
00:04:07,520 --> 00:04:09,760
PyTorch有之前的一点多版本

99
00:04:09,760 --> 00:04:11,200
到现在的2点多版本

100
00:04:11,200 --> 00:04:12,920
所以说不同的AI框架

101
00:04:12,920 --> 00:04:14,400
训练出来的网络模型

102
00:04:14,400 --> 00:04:15,320
还有算子

103
00:04:15,320 --> 00:04:17,080
它之间是有差异的

104
00:04:17,080 --> 00:04:18,600
而且不同版本之间

105
00:04:18,600 --> 00:04:20,560
它又会增加不同的算子

106
00:04:20,680 --> 00:04:21,520
不同的AI框架

107
00:04:21,520 --> 00:04:24,000
它的模型转换格式也是不一样的

108
00:04:24,000 --> 00:04:26,160
所以说我们会遇到非常多的

109
00:04:26,160 --> 00:04:28,120
工程性的问题

110
00:04:28,960 --> 00:04:31,320
针对上面一二三四个问题

111
00:04:31,320 --> 00:04:33,960
其实推理引擎都要逐一的去解决

112
00:04:33,960 --> 00:04:36,240
包括我们在思考整个架构的时候

113
00:04:36,240 --> 00:04:37,160
面对这些问题

114
00:04:37,160 --> 00:04:39,520
我们应该怎幺去设计好我们的架构

115
00:04:39,520 --> 00:04:41,520
才能够让我们整个模块

116
00:04:41,520 --> 00:04:44,640
或者让我们整个推理引擎做得更好

117
00:04:45,760 --> 00:04:48,520
第一个点是因为我们的算子非常的多

118
00:04:48,520 --> 00:04:49,280
不同的AI框架

119
00:04:49,280 --> 00:04:51,160
有不同的算子的格式的定义

120
00:04:51,160 --> 00:04:52,680
于是这里面就要求

121
00:04:52,680 --> 00:04:55,160
推理引擎需要有自己的算子的定义

122
00:04:55,160 --> 00:04:57,680
还有对应的格式

123
00:04:57,680 --> 00:04:58,520
有了这个之后

124
00:04:58,520 --> 00:04:59,840
就可以去对接到

125
00:04:59,840 --> 00:05:02,720
不同的AI框架的算子层了

126
00:05:03,720 --> 00:05:04,600
针对第二个问题

127
00:05:04,600 --> 00:05:07,280
我们需要支持非常多不同的AI框架

128
00:05:07,280 --> 00:05:10,280
每个AI框架都有自己的文档格式定义

129
00:05:10,280 --> 00:05:13,600
于是这里面就要求我们的一个推理引擎

130
00:05:13,600 --> 00:05:16,800
需要有自己自定义的计算图的IR

131
00:05:16,840 --> 00:05:20,000
去对接到不同的AI框架里面的计算图

132
00:05:21,080 --> 00:05:24,120
第三点要支持CNI GNI Transom等

133
00:05:24,120 --> 00:05:25,640
主流的网络模型结构

134
00:05:25,640 --> 00:05:27,560
这个时候对我们的推理引擎

135
00:05:27,560 --> 00:05:29,280
就要求我们有丰富的Demo

136
00:05:29,280 --> 00:05:30,360
还有Benchmark

137
00:05:30,360 --> 00:05:31,160
有了Benchmark

138
00:05:31,160 --> 00:05:32,760
就可以提供主流模型的

139
00:05:32,760 --> 00:05:34,360
性能和功能的基准

140
00:05:34,360 --> 00:05:36,240
来保证来去看护

141
00:05:36,240 --> 00:05:39,240
我们的整个推理引擎的可用性

142
00:05:40,520 --> 00:05:42,280
最后一步是因为深度学习

143
00:05:42,280 --> 00:05:43,920
有它的特殊性

144
00:05:43,920 --> 00:05:45,600
需要支持动态的Shape

145
00:05:45,640 --> 00:05:47,160
支持N1维度的输出

146
00:05:47,520 --> 00:05:48,760
支持空自流

147
00:05:48,760 --> 00:05:50,320
于是我们要求推理引擎

148
00:05:50,320 --> 00:05:52,520
要支持非常好的可扩展性

149
00:05:52,520 --> 00:05:55,240
还有AI的比较重要的一些相关的特性

150
00:05:55,240 --> 00:05:56,040
例如动态Shape

151
00:05:57,120 --> 00:05:58,400
针对不同的任务

152
00:05:58,400 --> 00:06:00,680
在CV里面例如检测分割分类

153
00:06:00,680 --> 00:06:01,600
在NLP里面

154
00:06:01,600 --> 00:06:02,960
MOM Marks

155
00:06:02,960 --> 00:06:06,800
这些我们需要做大量的基层测试和验证

156
00:06:06,800 --> 00:06:09,240
保证我们确实能够处理很多

157
00:06:09,240 --> 00:06:12,040
不同类型的网络模型

158
00:06:12,040 --> 00:06:13,240
特别是像动态Shape

159
00:06:13,400 --> 00:06:15,000
可能在分类里面是没有的

160
00:06:15,040 --> 00:06:16,720
但是当我们遇到一些

161
00:06:17,040 --> 00:06:19,040
但是当我们遇到一些分割的场景

162
00:06:19,040 --> 00:06:20,880
可能会用到很多的动态Shape

163
00:06:22,960 --> 00:06:24,640
把其他AI框架的网络模型

164
00:06:24,800 --> 00:06:27,880
转换成为自己推理引擎的一个网络模型

165
00:06:28,000 --> 00:06:30,120
接着我们就需要对网络模型

166
00:06:30,120 --> 00:06:32,200
或者计算图进行优化

167
00:06:33,280 --> 00:06:34,120
而在优化之前

168
00:06:34,240 --> 00:06:35,480
我们需要分析一下

169
00:06:35,480 --> 00:06:37,920
到底我们需要优化哪些内容

170
00:06:37,920 --> 00:06:40,040
在计算图里面到底有哪些用于

171
00:06:40,040 --> 00:06:42,800
才能更好的执行我们的一个优化

172
00:06:42,800 --> 00:06:44,680
所以我们首先来分析一下

173
00:06:44,720 --> 00:06:45,640
或者总结一下

174
00:06:45,640 --> 00:06:47,560
到底有哪些优化的挑战

175
00:06:47,560 --> 00:06:49,480
这里面总比总结了4条

176
00:06:49,480 --> 00:06:51,280
第一条是结构的用于

177
00:06:51,280 --> 00:06:52,640
第二条是精度的用于

178
00:06:52,760 --> 00:06:54,240
第三条是算法的用于

179
00:06:54,240 --> 00:06:56,120
第四条是读写的用于

180
00:06:56,120 --> 00:06:58,800
我们下面逐条的来去看一看

181
00:06:59,800 --> 00:07:01,960
首先是结构的用于

182
00:07:01,960 --> 00:07:04,240
结构的用于其实我们在AI编译器里面

183
00:07:04,680 --> 00:07:07,360
大量的去给大家普及过了

184
00:07:07,680 --> 00:07:09,000
这里面确实有很多

185
00:07:09,000 --> 00:07:11,280
跟AI编译器相关的一些内容

186
00:07:11,280 --> 00:07:13,200
我们的深度学习网络模型里面

187
00:07:13,240 --> 00:07:14,600
有非常大量的

188
00:07:15,280 --> 00:07:17,360
没有效果或者没有用的计算节点

189
00:07:17,360 --> 00:07:19,200
还有很多重复计算的词图

190
00:07:19,200 --> 00:07:20,480
还有相同的结构

191
00:07:20,920 --> 00:07:22,760
我们都可以在保留相同

192
00:07:22,760 --> 00:07:24,600
计算图语义的情况下

193
00:07:25,120 --> 00:07:27,600
去去掉这些用于的结构

194
00:07:27,600 --> 00:07:30,600
说白了就是我怎幺改这个图都好

195
00:07:30,600 --> 00:07:32,880
我保证计算图的语义

196
00:07:33,040 --> 00:07:34,440
它的执行的方式

197
00:07:34,440 --> 00:07:37,080
跟用户的期望是相同的

198
00:07:38,000 --> 00:07:38,720
所以就引出了

199
00:07:38,720 --> 00:07:40,600
我们在计算图优化的过程当中

200
00:07:40,600 --> 00:07:42,040
需要执行一些算子的融合

201
00:07:42,360 --> 00:07:43,120
算子的替换

202
00:07:43,440 --> 00:07:46,960
常量的值节等常用的优化的功能

203
00:07:46,960 --> 00:07:49,640
去对我们的结构用于进行优化

204
00:07:51,240 --> 00:07:54,040
第二个点就是精度用于

205
00:07:54,040 --> 00:07:55,520
实际上在我们推进引擎

206
00:07:55,720 --> 00:07:58,480
大部分存的数据都是张量

207
00:07:58,680 --> 00:08:02,080
一般我们以FP32浮点数来去一个存储的

208
00:08:02,080 --> 00:08:03,960
但是在某些情况下

209
00:08:03,960 --> 00:08:04,640
特别是分类

210
00:08:04,800 --> 00:08:05,880
我们确实可以压到

211
00:08:05,880 --> 00:08:08,760
IP16和int8甚至更低比特

212
00:08:09,040 --> 00:08:11,040
数据中可能存在大量的零

213
00:08:11,040 --> 00:08:12,480
或者重复的数据

214
00:08:13,640 --> 00:08:15,120
这个时候针对精度用于

215
00:08:15,320 --> 00:08:16,840
确实我们可以做很多

216
00:08:16,840 --> 00:08:19,120
模型压缩相关的工作

217
00:08:19,720 --> 00:08:20,200
这个功能

218
00:08:20,320 --> 00:08:22,240
其实我们在上一个内容里面

219
00:08:22,240 --> 00:08:23,640
给大家详细的介绍过

220
00:08:23,640 --> 00:08:24,800
做一些低比特的量化

221
00:08:24,920 --> 00:08:26,400
减字和征流

222
00:08:28,040 --> 00:08:30,880
第三个就是算法的用于

223
00:08:31,400 --> 00:08:33,880
算法的用于听上去有点虚

224
00:08:33,880 --> 00:08:36,280
就是算子扩了颗粒层面实现的算法

225
00:08:36,280 --> 00:08:39,240
本身就存在着计算的用于

226
00:08:40,120 --> 00:08:41,120
什幺叫计算用于

227
00:08:42,000 --> 00:08:43,440
这里面的钟敏就举了一个

228
00:08:43,440 --> 00:08:44,520
比较明确的例子

229
00:08:44,520 --> 00:08:46,800
我们做一个君子模糊的滑窗

230
00:08:46,800 --> 00:08:49,040
还有拉普拉斯的一个滑窗的时候

231
00:08:49,600 --> 00:08:51,800
实际上这里面都是通过一个卷积的方式

232
00:08:51,800 --> 00:08:52,400
去实现的

233
00:08:52,400 --> 00:08:54,800
只是这个卷积核比较特殊

234
00:08:54,800 --> 00:08:56,880
君子卷积可能它的卷积核

235
00:08:56,880 --> 00:08:59,320
是通过高斯定理来去实现的

236
00:08:59,320 --> 00:09:00,520
拉普拉斯的滑窗

237
00:09:00,680 --> 00:09:02,960
就是通过拉普拉斯定理来去实现的

238
00:09:02,960 --> 00:09:04,680
他们的计算原理都是一样的

239
00:09:04,680 --> 00:09:07,880
这个时候就存在着计算的用于了

240
00:09:08,320 --> 00:09:09,880
因为存在计算的用于

241
00:09:09,880 --> 00:09:12,200
于是就要求我们的推理引擎

242
00:09:12,400 --> 00:09:13,520
需要统一算子

243
00:09:13,520 --> 00:09:15,360
还有计算图的表达

244
00:09:15,360 --> 00:09:16,920
统一了算子计算图的表达

245
00:09:17,320 --> 00:09:20,000
我们就可以针对发现的计算用于

246
00:09:20,000 --> 00:09:21,640
进行一个统一

247
00:09:21,640 --> 00:09:23,040
然后整体去提升我们

248
00:09:23,040 --> 00:09:24,520
Kernel的泛化性

249
00:09:25,760 --> 00:09:28,760
第4点就是读写的用于

250
00:09:28,760 --> 00:09:30,360
在我们的计算场景当中

251
00:09:30,520 --> 00:09:32,320
确实会有大量

252
00:09:32,320 --> 00:09:34,360
存在大量的内存访问的问题

253
00:09:34,520 --> 00:09:36,040
内存是不是连续的

254
00:09:36,160 --> 00:09:38,040
要不要进行大量的内存访问

255
00:09:38,040 --> 00:09:40,400
都会是一个很严重的挑战

256
00:09:41,280 --> 00:09:42,600
针对读写用于这个问题

257
00:09:42,920 --> 00:09:44,960
于是在我们的优化模块里面

258
00:09:45,080 --> 00:09:47,160
就需要进行一些数据的排布的优化

259
00:09:47,160 --> 00:09:49,160
还有内存分配的优化

260
00:09:50,280 --> 00:09:51,360
了解完转化模块

261
00:09:51,360 --> 00:09:53,680
优化模块遇到的一些问题和挑战

262
00:09:53,680 --> 00:09:54,720
带着这些疑问

263
00:09:54,720 --> 00:09:55,800
或者带着这些目标

264
00:09:56,000 --> 00:09:57,800
我们就需要去设计好

265
00:09:57,800 --> 00:10:00,240
推理引擎整个离线模块的架构

266
00:10:00,240 --> 00:10:02,760
还有它的工作流程一些挑战

267
00:10:03,120 --> 00:10:04,280
现在我们看一下

268
00:10:04,400 --> 00:10:07,040
转化模块的整个架构

269
00:10:07,040 --> 00:10:08,440
直接看下面这个图

270
00:10:08,440 --> 00:10:11,840
转化模块我们分为一个图的转化

271
00:10:11,840 --> 00:10:13,600
还有图的优化

272
00:10:13,600 --> 00:10:14,800
两大个内容

273
00:10:14,800 --> 00:10:17,280
图的转化首先我们会遇到非常多

274
00:10:17,280 --> 00:10:18,920
不同的AI框架

275
00:10:18,920 --> 00:10:20,560
于是针对每个AI框架

276
00:10:20,560 --> 00:10:21,960
确实它有自己的API

277
00:10:21,960 --> 00:10:23,800
所以不可能通过一个converter

278
00:10:23,800 --> 00:10:26,360
能够把它所有的AI框架都转换过来

279
00:10:26,360 --> 00:10:28,840
于是就会针对Mathsport这个AI框架

280
00:10:28,840 --> 00:10:30,880
可能有Mathsport单独的converter

281
00:10:30,880 --> 00:10:33,080
Pytorch一般都会export到onix

282
00:10:33,200 --> 00:10:36,160
针对onix我们有自己独立的converter

283
00:10:36,160 --> 00:10:37,680
通过不同的converter

284
00:10:37,680 --> 00:10:39,960
都统一转换成为自己推理引擎的

285
00:10:39,960 --> 00:10:41,360
AI中间表达

286
00:10:41,360 --> 00:10:42,960
有了这个中间表达了

287
00:10:42,960 --> 00:10:44,880
后面我们在做图优化的时候

288
00:10:44,880 --> 00:10:46,000
都是基于这个AI

289
00:10:46,000 --> 00:10:47,840
都是基于自己定义的计算图

290
00:10:47,840 --> 00:10:51,160
进行一个改写或者修改的

291
00:10:52,160 --> 00:10:54,800
AI上面就是我们的模型转换

292
00:10:54,800 --> 00:10:56,000
或者格式转换

293
00:10:56,000 --> 00:10:59,320
AI下面就是我们的图优化的模块

294
00:10:59,760 --> 00:11:00,800
在图这个模块

295
00:11:00,880 --> 00:11:02,640
我们要做很多的算子融合

296
00:11:03,120 --> 00:11:05,120
替换内存重排

297
00:11:05,120 --> 00:11:06,280
数据重排

298
00:11:06,280 --> 00:11:07,560
还有内存分配

299
00:11:07,560 --> 00:11:09,080
计算图优化的这个功能

300
00:11:09,240 --> 00:11:11,160
其实不是说非常的新

301
00:11:11,160 --> 00:11:13,120
如果大家看过了解过

302
00:11:13,120 --> 00:11:14,440
AI编译器这个系列

303
00:11:14,600 --> 00:11:16,520
可以发现这里面有很多功能

304
00:11:16,520 --> 00:11:19,000
都类似于AI编译器的前端优化

305
00:11:19,000 --> 00:11:22,360
对这里面其实有很多功能可以附现的

306
00:11:22,360 --> 00:11:24,160
而我们现在公司

307
00:11:24,160 --> 00:11:26,160
其实有一部分专家就觉得

308
00:11:26,160 --> 00:11:27,440
计算图优化这个功能

309
00:11:27,640 --> 00:11:29,480
应该通过编译器来去做

310
00:11:29,480 --> 00:11:31,720
所以希望另一个编译器的项目

311
00:11:31,760 --> 00:11:35,040
但是总比觉得其实没有必要做的这幺的重

312
00:11:35,480 --> 00:11:37,400
很多时候在推定引擎

313
00:11:37,520 --> 00:11:39,720
其实没有必要去做一个编译器

314
00:11:39,720 --> 00:11:42,040
更多的基于那个pattern去优化

315
00:11:42,040 --> 00:11:44,760
更多的基于我们的规则进行优化就好了

316
00:11:44,760 --> 00:11:46,000
没有必要做那幺重

317
00:11:46,000 --> 00:11:47,120
因为一个离线模块

318
00:11:47,240 --> 00:11:48,800
可能它不需要太大

319
00:11:48,800 --> 00:11:51,480
简单的很小的几个M可以了

320
00:11:51,480 --> 00:11:53,080
因为我们推定引擎的AI

321
00:11:53,080 --> 00:11:55,520
尽可能的设计的比较简单

322
00:11:55,520 --> 00:11:57,440
跟我们训练的框架是不一样的

323
00:11:57,880 --> 00:11:58,760
训练的框架

324
00:11:58,880 --> 00:12:00,560
我们要考虑的问题非常多

325
00:12:00,600 --> 00:12:02,960
例如有三个特别重要的特性

326
00:12:03,240 --> 00:12:04,600
第一个就是自动规分

327
00:12:04,600 --> 00:12:07,400
第二个就是分布式的并行

328
00:12:07,560 --> 00:12:10,240
第三个点就是静态图和动态图的问题

329
00:12:10,480 --> 00:12:12,160
像现在这些大部分的问题

330
00:12:12,520 --> 00:12:14,760
在推定引擎我们没有必要考虑的太多

331
00:12:14,760 --> 00:12:16,680
所以没有必要搞一个编译器出来

332
00:12:16,680 --> 00:12:19,320
更多的去做一些pattern的修改就好了

333
00:12:19,920 --> 00:12:20,760
废话就不多说

334
00:12:20,760 --> 00:12:24,400
我们回到整个转化模块的工作流程里面

335
00:12:24,600 --> 00:12:26,080
去看一下刚才的架构图

336
00:12:26,200 --> 00:12:29,080
针对我们的工作流程有什幺不一样

337
00:12:29,200 --> 00:12:30,960
左边的这个是转化模块

338
00:12:30,960 --> 00:12:33,840
我们用了蓝色底来去给大家划分

339
00:12:33,840 --> 00:12:36,320
右边的这个就是优化的模块

340
00:12:36,320 --> 00:12:38,600
用了黄色底去进行划分

341
00:12:38,600 --> 00:12:41,360
我们可以看到确实也有很多个converter

342
00:12:41,360 --> 00:12:43,160
有非常多的转换器

343
00:12:43,520 --> 00:12:44,800
通过不同的转换器

344
00:12:44,800 --> 00:12:48,040
把不同的AI框架训练出来的一个网络模型

345
00:12:48,920 --> 00:12:51,000
转换成为我们推定引擎的IR

346
00:12:51,000 --> 00:12:52,520
有了推定引擎的IR之后

347
00:12:52,600 --> 00:12:54,720
我们现在就变成一个统一的表达了

348
00:12:54,720 --> 00:12:56,600
于是在做后面的优化

349
00:12:56,720 --> 00:12:58,240
我分开了三段

350
00:12:59,080 --> 00:13:00,920
一段叫做pre-optimize

351
00:13:00,920 --> 00:13:03,040
一段叫做正式的optimize

352
00:13:03,040 --> 00:13:05,600
最后一段叫post-optimize

353
00:13:05,600 --> 00:13:07,800
三个阶段有三个不同的内容

354
00:13:08,120 --> 00:13:11,640
这个也是Zombie看到很多推定引擎的项目代码

355
00:13:11,640 --> 00:13:12,960
所总结出来的

356
00:13:13,840 --> 00:13:17,080
首先会对转化模块传过来的一个计算图

357
00:13:17,080 --> 00:13:18,960
做一些公共表达式的消除

358
00:13:18,960 --> 00:13:19,880
死代码的消除

359
00:13:19,880 --> 00:13:21,240
还有怠速简化

360
00:13:21,520 --> 00:13:24,040
常用的怠速简化消除的功能

361
00:13:25,200 --> 00:13:26,640
自行一些公共的功能之后

362
00:13:26,800 --> 00:13:28,480
就正式的对我们的计算图

363
00:13:28,480 --> 00:13:31,360
对我们的神经网络的一些知识涌进来了

364
00:13:31,560 --> 00:13:33,600
第一点就有算子融合算子替换

365
00:13:33,600 --> 00:13:34,760
还有常量折叠

366
00:13:34,760 --> 00:13:37,440
这个就是最常用的一些方式

367
00:13:37,440 --> 00:13:39,640
也是我们作为中间优化层

368
00:13:40,840 --> 00:13:42,520
来到了post-optimize这个阶段

369
00:13:42,640 --> 00:13:45,520
其实代表我们的计算图基本上能换的就换了

370
00:13:46,040 --> 00:13:48,400
更多的是对数据的格式转换

371
00:13:48,480 --> 00:13:50,880
NCHW到NHWC这种

372
00:13:50,880 --> 00:13:52,400
还有内存的布局计算

373
00:13:52,400 --> 00:13:55,120
另外会把一些重复的算子把它和平掉

374
00:13:55,760 --> 00:13:58,080
到这个阶段已经没有太多了

375
00:13:58,160 --> 00:14:01,040
更多的在前面这两个阶段已经把它干完了

376
00:14:01,040 --> 00:14:02,280
在最后一个阶段

377
00:14:02,600 --> 00:14:03,880
主要是对我们的数据

378
00:14:03,880 --> 00:14:07,160
对我们的内存进行一些管理和预管理的工作

379
00:14:09,160 --> 00:14:10,760
好了今天的内容就这幺多

380
00:14:10,760 --> 00:14:11,760
我们回过一下

381
00:14:11,760 --> 00:14:14,640
这里面跟大家一起汇报了一下

382
00:14:14,640 --> 00:14:17,440
模型转换的遇到了一些挑战和目标

383
00:14:17,680 --> 00:14:21,360
接着我们看了一下计算图优化的一些挑战和目标

384
00:14:21,360 --> 00:14:22,600
带着这些挑战和目标

385
00:14:22,720 --> 00:14:25,600
我们就去设计了推理引擎的整体的架构

386
00:14:25,640 --> 00:14:28,760
然后把架构图变成我们的工作流程图

387
00:14:28,760 --> 00:14:31,040
把每一个模块都梳理清楚

388
00:14:32,440 --> 00:14:34,080
卷的不行了

389
00:14:34,080 --> 00:14:35,920
记得一键三连加关注哦

390
00:14:35,920 --> 00:14:38,920
所有的内容都会开源在下面这条链接里面

391
00:14:39,520 --> 00:14:40,240
摆了个掰

