0
0:00:00.800 --> 0:00:04.760
巴巴巴巴巴巴巴阿

1
0:00:05.240 --> 0:00:09.720
哈喽大家好下面的字词呢估计关注的人或者看的人呢就确实不多

2
0:00:09.720 --> 0:00:15.680
如果你正在看证明你可能是非常关注这个领域或者正在做这个领域的

3
0:00:15.680 --> 0:00:18.160
下面呢我们来到模型转换

4
0:00:18.200 --> 0:00:21.480
里面最重要的一个模块就是模型的优化

5
0:00:21.520 --> 0:00:24.760
那这里面的最重要的就是对计算图进行优化

6
0:00:24.760 --> 0:00:28.440
我们在上一集里面呢讲了一下怎么去制定一个计算图

7
0:00:28.440 --> 0:00:30.280
然后还有计算图的基本流程

8
0:00:30.280 --> 0:00:35.120
下面我们来看一下计算图的优化具体的细节内容

9
0:00:35.120 --> 0:00:38.560
就是这里面的details计算图优化的详解

10
0:00:38.560 --> 0:00:47.080
那在计算图优化呢其实我们在上一节里面去给大家普及过分为basic is time还有layout和memory三种的优化方式

11
0:00:47.080 --> 0:00:51.280
三种优化方式呢对应到整个推定型的计算流

12
0:00:51.280 --> 0:00:56.000
在预优化的阶段呢会进行很多代数相关的一些优化和简化

13
0:00:56.000 --> 0:01:01.280
接着呢在增增优化阶段会更多地结合我们神经网络的一些知识进行优化

14
0:01:01.280 --> 0:01:05.960
在后优化阶段呢更多的是对一些数据的格式啊内存的布局的重排

15
0:01:05.960 --> 0:01:10.960
还有一些很重要核心的重复的算子的坑路进行合并

16
0:01:10.960 --> 0:01:19.560
在正式进入后面任何的内容当中呢我想希望大家去看一看我之前发的一系列的视频就是AI编译器的前端优化

17
0:01:19.560 --> 0:01:23.880
因为后面的很多图优化的一些原理都会在这里面

18
0:01:23.920 --> 0:01:31.600
后面的基本上就不会讲原理了这里面更多的都是一些原理性的知识后面都是一些非常硬核的具体的内容

19
0:01:32.960 --> 0:01:53.840
哎注意了不是所有图优化都是基于模板去写的而是只有推定型或者大部分推定型呢都会基于模板来写在AI框架当中它是不一样的像AI框架呢我们回顾一下主要是我们在TVM我们假设以TVM为例子它的一个算子融合或者它的一个图优化的方式是建立了通过SD啊把派森的代码转换成

20
0:01:53.840 --> 0:02:19.600
为TVM里面的伪类的埃亚然后变立这个伪类的埃亚或者伪类数呢去建立整个DAG图通过DAG图呢用于后面的自配数的分析有了自配数之后呢就会应用真正的算子融合的一些算法去实现计算图的优化那可以看到像TVM这种呢更多的是去发现一些常用的规则去对计算图进行一个融合优化

21
0:02:20.600 --> 0:02:42.600
说明老师你好我有个问题哦像你刚才提到像AI框架或者AI编译器呢它的图优化采用基于规则数或者特殊的数的埃亚的方式进行融合优化吗那为什么推理引擎里面的图优化呢采用hardcore硬编码规则模型匹配的方式呢

22
0:02:42.600 --> 0:02:43.400
哎你问的这个问题非常好我简单复述理解一下其实像我们之前讲到的AI编译器或者AI框架更多的做一些计算图的优化呢是通过编译方式去做的而推理引擎的图优化了更多的是基于模板匹配或者一些hardcore的方式去写的大家都知道通过hardcore或者模板匹配的方式确实不能复盖非常多的场景只能复盖一些有用常用的场景而通过编译的方式呢确实我们可以复盖很多

23
0:03:12.600 --> 0:03:42.600
常为的一些应用正因为这个原因呢我们在推理引擎啊大部分都是针对于我们一些常用的一些模型进行部署而AI框架因为大家用来做创新的所以更多地去考虑到常为的问题而AI框架呢大部分都是在计算服务中心或者有很强的算力平台上面去执行的所以说时间对它来说不是说非常重要我们可以采取很多具IT的别的方式来去提升一些性能而推理引擎呢这个图优化大部分都是离线的或者我们叫做AOT

24
0:03:42.600 --> 0:03:49.920
的方式进行一些模板匹配或hardcore更好地去复盖我们主要的场景就可以了这也是它们最大的区别哦

25
0:03:51.160 --> 0:03:57.040
下面呢我们来看看计算图优化的详解计算图优化详解里面的内容特别特别的多

26
0:03:57.040 --> 0:03:57.960
项市基础的图优化的内容呢就特别的多了包括长量的止跌了勇于节点的消除算子的融合算子替换算子的迁移我们会讲非常多的内容可能呢里面会分开好几个内容来去讲更多的是去讲真正的融合规则而不是去讲为什么要这么融合了所以说下面的内容会越来越难或者也越来越难懂大家去记住就好了现在我们看第一个内容就是我们的欧一一constant floating长量则跌那长量止跌呢

27
0:04:27.040 --> 0:04:57.040
这是编译优化的一个技术啊对我们编译时的长量或者长量的表达是进行计算来去简化代码的在计算图里面呢就可以预先地去确定输出节点的值替换成长量就把长量这个则跌引掉了然后对计算图呢进行一些结构简化的操作我们下面呢看一些具体的例子好像现在有的一些constant float呢就是长量的止跌还有binary的止跌我们看一下具体的图这里面后面的字呢我就不简单地读了像这种呢

28
0:04:57.040 --> 0:05:27.040
有两个长量输进去然后有个op一和op二但是呢可以看到op一啊它是接收两个长量作为输入这些长量呢在我们离线的时候呢其实可以把它算出来把它算完之后呢作为一个新的长量输给我们的op二这种就是最常见的长量止跌推定引擎里面最重要的或者宗米之前写过的这种一个长量止跌的公式呢其实是bin则跌bin则跌也就是利用了这个原理所以大家简单地理解一下就好了下面呢我们看一下

29
0:05:28.040 --> 0:05:29.120
一种止跌方式

30
0:05:30.400 --> 0:05:39.400
当止跌定的第二个维度就指定维度的数的时候是长量那么就可以把它止跌进去参数的方式放在止跌定这个算子里面

31
0:05:39.520 --> 0:05:46.240
然后呢就少了一个算子因为const它有可能是一个算子或者有可能呢是占用内存的一块空间

32
0:05:46.480 --> 0:05:55.560
那下面呢还有binary止跌那binary止跌呢其实跟刚才的止跌定的止跌差不多它里面的数呢可能是个标量那这时候呢就把标量

33
0:05:55.560 --> 0:06:04.520
变成binary的一个参数然后进行一个计算的那这个时候呢可以看到少了一个计算的节点对我们的计算来说呢确实有很多的好处

34
0:06:05.520 --> 0:06:15.400
接着呢我们看一下第二个内容就是计算图的优化勇于节点的消除勇于节点的消除里面这些内容啊特别特别多啊就没有用的节点进行消除

35
0:06:15.400 --> 0:06:23.400
那这里面呢我们分开好几个分类第一个呢就是op本身没有意义就这个算子本身是没有意义的所以我们就会把它去掉

36
0:06:23.400 --> 0:06:53.400
例如开始转换之前的前后类型都相同的concat只有一个输入的tensor还有runops和print和set和stopgradient和split这些算子其实都可以干掉那这个时候呢我们就会有一系列的规则去写一系列的模板去删掉这些没有用的算子包括dropout这种在训练的过程当中呢去有用的算子我们在退役的时候呢或者在退役引擎转换的时候呢都会把它干掉那么简单地看几个图像这里面呢有一个勇于的算

37
0:06:53.400 --> 0:07:23.400
子输入进来的时候呢我们就会把这个算子干掉那这种啊这个算子的输入跟输出会把上一个算子的输跟输出把它连回来但是呢有种就是这个算子的输出呢对下一个算子是没有意义的那这个时候呢我们就会把它切成两个子图一个子图呢就是op一input一个子图呢就是op二进行一个具体的计算那第三种情况就是像这个勇于的算子它的输入对它来说是没用的既然这个输入对它来说

38
0:07:23.400 --> 0:07:53.400
是没用的那前面这个计算是不是也是没用的呀既然是这样那就会迭代式地去网上轮寻删除网上的节点只要这个节点的输入没有意义那证明这个节点它是没有意义的因为它的输出没有人接那这个时候就可以把这个算子干掉它的算子如果也是这种情况呢也会一直一直轮寻把它网上的算子也干掉这种呢就是删除op就是这个算子本身没有意义的算子它就有三种方式呢

39
0:07:53.400 --> 0:08:23.400
我们看一下op的参数没有意义也就是说这个算子呢其实有意义的但是呢当你设定为具体某些参数的时候或者某些区别的时候呢它就没有意义我们举个最典型的例子就tensor的costcost的算子呢主要是对数据的排布进行一个转换的假设我的输入的参数等于输出的参数的时候假设我现在有个数据呢nchw我把它cost成nchw

40
0:08:23.400 --> 0:08:53.400
相同的那我干嘛要超这个算子呢所以呢就可以把这个算子干掉当然了我们还有很多种情况啊像是类似的index大等于零index end呢等于channel减一的时候呢这个算子是没有意义的像isbring的时候呢当输出的shift等于输入的shift的时候也是没有意义的当破脸的参数或者划窗了等于乘一它也是没有用的所以我们看一下下面的这个图呢假设这个cost的算子它的source等于destination的时候呢这个算子就没有意义

41
0:08:53.400 --> 0:09:23.400
把这个算子干掉那像这种的isbring定的时候假设这个shift呢跟输入的shift是一样的我们就把这个算子干掉那像破脸的时候呢我们等于一乘一也是没有用的像start等于某些特殊特性的时候也是没有用的也把这个算子干掉那像这种的确实在我们的神经网络里面呢会出现大量的冗余节点而总比在具体的实践当中就我之前在项目交付的时候呢会做过相关的工作确实也会把这些算子干掉之后呢性能提升了非常的多

42
0:09:23.400 --> 0:09:26.360
而且网络模型啊确实简化了非常的多

43
0:09:26.360 --> 0:09:30.120
另外还有一些OP的位置没有意义的

44
0:09:30.120 --> 0:09:31.680
这没有非常的多呀

45
0:09:31.680 --> 0:09:45.960
所以大家如果真想了解里面的细节呢可以翻看我gear上面里面的关于推理引擎的很多的内容很多的slide啊就PPT我都已经开源开放了还有一些对应的video也会放在这里面

46
0:09:45.960 --> 0:09:53.360
那回到我们的话题我们这里面就不一一去过了我们看一个具体的一些例子啊确实非常的多呀

47
0:09:53.360 --> 0:10:22.600
这里面除了飞腾的消除重复的消除还有很多那我们看一些具体的一些图那看图说话确实非常的乐观像这种开始的算子假设没有意义的我们就会把它删掉像恩西肯的算子时候没有意义也把它删掉那有时候呢我的input给OP一的时候呢OP一的输出它是没有输出的没有算子接的那我算来干嘛了这个它没有人要那所以这个时候呢就可以把这条分支给干掉最后还有就是google pooling

48
0:10:22.600 --> 0:10:52.600
它后面接一些wechat啊或者飞腾的时候呢其实也是没有意义的我们也其实可以把这些算子干掉所以说干掉的这些算子可以哎还有非常多哎像咏玉节点消除确实里面写了非常多的passed后面这两个图呢其实比较相似假设我有两个wechat两个wechat都是相反的时候这个时候呢我们就可以把它都干掉cast a到b然后cast b到a那我还不如把这两个算子直接干掉就好了它的语义相反哎这里面就讲到了ops

49
0:10:52.600 --> 0:11:22.600
然后两个的语义相反那这个时候呢两个OP都可以把它干掉哎那这里面呢也有非常非常的多例如squeeze跟sprint呢它却是语义相反的还有两个cast呢它可能也是语义相反的像我们量化和反量化假设它连在一起也是没有意义的可以把它删掉contact跟slist也是可以删掉语义相反的我们都可以干掉我们可以看一下下面的具体的图像sprint定了就是扩充的维度

50
0:11:22.600 --> 0:11:52.600
不同的维度合在一起这种呢确实也可以干掉我一个扩充一个合并那我就干脆啥都不做就行了当然它里面的参数或者里面的轴啊大家要注意一下就得相对应才行那后面像这种contact在slist呢还不如我直接slist就完了所以说它基本上呢很多很相似的地方下面呢我们看一下呃勇于节点消除的第四个内容呢勇于节点消除特别特别的多第四个内容就是公共纸图的消除

51
0:11:52.600 --> 0:12:22.600
就是把一些有大模块的两个完全相同的纸图把它干掉那我们看一下这个图我们假设有三个输入三个输入呢对应的是给op一op二op三去执行假设我们右边又有一个分支同样给op一op二op三去执行的时候这个时候呢我们可以把红色把黄色这块干掉黄色啊我没有色盲后把黄色这块干掉就剩下左边的这个纸图了这种就是公共纸图的消除其实呢刚才讲了很多算法怎么去

52
0:12:22.600 --> 0:12:52.600
寻找图其实大部分都是一些经典的立刻的算法的题目例如公共纸图消除呢就是寻找公共纸素则有兴趣的同学啊或者你不明白为什么要刷立刻之后你发现哎为什么我写的代码基本都跟立刻相关的为什么都都是做数的检索所以呢其实这个就是刷立刻的好处或者为什么我们很多大厂做面试的时候呢也是需要进行一个立刻的面试这也真的是有它的好处和理由的可能你平时在做一些简单的应用API的时候

53
0:12:52.600 --> 0:13:22.600
只是调用人家一些库你觉得你根本没有必要去写但是你呢去写一些核心的代码或者真正做一些开创性的内容的时候你就会发现哎你们很多知识可以真的是可以接近过来的好了今天的内容就这么多我们将会在下一节当中分享更多的图优化的一些算法和内容谢谢各位摆了个杯卷得不行了卷得不行了记得一键三连加关注哦所有的内容都会开源在下面这条链接里面

54
0:13:22.600 --> 0:13:24.450
摆了个杯

