1
00:00:00,000 --> 00:00:06,240
Hello大家好

2
00:00:06,240 --> 00:00:09,720
下面的支持估计关注的人或者看的人就确实不多

3
00:00:09,720 --> 00:00:13,680
如果你正在看证明你可能是非常关注这个领域

4
00:00:13,680 --> 00:00:15,520
或者正在做这个领域的

5
00:00:15,520 --> 00:00:18,080
下面我们来到模型转换

6
00:00:18,080 --> 00:00:21,480
里面最重要的一个模块就是模型的优化

7
00:00:21,480 --> 00:00:24,760
这里面最重要的就是对计算图进行优化

8
00:00:24,760 --> 00:00:28,480
我们在上一节里面讲了一下怎幺去制定一个计算图

9
00:00:28,480 --> 00:00:30,240
然后还有计算图的基本流程

10
00:00:30,240 --> 00:00:35,080
下面我们来看一下计算图的优化具体的细节内容

11
00:00:35,080 --> 00:00:38,520
就是这里面的details计算图优化的详解

12
00:00:38,520 --> 00:00:42,440
在计算图优化其实我们在上一节里面去给大家普及过

13
00:00:42,440 --> 00:00:47,080
分为basic, extend,还有layout和memory三种的优化方式

14
00:00:47,080 --> 00:00:51,280
三种优化方式对应到整个推定引擎的计算流

15
00:00:51,280 --> 00:00:55,960
在预优化的阶段会进行很多代数相关的一些优化和简化

16
00:00:56,000 --> 00:01:01,240
接着在增增优化阶段会更多的结合我们神经网络的一些知识进行优化

17
00:01:01,240 --> 00:01:04,320
在后优化阶段更多的是对一些数据的格式

18
00:01:04,320 --> 00:01:05,880
内存的布局的重排

19
00:01:05,880 --> 00:01:09,880
还有一些很重要内核的重复的算子的kernel进行合并

20
00:01:10,880 --> 00:01:13,640
在正式进入后面硬核的内容当中

21
00:01:13,640 --> 00:01:17,440
我想希望大家去看一看我之前发的一系列的视频

22
00:01:17,440 --> 00:01:19,480
就是AI编译器的前端优化

23
00:01:19,480 --> 00:01:23,840
因为后面的很多图优化的一些原理都会在这里面

24
00:01:23,840 --> 00:01:26,200
后面基本上就不会讲原理了

25
00:01:26,200 --> 00:01:29,080
这里面更多的都是一些原理性的知识

26
00:01:29,080 --> 00:01:31,760
后面都是一些非常硬核的具体的内容

27
00:01:32,760 --> 00:01:36,680
注意了不是所有图优化都是基于模板去写的

28
00:01:36,680 --> 00:01:40,440
而是只有推定引擎或者大部分推定引擎都会基于模板来写

29
00:01:40,440 --> 00:01:42,640
在AI框架当中它是不一样的

30
00:01:42,640 --> 00:01:44,440
像AI框架我们回顾一下

31
00:01:44,440 --> 00:01:47,200
主要是我们在TVM我们假设以TM为例子

32
00:01:47,200 --> 00:01:50,320
它的一个算子融合或者它的一个图优化的方式

33
00:01:50,400 --> 00:01:55,640
是创建了通过SD把python的代码转换成为TVM里面的relate IR

34
00:01:55,640 --> 00:02:01,160
然后便利这个relate IR或者relate数去创建整个DAG图

35
00:02:01,160 --> 00:02:05,080
通过DAG图用于后面的自配数的分析

36
00:02:05,080 --> 00:02:09,640
有了自配数之后就会应用真正的算子融合的一些算法

37
00:02:09,640 --> 00:02:11,800
去实现计算图的优化

38
00:02:11,800 --> 00:02:16,960
可以看到像TVM这种更多的是去发现一些常用的规则

39
00:02:16,960 --> 00:02:19,720
去对计算图进行一个融合优化

40
00:02:21,280 --> 00:02:22,720
说明老师你好

41
00:02:22,720 --> 00:02:24,240
我有个问题

42
00:02:24,240 --> 00:02:29,320
像你刚才提到像AI框架或者AI编译器它的图优化

43
00:02:29,320 --> 00:02:34,920
采用基于规则数或者特殊的数的IR的方式进行融合优化吗

44
00:02:34,920 --> 00:02:39,160
那为什幺推理引擎里面的图优化采用hardcore

45
00:02:39,160 --> 00:02:42,600
硬编码或者模型匹配的方式呢

46
00:02:43,200 --> 00:02:46,080
你问的这个问题非常好

47
00:02:46,080 --> 00:02:48,000
我简单复述理解一下

48
00:02:48,000 --> 00:02:51,360
其实像我们之前讲到的AI编译器或者AI框架

49
00:02:51,360 --> 00:02:55,720
更多的做一些计算图的优化是通过编译方式去做的

50
00:02:55,720 --> 00:02:57,800
而推理引擎的图优化

51
00:02:57,800 --> 00:03:01,640
更多的是基于模板匹配或者一些hardcore的方式去写的

52
00:03:01,640 --> 00:03:04,880
大家都知道通过hardcore或者模板匹配的方式

53
00:03:04,880 --> 00:03:07,200
确实不能覆盖非常多的场景

54
00:03:07,200 --> 00:03:09,600
只能覆盖一些有用常用的场景

55
00:03:09,600 --> 00:03:11,200
而通过编译的方式

56
00:03:11,320 --> 00:03:14,080
确实我们可以覆盖很多常规的一些应用

57
00:03:14,080 --> 00:03:15,000
正因为这个原因

58
00:03:15,120 --> 00:03:20,240
我们在推理引擎大部分都是针对于我们一些常用的一些模型进行部署

59
00:03:20,240 --> 00:03:22,840
而AI框架因为大家用来做创新的

60
00:03:22,840 --> 00:03:25,160
所以更多的去考虑到场尾的问题

61
00:03:25,160 --> 00:03:28,560
而AI框架大部分都是在计算服务中心

62
00:03:28,560 --> 00:03:31,240
或者有很强的算力平台上面去执行的

63
00:03:31,240 --> 00:03:33,960
所以说时间对它来说不是说非常重要

64
00:03:33,960 --> 00:03:37,960
我们可以采取很多GIT的编译方式来去提升一些性能

65
00:03:37,960 --> 00:03:41,240
而推理引擎图优化大部分都是离线的

66
00:03:41,240 --> 00:03:43,360
或者我们叫做AOT的方式

67
00:03:43,400 --> 00:03:45,120
进行一些模板匹配或Hardcore

68
00:03:45,120 --> 00:03:48,280
更好的去覆盖我们主要的场景就可以了

69
00:03:48,280 --> 00:03:49,680
这也是它们最大的区别

70
00:03:51,080 --> 00:03:53,880
下面我们来看看计算图优化的详解

71
00:03:53,880 --> 00:03:57,080
计算图优化详解里面的内容特别的多

72
00:03:57,880 --> 00:04:01,600
像是基础的图优化的内容就特别的多了

73
00:04:01,600 --> 00:04:02,800
包括常量的字叠

74
00:04:02,920 --> 00:04:04,080
勇于节点的消除

75
00:04:04,080 --> 00:04:04,840
算子的融合

76
00:04:04,840 --> 00:04:05,520
算子替换

77
00:04:05,520 --> 00:04:06,800
用算子的潜移

78
00:04:06,800 --> 00:04:08,320
我们会讲非常多的内容

79
00:04:08,320 --> 00:04:10,600
可能里面会分开好几个内容来去讲

80
00:04:10,600 --> 00:04:13,000
更多的是去讲真正的融合规则

81
00:04:13,120 --> 00:04:15,160
而不是去讲为什幺要这幺融合了

82
00:04:15,160 --> 00:04:17,120
所以说下面的内容会越来越难

83
00:04:17,120 --> 00:04:18,800
或者也越来越难懂

84
00:04:18,800 --> 00:04:20,320
大家去记住就好了

85
00:04:20,320 --> 00:04:21,920
现在我们看第一个内容

86
00:04:21,920 --> 00:04:24,520
就是我们的OE constant floating

87
00:04:24,520 --> 00:04:25,760
常量字叠

88
00:04:25,760 --> 00:04:28,560
那常量字叠它其实是编译优化的一个技术

89
00:04:28,840 --> 00:04:31,640
对我们编译时的常量或者常量的表达式

90
00:04:31,640 --> 00:04:34,280
进行计算来去简化代码的

91
00:04:34,800 --> 00:04:37,160
在计算图里面就可以预先的去确定

92
00:04:37,160 --> 00:04:39,920
输出节点的值替换成常量

93
00:04:39,920 --> 00:04:42,200
就把常量这个字叠引掉了

94
00:04:42,320 --> 00:04:45,400
然后对计算图进行一些结构简化的操作

95
00:04:45,400 --> 00:04:47,840
我们下面看一些具体的例子

96
00:04:47,840 --> 00:04:49,680
好像现在有的一些constant float

97
00:04:49,760 --> 00:04:51,200
就是常量的值叠

98
00:04:51,200 --> 00:04:52,560
还有binary的值叠

99
00:04:52,560 --> 00:04:53,680
我们看一下具体的图

100
00:04:53,680 --> 00:04:55,760
这里面后面的字我就不简单的读了

101
00:04:56,080 --> 00:04:58,480
像这种我有两个常量输进去

102
00:04:58,480 --> 00:05:00,360
然后有一个OP1和OP2

103
00:05:00,800 --> 00:05:02,080
但是可以看到OP1

104
00:05:02,200 --> 00:05:04,600
它是接收两个常量作为输入

105
00:05:04,600 --> 00:05:06,560
这些常量在我们离线的时候

106
00:05:06,840 --> 00:05:08,480
其实可以把它算出来

107
00:05:08,480 --> 00:05:09,360
把它算完之后

108
00:05:09,520 --> 00:05:10,880
作为一个新的常量

109
00:05:10,920 --> 00:05:11,920
输给我们的OP2

110
00:05:11,920 --> 00:05:14,040
这种就是最常见的常量值叠

111
00:05:14,920 --> 00:05:16,160
推进引擎里面最重要的

112
00:05:16,160 --> 00:05:17,600
或者周米之前写过的

113
00:05:17,600 --> 00:05:19,120
这种一个常量值叠的公式

114
00:05:19,640 --> 00:05:21,360
其实是bin值叠

115
00:05:21,360 --> 00:05:23,280
bin值叠也就是利用了这个原理

116
00:05:23,280 --> 00:05:25,080
所以大家简单的理解一下就好了

117
00:05:25,720 --> 00:05:26,880
下面我们看一下

118
00:05:26,880 --> 00:05:29,120
explain dim的一种字叠方式

119
00:05:30,280 --> 00:05:32,320
当explain dim的第二个维度

120
00:05:32,320 --> 00:05:34,440
就指定维度的输入的时候是常量

121
00:05:34,440 --> 00:05:37,120
那我们就可以把它直叠进去参数的方式

122
00:05:37,120 --> 00:05:39,320
放在explain dim这个算子里面

123
00:05:39,440 --> 00:05:41,240
然后就少了一个算子

124
00:05:41,240 --> 00:05:43,200
因为constant它有可能是一个算子

125
00:05:43,200 --> 00:05:46,200
或者有可能是占用内存的一块空间

126
00:05:46,520 --> 00:05:48,440
下面还有binary值叠

127
00:05:48,560 --> 00:05:49,320
binary值叠

128
00:05:49,440 --> 00:05:51,360
其实跟刚才的explain dim的

129
00:05:51,360 --> 00:05:52,240
值叠差不多

130
00:05:52,240 --> 00:05:54,160
它里面的输入可能是个标量

131
00:05:54,280 --> 00:05:57,600
这时候就把标量变成binary的一个参数

132
00:05:57,600 --> 00:05:59,440
然后进行一个计算的

133
00:05:59,600 --> 00:06:00,520
这个时候可以看到

134
00:06:00,520 --> 00:06:01,840
少了一个计算的节点

135
00:06:01,840 --> 00:06:02,920
对我们的计算来说

136
00:06:03,040 --> 00:06:04,560
确实有很多的好处

137
00:06:05,560 --> 00:06:07,080
接着我们看一下第二个内容

138
00:06:07,080 --> 00:06:08,200
就是计算图的优化

139
00:06:08,200 --> 00:06:10,320
勇于节点的消除

140
00:06:10,320 --> 00:06:11,160
勇于节点的消除

141
00:06:11,160 --> 00:06:12,760
里面这些内容特别的多

142
00:06:12,880 --> 00:06:15,400
就没有用的节点进行消除

143
00:06:15,600 --> 00:06:17,000
这里面我们分开好几个分类

144
00:06:17,000 --> 00:06:19,000
第一个就是op本身没有意义

145
00:06:19,000 --> 00:06:21,600
就这个算子本身是没有意义的

146
00:06:21,600 --> 00:06:23,440
所以我们就会把它去掉

147
00:06:23,440 --> 00:06:25,720
例如const转换之前的前后类型

148
00:06:25,720 --> 00:06:26,600
都是相同的

149
00:06:26,600 --> 00:06:29,160
concat只有一个输入的tensor

150
00:06:29,160 --> 00:06:30,520
还有learn-opposite print

151
00:06:30,520 --> 00:06:31,760
assert stop gradient

152
00:06:31,760 --> 00:06:32,360
split

153
00:06:32,440 --> 00:06:35,040
这些算子其实都可以干掉

154
00:06:35,240 --> 00:06:37,200
这个时候我们就会有一系列的规则

155
00:06:37,200 --> 00:06:38,600
去写一系列的模板

156
00:06:38,600 --> 00:06:40,600
去删掉这些没有用的算子

157
00:06:40,600 --> 00:06:41,680
包括dropout这种

158
00:06:41,680 --> 00:06:42,800
在训练的过程当中

159
00:06:42,920 --> 00:06:44,000
去有用的算子

160
00:06:44,000 --> 00:06:44,920
我们在退集的时候

161
00:06:45,120 --> 00:06:46,480
或者在推理引擎转换的时候

162
00:06:46,920 --> 00:06:48,440
都会把它干掉

163
00:06:48,440 --> 00:06:51,080
那幺简单的看几个图

164
00:06:51,080 --> 00:06:53,760
像这里面有一个勇于的算子

165
00:06:53,760 --> 00:06:54,720
输入进来的时候

166
00:06:54,800 --> 00:06:56,640
我们就会把这个算子干掉

167
00:06:56,840 --> 00:06:59,400
这种算子的输入跟输出

168
00:06:59,400 --> 00:07:01,080
会把上一个算子的输入跟输出

169
00:07:01,080 --> 00:07:02,320
把它连回来

170
00:07:02,520 --> 00:07:05,080
但是有种就是这个算子的输出

171
00:07:05,240 --> 00:07:07,080
对下一个算子是没有意义的

172
00:07:07,280 --> 00:07:10,360
这个时候我们就会把它切成两个子图

173
00:07:10,360 --> 00:07:12,080
一个子图就是op1 input

174
00:07:12,080 --> 00:07:15,080
一个子图就是op2进行一个具体的计算

175
00:07:16,200 --> 00:07:19,600
第三种情况就是像勇于的算子

176
00:07:19,600 --> 00:07:21,880
它的输入对它来说是没用的

177
00:07:22,160 --> 00:07:24,400
既然这个输入对它来说是没用的

178
00:07:24,400 --> 00:07:26,880
那前面的计算是不是也是没用的

179
00:07:27,360 --> 00:07:28,080
既然是这样

180
00:07:28,080 --> 00:07:30,640
那就会迭代式的去网上

181
00:07:30,640 --> 00:07:33,160
轮循删除网上的节点

182
00:07:33,160 --> 00:07:34,840
只要这个节点的输入没有意义

183
00:07:35,000 --> 00:07:37,280
证明这个节点它是没有意义的

184
00:07:37,280 --> 00:07:39,000
因为它的输出没有人接

185
00:07:39,160 --> 00:07:41,160
这个时候就可以把这个算子干掉

186
00:07:41,160 --> 00:07:42,840
它的算子如果也是这种情况

187
00:07:43,000 --> 00:07:44,560
也会一直轮循

188
00:07:44,560 --> 00:07:46,640
把它网上的算子也干掉

189
00:07:46,640 --> 00:07:49,760
这种就是删除op就是这个算子

190
00:07:49,760 --> 00:07:50,920
本身没有意义的算子

191
00:07:50,920 --> 00:07:52,040
它就有三种方式

192
00:07:52,800 --> 00:07:56,040
接下来我们看一下op的参数没有意义

193
00:07:56,040 --> 00:07:58,600
也就是说这个算子其实是有意义的

194
00:07:58,600 --> 00:08:02,400
但是当你设定为具体某些参数的时候

195
00:08:02,400 --> 00:08:03,520
或者某些区别的时候

196
00:08:03,760 --> 00:08:04,720
它就没有意义

197
00:08:04,800 --> 00:08:06,920
我们举个最典型的例子

198
00:08:06,920 --> 00:08:08,000
就tensor的cust

199
00:08:08,360 --> 00:08:11,000
cust的算子主要是对数据的排布

200
00:08:11,000 --> 00:08:12,560
进行一个转换的

201
00:08:12,560 --> 00:08:14,400
假设我的输入的参数

202
00:08:14,400 --> 00:08:16,280
等于输出的参数的时候

203
00:08:16,880 --> 00:08:18,040
假设我现在有个数据

204
00:08:18,160 --> 00:08:19,080
nchw

205
00:08:19,080 --> 00:08:21,640
我把它cust成nchw

206
00:08:22,080 --> 00:08:23,760
我cust前后都是相同的

207
00:08:23,880 --> 00:08:25,000
我干嘛要操这个算子

208
00:08:25,320 --> 00:08:27,800
所以就可以把这个算子干掉

209
00:08:28,840 --> 00:08:30,320
当然了我们还有很多种情况

210
00:08:30,440 --> 00:08:32,280
像是list的index大等于0

211
00:08:32,280 --> 00:08:34,920
index end等于channel-1的时候

212
00:08:35,080 --> 00:08:36,160
这个算子是没有意义的

213
00:08:36,160 --> 00:08:37,280
像exprime的时候

214
00:08:37,440 --> 00:08:40,320
当输出的shape等于输入的shape的时候

215
00:08:40,320 --> 00:08:41,520
也是没有意义的

216
00:08:41,760 --> 00:08:43,800
当破脸的参数或者滑窗的等于乘1

217
00:08:43,800 --> 00:08:45,200
它也是没有用的

218
00:08:45,200 --> 00:08:47,200
所以我们看一下下面的图

219
00:08:48,120 --> 00:08:49,480
假设cust的算子

220
00:08:49,480 --> 00:08:51,440
它的source等于destination的时候

221
00:08:51,640 --> 00:08:52,600
这个算子就没有意义

222
00:08:52,600 --> 00:08:54,640
我们把cust的算子干掉

223
00:08:54,920 --> 00:08:56,320
像这种exprime dim的时候

224
00:08:56,320 --> 00:08:58,720
假设这个shape跟输入的shape是一样的

225
00:08:58,920 --> 00:09:01,520
我们就把这个算子干掉

226
00:09:01,840 --> 00:09:02,800
像pooling的时候

227
00:09:02,880 --> 00:09:04,480
我们等于1乘1也是没有用的

228
00:09:04,480 --> 00:09:06,880
像start等于某些特殊特性的时候

229
00:09:06,880 --> 00:09:07,720
也是没有用的

230
00:09:07,720 --> 00:09:09,760
也把这个算子干掉

231
00:09:10,040 --> 00:09:12,240
像这种确实在我们的神经网络里面

232
00:09:12,360 --> 00:09:14,040
会出现大量的冗余节点

233
00:09:14,040 --> 00:09:15,960
而总比在具体的实践当中

234
00:09:16,160 --> 00:09:17,800
就我之前在项目交付的时候

235
00:09:17,960 --> 00:09:19,600
会做过相关的工作

236
00:09:19,600 --> 00:09:21,640
确实也会把这些算子干掉之后

237
00:09:21,800 --> 00:09:23,240
性能提升了非常的多

238
00:09:23,240 --> 00:09:26,200
而且网络模型确实简化了非常的多

239
00:09:26,200 --> 00:09:29,560
另外还有一些OP的位置没有意义的

240
00:09:30,080 --> 00:09:31,440
这里面有非常的多

241
00:09:31,600 --> 00:09:34,400
所以大家如果真想了解里面的细节

242
00:09:34,600 --> 00:09:36,720
可以翻看我Github上面里面的

243
00:09:36,720 --> 00:09:39,000
关于推理引擎的很多的内容

244
00:09:39,000 --> 00:09:40,640
很多的slide就PPT

245
00:09:40,640 --> 00:09:42,440
我都已经开源开放了

246
00:09:42,440 --> 00:09:43,720
还有一些对应的video

247
00:09:43,720 --> 00:09:45,000
也会放在这里面

248
00:09:45,920 --> 00:09:47,280
回到我们的话题

249
00:09:47,280 --> 00:09:49,320
我们这里面就不一一去过了

250
00:09:49,320 --> 00:09:51,760
我们看一个具体的一些例子

251
00:09:52,000 --> 00:09:53,240
确实非常的多

252
00:09:53,400 --> 00:09:54,680
这里面除了factor的消除

253
00:09:54,680 --> 00:09:55,680
重复的消除

254
00:09:55,680 --> 00:09:56,760
还有很多

255
00:09:56,960 --> 00:09:59,280
我们看一些具体的一些图

256
00:09:59,400 --> 00:10:02,360
看图说话确实非常之乐观

257
00:10:02,360 --> 00:10:04,200
像这种Custom的算子脚是没有意义的

258
00:10:04,200 --> 00:10:05,560
我们就会把它删掉

259
00:10:05,560 --> 00:10:07,320
像Nsequence的算子时候没有意义

260
00:10:07,320 --> 00:10:08,600
也把它删掉

261
00:10:08,840 --> 00:10:11,040
有时候我的input给OP1的时候

262
00:10:11,200 --> 00:10:12,480
OP1的输出

263
00:10:12,480 --> 00:10:13,440
它是没有输出的

264
00:10:13,440 --> 00:10:14,240
没有算子接的

265
00:10:14,360 --> 00:10:15,200
我算来干嘛了

266
00:10:15,640 --> 00:10:16,440
它没有人要

267
00:10:16,560 --> 00:10:20,080
所以这个时候就可以把这条分支给干掉

268
00:10:20,920 --> 00:10:22,560
最后还有就是Google pooling

269
00:10:22,560 --> 00:10:23,920
它后面接一些we shape

270
00:10:23,920 --> 00:10:24,800
或者factor的时候

271
00:10:24,880 --> 00:10:26,040
其实也是没有意义的

272
00:10:26,040 --> 00:10:28,680
我们也其实可以把这些算子干掉

273
00:10:28,800 --> 00:10:30,440
所以说干掉的这些算子

274
00:10:30,440 --> 00:10:32,040
可以还有非常多

275
00:10:32,720 --> 00:10:33,840
像勇于节点消除

276
00:10:33,840 --> 00:10:35,880
确实里面写了非常多的past

277
00:10:36,320 --> 00:10:38,240
后面这两个图其实比较相似

278
00:10:38,240 --> 00:10:39,600
假设我有两个we shape

279
00:10:39,600 --> 00:10:41,440
两个we shape都是相反的时候

280
00:10:41,440 --> 00:10:43,640
这个时候我们就可以把它都干掉

281
00:10:43,640 --> 00:10:45,040
Custom A到B

282
00:10:45,040 --> 00:10:46,600
然后Custom B到A

283
00:10:46,800 --> 00:10:49,240
我还不如把这两个算子直接干掉就好了

284
00:10:49,240 --> 00:10:50,440
它的语义相反

285
00:10:50,840 --> 00:10:54,840
这里面就讲到了OPS前后两个的语义相反

286
00:10:55,080 --> 00:10:57,960
这个时候两个OP都可以把它干掉

287
00:10:58,560 --> 00:11:00,880
这里面也有非常多

288
00:11:00,880 --> 00:11:02,520
例如Squish跟Spline

289
00:11:02,600 --> 00:11:04,040
它确实语义相反的

290
00:11:04,040 --> 00:11:05,240
还有两个Custom

291
00:11:05,480 --> 00:11:07,160
它可能也是语义相反的

292
00:11:07,160 --> 00:11:09,360
像我们量化和反量化

293
00:11:09,360 --> 00:11:10,680
假设它连在一起

294
00:11:10,680 --> 00:11:11,840
也是没有意义的

295
00:11:11,840 --> 00:11:12,920
可以把它删掉

296
00:11:12,920 --> 00:11:15,680
Concate跟Splist也是可以删掉

297
00:11:15,680 --> 00:11:17,400
语义相反的我们都可以干掉

298
00:11:17,400 --> 00:11:19,400
我们可以看一下下面的具体的图

299
00:11:19,400 --> 00:11:21,800
像Spline dim就是扩充的维度

300
00:11:21,800 --> 00:11:23,880
Squish就把不同的维度合在一起

301
00:11:23,880 --> 00:11:25,640
这种确实也可以干掉

302
00:11:25,640 --> 00:11:27,040
我一个扩充一个合并

303
00:11:27,160 --> 00:11:28,960
我就干脆啥都不做就行了

304
00:11:28,960 --> 00:11:31,240
当然它里面的参数或者里面的轴

305
00:11:31,680 --> 00:11:32,680
大家要注意一下

306
00:11:32,680 --> 00:11:35,080
就得相对应才行

307
00:11:35,440 --> 00:11:37,040
后面像这种Concate跟Splist

308
00:11:37,640 --> 00:11:39,440
还不如我直接Splist就完了

309
00:11:39,440 --> 00:11:42,560
所以说它基本上很多很相似的地方

310
00:11:42,560 --> 00:11:44,160
下面我们看一下

311
00:11:44,640 --> 00:11:46,640
勇于节点消除的第4个内容

312
00:11:46,760 --> 00:11:48,680
勇于节点消除特别的多

313
00:11:48,680 --> 00:11:51,720
第4个内容就是公共纸图的消除

314
00:11:51,720 --> 00:11:54,800
公共纸图就是把一些有大模块的

315
00:11:54,800 --> 00:11:56,320
两个完全相同的纸图

316
00:11:56,480 --> 00:11:57,280
把它干掉

317
00:11:57,360 --> 00:11:58,240
我们看一下这个图

318
00:11:58,240 --> 00:12:00,160
我们假设有三个输入

319
00:12:00,160 --> 00:12:03,600
三个输入对应的是给OP1 OP2 OP3去执行

320
00:12:03,600 --> 00:12:05,280
假设我们右边又有一个分支

321
00:12:05,280 --> 00:12:07,840
同样给OP1 OP2 OP3去执行的时候

322
00:12:07,840 --> 00:12:09,720
这个时候我们可以把红色

323
00:12:09,720 --> 00:12:11,200
把黄色的这块干掉

324
00:12:11,200 --> 00:12:12,600
黄色我没有色盲

325
00:12:13,240 --> 00:12:14,240
把黄色的这块干掉

326
00:12:14,240 --> 00:12:16,080
就剩下左边的纸图了

327
00:12:16,080 --> 00:12:18,160
这种就是公共纸图的消除

328
00:12:19,520 --> 00:12:21,920
其实刚才讲了很多算法

329
00:12:21,920 --> 00:12:23,120
怎幺去能寻的图

330
00:12:23,120 --> 00:12:25,000
其实大部分都是一些经典的

331
00:12:25,040 --> 00:12:26,680
leetcode算法的题目

332
00:12:26,680 --> 00:12:27,880
例如公共纸图消除

333
00:12:28,000 --> 00:12:29,760
就是寻找公共子数

334
00:12:29,760 --> 00:12:31,000
所以有兴趣的同学

335
00:12:31,160 --> 00:12:33,240
或者你不明白为什幺要刷leetcode

336
00:12:33,240 --> 00:12:35,360
就你发现为什幺我写的代码

337
00:12:35,360 --> 00:12:36,480
基本上都跟你关系相关的

338
00:12:36,480 --> 00:12:38,520
为什幺都是做数的检索

339
00:12:38,520 --> 00:12:41,960
所以其实这个就是刷leetcode的好处

340
00:12:41,960 --> 00:12:44,640
或者为什幺我们很多大厂做面试的时候

341
00:12:44,800 --> 00:12:47,400
也是需要进行一个leetcode的面试

342
00:12:47,400 --> 00:12:49,800
这真的是有它的好处和理由的

343
00:12:49,800 --> 00:12:52,000
可能你平时在做一些简单的应用

344
00:12:52,000 --> 00:12:52,640
API的时候

345
00:12:52,640 --> 00:12:54,160
只是调用人家一些库

346
00:12:54,200 --> 00:12:56,080
你觉得你根本没有必要去写

347
00:12:56,080 --> 00:12:58,200
但是你去写一些内核的代码

348
00:12:58,200 --> 00:13:00,560
或者真正做一些开创性的内容的时候

349
00:13:00,560 --> 00:13:01,360
你就会发现

350
00:13:01,840 --> 00:13:02,720
你们很多知识

351
00:13:02,720 --> 00:13:04,560
真的是可以借鉴过来的

352
00:13:04,920 --> 00:13:06,640
好了今天的内容就这幺多

353
00:13:06,640 --> 00:13:08,240
我们将会在下一节当中

354
00:13:08,240 --> 00:13:12,560
分享更多的图优化的一些算法和内容

355
00:13:12,560 --> 00:13:13,520
谢谢各位

356
00:13:13,520 --> 00:13:14,960
拜了个拜

357
00:13:15,920 --> 00:13:17,600
卷的不行了

358
00:13:17,600 --> 00:13:19,040
记得一键三连加关注

359
00:13:19,440 --> 00:13:20,800
所有的内容都会开源

360
00:13:20,800 --> 00:13:22,600
在下面这条链接里面

361
00:13:22,600 --> 00:13:24,000
拜了个拜

