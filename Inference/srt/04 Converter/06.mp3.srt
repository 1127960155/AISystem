0
0:00:00.000 --> 0:00:06.240
Hello大家好

1
0:00:06.240 --> 0:00:09.720
下面的支持估计关注的人或者看的人就确实不多

2
0:00:09.720 --> 0:00:13.680
如果你正在看证明你可能是非常关注这个领域

3
0:00:13.680 --> 0:00:15.520
或者正在做这个领域的

4
0:00:15.520 --> 0:00:18.080
下面我们来到模型转换

5
0:00:18.080 --> 0:00:21.480
里面最重要的一个模块就是模型的优化

6
0:00:21.480 --> 0:00:24.760
这里面最重要的就是对计算图进行优化

7
0:00:24.760 --> 0:00:28.480
我们在上一节里面讲了一下怎幺去制定一个计算图

8
0:00:28.480 --> 0:00:30.240
然后还有计算图的基本流程

9
0:00:30.240 --> 0:00:35.080
下面我们来看一下计算图的优化具体的细节内容

10
0:00:35.080 --> 0:00:38.520
就是这里面的details计算图优化的详解

11
0:00:38.520 --> 0:00:42.440
在计算图优化其实我们在上一节里面去给大家普及过

12
0:00:42.440 --> 0:00:47.080
分为basic, extend,还有layout和memory三种的优化方式

13
0:00:47.080 --> 0:00:51.280
三种优化方式对应到整个推定引擎的计算流

14
0:00:51.280 --> 0:00:55.960
在预优化的阶段会进行很多代数相关的一些优化和简化

15
0:00:56.000 --> 0:01:01.240
接着在增增优化阶段会更多的结合我们神经网络的一些知识进行优化

16
0:01:01.240 --> 0:01:04.320
在后优化阶段更多的是对一些数据的格式

17
0:01:04.320 --> 0:01:05.880
内存的布局的重排

18
0:01:05.880 --> 0:01:09.880
还有一些很重要内核的重复的算子的kernel进行合并

19
0:01:10.880 --> 0:01:13.640
在正式进入后面硬核的内容当中

20
0:01:13.640 --> 0:01:17.440
我想希望大家去看一看我之前发的一系列的视频

21
0:01:17.440 --> 0:01:19.480
就是AI编译器的前端优化

22
0:01:19.480 --> 0:01:23.840
因为后面的很多图优化的一些原理都会在这里面

23
0:01:23.840 --> 0:01:26.200
后面基本上就不会讲原理了

24
0:01:26.200 --> 0:01:29.080
这里面更多的都是一些原理性的知识

25
0:01:29.080 --> 0:01:31.760
后面都是一些非常硬核的具体的内容

26
0:01:32.760 --> 0:01:36.680
注意了不是所有图优化都是基于模板去写的

27
0:01:36.680 --> 0:01:40.440
而是只有推定引擎或者大部分推定引擎都会基于模板来写

28
0:01:40.440 --> 0:01:42.640
在AI框架当中它是不一样的

29
0:01:42.640 --> 0:01:44.440
像AI框架我们回顾一下

30
0:01:44.440 --> 0:01:47.200
主要是我们在TVM我们假设以TM为例子

31
0:01:47.200 --> 0:01:50.320
它的一个算子融合或者它的一个图优化的方式

32
0:01:50.400 --> 0:01:55.640
是创建了通过SD把python的代码转换成为TVM里面的relate IR

33
0:01:55.640 --> 0:02:01.160
然后便利这个relate IR或者relate数去创建整个DAG图

34
0:02:01.160 --> 0:02:05.080
通过DAG图用于后面的自配数的分析

35
0:02:05.080 --> 0:02:09.640
有了自配数之后就会应用真正的算子融合的一些算法

36
0:02:09.640 --> 0:02:11.800
去实现计算图的优化

37
0:02:11.800 --> 0:02:16.960
可以看到像TVM这种更多的是去发现一些常用的规则

38
0:02:16.960 --> 0:02:19.720
去对计算图进行一个融合优化

39
0:02:21.280 --> 0:02:22.720
说明老师你好

40
0:02:22.720 --> 0:02:24.240
我有个问题

41
0:02:24.240 --> 0:02:29.320
像你刚才提到像AI框架或者AI编译器它的图优化

42
0:02:29.320 --> 0:02:34.920
采用基于规则数或者特殊的数的IR的方式进行融合优化吗

43
0:02:34.920 --> 0:02:39.160
那为什幺推理引擎里面的图优化采用hardcore

44
0:02:39.160 --> 0:02:42.600
硬编码或者模型匹配的方式呢

45
0:02:43.200 --> 0:02:46.080
你问的这个问题非常好

46
0:02:46.080 --> 0:02:48.000
我简单复述理解一下

47
0:02:48.000 --> 0:02:51.360
其实像我们之前讲到的AI编译器或者AI框架

48
0:02:51.360 --> 0:02:55.720
更多的做一些计算图的优化是通过编译方式去做的

49
0:02:55.720 --> 0:02:57.800
而推理引擎的图优化

50
0:02:57.800 --> 0:03:01.640
更多的是基于模板匹配或者一些hardcore的方式去写的

51
0:03:01.640 --> 0:03:04.880
大家都知道通过hardcore或者模板匹配的方式

52
0:03:04.880 --> 0:03:07.200
确实不能覆盖非常多的场景

53
0:03:07.200 --> 0:03:09.600
只能覆盖一些有用常用的场景

54
0:03:09.600 --> 0:03:11.200
而通过编译的方式

55
0:03:11.320 --> 0:03:14.080
确实我们可以覆盖很多常规的一些应用

56
0:03:14.080 --> 0:03:15.000
正因为这个原因

57
0:03:15.120 --> 0:03:20.240
我们在推理引擎大部分都是针对于我们一些常用的一些模型进行部署

58
0:03:20.240 --> 0:03:22.840
而AI框架因为大家用来做创新的

59
0:03:22.840 --> 0:03:25.160
所以更多的去考虑到场尾的问题

60
0:03:25.160 --> 0:03:28.560
而AI框架大部分都是在计算服务中心

61
0:03:28.560 --> 0:03:31.240
或者有很强的算力平台上面去执行的

62
0:03:31.240 --> 0:03:33.960
所以说时间对它来说不是说非常重要

63
0:03:33.960 --> 0:03:37.960
我们可以采取很多GIT的编译方式来去提升一些性能

64
0:03:37.960 --> 0:03:41.240
而推理引擎图优化大部分都是离线的

65
0:03:41.240 --> 0:03:43.360
或者我们叫做AOT的方式

66
0:03:43.400 --> 0:03:45.120
进行一些模板匹配或Hardcore

67
0:03:45.120 --> 0:03:48.280
更好的去覆盖我们主要的场景就可以了

68
0:03:48.280 --> 0:03:49.680
这也是它们最大的区别

69
0:03:51.080 --> 0:03:53.880
下面我们来看看计算图优化的详解

70
0:03:53.880 --> 0:03:57.080
计算图优化详解里面的内容特别的多

71
0:03:57.880 --> 0:04:01.600
像是基础的图优化的内容就特别的多了

72
0:04:01.600 --> 0:04:02.800
包括常量的字叠

73
0:04:02.920 --> 0:04:04.080
勇于节点的消除

74
0:04:04.080 --> 0:04:04.840
算子的融合

75
0:04:04.840 --> 0:04:05.520
算子替换

76
0:04:05.520 --> 0:04:06.800
用算子的潜移

77
0:04:06.800 --> 0:04:08.320
我们会讲非常多的内容

78
0:04:08.320 --> 0:04:10.600
可能里面会分开好几个内容来去讲

79
0:04:10.600 --> 0:04:13.000
更多的是去讲真正的融合规则

80
0:04:13.120 --> 0:04:15.160
而不是去讲为什幺要这幺融合了

81
0:04:15.160 --> 0:04:17.120
所以说下面的内容会越来越难

82
0:04:17.120 --> 0:04:18.800
或者也越来越难懂

83
0:04:18.800 --> 0:04:20.320
大家去记住就好了

84
0:04:20.320 --> 0:04:21.920
现在我们看第一个内容

85
0:04:21.920 --> 0:04:24.520
就是我们的OE constant floating

86
0:04:24.520 --> 0:04:25.760
常量字叠

87
0:04:25.760 --> 0:04:28.560
那常量字叠它其实是编译优化的一个技术

88
0:04:28.840 --> 0:04:31.640
对我们编译时的常量或者常量的表达式

89
0:04:31.640 --> 0:04:34.280
进行计算来去简化代码的

90
0:04:34.800 --> 0:04:37.160
在计算图里面就可以预先的去确定

91
0:04:37.160 --> 0:04:39.920
输出节点的值替换成常量

92
0:04:39.920 --> 0:04:42.200
就把常量这个字叠引掉了

93
0:04:42.320 --> 0:04:45.400
然后对计算图进行一些结构简化的操作

94
0:04:45.400 --> 0:04:47.840
我们下面看一些具体的例子

95
0:04:47.840 --> 0:04:49.680
好像现在有的一些constant float

96
0:04:49.760 --> 0:04:51.200
就是常量的值叠

97
0:04:51.200 --> 0:04:52.560
还有binary的值叠

98
0:04:52.560 --> 0:04:53.680
我们看一下具体的图

99
0:04:53.680 --> 0:04:55.760
这里面后面的字我就不简单的读了

100
0:04:56.080 --> 0:04:58.480
像这种我有两个常量输进去

101
0:04:58.480 --> 0:05:00.360
然后有一个OP1和OP2

102
0:05:00.800 --> 0:05:02.080
但是可以看到OP1

103
0:05:02.200 --> 0:05:04.600
它是接收两个常量作为输入

104
0:05:04.600 --> 0:05:06.560
这些常量在我们离线的时候

105
0:05:06.840 --> 0:05:08.480
其实可以把它算出来

106
0:05:08.480 --> 0:05:09.360
把它算完之后

107
0:05:09.520 --> 0:05:10.880
作为一个新的常量

108
0:05:10.920 --> 0:05:11.920
输给我们的OP2

109
0:05:11.920 --> 0:05:14.040
这种就是最常见的常量值叠

110
0:05:14.920 --> 0:05:16.160
推进引擎里面最重要的

111
0:05:16.160 --> 0:05:17.600
或者周米之前写过的

112
0:05:17.600 --> 0:05:19.120
这种一个常量值叠的公式

113
0:05:19.640 --> 0:05:21.360
其实是bin值叠

114
0:05:21.360 --> 0:05:23.280
bin值叠也就是利用了这个原理

115
0:05:23.280 --> 0:05:25.080
所以大家简单的理解一下就好了

116
0:05:25.720 --> 0:05:26.880
下面我们看一下

117
0:05:26.880 --> 0:05:29.120
explain dim的一种字叠方式

118
0:05:30.280 --> 0:05:32.320
当explain dim的第二个维度

119
0:05:32.320 --> 0:05:34.440
就指定维度的输入的时候是常量

120
0:05:34.440 --> 0:05:37.120
那我们就可以把它直叠进去参数的方式

121
0:05:37.120 --> 0:05:39.320
放在explain dim这个算子里面

122
0:05:39.440 --> 0:05:41.240
然后就少了一个算子

123
0:05:41.240 --> 0:05:43.200
因为constant它有可能是一个算子

124
0:05:43.200 --> 0:05:46.200
或者有可能是占用内存的一块空间

125
0:05:46.520 --> 0:05:48.440
下面还有binary值叠

126
0:05:48.560 --> 0:05:49.320
binary值叠

127
0:05:49.440 --> 0:05:51.360
其实跟刚才的explain dim的

128
0:05:51.360 --> 0:05:52.240
值叠差不多

129
0:05:52.240 --> 0:05:54.160
它里面的输入可能是个标量

130
0:05:54.280 --> 0:05:57.600
这时候就把标量变成binary的一个参数

131
0:05:57.600 --> 0:05:59.440
然后进行一个计算的

132
0:05:59.600 --> 0:06:00.520
这个时候可以看到

133
0:06:00.520 --> 0:06:01.840
少了一个计算的节点

134
0:06:01.840 --> 0:06:02.920
对我们的计算来说

135
0:06:03.040 --> 0:06:04.560
确实有很多的好处

136
0:06:05.560 --> 0:06:07.080
接着我们看一下第二个内容

137
0:06:07.080 --> 0:06:08.200
就是计算图的优化

138
0:06:08.200 --> 0:06:10.320
勇于节点的消除

139
0:06:10.320 --> 0:06:11.160
勇于节点的消除

140
0:06:11.160 --> 0:06:12.760
里面这些内容特别的多

141
0:06:12.880 --> 0:06:15.400
就没有用的节点进行消除

142
0:06:15.600 --> 0:06:17.000
这里面我们分开好几个分类

143
0:06:17.000 --> 0:06:19.000
第一个就是op本身没有意义

144
0:06:19.000 --> 0:06:21.600
就这个算子本身是没有意义的

145
0:06:21.600 --> 0:06:23.440
所以我们就会把它去掉

146
0:06:23.440 --> 0:06:25.720
例如const转换之前的前后类型

147
0:06:25.720 --> 0:06:26.600
都是相同的

148
0:06:26.600 --> 0:06:29.160
concat只有一个输入的tensor

149
0:06:29.160 --> 0:06:30.520
还有learn-opposite print

150
0:06:30.520 --> 0:06:31.760
assert stop gradient

151
0:06:31.760 --> 0:06:32.360
split

152
0:06:32.440 --> 0:06:35.040
这些算子其实都可以干掉

153
0:06:35.240 --> 0:06:37.200
这个时候我们就会有一系列的规则

154
0:06:37.200 --> 0:06:38.600
去写一系列的模板

155
0:06:38.600 --> 0:06:40.600
去删掉这些没有用的算子

156
0:06:40.600 --> 0:06:41.680
包括dropout这种

157
0:06:41.680 --> 0:06:42.800
在训练的过程当中

158
0:06:42.920 --> 0:06:44.000
去有用的算子

159
0:06:44.000 --> 0:06:44.920
我们在退集的时候

160
0:06:45.120 --> 0:06:46.480
或者在推理引擎转换的时候

161
0:06:46.920 --> 0:06:48.440
都会把它干掉

162
0:06:48.440 --> 0:06:51.080
那幺简单的看几个图

163
0:06:51.080 --> 0:06:53.760
像这里面有一个勇于的算子

164
0:06:53.760 --> 0:06:54.720
输入进来的时候

165
0:06:54.800 --> 0:06:56.640
我们就会把这个算子干掉

166
0:06:56.840 --> 0:06:59.400
这种算子的输入跟输出

167
0:06:59.400 --> 0:07:01.080
会把上一个算子的输入跟输出

168
0:07:01.080 --> 0:07:02.320
把它连回来

169
0:07:02.520 --> 0:07:05.080
但是有种就是这个算子的输出

170
0:07:05.240 --> 0:07:07.080
对下一个算子是没有意义的

171
0:07:07.280 --> 0:07:10.360
这个时候我们就会把它切成两个子图

172
0:07:10.360 --> 0:07:12.080
一个子图就是op1 input

173
0:07:12.080 --> 0:07:15.080
一个子图就是op2进行一个具体的计算

174
0:07:16.200 --> 0:07:19.600
第三种情况就是像勇于的算子

175
0:07:19.600 --> 0:07:21.880
它的输入对它来说是没用的

176
0:07:22.160 --> 0:07:24.400
既然这个输入对它来说是没用的

177
0:07:24.400 --> 0:07:26.880
那前面的计算是不是也是没用的

178
0:07:27.360 --> 0:07:28.080
既然是这样

179
0:07:28.080 --> 0:07:30.640
那就会迭代式的去网上

180
0:07:30.640 --> 0:07:33.160
轮循删除网上的节点

181
0:07:33.160 --> 0:07:34.840
只要这个节点的输入没有意义

182
0:07:35.000 --> 0:07:37.280
证明这个节点它是没有意义的

183
0:07:37.280 --> 0:07:39.000
因为它的输出没有人接

184
0:07:39.160 --> 0:07:41.160
这个时候就可以把这个算子干掉

185
0:07:41.160 --> 0:07:42.840
它的算子如果也是这种情况

186
0:07:43.000 --> 0:07:44.560
也会一直轮循

187
0:07:44.560 --> 0:07:46.640
把它网上的算子也干掉

188
0:07:46.640 --> 0:07:49.760
这种就是删除op就是这个算子

189
0:07:49.760 --> 0:07:50.920
本身没有意义的算子

190
0:07:50.920 --> 0:07:52.040
它就有三种方式

191
0:07:52.800 --> 0:07:56.040
接下来我们看一下op的参数没有意义

192
0:07:56.040 --> 0:07:58.600
也就是说这个算子其实是有意义的

193
0:07:58.600 --> 0:08:02.400
但是当你设定为具体某些参数的时候

194
0:08:02.400 --> 0:08:03.520
或者某些区别的时候

195
0:08:03.760 --> 0:08:04.720
它就没有意义

196
0:08:04.800 --> 0:08:06.920
我们举个最典型的例子

197
0:08:06.920 --> 0:08:08.000
就tensor的cust

198
0:08:08.360 --> 0:08:11.000
cust的算子主要是对数据的排布

199
0:08:11.000 --> 0:08:12.560
进行一个转换的

200
0:08:12.560 --> 0:08:14.400
假设我的输入的参数

201
0:08:14.400 --> 0:08:16.280
等于输出的参数的时候

202
0:08:16.880 --> 0:08:18.040
假设我现在有个数据

203
0:08:18.160 --> 0:08:19.080
nchw

204
0:08:19.080 --> 0:08:21.640
我把它cust成nchw

205
0:08:22.080 --> 0:08:23.760
我cust前后都是相同的

206
0:08:23.880 --> 0:08:25.000
我干嘛要操这个算子

207
0:08:25.320 --> 0:08:27.800
所以就可以把这个算子干掉

208
0:08:28.840 --> 0:08:30.320
当然了我们还有很多种情况

209
0:08:30.440 --> 0:08:32.280
像是list的index大等于0

210
0:08:32.280 --> 0:08:34.920
index end等于channel-1的时候

211
0:08:35.080 --> 0:08:36.160
这个算子是没有意义的

212
0:08:36.160 --> 0:08:37.280
像exprime的时候

213
0:08:37.440 --> 0:08:40.320
当输出的shape等于输入的shape的时候

214
0:08:40.320 --> 0:08:41.520
也是没有意义的

215
0:08:41.760 --> 0:08:43.800
当破脸的参数或者滑窗的等于乘1

216
0:08:43.800 --> 0:08:45.200
它也是没有用的

217
0:08:45.200 --> 0:08:47.200
所以我们看一下下面的图

218
0:08:48.120 --> 0:08:49.480
假设cust的算子

219
0:08:49.480 --> 0:08:51.440
它的source等于destination的时候

220
0:08:51.640 --> 0:08:52.600
这个算子就没有意义

221
0:08:52.600 --> 0:08:54.640
我们把cust的算子干掉

222
0:08:54.920 --> 0:08:56.320
像这种exprime dim的时候

223
0:08:56.320 --> 0:08:58.720
假设这个shape跟输入的shape是一样的

224
0:08:58.920 --> 0:09:01.520
我们就把这个算子干掉

225
0:09:01.840 --> 0:09:02.800
像pooling的时候

226
0:09:02.880 --> 0:09:04.480
我们等于1乘1也是没有用的

227
0:09:04.480 --> 0:09:06.880
像start等于某些特殊特性的时候

228
0:09:06.880 --> 0:09:07.720
也是没有用的

229
0:09:07.720 --> 0:09:09.760
也把这个算子干掉

230
0:09:10.040 --> 0:09:12.240
像这种确实在我们的神经网络里面

231
0:09:12.360 --> 0:09:14.040
会出现大量的冗余节点

232
0:09:14.040 --> 0:09:15.960
而总比在具体的实践当中

233
0:09:16.160 --> 0:09:17.800
就我之前在项目交付的时候

234
0:09:17.960 --> 0:09:19.600
会做过相关的工作

235
0:09:19.600 --> 0:09:21.640
确实也会把这些算子干掉之后

236
0:09:21.800 --> 0:09:23.240
性能提升了非常的多

237
0:09:23.240 --> 0:09:26.200
而且网络模型确实简化了非常的多

238
0:09:26.200 --> 0:09:29.560
另外还有一些OP的位置没有意义的

239
0:09:30.080 --> 0:09:31.440
这里面有非常的多

240
0:09:31.600 --> 0:09:34.400
所以大家如果真想了解里面的细节

241
0:09:34.600 --> 0:09:36.720
可以翻看我Github上面里面的

242
0:09:36.720 --> 0:09:39.000
关于推理引擎的很多的内容

243
0:09:39.000 --> 0:09:40.640
很多的slide就PPT

244
0:09:40.640 --> 0:09:42.440
我都已经开源开放了

245
0:09:42.440 --> 0:09:43.720
还有一些对应的video

246
0:09:43.720 --> 0:09:45.000
也会放在这里面

247
0:09:45.920 --> 0:09:47.280
回到我们的话题

248
0:09:47.280 --> 0:09:49.320
我们这里面就不一一去过了

249
0:09:49.320 --> 0:09:51.760
我们看一个具体的一些例子

250
0:09:52.000 --> 0:09:53.240
确实非常的多

251
0:09:53.400 --> 0:09:54.680
这里面除了factor的消除

252
0:09:54.680 --> 0:09:55.680
重复的消除

253
0:09:55.680 --> 0:09:56.760
还有很多

254
0:09:56.960 --> 0:09:59.280
我们看一些具体的一些图

255
0:09:59.400 --> 0:10:02.360
看图说话确实非常之乐观

256
0:10:02.360 --> 0:10:04.200
像这种Custom的算子脚是没有意义的

257
0:10:04.200 --> 0:10:05.560
我们就会把它删掉

258
0:10:05.560 --> 0:10:07.320
像Nsequence的算子时候没有意义

259
0:10:07.320 --> 0:10:08.600
也把它删掉

260
0:10:08.840 --> 0:10:11.040
有时候我的input给OP1的时候

261
0:10:11.200 --> 0:10:12.480
OP1的输出

262
0:10:12.480 --> 0:10:13.440
它是没有输出的

263
0:10:13.440 --> 0:10:14.240
没有算子接的

264
0:10:14.360 --> 0:10:15.200
我算来干嘛了

265
0:10:15.640 --> 0:10:16.440
它没有人要

266
0:10:16.560 --> 0:10:20.080
所以这个时候就可以把这条分支给干掉

267
0:10:20.920 --> 0:10:22.560
最后还有就是Google pooling

268
0:10:22.560 --> 0:10:23.920
它后面接一些we shape

269
0:10:23.920 --> 0:10:24.800
或者factor的时候

270
0:10:24.880 --> 0:10:26.040
其实也是没有意义的

271
0:10:26.040 --> 0:10:28.680
我们也其实可以把这些算子干掉

272
0:10:28.800 --> 0:10:30.440
所以说干掉的这些算子

273
0:10:30.440 --> 0:10:32.040
可以还有非常多

274
0:10:32.720 --> 0:10:33.840
像勇于节点消除

275
0:10:33.840 --> 0:10:35.880
确实里面写了非常多的past

276
0:10:36.320 --> 0:10:38.240
后面这两个图其实比较相似

277
0:10:38.240 --> 0:10:39.600
假设我有两个we shape

278
0:10:39.600 --> 0:10:41.440
两个we shape都是相反的时候

279
0:10:41.440 --> 0:10:43.640
这个时候我们就可以把它都干掉

280
0:10:43.640 --> 0:10:45.040
Custom A到B

281
0:10:45.040 --> 0:10:46.600
然后Custom B到A

282
0:10:46.800 --> 0:10:49.240
我还不如把这两个算子直接干掉就好了

283
0:10:49.240 --> 0:10:50.440
它的语义相反

284
0:10:50.840 --> 0:10:54.840
这里面就讲到了OPS前后两个的语义相反

285
0:10:55.080 --> 0:10:57.960
这个时候两个OP都可以把它干掉

286
0:10:58.560 --> 0:11:00.880
这里面也有非常多

287
0:11:00.880 --> 0:11:02.520
例如Squish跟Spline

288
0:11:02.600 --> 0:11:04.040
它确实语义相反的

289
0:11:04.040 --> 0:11:05.240
还有两个Custom

290
0:11:05.480 --> 0:11:07.160
它可能也是语义相反的

291
0:11:07.160 --> 0:11:09.360
像我们量化和反量化

292
0:11:09.360 --> 0:11:10.680
假设它连在一起

293
0:11:10.680 --> 0:11:11.840
也是没有意义的

294
0:11:11.840 --> 0:11:12.920
可以把它删掉

295
0:11:12.920 --> 0:11:15.680
Concate跟Splist也是可以删掉

296
0:11:15.680 --> 0:11:17.400
语义相反的我们都可以干掉

297
0:11:17.400 --> 0:11:19.400
我们可以看一下下面的具体的图

298
0:11:19.400 --> 0:11:21.800
像Spline dim就是扩充的维度

299
0:11:21.800 --> 0:11:23.880
Squish就把不同的维度合在一起

300
0:11:23.880 --> 0:11:25.640
这种确实也可以干掉

301
0:11:25.640 --> 0:11:27.040
我一个扩充一个合并

302
0:11:27.160 --> 0:11:28.960
我就干脆啥都不做就行了

303
0:11:28.960 --> 0:11:31.240
当然它里面的参数或者里面的轴

304
0:11:31.680 --> 0:11:32.680
大家要注意一下

305
0:11:32.680 --> 0:11:35.080
就得相对应才行

306
0:11:35.440 --> 0:11:37.040
后面像这种Concate跟Splist

307
0:11:37.640 --> 0:11:39.440
还不如我直接Splist就完了

308
0:11:39.440 --> 0:11:42.560
所以说它基本上很多很相似的地方

309
0:11:42.560 --> 0:11:44.160
下面我们看一下

310
0:11:44.640 --> 0:11:46.640
勇于节点消除的第4个内容

311
0:11:46.760 --> 0:11:48.680
勇于节点消除特别的多

312
0:11:48.680 --> 0:11:51.720
第4个内容就是公共纸图的消除

313
0:11:51.720 --> 0:11:54.800
公共纸图就是把一些有大模块的

314
0:11:54.800 --> 0:11:56.320
两个完全相同的纸图

315
0:11:56.480 --> 0:11:57.280
把它干掉

316
0:11:57.360 --> 0:11:58.240
我们看一下这个图

317
0:11:58.240 --> 0:12:00.160
我们假设有三个输入

318
0:12:00.160 --> 0:12:03.600
三个输入对应的是给OP1 OP2 OP3去执行

319
0:12:03.600 --> 0:12:05.280
假设我们右边又有一个分支

320
0:12:05.280 --> 0:12:07.840
同样给OP1 OP2 OP3去执行的时候

321
0:12:07.840 --> 0:12:09.720
这个时候我们可以把红色

322
0:12:09.720 --> 0:12:11.200
把黄色的这块干掉

323
0:12:11.200 --> 0:12:12.600
黄色我没有色盲

324
0:12:13.240 --> 0:12:14.240
把黄色的这块干掉

325
0:12:14.240 --> 0:12:16.080
就剩下左边的纸图了

326
0:12:16.080 --> 0:12:18.160
这种就是公共纸图的消除

327
0:12:19.520 --> 0:12:21.920
其实刚才讲了很多算法

328
0:12:21.920 --> 0:12:23.120
怎幺去能寻的图

329
0:12:23.120 --> 0:12:25.000
其实大部分都是一些经典的

330
0:12:25.040 --> 0:12:26.680
leetcode算法的题目

331
0:12:26.680 --> 0:12:27.880
例如公共纸图消除

332
0:12:28.000 --> 0:12:29.760
就是寻找公共子数

333
0:12:29.760 --> 0:12:31.000
所以有兴趣的同学

334
0:12:31.160 --> 0:12:33.240
或者你不明白为什幺要刷leetcode

335
0:12:33.240 --> 0:12:35.360
就你发现为什幺我写的代码

336
0:12:35.360 --> 0:12:36.480
基本上都跟你关系相关的

337
0:12:36.480 --> 0:12:38.520
为什幺都是做数的检索

338
0:12:38.520 --> 0:12:41.960
所以其实这个就是刷leetcode的好处

339
0:12:41.960 --> 0:12:44.640
或者为什幺我们很多大厂做面试的时候

340
0:12:44.800 --> 0:12:47.400
也是需要进行一个leetcode的面试

341
0:12:47.400 --> 0:12:49.800
这真的是有它的好处和理由的

342
0:12:49.800 --> 0:12:52.000
可能你平时在做一些简单的应用

343
0:12:52.000 --> 0:12:52.640
API的时候

344
0:12:52.640 --> 0:12:54.160
只是调用人家一些库

345
0:12:54.200 --> 0:12:56.080
你觉得你根本没有必要去写

346
0:12:56.080 --> 0:12:58.200
但是你去写一些内核的代码

347
0:12:58.200 --> 0:13:00.560
或者真正做一些开创性的内容的时候

348
0:13:00.560 --> 0:13:01.360
你就会发现

349
0:13:01.840 --> 0:13:02.720
你们很多知识

350
0:13:02.720 --> 0:13:04.560
真的是可以借鉴过来的

351
0:13:04.920 --> 0:13:06.640
好了今天的内容就这幺多

352
0:13:06.640 --> 0:13:08.240
我们将会在下一节当中

353
0:13:08.240 --> 0:13:12.560
分享更多的图优化的一些算法和内容

354
0:13:12.560 --> 0:13:13.520
谢谢各位

355
0:13:13.520 --> 0:13:14.960
拜了个拜

356
0:13:15.920 --> 0:13:17.600
卷的不行了

357
0:13:17.600 --> 0:13:19.040
记得一键三连加关注

358
0:13:19.440 --> 0:13:20.800
所有的内容都会开源

359
0:13:20.800 --> 0:13:22.600
在下面这条链接里面

360
0:13:22.600 --> 0:13:24.000
拜了个拜

