0
0:00:00.000 --> 0:00:11.200
Hello,大家好,我们还是回到模型小型化里面的CNN小型化

1
0:00:11.200 --> 0:00:16.200
CNN小型化里面主要是聚焦于网络模型结构的小型化

2
0:00:16.200 --> 0:00:19.000
可以看到其实我们在上一节课里面

3
0:00:19.000 --> 0:00:21.800
就给大家分享了SquishNet,SquashNet,MobileNet

4
0:00:21.800 --> 0:00:25.800
这些网络模型结构更多的是提出了新的模型结构

5
0:00:25.800 --> 0:00:27.000
新的卷迹的算法

6
0:00:28.000 --> 0:00:31.600
像SquishNet就提出了自己的一个Squish的结构

7
0:00:31.600 --> 0:00:34.600
ServeNet就提出了对Channel进行Serve的操作

8
0:00:34.600 --> 0:00:38.400
MobileNet就提出了一个新的卷迹的压缩的方式

9
0:00:38.400 --> 0:00:41.000
而下面我们要讲的几个系列

10
0:00:41.000 --> 0:00:44.600
主要是ESPNet,FBNet,EfficientNet和GhostNet

11
0:00:44.600 --> 0:00:49.000
四个内容,下面我们看一下第一个内容ESPNet

12
0:00:51.800 --> 0:00:55.200
下面这个就是ESPNet的一个论文

13
0:00:55.200 --> 0:00:59.400
很有意思的就是ESPNet跟我们刚才的一些网络模型

14
0:00:59.400 --> 0:01:02.000
或者刚才介绍的一些组件网络模型不一样

15
0:01:02.000 --> 0:01:04.400
它主要是针对具体的小任务的

16
0:01:04.400 --> 0:01:08.200
就是Semantic Segmentation,图像的分割

17
0:01:08.200 --> 0:01:11.200
其实在做图像的分割有一种方法

18
0:01:11.200 --> 0:01:14.200
就是把我们刚才介绍的一些轻量化的模型

19
0:01:14.200 --> 0:01:16.000
作为网络模型的主干

20
0:01:16.000 --> 0:01:22.600
然后在具体的Bitbomb或者Head就替换掉YOLO或者SSD了

21
0:01:22.600 --> 0:01:25.600
这里面就提出了两个新的概念

22
0:01:25.600 --> 0:01:29.400
Efficient Special Pyramid,高效的金字塔的结构

23
0:01:29.400 --> 0:01:33.800
第二个就是Hawaki Feature Fusion,多层的特征融合

24
0:01:33.800 --> 0:01:36.000
现在我们看看有什幺不一样哦

25
0:01:36.000 --> 0:01:38.800
第一个左边的这个就是Stand的卷机

26
0:01:38.800 --> 0:01:42.200
普通的卷机是不断的卷卷卷卷卷,然后不断的卷机

27
0:01:42.200 --> 0:01:45.200
这里面这个高效的金字塔结构

28
0:01:45.200 --> 0:01:48.200
就是前面我有一个Pointwise的卷机

29
0:01:48.200 --> 0:01:51.200
接下来就提供了几种不同的Kernel大小

30
0:01:51.200 --> 0:01:53.200
进行一个空洞化的卷机

31
0:01:53.200 --> 0:01:56.000
分开两步叫做ESP的结构

32
0:01:56.000 --> 0:01:58.800
第二个就是HFF的结构

33
0:01:58.800 --> 0:02:00.400
像HFF的结构我们可以看到

34
0:02:00.400 --> 0:02:05.800
其实这里面有很多种不同的金字塔的网络模型的大小

35
0:02:05.800 --> 0:02:11.000
最后通过这种加SUM加Concate的网络模型结构的组合

36
0:02:11.000 --> 0:02:13.000
就变成了一个Hawaki Feature Fusion

37
0:02:13.000 --> 0:02:17.200
通过这种方式有效的去对我们的下午任务图像分割

38
0:02:17.200 --> 0:02:18.600
保持一定的精度

39
0:02:18.600 --> 0:02:21.600
但是降低了整个网络模型的参数量

40
0:02:24.000 --> 0:02:27.400
接下来我们看看第二篇文章

41
0:02:27.400 --> 0:02:30.200
这个系列的第二篇文章ESPNet V2

42
0:02:30.200 --> 0:02:35.200
下面第二个就是ESPNet V2第二个版本了

43
0:02:35.200 --> 0:02:37.200
确实好多这种系列版本

44
0:02:37.200 --> 0:02:40.200
我们看看它的网络模型结构的组成

45
0:02:40.200 --> 0:02:43.800
下面这个就是ESPNet的网络模型结构

46
0:02:43.800 --> 0:02:47.800
左边的这次刚才我们介绍的V1的一个ESP的Block

47
0:02:47.800 --> 0:02:50.800
右边的这个就是ESP的一个A的版本

48
0:02:50.800 --> 0:02:53.400
最后这个C图就是最终的版本

49
0:02:53.400 --> 0:02:54.800
我们看一下有什幺区别

50
0:02:54.800 --> 0:02:57.800
这种就是普通的Deadwise的卷机

51
0:02:57.800 --> 0:03:00.600
而作者换成GOOP的一个Deadwise的卷机

52
0:03:00.600 --> 0:03:04.200
就是进一步的提升了网络模型的一个轻量化

53
0:03:04.200 --> 0:03:07.600
接着第二个改变就是我们把Deadwise的卷机

54
0:03:07.600 --> 0:03:09.400
就是后面的一些卷机

55
0:03:09.400 --> 0:03:11.800
后面就增加了一个1x1的卷机

56
0:03:11.800 --> 0:03:15.800
进一步的提取它的一些特征的空间出来

57
0:03:15.800 --> 0:03:17.600
作者通过实验就发现

58
0:03:17.600 --> 0:03:19.200
再提取一层特征出来了

59
0:03:19.200 --> 0:03:22.000
其实我还不如先Concate之后

60
0:03:22.000 --> 0:03:24.200
再做一个GOOP的一个卷机

61
0:03:24.200 --> 0:03:27.200
那就把这种卷机的方式替换成GOOP卷机

62
0:03:27.200 --> 0:03:28.600
这种方式进行提取

63
0:03:28.600 --> 0:03:32.800
可以看到从左边的一个卷机换成GOOP的卷机

64
0:03:32.800 --> 0:03:35.200
然后增加一个GOOP特征的提取

65
0:03:35.200 --> 0:03:37.400
最后汇集起来

66
0:03:37.400 --> 0:03:40.800
这种就是ESPNet V2的特征

67
0:03:42.400 --> 0:03:44.800
介绍完ESPNet这种座椅一分个特征之后

68
0:03:44.800 --> 0:03:48.000
我们看看FBNet这个系列

69
0:03:48.800 --> 0:03:50.400
针对FBNet和EfficientNet

70
0:03:50.400 --> 0:03:53.400
我就不单独的去展开它们的论文了

71
0:03:53.400 --> 0:03:57.200
像FBNet其实已经推出了VV2 V3 V5

72
0:03:57.200 --> 0:03:59.200
不同的版本 版本量非常多

73
0:03:59.200 --> 0:04:02.400
而EfficientNet同样也推出了VV2的版本

74
0:04:02.400 --> 0:04:06.600
这两个版本是发生在2018年和2019年的

75
0:04:06.600 --> 0:04:09.000
这段时间最火的一个技术是什幺

76
0:04:09.600 --> 0:04:11.800
三顶的朋友们请告诉我

77
0:04:11.800 --> 0:04:17.600
2018年到2019年最火的一个网络模型搜索的技术是什幺

78
0:04:20.400 --> 0:04:21.600
NAS搜索

79
0:04:21.600 --> 0:04:22.000
对

80
0:04:22.000 --> 0:04:24.000
像FBNet和EfficientNet

81
0:04:24.000 --> 0:04:26.200
其实都是用了NAS的搜索方法

82
0:04:26.200 --> 0:04:30.200
只是搜索的网络模型的结构和方向各有所不同

83
0:04:30.200 --> 0:04:33.200
所以说它们两个的方式都是基于NAS的

84
0:04:33.200 --> 0:04:34.600
而基于NAS的搜索方法

85
0:04:34.600 --> 0:04:37.400
其实经过了这几年的一个延续

86
0:04:37.800 --> 0:04:41.600
我们发现它不是一个可持续的一个场景

87
0:04:41.600 --> 0:04:43.800
因为网络模型搜索的空间太大

88
0:04:43.800 --> 0:04:45.600
占用的资源量太多

89
0:04:45.600 --> 0:04:49.800
所以说它已经不再是我们现在的一个研究的重点

90
0:04:49.800 --> 0:04:51.600
那今天的最后的内容

91
0:04:51.600 --> 0:04:53.800
我们去读一读

92
0:04:53.800 --> 0:04:55.200
诺亚提出来的

93
0:04:55.200 --> 0:04:58.000
涵凯提出来的一个GhostNet这篇文章

94
0:04:59.800 --> 0:05:01.800
下面我们看看GhostNet这篇文章

95
0:05:01.800 --> 0:05:04.400
GhostNet这篇文章是由华为的涵凯

96
0:05:04.400 --> 0:05:07.400
还有王源赫来去作为一个第一作者的

97
0:05:07.400 --> 0:05:10.600
我们现在看看它的一个主要的网络模型结构

98
0:05:10.600 --> 0:05:14.600
那下面A图就是普通的一个卷积层

99
0:05:14.600 --> 0:05:16.000
这个没啥好讲的

100
0:05:16.000 --> 0:05:19.000
我们看看GhostModel这一个模块

101
0:05:19.000 --> 0:05:20.800
像GhostModel这个模块

102
0:05:20.800 --> 0:05:23.800
我们可以看到输入的时候还是有一个卷积

103
0:05:23.800 --> 0:05:25.000
但是这个卷积

104
0:05:25.000 --> 0:05:26.600
它的一个channel数

105
0:05:26.600 --> 0:05:28.600
其实是急剧的减少的

106
0:05:28.600 --> 0:05:30.800
然后减少了这个channel数之外

107
0:05:30.800 --> 0:05:31.800
其实没关系

108
0:05:31.800 --> 0:05:33.800
它通过一个数学的线性映射

109
0:05:33.800 --> 0:05:36.200
又产生了很多不同的feature map

110
0:05:36.200 --> 0:05:38.600
最后把卷积得到的feature map

111
0:05:38.600 --> 0:05:41.000
还有一些做线性映射得到的feature map

112
0:05:41.000 --> 0:05:42.800
把它们concate到一起

113
0:05:42.800 --> 0:05:44.800
最后做一个统一的输出

114
0:05:44.800 --> 0:05:47.200
通过这种减少我们单个卷积的方式

115
0:05:47.200 --> 0:05:50.400
替换成为普通的其他的一个数学的操作

116
0:05:50.400 --> 0:05:53.400
这种方式就是剪辑我们单层的卷积的运算

117
0:05:53.400 --> 0:05:56.800
然后用线性的一个运算方式去替换掉

118
0:05:56.800 --> 0:05:59.400
那我们继续往下看看

119
0:05:59.400 --> 0:06:03.200
那这个就是stripe等于1的一个的bottleneck

120
0:06:03.200 --> 0:06:05.600
这种我们叫做ghost bottleneck

121
0:06:05.600 --> 0:06:07.200
那这个方式一看这个图

122
0:06:07.200 --> 0:06:08.800
其实大家很容易联想

123
0:06:08.800 --> 0:06:12.400
它就是基于一个webnet的一个模型的结构

124
0:06:12.400 --> 0:06:14.400
进行一个改动的

125
0:06:15.600 --> 0:06:16.200
好了

126
0:06:16.200 --> 0:06:19.600
那我们现在在整个轻量级的网络模型里面

127
0:06:19.600 --> 0:06:22.200
基本上就介绍完它整体的系列了

128
0:06:22.200 --> 0:06:24.400
我们继续往下看一看

129
0:06:24.400 --> 0:06:26.200
做一个简单的总结

130
0:06:27.400 --> 0:06:29.200
那在卷积盒的大小方面

131
0:06:29.200 --> 0:06:31.800
其实我们会把一些大的卷积盒

132
0:06:32.000 --> 0:06:35.200
用了很多个小的卷积盒进行汰递

133
0:06:35.200 --> 0:06:36.800
例如我们三层三的卷积盒

134
0:06:36.800 --> 0:06:39.800
会用一个三层一加一个一层三的卷积盒

135
0:06:39.800 --> 0:06:42.400
就是低字分解的方式进行替换

136
0:06:42.400 --> 0:06:45.200
那第二种方式就是单一尺寸的卷积盒

137
0:06:45.200 --> 0:06:47.800
我们使用了更多尺寸的卷积盒

138
0:06:47.800 --> 0:06:49.800
进行一个替换的

139
0:06:49.800 --> 0:06:53.200
那第三种方式就是使用固定的形状的卷积盒

140
0:06:53.200 --> 0:06:56.200
慢慢的使用了可变形状的卷积盒

141
0:06:56.200 --> 0:06:58.600
例如普通卷积变成一个device卷积

142
0:06:58.600 --> 0:07:01.000
然后变成device加provice卷积

143
0:07:01.000 --> 0:07:03.200
或者变成一个provice卷积

144
0:07:04.000 --> 0:07:08.000
第四点就是大量的去使用了一层一的卷积盒

145
0:07:08.000 --> 0:07:09.600
作为我们buttonlet的一个结构

146
0:07:09.600 --> 0:07:12.200
其中的一个输入或者输出的

147
0:07:12.200 --> 0:07:16.400
确实在卷积盒方面做了很多不同的演进

148
0:07:16.400 --> 0:07:20.200
那第二点就是在卷积层方面做的一个演进方式

149
0:07:20.200 --> 0:07:23.800
就是把标准的卷积盒变成一个device的卷积盒

150
0:07:23.800 --> 0:07:25.600
或者一个provice的卷积盒

151
0:07:25.600 --> 0:07:29.600
第二个相册方的这种就使用了分组的卷积

152
0:07:29.600 --> 0:07:33.200
第三个就是分组的卷积之前或者卷积之后

153
0:07:33.200 --> 0:07:35.400
使用了一个channel server的一个方式

154
0:07:36.800 --> 0:07:39.400
第四个就是信道加全的计算

155
0:07:39.400 --> 0:07:43.200
例如Squishnet把一层一加三层三进行一个concate

156
0:07:43.200 --> 0:07:45.600
那在卷积层连接方面

157
0:07:45.600 --> 0:07:48.400
其实用了更多的skip connection

158
0:07:48.400 --> 0:07:50.800
让我们的模型更加深

159
0:07:50.800 --> 0:07:53.600
那第二个就使用了dense connection

160
0:07:53.600 --> 0:07:54.800
融合起来了

161
0:07:54.800 --> 0:07:57.800
然后把其他的特征融合更多的特征

162
0:07:57.800 --> 0:08:00.000
从而提升我们的网络模型的精度

163
0:08:00.000 --> 0:08:01.200
那所以总结起来

164
0:08:01.200 --> 0:08:04.800
整个CNN网络模型的一个结构的轻量化

165
0:08:04.800 --> 0:08:06.400
总有三方面

166
0:08:06.400 --> 0:08:09.400
第一方面就是卷积盒进行改进

167
0:08:09.400 --> 0:08:13.800
第二个就是卷积层的信道数进行改进

168
0:08:13.800 --> 0:08:18.200
第三个就是卷积层的连接的方式进行改进

169
0:08:18.200 --> 0:08:21.000
从而把我们的网络模型变得更小

170
0:08:21.000 --> 0:08:23.000
变得精度更高

171
0:08:23.000 --> 0:08:23.400
好了

172
0:08:23.400 --> 0:08:25.200
今天的分享就到这里为止

173
0:08:25.200 --> 0:08:25.800
谢谢各位

174
0:08:25.800 --> 0:08:27.000
拜拜

175
0:08:27.000 --> 0:08:27.800
卷得不行了

176
0:08:27.800 --> 0:08:28.600
卷得不行了

177
0:08:28.600 --> 0:08:30.400
记得一键三连加关注哦

178
0:08:30.400 --> 0:08:34.000
所有的内容都会开源在下面这条链接里面

179
0:08:34.000 --> 0:08:35.400
拜拜

