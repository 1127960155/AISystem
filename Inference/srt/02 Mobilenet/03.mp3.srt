1
00:00:00,000 --> 00:00:11,200
Hello,大家好,我们还是回到模型小型化里面的CNN小型化

2
00:00:11,200 --> 00:00:16,200
CNN小型化里面主要是聚焦于网络模型结构的小型化

3
00:00:16,200 --> 00:00:19,000
可以看到其实我们在上一节课里面

4
00:00:19,000 --> 00:00:21,800
就给大家分享了SquishNet,SquashNet,MobileNet

5
00:00:21,800 --> 00:00:25,800
这些网络模型结构更多的是提出了新的模型结构

6
00:00:25,800 --> 00:00:27,000
新的卷迹的算法

7
00:00:28,000 --> 00:00:31,600
像SquishNet就提出了自己的一个Squish的结构

8
00:00:31,600 --> 00:00:34,600
ServeNet就提出了对Channel进行Serve的操作

9
00:00:34,600 --> 00:00:38,400
MobileNet就提出了一个新的卷迹的压缩的方式

10
00:00:38,400 --> 00:00:41,000
而下面我们要讲的几个系列

11
00:00:41,000 --> 00:00:44,600
主要是ESPNet,FBNet,EfficientNet和GhostNet

12
00:00:44,600 --> 00:00:49,000
四个内容,下面我们看一下第一个内容ESPNet

13
00:00:51,800 --> 00:00:55,200
下面这个就是ESPNet的一个论文

14
00:00:55,200 --> 00:00:59,400
很有意思的就是ESPNet跟我们刚才的一些网络模型

15
00:00:59,400 --> 00:01:02,000
或者刚才介绍的一些组件网络模型不一样

16
00:01:02,000 --> 00:01:04,400
它主要是针对具体的小任务的

17
00:01:04,400 --> 00:01:08,200
就是Semantic Segmentation,图像的分割

18
00:01:08,200 --> 00:01:11,200
其实在做图像的分割有一种方法

19
00:01:11,200 --> 00:01:14,200
就是把我们刚才介绍的一些轻量化的模型

20
00:01:14,200 --> 00:01:16,000
作为网络模型的主干

21
00:01:16,000 --> 00:01:22,600
然后在具体的Bitbomb或者Head就替换掉YOLO或者SSD了

22
00:01:22,600 --> 00:01:25,600
这里面就提出了两个新的概念

23
00:01:25,600 --> 00:01:29,400
Efficient Special Pyramid,高效的金字塔的结构

24
00:01:29,400 --> 00:01:33,800
第二个就是Hawaki Feature Fusion,多层的特征融合

25
00:01:33,800 --> 00:01:36,000
现在我们看看有什幺不一样哦

26
00:01:36,000 --> 00:01:38,800
第一个左边的这个就是Stand的卷机

27
00:01:38,800 --> 00:01:42,200
普通的卷机是不断的卷卷卷卷卷,然后不断的卷机

28
00:01:42,200 --> 00:01:45,200
这里面这个高效的金字塔结构

29
00:01:45,200 --> 00:01:48,200
就是前面我有一个Pointwise的卷机

30
00:01:48,200 --> 00:01:51,200
接下来就提供了几种不同的Kernel大小

31
00:01:51,200 --> 00:01:53,200
进行一个空洞化的卷机

32
00:01:53,200 --> 00:01:56,000
分开两步叫做ESP的结构

33
00:01:56,000 --> 00:01:58,800
第二个就是HFF的结构

34
00:01:58,800 --> 00:02:00,400
像HFF的结构我们可以看到

35
00:02:00,400 --> 00:02:05,800
其实这里面有很多种不同的金字塔的网络模型的大小

36
00:02:05,800 --> 00:02:11,000
最后通过这种加SUM加Concate的网络模型结构的组合

37
00:02:11,000 --> 00:02:13,000
就变成了一个Hawaki Feature Fusion

38
00:02:13,000 --> 00:02:17,200
通过这种方式有效的去对我们的下午任务图像分割

39
00:02:17,200 --> 00:02:18,600
保持一定的精度

40
00:02:18,600 --> 00:02:21,600
但是降低了整个网络模型的参数量

41
00:02:24,000 --> 00:02:27,400
接下来我们看看第二篇文章

42
00:02:27,400 --> 00:02:30,200
这个系列的第二篇文章ESPNet V2

43
00:02:30,200 --> 00:02:35,200
下面第二个就是ESPNet V2第二个版本了

44
00:02:35,200 --> 00:02:37,200
确实好多这种系列版本

45
00:02:37,200 --> 00:02:40,200
我们看看它的网络模型结构的组成

46
00:02:40,200 --> 00:02:43,800
下面这个就是ESPNet的网络模型结构

47
00:02:43,800 --> 00:02:47,800
左边的这次刚才我们介绍的V1的一个ESP的Block

48
00:02:47,800 --> 00:02:50,800
右边的这个就是ESP的一个A的版本

49
00:02:50,800 --> 00:02:53,400
最后这个C图就是最终的版本

50
00:02:53,400 --> 00:02:54,800
我们看一下有什幺区别

51
00:02:54,800 --> 00:02:57,800
这种就是普通的Deadwise的卷机

52
00:02:57,800 --> 00:03:00,600
而作者换成GOOP的一个Deadwise的卷机

53
00:03:00,600 --> 00:03:04,200
就是进一步的提升了网络模型的一个轻量化

54
00:03:04,200 --> 00:03:07,600
接着第二个改变就是我们把Deadwise的卷机

55
00:03:07,600 --> 00:03:09,400
就是后面的一些卷机

56
00:03:09,400 --> 00:03:11,800
后面就增加了一个1x1的卷机

57
00:03:11,800 --> 00:03:15,800
进一步的提取它的一些特征的空间出来

58
00:03:15,800 --> 00:03:17,600
作者通过实验就发现

59
00:03:17,600 --> 00:03:19,200
再提取一层特征出来了

60
00:03:19,200 --> 00:03:22,000
其实我还不如先Concate之后

61
00:03:22,000 --> 00:03:24,200
再做一个GOOP的一个卷机

62
00:03:24,200 --> 00:03:27,200
那就把这种卷机的方式替换成GOOP卷机

63
00:03:27,200 --> 00:03:28,600
这种方式进行提取

64
00:03:28,600 --> 00:03:32,800
可以看到从左边的一个卷机换成GOOP的卷机

65
00:03:32,800 --> 00:03:35,200
然后增加一个GOOP特征的提取

66
00:03:35,200 --> 00:03:37,400
最后汇集起来

67
00:03:37,400 --> 00:03:40,800
这种就是ESPNet V2的特征

68
00:03:42,400 --> 00:03:44,800
介绍完ESPNet这种座椅一分个特征之后

69
00:03:44,800 --> 00:03:48,000
我们看看FBNet这个系列

70
00:03:48,800 --> 00:03:50,400
针对FBNet和EfficientNet

71
00:03:50,400 --> 00:03:53,400
我就不单独的去展开它们的论文了

72
00:03:53,400 --> 00:03:57,200
像FBNet其实已经推出了VV2 V3 V5

73
00:03:57,200 --> 00:03:59,200
不同的版本 版本量非常多

74
00:03:59,200 --> 00:04:02,400
而EfficientNet同样也推出了VV2的版本

75
00:04:02,400 --> 00:04:06,600
这两个版本是发生在2018年和2019年的

76
00:04:06,600 --> 00:04:09,000
这段时间最火的一个技术是什幺

77
00:04:09,600 --> 00:04:11,800
三顶的朋友们请告诉我

78
00:04:11,800 --> 00:04:17,600
2018年到2019年最火的一个网络模型搜索的技术是什幺

79
00:04:20,400 --> 00:04:21,600
NAS搜索

80
00:04:21,600 --> 00:04:22,000
对

81
00:04:22,000 --> 00:04:24,000
像FBNet和EfficientNet

82
00:04:24,000 --> 00:04:26,200
其实都是用了NAS的搜索方法

83
00:04:26,200 --> 00:04:30,200
只是搜索的网络模型的结构和方向各有所不同

84
00:04:30,200 --> 00:04:33,200
所以说它们两个的方式都是基于NAS的

85
00:04:33,200 --> 00:04:34,600
而基于NAS的搜索方法

86
00:04:34,600 --> 00:04:37,400
其实经过了这几年的一个延续

87
00:04:37,800 --> 00:04:41,600
我们发现它不是一个可持续的一个场景

88
00:04:41,600 --> 00:04:43,800
因为网络模型搜索的空间太大

89
00:04:43,800 --> 00:04:45,600
占用的资源量太多

90
00:04:45,600 --> 00:04:49,800
所以说它已经不再是我们现在的一个研究的重点

91
00:04:49,800 --> 00:04:51,600
那今天的最后的内容

92
00:04:51,600 --> 00:04:53,800
我们去读一读

93
00:04:53,800 --> 00:04:55,200
诺亚提出来的

94
00:04:55,200 --> 00:04:58,000
涵凯提出来的一个GhostNet这篇文章

95
00:04:59,800 --> 00:05:01,800
下面我们看看GhostNet这篇文章

96
00:05:01,800 --> 00:05:04,400
GhostNet这篇文章是由华为的涵凯

97
00:05:04,400 --> 00:05:07,400
还有王源赫来去作为一个第一作者的

98
00:05:07,400 --> 00:05:10,600
我们现在看看它的一个主要的网络模型结构

99
00:05:10,600 --> 00:05:14,600
那下面A图就是普通的一个卷积层

100
00:05:14,600 --> 00:05:16,000
这个没啥好讲的

101
00:05:16,000 --> 00:05:19,000
我们看看GhostModel这一个模块

102
00:05:19,000 --> 00:05:20,800
像GhostModel这个模块

103
00:05:20,800 --> 00:05:23,800
我们可以看到输入的时候还是有一个卷积

104
00:05:23,800 --> 00:05:25,000
但是这个卷积

105
00:05:25,000 --> 00:05:26,600
它的一个channel数

106
00:05:26,600 --> 00:05:28,600
其实是急剧的减少的

107
00:05:28,600 --> 00:05:30,800
然后减少了这个channel数之外

108
00:05:30,800 --> 00:05:31,800
其实没关系

109
00:05:31,800 --> 00:05:33,800
它通过一个数学的线性映射

110
00:05:33,800 --> 00:05:36,200
又产生了很多不同的feature map

111
00:05:36,200 --> 00:05:38,600
最后把卷积得到的feature map

112
00:05:38,600 --> 00:05:41,000
还有一些做线性映射得到的feature map

113
00:05:41,000 --> 00:05:42,800
把它们concate到一起

114
00:05:42,800 --> 00:05:44,800
最后做一个统一的输出

115
00:05:44,800 --> 00:05:47,200
通过这种减少我们单个卷积的方式

116
00:05:47,200 --> 00:05:50,400
替换成为普通的其他的一个数学的操作

117
00:05:50,400 --> 00:05:53,400
这种方式就是剪辑我们单层的卷积的运算

118
00:05:53,400 --> 00:05:56,800
然后用线性的一个运算方式去替换掉

119
00:05:56,800 --> 00:05:59,400
那我们继续往下看看

120
00:05:59,400 --> 00:06:03,200
那这个就是stripe等于1的一个的bottleneck

121
00:06:03,200 --> 00:06:05,600
这种我们叫做ghost bottleneck

122
00:06:05,600 --> 00:06:07,200
那这个方式一看这个图

123
00:06:07,200 --> 00:06:08,800
其实大家很容易联想

124
00:06:08,800 --> 00:06:12,400
它就是基于一个webnet的一个模型的结构

125
00:06:12,400 --> 00:06:14,400
进行一个改动的

126
00:06:15,600 --> 00:06:16,200
好了

127
00:06:16,200 --> 00:06:19,600
那我们现在在整个轻量级的网络模型里面

128
00:06:19,600 --> 00:06:22,200
基本上就介绍完它整体的系列了

129
00:06:22,200 --> 00:06:24,400
我们继续往下看一看

130
00:06:24,400 --> 00:06:26,200
做一个简单的总结

131
00:06:27,400 --> 00:06:29,200
那在卷积盒的大小方面

132
00:06:29,200 --> 00:06:31,800
其实我们会把一些大的卷积盒

133
00:06:32,000 --> 00:06:35,200
用了很多个小的卷积盒进行汰递

134
00:06:35,200 --> 00:06:36,800
例如我们三层三的卷积盒

135
00:06:36,800 --> 00:06:39,800
会用一个三层一加一个一层三的卷积盒

136
00:06:39,800 --> 00:06:42,400
就是低字分解的方式进行替换

137
00:06:42,400 --> 00:06:45,200
那第二种方式就是单一尺寸的卷积盒

138
00:06:45,200 --> 00:06:47,800
我们使用了更多尺寸的卷积盒

139
00:06:47,800 --> 00:06:49,800
进行一个替换的

140
00:06:49,800 --> 00:06:53,200
那第三种方式就是使用固定的形状的卷积盒

141
00:06:53,200 --> 00:06:56,200
慢慢的使用了可变形状的卷积盒

142
00:06:56,200 --> 00:06:58,600
例如普通卷积变成一个device卷积

143
00:06:58,600 --> 00:07:01,000
然后变成device加provice卷积

144
00:07:01,000 --> 00:07:03,200
或者变成一个provice卷积

145
00:07:04,000 --> 00:07:08,000
第四点就是大量的去使用了一层一的卷积盒

146
00:07:08,000 --> 00:07:09,600
作为我们buttonlet的一个结构

147
00:07:09,600 --> 00:07:12,200
其中的一个输入或者输出的

148
00:07:12,200 --> 00:07:16,400
确实在卷积盒方面做了很多不同的演进

149
00:07:16,400 --> 00:07:20,200
那第二点就是在卷积层方面做的一个演进方式

150
00:07:20,200 --> 00:07:23,800
就是把标准的卷积盒变成一个device的卷积盒

151
00:07:23,800 --> 00:07:25,600
或者一个provice的卷积盒

152
00:07:25,600 --> 00:07:29,600
第二个相册方的这种就使用了分组的卷积

153
00:07:29,600 --> 00:07:33,200
第三个就是分组的卷积之前或者卷积之后

154
00:07:33,200 --> 00:07:35,400
使用了一个channel server的一个方式

155
00:07:36,800 --> 00:07:39,400
第四个就是信道加全的计算

156
00:07:39,400 --> 00:07:43,200
例如Squishnet把一层一加三层三进行一个concate

157
00:07:43,200 --> 00:07:45,600
那在卷积层连接方面

158
00:07:45,600 --> 00:07:48,400
其实用了更多的skip connection

159
00:07:48,400 --> 00:07:50,800
让我们的模型更加深

160
00:07:50,800 --> 00:07:53,600
那第二个就使用了dense connection

161
00:07:53,600 --> 00:07:54,800
融合起来了

162
00:07:54,800 --> 00:07:57,800
然后把其他的特征融合更多的特征

163
00:07:57,800 --> 00:08:00,000
从而提升我们的网络模型的精度

164
00:08:00,000 --> 00:08:01,200
那所以总结起来

165
00:08:01,200 --> 00:08:04,800
整个CNN网络模型的一个结构的轻量化

166
00:08:04,800 --> 00:08:06,400
总有三方面

167
00:08:06,400 --> 00:08:09,400
第一方面就是卷积盒进行改进

168
00:08:09,400 --> 00:08:13,800
第二个就是卷积层的信道数进行改进

169
00:08:13,800 --> 00:08:18,200
第三个就是卷积层的连接的方式进行改进

170
00:08:18,200 --> 00:08:21,000
从而把我们的网络模型变得更小

171
00:08:21,000 --> 00:08:23,000
变得精度更高

172
00:08:23,000 --> 00:08:23,400
好了

173
00:08:23,400 --> 00:08:25,200
今天的分享就到这里为止

174
00:08:25,200 --> 00:08:25,800
谢谢各位

175
00:08:25,800 --> 00:08:27,000
拜拜

176
00:08:27,000 --> 00:08:27,800
卷得不行了

177
00:08:27,800 --> 00:08:28,600
卷得不行了

178
00:08:28,600 --> 00:08:30,400
记得一键三连加关注哦

179
00:08:30,400 --> 00:08:34,000
所有的内容都会开源在下面这条链接里面

180
00:08:34,000 --> 00:08:35,400
拜拜

