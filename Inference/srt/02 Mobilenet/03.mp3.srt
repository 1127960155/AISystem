0
0:00:00.000 --> 0:00:09.300
大家好我们还是回到模型小型化里面的

1
0:00:09.300 --> 0:00:11.360
寺恩恩小型化

2
0:00:11.360 --> 0:00:16.200
寺恩小型化里面呢主要是聚焦于网络模型结构的小型化

3
0:00:16.200 --> 0:00:19.200
那可以看到啊其实我们在上一节课里面呢

4
0:00:19.200 --> 0:00:21.900
就给大家分享了思惟思内思方的模块内

5
0:00:22.020 --> 0:00:27.100
这些网络模型结构更多的是提出了新的模型结构新的卷机的算法

6
0:00:27.100 --> 0:00:31.700
像SqueezeNet就提出了自己的一个Squeeze的结构

7
0:00:31.700 --> 0:00:34.700
SurferNet就提出了对圈头进行Surfer的操作

8
0:00:34.700 --> 0:00:38.500
那MobileNet就提出了一个新的转机的压缩的方式

9
0:00:38.500 --> 0:00:41.500
而下面我们要讲的几个系列主要是

10
0:00:41.500 --> 0:00:45.900
ESPNet FBNet还有IphigyNet和GhostNet四个类型

11
0:00:45.900 --> 0:00:49.100
下面我们看一下第一个类型ESPNet

12
0:00:49.100 --> 0:00:55.300
下面这个就是ESPNet的一个论文

13
0:00:55.300 --> 0:00:57.500
那很有意思的就是ESPNet呢

14
0:00:57.500 --> 0:01:01.900
跟我们刚才的一些网络模型或者刚才介绍的一些组件网络模型呢不一样

15
0:01:01.900 --> 0:01:08.100
它主要是针对具体的小任务的就是Semantic Segmentation图像的分割

16
0:01:08.100 --> 0:01:15.900
那其实在做图像的分割呢有一种方法就是把我们刚才介绍的一些轻量化的模型呢作为网络模型的主干

17
0:01:15.900 --> 0:01:22.500
然后呢在具体的笨或者汉字呢就替换掉那个YOLO或者SSD了

18
0:01:22.700 --> 0:01:29.500
这里面呢就提出了两个新的概念Efficient Special Pyramid高效的金字塔的结构

19
0:01:29.500 --> 0:01:33.900
第二个呢就是好维基Feature Fusion多层的特征融合

20
0:01:33.900 --> 0:01:36.100
现在我们看看有什么不一样哦

21
0:01:36.100 --> 0:01:42.300
那第一个呢左边的这个就是stand的卷机那普通的卷机是不断的卷卷卷卷然后不断的卷机

22
0:01:42.300 --> 0:01:48.300
那这里面呢这个高效的金字塔结构呢就是前面我有一个Pointwise的卷机

23
0:01:48.300 --> 0:01:53.300
接下来就提供了几种不同的konos大小进行一个空洞化的卷机

24
0:01:53.300 --> 0:01:55.900
分开两部呢叫做ESP的结构

25
0:01:55.900 --> 0:02:05.900
那第二个呢就是HFF的结构像HF结构我们可以看到其实这里面呢有很多种不同的金字塔的一个网络模型的大小

26
0:02:05.900 --> 0:02:13.100
那最后呢通过这种加SUM然后加CONCAT这种的网络模型结构的组合呢就变成了一个好维基Feature Fusion

27
0:02:13.100 --> 0:02:17.300
通过这种方式呢有效地去对我们的小人物图像分割呢

28
0:02:17.300 --> 0:02:24.100
保持一定的精度呢但是降低了整个网络模型的参数量

29
0:02:24.100 --> 0:02:30.900
接下来我们看看第二篇文章这个系列的第二篇文章ESPNET V2

30
0:02:30.900 --> 0:02:40.600
下面的第二个就是ESPNET V二第二个版本了确实好多这种系列版本我们看看它的网络模型结构的组成

31
0:02:40.600 --> 0:02:47.800
那下面的这个就是ESPNET的网络模型结构左边的这是刚才我们介绍的V一的一个ESP的block

32
0:02:47.800 --> 0:02:55.000
那右边的这个呢就是ESP的一个A的版本一最后这个系图呢就是最终的版本我们看一下有什么区别

33
0:02:55.000 --> 0:03:04.200
那这种呢就是普通的device的卷机而作者换成GOOP的一个device的卷机呢就是进一步地提升了网络模型的一个轻量化

34
0:03:04.200 --> 0:03:09.400
那接着呢第二个改变就是我们把device卷机就是后面的一些卷机啊

35
0:03:09.400 --> 0:03:15.900
后面呢就增加了一个一层一的卷机呢进一步地提取它的一些特征的空间出来

36
0:03:15.900 --> 0:03:24.300
作者通过实验呢就发现再提取一层特征出来呢其实我还不如先看开之后再做一个GOOP的一个卷机

37
0:03:24.300 --> 0:03:28.600
那就把这种卷机的方式替换成GOOP卷机这种方式进行提取

38
0:03:28.600 --> 0:03:37.600
那可以看到从左边的一个卷机换成GOOP的卷机然后增加一个GOOP特征的提取最后汇集起来

39
0:03:37.700 --> 0:03:40.500
这种呢就是ESPN的v二的特征

40
0:03:42.500 --> 0:03:48.000
介绍完ESPN这种作与一分个的特征之后呢我们看看FB内这个系列

41
0:03:48.900 --> 0:03:53.600
针对FB内和Efficient内呢我就不单独地去展开他们的论文了

42
0:03:53.600 --> 0:03:59.300
那像FB内呢其实已经推出了v二v三v五不同的版本版本量非常多

43
0:03:59.300 --> 0:04:02.500
而Efficient内呢同样也推出了v二的版本

44
0:04:02.500 --> 0:04:06.600
那这两个版本呢是发生在二零一八年和二零一九年的

45
0:04:06.600 --> 0:04:09.600
而这段时间最火的一个技术是什么

46
0:04:09.600 --> 0:04:17.700
山顶的朋友们请告诉我二零一八年到二零一九年最火的一个网络模型搜索的技术是什么

47
0:04:20.400 --> 0:04:30.500
NAS搜索对像FB内和Efficient内呢其实都是用了NAS的搜索方法只是搜索的网络模型的结构和方向呢各有所不同

48
0:04:30.500 --> 0:04:33.400
所以说他们两个的方式呢都是基于NAS的

49
0:04:33.400 --> 0:04:41.700
而据NAS的搜索方法其实经过了这几年的一个延续啊我们发现它不是一个嗯可持续的一个场景

50
0:04:41.700 --> 0:04:45.600
因为网络模型搜索的空间太大占用的资源量太多

51
0:04:45.600 --> 0:04:49.900
所以说它已经不再是我们现在的一个研究的重点

52
0:04:49.900 --> 0:04:57.900
那今天的最后的内容呢我们去读一读呃洛亚提出来的韩凯提出来的一个ghostnet这篇文章

53
0:04:59.900 --> 0:05:01.700
下面我们看看ghostnet这篇文章

54
0:05:01.700 --> 0:05:07.400
ghostnet这篇文章呢是由华为的韩凯还有那个王赫来去作为一个第一作者的

55
0:05:07.400 --> 0:05:10.500
那我们现在看看它的一个主要的网络模型结构

56
0:05:10.500 --> 0:05:19.000
那下面A图呢就是普通的一个卷积层啊那这个没啥好讲的我们看看ghostmodel这一个模块

57
0:05:19.000 --> 0:05:23.700
那像ghostmodel这个模块呢我们可以看到输入的时候还是有一个卷积

58
0:05:23.700 --> 0:05:28.500
但是呢这个卷积呢它的一个圈入数额其实是急剧的减少的

59
0:05:28.500 --> 0:05:31.800
然后呢减少了这个圈入数之外呢其实没关系

60
0:05:31.800 --> 0:05:36.300
它通过一个数学的先先行映射呢又产生了很多不同的feature map

61
0:05:36.300 --> 0:05:41.200
最后呢把卷积得到的feature map还有一些做先行映射得到的feature map

62
0:05:41.200 --> 0:05:50.500
把它们肯定到一起最后做一个统一的输出通过这种减少我们单个卷积的方式替换成为普通的其他的一个数学的操作

63
0:05:50.500 --> 0:05:57.000
这种方式呢就是减去我们单层的卷积的运算然后用线性的一个运算方式呢去替换掉

64
0:05:57.000 --> 0:06:05.700
那我们继续往下看一看那这个呢就是stripe等于一的一个的button neck这个我们叫做ghost button neck

65
0:06:05.700 --> 0:06:15.700
那这个方式一看这个图啊其实大家很容易联想它就是基于一个webnet的一个模型的结构进行一个改动的

66
0:06:15.700 --> 0:06:22.400
好了那我们现在在整个轻量级的网络模型里面呢基本上就介绍完它整体的系列了

67
0:06:22.400 --> 0:06:27.500
我们继续往下看一看做一个简单的总结

68
0:06:27.500 --> 0:06:35.200
那在卷积盒的大小方面呢其实我们会把一些大的卷积盒呢用了很多更小的卷积盒进行派递

69
0:06:35.200 --> 0:06:42.500
例如我们三乘三的卷积盒会用一个三乘一加加一个一乘三的卷积盒就是低字分解的方式进行替换

70
0:06:42.500 --> 0:06:49.800
那第二种方式就是单一尺寸的卷积盒呢我们使用了更多尺寸的卷积盒进行一个替换的

71
0:06:49.800 --> 0:06:58.600
那第三种方式就是使用固定的形状的卷积盒呢慢慢地使用了可变形状的卷积盒例如普通卷积变成一个device卷积呢

72
0:06:58.600 --> 0:07:03.900
然后变成device加provice卷积呢或者变成一个coopvice的卷积

73
0:07:03.900 --> 0:07:12.100
第四点呢就是大量地去使用了一乘一的卷积盒作为我们botnet的一个结构其中一个一个输入或者输出的

74
0:07:12.100 --> 0:07:16.300
确实在卷积盒方面呢做了很多不同的演进

75
0:07:16.300 --> 0:07:25.500
那第二点就是在卷积层方面做的一个演进的方式就是把标准的卷积盒变成一个device的卷积盒或者一个provice的卷积盒

76
0:07:25.500 --> 0:07:29.500
第二个呢像测发带这种呢就使用了分组的卷积

77
0:07:29.500 --> 0:07:36.800
第三个呢就是分组的卷积之前呢或者卷积之后呢使用了一个channel server的一个方式

78
0:07:36.800 --> 0:07:43.400
第四个呢就是通道加权的计算例如sqsnet把一层一加三层三进行一个concat

79
0:07:43.400 --> 0:07:50.900
那在卷积层连接方面呢其实用了更多的skip connection让我们的模型更加深

80
0:07:50.900 --> 0:08:00.100
那第二个呢就使用了dense connection融合起来了然后把其他的特征融合更多的特征从而提升我们的网络模型的精度

81
0:08:00.100 --> 0:08:09.500
那所以总结起来整个CNN网络模型的一个结构的清亮化呢则有三方面第一方面呢就是卷积和进行改进

82
0:08:09.500 --> 0:08:35.100
第二个呢就是卷积层的通道数进行改进第三个就是卷积层的连接的方式进行改进从而把我们的网络模型变得更小变得精度更高

