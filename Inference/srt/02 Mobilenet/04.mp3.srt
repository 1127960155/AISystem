0
0:00:00.000 --> 0:00:06.720


1
0:00:06.720 --> 0:00:10.280
哈喽大家好今天晚上的更新呢应该是比较早的

2
0:00:10.280 --> 0:00:17.620
现在才嗯二十二点五十二分哦也就是十一点左右啊没有像以前那么晚了

3
0:00:17.660 --> 0:00:23.960
那今天呢我给大家带来的一个内容呢就是推理系统或者推理引擎里面的模型小型化

4
0:00:24.000 --> 0:00:27.360
这里面呢额对提点我是宗秘

5
0:00:27.360 --> 0:00:30.360
今天我要给大家去带来一个比较新的内容

6
0:00:30.360 --> 0:00:34.360
就是全双嘛这个结构的一些模型小型化

7
0:00:35.160 --> 0:00:38.960
在前两节内容里面呢我们主要是关心传统的

8
0:00:38.960 --> 0:00:42.160
CNN网络模型结构的一个小型化

9
0:00:42.160 --> 0:00:47.560
那这个内容呢也是比较传统或者研究的已经算是比较明白了

10
0:00:47.560 --> 0:00:54.160
现在呢我们来看一个新的内容业界也是玩得不是说非常明白或者研究的不是说非常明白

11
0:00:54.160 --> 0:00:57.760
也有很多可以发paper还有研究的点

12
0:00:57.760 --> 0:01:03.160
这个内容呢也就是全双嘛结构的小型化了

13
0:01:03.160 --> 0:01:07.960
字里面呢就非常欢迎大家可以在全双嘛小型化里面呢

14
0:01:07.960 --> 0:01:11.160
做非常多的科研的创新和研究了

15
0:01:12.760 --> 0:01:18.960
现在呢我们来看一下这个图啊这个图其实在我们分布式并行里面已经讲过很多次了

16
0:01:18.960 --> 0:01:25.160
可以看到随着应该是从二零一六年到一七年全双嘛结构推出之后

17
0:01:25.160 --> 0:01:33.960
到那个二零二二年也就截止到目前为止我们的网络模型需要的参数量和了计算量是节节的攀升

18
0:01:33.960 --> 0:01:38.960
而这里面有一个比较大的一个起点就是拉举scale大规模的数据块

19
0:01:38.960 --> 0:01:48.860
或者大规模的分布式并行而这个呢得益于我们的全双嘛结构出现之后呢可以看到这里面的不管是麦克传吉比干三还有盘骨这些大模型啊

20
0:01:48.860 --> 0:01:58.660
所以导致我们的网络模型参数量急剧的膨胀这个网络模型的参数量急剧的膨胀引起了我们现在来到了一个大模型的阶段

21
0:01:58.660 --> 0:02:08.860
也同时催生了我们很多预训练模型的出现有了预训练模型之后呢我们只需要去开发对应的校任务就可以解决很多像以前模型的碎片化的问题

22
0:02:10.660 --> 0:02:17.860
关于大模型这个结构呢非常欢迎大家去看一下分布式并行里面的大模型训练和挑战大模型

23
0:02:17.860 --> 0:02:28.860
经典的算法结构这里面呢就是讲全双嘛还有BERT这些大模型的引起还有针对一些千亿万亿规模的搜塔大模型的一些深度的剖析

24
0:02:30.060 --> 0:02:38.260
回到我们今天的内容里面呢其实我们可以看到大模型的最开始呢是由我们的全双嘛就是attention is all you need这篇文章所引起的

25
0:02:38.560 --> 0:02:47.760
而左边的这个呢就是我们的全双嘛的网络模型的结构那看上去比较复杂其实呢就是由一个多头助力机制呢去叠加两层然后加一个链浪

26
0:02:47.760 --> 0:02:52.460
再加一个fit forward然后不断地一层层累积上去的这种网络模型结构

27
0:02:53.360 --> 0:03:01.360
但原来左边的这种结构呢主要是针对NLP的一些数据进行处理的后来呢谷歌又发了一篇文章叫做VIT

28
0:03:01.760 --> 0:03:12.560
这里面呢就提出了一种新的idea把图片的数据呢变成一个派取然后做一些embedded的操作最后呢输进去我们的网络模型结构就是transformer的encoder

29
0:03:12.660 --> 0:03:16.160
transformer左边的这个是encoder右边的这个是decoder嘛

30
0:03:16.360 --> 0:03:24.160
通过这种方式呢把视觉引到我们的transformer结构里面所以transformer的结构啊又进一步地燃烧到不同的领域

31
0:03:25.660 --> 0:03:32.460
下面的这个就是整个transformer的结构了那我们往下看一下假设我们的输入呢是我是李蕾

32
0:03:33.060 --> 0:03:39.260
把这个数据呢进行一个embedded的操作之后去给很多个transformer的encoderencoderencoder之后

33
0:03:39.460 --> 0:04:01.360
再把最后一层的encoder给不同的decoderdecoderdecoder最后再output那这种方式呢主要是由很多个我们刚才介绍的左边的这个encoder跟decoder去进行一个组合通过这种方式呢使得我们的网络模型的参数量进一步地增加就是这种方式去组成了我们的大模型的时代

34
0:04:02.060 --> 0:04:11.860
可以看到啊其实transformer的效果还是非常的好基本上呢完胜了传统的LSTM跟RNN的能够处理非常长的时序的问题

35
0:04:13.360 --> 0:04:30.560
但是有一个比较大的问题我们的网络模型的参数量大上去之后精度确实提升了但是我们要考虑一点就是怎么让我们的大模型进行部署起来了怎么把我们的大模型真正地用起来呢这个时候又有很多人去研究大模型的小型化了

36
0:04:30.560 --> 0:04:32.460
而大模型缩食化呀

37
0:04:32.460 --> 0:04:56.160
全数码的参数量呢从千万的级别开始到伯腾的到一级别后来全数码的这种大模型结构呢从十万百万千万到后来加上全数码M1的这种结构到了万亿规模规模的参数量这么大单纯纯一个网络模型的都已经到几个GB了怎么对网络模型进行压缩就变得非常之热门的话题了

38
0:04:56.160 --> 0:05:26.160
于是呢我们可以看到啊其实这里面influence就是参考的文献我分开了三段前面第一段呢就是针对NLP领域的一些压缩那中间的这个就针对CV的压缩后面的这个呢主要是针对多模态的压缩时间还是都比较新的有些可能才刚发表没多久Q八bitdistwobittinybit这些都是用了一些量化或者压缩减资的方法那下面这种tiny VITdynamic VIT和mini VIT呢

39
0:05:26.160 --> 0:05:56.160
大部分也是用了减资增流压缩量化的方法而下面这个也是大模型或者全数模结构里面呢进行一个模型的压缩小型化大部分呢都是采用了传统的压缩方法的四件套量化减资增流低资分解通过这四种方法来进行一个网络模型的压缩的但是呢我们今天的主角还是回到我们的网络模型的结构的轻量化通过优化我们的网络模型的结构呢

40
0:05:56.160 --> 0:06:26.160
使用的网络模型的在保存相同的精度的前提下呢网络模型的参数量进一步的减少那这里面呢有比较著名的就是摩拜VIT是Facebook发明的后来又出现了摩拜formerefficientformer各种former的这种结构而下面呢我们来去看一些搜塔的轻量级的全数模的网络模型结构现在我们来到摩拜VIT这篇文章在这篇文章提出的时候呢作者就在想像传统的

41
0:06:26.160 --> 0:06:56.160
CNN网络模型它的一个模型或者网络模型的小型化其实是非常成功的那有没有可能我们去结合传统的摩拜Net这种网络模型的轻量化的结构然后再把全数模的网络模型结构的优势加进去呢基于这个想法呢于是呢作者就提出了摩拜VIT的这个网络模型而且里面呢就提出了一个摩拜VITblock的这种结构那么首先来看看网络模型的性能可以看到啊

42
0:06:56.160 --> 0:07:26.160
从这个图呢网络模型的性能基本上呢摩拜VIT的一个网络模型的参数量呢大概有六百万左右它只有两点七MB它的网络模型的参数量的大小呢确实比摩拜的V三呢还有纳斯雷呢确实要小而且呢性能有很好的提升在一米九的分类模型里面呢就已经达到了百分之七十八的一个网络模型的精度那了解完它的一个基本的内容之后我们来看看它的网络

43
0:07:26.160 --> 0:07:56.160
模型结构那下面的这个图就是摩拜VIT的网络模型结构可以看到啊我们假设一张图遍区进去之后呢经过一个卷积MV二MV二MV二接着有一个摩拜VITblock的这种结构它分颜色哦红色的这个呢其实叫做摩拜纳V二的这个block引用了摩拜纳V二的网络模型轻量化的结构但是呢在中间插入了一个摩拜VIT

44
0:07:56.160 --> 0:08:26.160
的block那摩拜VIT的block呢就像上面所示首先它的输呢是经过一个卷积层的feature map那这个feature map呢给一个卷积N层N的网络模型进行一个卷积那这个卷积的作用呢主要是做一些局部的特征提取接着执行一个一层一的卷积把网络模型的feature map呢投射到一个高危的空间里面接着呢通过一个unfold把网络模型结构呢展开成为全数码可以输入的网络模型的方式

45
0:08:26.160 --> 0:08:56.160
那利用全数码QKB的这种方式呢去对全局的feature map就全局的特征进行一个提取然后呢在float成原来的这种格式接着再做一个一层一的卷积再执行一个N层N的卷积最后输出输入呢跟输出是相同的中间为了给我们的全数码的网络模型结构去做一个全局的感知于是呢做了一些简单的变换那通过这种方式呢就实现了一个摩拜VIT结构了

46
0:08:57.160 --> 0:09:04.360
好现在我们来看看同联的第二个工作就是摩拜former这篇文章

47
0:09:05.360 --> 0:09:12.120
像摩拜former这篇文章呢其实跟刚才有点异曲同工自妙啊同样也是bling bling

48
0:09:12.120 --> 0:09:17.160
摩拜net和全数码把摩拜net跟全数码融合起来

49
0:09:17.360 --> 0:09:25.200
刚才那篇文章呢是apple发明的而这篇文章呢是Microsoft去提出来的确实还挺有趣的哈

50
0:09:25.200 --> 0:09:29.600
下面呢我们简单的去看看这一篇文章有什么不一样

51
0:09:29.800 --> 0:09:34.000
那上面的这个图呢就是摩拜former的整个网络模型的结构

52
0:09:34.000 --> 0:09:39.000
那可以看到呢左边这个呢就是摩拜net一个结构模型

53
0:09:39.000 --> 0:09:42.600
右边的这条分支呢就是全数码的一个结构模型

54
0:09:42.600 --> 0:09:47.000
这边呢就以并行的方式呢去执行两个不同的block

55
0:09:47.000 --> 0:09:49.800
那左边就是摩拜net block右边就是全数码的block

56
0:09:50.000 --> 0:09:53.600
可以看到啊我们下面以这个虚线的框为例子

57
0:09:53.800 --> 0:09:58.400
左边的这个摩拜net block呢会把一些参数传给我们的全数码

58
0:09:58.600 --> 0:10:05.800
那这个呢我们叫做摩拜net图former全数码学习完之后呢也会把它的一些feature map呢传给摩拜net

59
0:10:06.000 --> 0:10:11.200
这个呢就是它最简单的摩拜former的结构我们往下再看一下

60
0:10:11.400 --> 0:10:16.600
那这个图呢会更加清晰去讲讲它整个摩拜former的block

61
0:10:16.800 --> 0:10:23.400
左下角呢这个就是摩拜net一个block可以看到基本上呢就一个卷机维路device卷机维路

62
0:10:23.600 --> 0:10:29.200
然后呢我们再看看右上角那右上角呢就是一个全数码的简单的encoder

63
0:10:29.400 --> 0:10:34.000
通过一个multi high attention然后加一加一个ffn的层最后输出

64
0:10:34.200 --> 0:10:38.400
那很有意思的就是我们要把摩拜net转成former的结构

65
0:10:38.600 --> 0:10:42.400
这边呢就需要做一个mobile to former的一个结构网络模型

66
0:10:42.400 --> 0:10:44.400
通过它的一个特殊的设计

67
0:10:44.400 --> 0:10:47.400
然后呢就把摩拜net的输出给到我们的former

68
0:10:47.600 --> 0:10:52.200
这里面呢还有一个former to mobile net一个红色的方式去展示

69
0:10:52.400 --> 0:10:59.200
同样也会把全数码的一些网络模型的结构通过一些转换最后给到我们的摩拜net

70
0:10:59.600 --> 0:11:04.200
通过这种左右并行的方式呢去实现了摩拜former的结构

71
0:11:06.000 --> 0:11:09.600
我们看一下下一个场景就是efficient former

72
0:11:09.800 --> 0:11:15.000
那大家看到这篇文章会不会想到efficient former摩拜former

73
0:11:15.200 --> 0:11:21.200
不就把efficient net我们之前讲的efficient net的一个系列的一些网络模型的经典的结构

74
0:11:21.400 --> 0:11:26.000
跟全数码融合起来嘛对这篇文章就是这个概念

75
0:11:27.400 --> 0:11:33.600
下面这个文章呢就是efficient former的标题vision transformer at mobile net speed

76
0:11:33.800 --> 0:11:36.000
马上来看看它有什么不一样啊

77
0:11:36.200 --> 0:11:46.200
哎这个图呢很有代表性啊这个图可以看到efficient former的性能确实确实很优啊比刚才的摩拜former要高不少

78
0:11:46.200 --> 0:11:49.200
刚才摩拜former呢只有到百分之七十六到七十八

79
0:11:49.200 --> 0:11:56.400
虽然它的一个网络模型的参数量呢确实大了一点但是不妨碍它的性能确实能够做的很好

80
0:11:56.400 --> 0:12:02.000
我们再往下看一看大家看一下这个图啊这个图还是很有意思的

81
0:12:02.000 --> 0:12:10.000
作者呢去做了大量的消融实验还有分解实验呢去把一些摩拜类的efficient类的还有一些proof former efficient former

82
0:12:10.000 --> 0:12:16.000
这些网络模型的结构都按照一些大的block或者大的一些结构体去展开

83
0:12:16.000 --> 0:12:21.000
我们可以看到它里面的去算patch embedding占了多少的时间

84
0:12:21.000 --> 0:12:30.000
attention层又占了多少的时限还有一个先行激活层呢还有with shape normalize activation分别都占了多少的时间

85
0:12:30.000 --> 0:12:35.000
而像这种的确实啊大量的卷迹去堆叠就没有必要去拆分了

86
0:12:35.000 --> 0:12:43.400
而transform的结构最大的问题就是它有很多的embedding而且很多的multi attention还有linerwith shape normalization

87
0:12:43.400 --> 0:13:00.400
作者通过这种消融实验的分析呢那可以找到我们要优化的点就是在embedding层要做优化第二个我们需要在liner层做优化第三个我们要在multi attention层做优化另外还有我们需要把这些with shape的工作给它砍掉

88
0:13:00.400 --> 0:13:12.400
于是呢就出现了efficient former这个网络模型结构可以看到它对比起刚才的一些消融实验里面很多的消耗实验的工作呢确实把它压缩的还挺厉害的我们具体看看它的网络模型结构

89
0:13:12.400 --> 0:13:40.400
那这个呢就是efficient former的网络模型结构这里面呢有一个mb四d有一个mb三d我们先来看看这个efficient former的网络模型结构上面的这个就是它的整体的网络模型的结构从输入卷迹卷迹然后有一个mb四d的层那mb四d呢是下面的展开我们先不来看然后呢有个embeddingmb四dembeddingmb四d接着呢我们这里面有个分水岭就是四d转三d

90
0:13:40.400 --> 0:14:10.400
它简单的做了一个映射变换之后呢到了另外一个新的结构mb三d然后embeddingmb三d在最后输出的而mb三d呢就是右下角的这个模块那现在我们分别的去看看mb三d跟mb四d有什么区别那mb三d其实这里面呢确实像传统的卷迹升级网络模型做一个铺垒卷迹避入这种工作那没有太多的创新那像mb三d因为我们引入了一个

91
0:14:10.400 --> 0:14:40.400
全数码的结构网络模型它的输入呢跟输出不一样像之前的mobile phone嘛确实你把mobile line跟那个efficient line进行一个捆绞你要大量的we shape之间的一些性性的转换那这里面呢我只转换一次接着呢有大量的全数码组成的mb三d的结构进行一个相累加通过这种方式呢去解决了刚才的patch embedding还有attention还有nano we shape这些耗时的工作好了那我们今天的内容呢

92
0:14:40.400 --> 0:15:10.400
是讲了轻量级的全数码的网络模型结构从mobile vip呢到mobile phone嘛呢然后再到efficient phone嘛现在呢我们对轻量化的网络模型呢做了一个简单的总结首先第一点就是不同的网络模型架构即使它的flow是相同但是mac也有可能有很大的差异所以大家要去看一下不仅仅要看flow实际上我们在真正的客户交付或者真正客户跑起来

93
0:15:10.400 --> 0:15:40.400
仅仅只是看我们算网络模型的一个性能我们还要看mac那基于flow跟mac可以回看一下网络模型小型化的第一个视频接着第二点就是flow虽然低但是不等于或者不代表我们的实验低哦具体还要结合具体的硬件架构去分析所以我们在做轻量化网络模型结构或者在做一个综合测评的时候呢确实要看很多的指标于是呢

94
0:15:40.400 --> 0:16:10.400
我在第一个视频的时候呢去给大家汇报了一下不同的参数所代表的意义接着第三点就是很多加速芯片特别是一些特殊的AI加速芯片最大的瓶颈是在仿存的带宽而不是在它的算力仿存的带宽呢确实很大程度呢去制约了我们整体的性能第四个呢就是不同的硬件平台去部署轻量化的一个网络模型呢需要根据具体的业务进行选择的这一点呢终也是

95
0:16:10.400 --> 0:16:40.400
深有体会的因为其实我们在面向CPU消费者业务去交付HMS扣的时候呢也就是你们用华为手机后面跑的所有的一些轻量化的算法一开始呢说实话我们采用了摩巴的V三这个网络模型结构摩巴的V三呢在谷歌的那个手机里面确实跑得又快精度又高但是实际上我们在华为的手机和高通的手机测下来摩巴的V三的性能反倒没有摩巴的V二要好而且精度可能还差不多

96
0:16:40.400 --> 0:17:09.010
所以说不同硬件部署的轻量级网络模型呢要根据具体的业务来决定而具体的业务我们还要看很多指标不同的事业呢不同的房存带宽呢都会影响我们轻量化的模型结构

