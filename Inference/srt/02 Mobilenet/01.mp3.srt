0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴巴

1
0:00:06.200 --> 0:00:10.640
唉,现在看看时间已经到了凌晨35分

2
0:00:10.640 --> 0:00:12.160
就晚上12点半了

3
0:00:13.520 --> 0:00:17.040
我终于知道为什么大家叫做这种晚上更新视频的up主

4
0:00:17.040 --> 0:00:19.200
或者人呢叫做干弟啊

5
0:00:19.200 --> 0:00:21.080
原来确实很伤肝

6
0:00:21.080 --> 0:00:22.960
我不是说你很能干

7
0:00:23.720 --> 0:00:25.920
嘿,这还不错

8
0:00:27.040 --> 0:00:29.440
录完这个视频呢我就要去休息了

9
0:00:29.440 --> 0:00:31.680
那今天我给大家去汇报的一个内容呢

10
0:00:31.680 --> 0:00:34.720
就是推理引擎的模型小型化

11
0:00:34.720 --> 0:00:35.560
一个新的内容啊

12
0:00:35.560 --> 0:00:36.960
模型小型化哦

13
0:00:36.960 --> 0:00:37.480
然后呢

14
0:00:37.480 --> 0:00:40.360
我们在进入正式的一些算法之前呢

15
0:00:40.360 --> 0:00:44.800
我想给大家去介绍一下推理的一个具体的参数

16
0:00:44.800 --> 0:00:46.520
或者相关的参数

17
0:00:46.520 --> 0:00:47.560
那在模型小型化

18
0:00:47.560 --> 0:00:49.600
我们会分开三个内容去介绍的

19
0:00:49.600 --> 0:00:52.280
第一个就是基础的参数的概念

20
0:00:52.280 --> 0:00:53.800
了解完基础的参数

21
0:00:53.800 --> 0:00:58.680
我们才知道我们的模型小型化到底什么样才是有效的

22
0:00:58.680 --> 0:01:04.080
我们应该用什么参数或者什么指针去衡量我们的小型化

23
0:01:04.080 --> 0:01:04.680
那接着呢

24
0:01:04.680 --> 0:01:06.920
我们去看看CNN小型化

25
0:01:06.920 --> 0:01:11.120
最后我们再看看Transformer小型化的一些内容

26
0:01:14.480 --> 0:01:14.960
其实呢

27
0:01:14.960 --> 0:01:16.760
随着我们的AI业务的发展呢

28
0:01:16.760 --> 0:01:17.680
我们的模型啊

29
0:01:17.680 --> 0:01:20.360
应该相对来说是越来越大

30
0:01:20.360 --> 0:01:21.320
那这个圈圈呢

31
0:01:21.320 --> 0:01:23.360
就代表我们的模型的参数量

32
0:01:23.360 --> 0:01:24.800
模型的参数量越大

33
0:01:24.800 --> 0:01:27.000
我们的精度呢是越高

34
0:01:27.000 --> 0:01:28.440
不管是哪种方式也好

35
0:01:28.440 --> 0:01:30.080
走上面这条也好啊

36
0:01:30.080 --> 0:01:31.840
确实越大呢

37
0:01:31.840 --> 0:01:33.920
精度是越高的

38
0:01:33.920 --> 0:01:34.600
那这一点呢

39
0:01:34.600 --> 0:01:35.440
是毋庸置疑的

40
0:01:35.440 --> 0:01:37.640
只是我们大到什么程度

41
0:01:37.640 --> 0:01:38.920
高到什么程度

42
0:01:38.920 --> 0:01:39.640
中间呢

43
0:01:39.640 --> 0:01:42.160
可能会有一个权衡

44
0:01:42.160 --> 0:01:43.520
那往右边一看呢

45
0:01:43.520 --> 0:01:46.120
现在的大模型越来越多

46
0:01:46.120 --> 0:01:47.880
确实已经变成大模型了

47
0:01:47.880 --> 0:01:49.240
既然叫大模型

48
0:01:49.240 --> 0:01:50.520
这个红色这条线呢

49
0:01:50.520 --> 0:01:51.560
就是Large Scale了

50
0:01:51.560 --> 0:01:57.400
就大规模的使得到了我们的模型的参数量进一步的增加

51
0:01:57.400 --> 0:01:58.040
这个时候呢

52
0:01:58.040 --> 0:02:01.440
我们的计算量要求也是非常的高

53
0:02:01.440 --> 0:02:02.720
那基于这一点呢

54
0:02:02.720 --> 0:02:06.760
我们来看看具体的参数量怎么去评价的

55
0:02:09.760 --> 0:02:10.320
那现在呢

56
0:02:10.320 --> 0:02:11.320
我们有几个指针

57
0:02:11.320 --> 0:02:11.840
第一个呢

58
0:02:11.840 --> 0:02:13.960
就是Flops

59
0:02:13.960 --> 0:02:17.080
指的是我们的浮点运算的次数

60
0:02:17.080 --> 0:02:19.160
Floating Point Operation

61
0:02:19.160 --> 0:02:19.880
那这个时候呢

62
0:02:19.880 --> 0:02:22.640
我们一般的会以Flops作为我们的计算量

63
0:02:22.640 --> 0:02:25.560
来去衡量算法模型的时间的复杂度

64
0:02:25.560 --> 0:02:26.360
那接着呢

65
0:02:26.360 --> 0:02:28.280
我们看一个概念

66
0:02:28.280 --> 0:02:29.720
它也叫Flops

67
0:02:29.720 --> 0:02:31.000
但是那个S呢

68
0:02:31.000 --> 0:02:32.760
就变成大写了

69
0:02:32.760 --> 0:02:33.760
那这种情况下呢

70
0:02:33.760 --> 0:02:37.440
我们要做每秒所运行的浮点的运算的次数

71
0:02:37.440 --> 0:02:40.600
Floating Point Operation Per Second

72
0:02:40.600 --> 0:02:41.800
Per Second这个S呢

73
0:02:41.800 --> 0:02:43.720
就变成了一个缩写

74
0:02:43.720 --> 0:02:45.880
每秒所运行的浮点运算次数呢

75
0:02:45.880 --> 0:02:47.760
我们叫做运算的

76
0:02:47.760 --> 0:02:49.600
或者叫做充足的理解

77
0:02:49.600 --> 0:02:50.720
计算的速率

78
0:02:50.720 --> 0:02:52.800
去衡量我们硬件的一个指针

79
0:02:52.800 --> 0:02:54.640
还有模型速度的一个指针

80
0:02:54.640 --> 0:02:57.560
作为芯片的一个算力指针

81
0:02:57.560 --> 0:02:58.120
接下来呢

82
0:02:58.120 --> 0:02:59.400
我们看一下第三个概念

83
0:02:59.400 --> 0:03:00.960
就是MACCS

84
0:03:00.960 --> 0:03:03.000
乘加的操作次数

85
0:03:03.000 --> 0:03:06.200
Multiple Accumulator Operations

86
0:03:06.200 --> 0:03:07.440
通常来说呢

87
0:03:07.440 --> 0:03:09.600
乘加的操作次数就MACCS呢

88
0:03:09.600 --> 0:03:11.920
是第一个点浮点运算次数

89
0:03:11.920 --> 0:03:13.720
Flops的一半

90
0:03:13.720 --> 0:03:14.280
举个例子

91
0:03:14.280 --> 0:03:15.280
就是我们现在呢

92
0:03:15.280 --> 0:03:17.040
有很多矩阵的相乘呢

93
0:03:17.040 --> 0:03:19.120
W0乘以X0呢

94
0:03:19.120 --> 0:03:23.240
我们把它视为简单的一个乘法的操作

95
0:03:23.240 --> 0:03:24.360
那大部分时候呢

96
0:03:24.360 --> 0:03:27.320
我们都会做非常大量的乘法的运算

97
0:03:27.320 --> 0:03:28.560
或者乘加的运算

98
0:03:28.560 --> 0:03:29.720
在我们的推理芯片

99
0:03:29.720 --> 0:03:31.320
或者AI加速芯片里面

100
0:03:31.320 --> 0:03:33.320
那这也是其中一个指针

101
0:03:33.320 --> 0:03:34.640
那第四个指针呢

102
0:03:34.640 --> 0:03:36.360
就是我们的Primiter

103
0:03:36.360 --> 0:03:40.200
这个是我们去衡量刚才的那些图里面

104
0:03:40.200 --> 0:03:43.040
非常有利的一个指针数

105
0:03:43.040 --> 0:03:44.440
那就我们的模型的大小

106
0:03:44.440 --> 0:03:46.880
说白了就是模型的大小数

107
0:03:46.880 --> 0:03:47.640
那这个时候呢

108
0:03:47.640 --> 0:03:50.760
会直接影响到我们对内存的占用量

109
0:03:50.760 --> 0:03:51.240
单位呢

110
0:03:51.240 --> 0:03:52.120
通常为M

111
0:03:52.120 --> 0:03:54.040
就是KBMB这个M

112
0:03:54.040 --> 0:03:54.760
那这个M呢

113
0:03:54.760 --> 0:03:56.280
主要是指MB

114
0:03:56.280 --> 0:03:57.680
而我们的参数量呢

115
0:03:57.680 --> 0:04:00.760
一般用Float32去表示

116
0:04:00.760 --> 0:04:02.800
因为我们一般存储或训练的时候呢

117
0:04:02.800 --> 0:04:05.160
都是用FP32去训练的

118
0:04:05.160 --> 0:04:05.920
那这个时候呢

119
0:04:05.920 --> 0:04:06.960
模型的大小呢

120
0:04:06.960 --> 0:04:08.400
是参数量的4倍

121
0:04:09.880 --> 0:04:12.360
下面我们来再看另外两个指针

122
0:04:12.360 --> 0:04:13.720
另外两个指针呢

123
0:04:13.720 --> 0:04:14.640
叫做MAC

124
0:04:14.640 --> 0:04:16.000
跟MACC是不一样的

125
0:04:16.000 --> 0:04:17.440
大家要注意一下

126
0:04:17.440 --> 0:04:18.320
MAC呢

127
0:04:18.320 --> 0:04:20.920
这个指针我们经常去用到

128
0:04:20.920 --> 0:04:22.640
就是我们的内存访问的代价

129
0:04:22.680 --> 0:04:24.200
Memory Assessed Post

130
0:04:25.480 --> 0:04:26.120
MAC呢

131
0:04:26.120 --> 0:04:28.160
主要是指我们输一个简单的样本

132
0:04:28.160 --> 0:04:29.600
那我们以图像为例子

133
0:04:29.600 --> 0:04:31.040
我输入对我们的系统呢

134
0:04:31.040 --> 0:04:32.400
输入一个图片

135
0:04:32.400 --> 0:04:34.800
那我们完成一个整体的前线传播

136
0:04:34.800 --> 0:04:36.760
或者一个简单的卷机之后呢

137
0:04:36.760 --> 0:04:39.600
摸索对内存的一个交换的总量

138
0:04:39.600 --> 0:04:40.960
就模型的空间复杂度

139
0:04:40.960 --> 0:04:41.560
单位呢

140
0:04:41.560 --> 0:04:45.120
我们用byte来去做一个统计

141
0:04:45.120 --> 0:04:46.240
那最后一个呢

142
0:04:46.240 --> 0:04:47.880
就是内存的带宽

143
0:04:47.880 --> 0:04:48.680
内存的带宽

144
0:04:48.680 --> 0:04:50.480
这一个参数量是非常重要的

145
0:04:50.480 --> 0:04:52.520
就我觉得里面比较重要的

146
0:04:52.520 --> 0:04:54.440
几个参数量吧

147
0:04:54.440 --> 0:04:55.440
或者几个指针

148
0:04:55.440 --> 0:04:56.840
就是内存的带宽

149
0:04:56.840 --> 0:04:57.840
MAC

150
0:04:57.840 --> 0:04:58.440
Flop

151
0:04:58.440 --> 0:04:59.600
模型的参数

152
0:04:59.600 --> 0:05:01.760
这四个其实是比较重要的

153
0:05:01.760 --> 0:05:02.560
内存的带宽呢

154
0:05:02.560 --> 0:05:04.320
主要决定了我们将数据呢

155
0:05:04.320 --> 0:05:06.280
从内存里面移到我们的LU

156
0:05:06.280 --> 0:05:07.800
或者我们的kernel的内核

157
0:05:07.800 --> 0:05:11.200
或者TensorRT里面去做计算的速率

158
0:05:11.200 --> 0:05:13.840
就是搬运内存的一个速率

159
0:05:15.040 --> 0:05:16.600
那内存带宽的值呢

160
0:05:16.600 --> 0:05:17.200
这个值呢

161
0:05:17.200 --> 0:05:19.600
决定于我们内存和计算内核之间的数据

162
0:05:19.600 --> 0:05:21.040
传输的速率

163
0:05:21.040 --> 0:05:22.520
这个值是越高越好

164
0:05:22.520 --> 0:05:23.480
但肯定了

165
0:05:23.480 --> 0:05:25.320
因为我们的硬件的设计的问题

166
0:05:25.320 --> 0:05:26.640
或者我们的功耗的问题

167
0:05:26.640 --> 0:05:28.560
还有那个价格的问题呢

168
0:05:28.560 --> 0:05:29.520
这个内存带宽呢

169
0:05:29.520 --> 0:05:31.040
会有一定的峰值的

170
0:05:33.680 --> 0:05:35.560
那我们现在来看看几个

171
0:05:35.560 --> 0:05:37.680
比较典型的一个计算

172
0:05:37.680 --> 0:05:39.880
那我们在标准的卷机藏久机

173
0:05:39.880 --> 0:05:40.600
那这个时候呢

174
0:05:40.600 --> 0:05:41.640
我们的参数量呢

175
0:05:41.640 --> 0:05:43.160
就等于我们的kernel的hive

176
0:05:43.160 --> 0:05:43.920
kernel的wive

177
0:05:43.920 --> 0:05:45.920
乘以kernel的in和kernel的out

178
0:05:45.920 --> 0:05:47.040
我们的参数量呢

179
0:05:47.040 --> 0:05:48.600
大部分是这么去计算的

180
0:05:48.600 --> 0:05:49.840
为啥都是kernel呢

181
0:05:51.040 --> 0:05:53.080
为啥都是kernel跟输入的数据呢

182
0:05:53.080 --> 0:05:55.440
是因为我们大部分的参数量啊

183
0:05:56.040 --> 0:05:57.040
都是从我们的kernel

184
0:05:57.040 --> 0:05:58.480
或者kernel我们的channel

185
0:05:58.480 --> 0:06:00.400
c in c out的一些参数量

186
0:06:01.240 --> 0:06:02.480
那在算flops

187
0:06:02.480 --> 0:06:04.120
浮点运算精度的时候呢

188
0:06:04.120 --> 0:06:05.880
就会再乘以一个h和w

189
0:06:05.880 --> 0:06:07.840
就是图像的长和宽

190
0:06:08.280 --> 0:06:08.880
那下面呢

191
0:06:08.880 --> 0:06:10.720
我们还有好几个就是全连接了

192
0:06:10.720 --> 0:06:11.880
全连接比较简单

193
0:06:11.920 --> 0:06:13.920
基本上都是c in c out去计算

194
0:06:13.960 --> 0:06:15.440
然后也没有其他了

195
0:06:15.560 --> 0:06:16.320
另外的话

196
0:06:16.320 --> 0:06:17.400
我们卷机呢

197
0:06:17.400 --> 0:06:18.800
还有gup卷机

198
0:06:18.800 --> 0:06:20.040
所以说gup卷机呢

199
0:06:20.040 --> 0:06:21.240
可能里面呢

200
0:06:21.240 --> 0:06:22.880
就会做成一个group

201
0:06:22.920 --> 0:06:24.320
然后做flops的时候呢

202
0:06:24.320 --> 0:06:26.280
会对w除一个group

203
0:06:27.720 --> 0:06:29.240
当然还有device卷机

204
0:06:29.240 --> 0:06:30.160
device卷机呢

205
0:06:30.160 --> 0:06:31.720
就会除一个c in

206
0:06:31.720 --> 0:06:32.360
然后呢

207
0:06:32.360 --> 0:06:33.080
这个flops呢

208
0:06:33.080 --> 0:06:34.400
也是相同的

209
0:06:34.440 --> 0:06:37.200
所以说我们要了解算法

210
0:06:37.200 --> 0:06:38.520
为啥要了解算法

211
0:06:38.520 --> 0:06:39.840
要了解kernel呢

212
0:06:39.880 --> 0:06:41.600
就是因为我们有很多不同的

213
0:06:41.600 --> 0:06:42.560
计算的方式

214
0:06:42.600 --> 0:06:45.480
都会影响我们整个的系统的效率

215
0:06:45.480 --> 0:06:45.880
嗯

216
0:06:50.000 --> 0:06:50.720
那下面呢

217
0:06:50.720 --> 0:06:52.200
我们以英伟达的t4呢

218
0:06:52.200 --> 0:06:53.160
去了解一下

219
0:06:53.160 --> 0:06:55.480
这些具体的参数指针

220
0:06:55.520 --> 0:06:57.640
那这个就是英伟达t4的一个

221
0:06:57.640 --> 0:07:00.440
GPU双联路的一个具体的推理的性能

222
0:07:00.480 --> 0:07:01.880
还有训练的性能

223
0:07:01.920 --> 0:07:02.520
可以看到啊

224
0:07:02.520 --> 0:07:04.200
t4大部分都是做推理的

225
0:07:04.200 --> 0:07:05.720
所以训练我们可以不看

226
0:07:05.760 --> 0:07:07.320
那推理的性能可以看到

227
0:07:07.320 --> 0:07:09.080
确实它有非常对比起

228
0:07:09.200 --> 0:07:09.880
CPU呢

229
0:07:09.880 --> 0:07:11.560
有非常大的

230
0:07:11.560 --> 0:07:14.480
或者非常高的一个性能的提升

231
0:07:14.480 --> 0:07:15.880
而这些性能的提升呢

232
0:07:15.880 --> 0:07:17.520
我们看看它具体的规格

233
0:07:17.800 --> 0:07:20.440
里面的一个tensor的内核数

234
0:07:20.440 --> 0:07:21.760
还有cuda的内核数

235
0:07:21.960 --> 0:07:22.920
cuda的内核数呢

236
0:07:22.920 --> 0:07:24.080
就意味着我们对于vector的

237
0:07:24.080 --> 0:07:25.040
计算或者在线数呢

238
0:07:25.040 --> 0:07:26.640
可以做的非常的多

239
0:07:26.680 --> 0:07:28.520
而tensor的内核数呢

240
0:07:28.520 --> 0:07:31.320
就我们对一些稠密的矩阵的预算

241
0:07:31.520 --> 0:07:31.920
接着呢

242
0:07:31.920 --> 0:07:32.800
我们可以看一下

243
0:07:32.800 --> 0:07:33.760
其实很多

244
0:07:33.760 --> 0:07:34.720
不管是单精度

245
0:07:34.720 --> 0:07:35.440
否点精度

246
0:07:35.440 --> 0:07:37.320
还是int4呢

247
0:07:37.320 --> 0:07:39.240
我们都会有一个tflops

248
0:07:39.280 --> 0:07:40.360
那这个s呢

249
0:07:40.360 --> 0:07:42.840
是大小每秒处理的数据量

250
0:07:42.840 --> 0:07:45.200
那为啥int8,int4少了个flop呢

251
0:07:45.240 --> 0:07:47.080
因为flop是指floating

252
0:07:47.080 --> 0:07:48.480
浮点运算

253
0:07:48.480 --> 0:07:49.160
所以里面呢

254
0:07:49.160 --> 0:07:50.840
就少了个f

255
0:07:51.760 --> 0:07:52.240
接着呢

256
0:07:52.240 --> 0:07:54.320
我们可以看看比较留意的参数

257
0:07:54.320 --> 0:07:56.880
就GPU的显存300GbE

258
0:07:56.880 --> 0:07:57.440
那这个呢

259
0:07:57.440 --> 0:08:00.080
就是我们显存的一个传输的速率

260
0:08:00.080 --> 0:08:02.400
也是非常的重要的参数

261
0:08:02.440 --> 0:08:03.120
另外的话

262
0:08:03.120 --> 0:08:05.520
还有一个互联的带宽

263
0:08:05.520 --> 0:08:06.640
就我们的PCIe桥

264
0:08:06.640 --> 0:08:08.760
会传输多高的速率

265
0:08:08.760 --> 0:08:09.360
那这个呢

266
0:08:09.360 --> 0:08:11.160
就是设备外的传输的速率了

267
0:08:11.160 --> 0:08:12.640
而设备内的传输的速率呢

268
0:08:12.640 --> 0:08:13.680
就300GbE

269
0:08:13.680 --> 0:08:14.080
这个呢

270
0:08:14.080 --> 0:08:15.360
也是非常重要的

271
0:08:15.360 --> 0:08:18.160
对应到我们刚才聊到的内存的带宽

272
0:08:20.800 --> 0:08:21.200
好了

273
0:08:21.200 --> 0:08:21.840
今天内容呢

274
0:08:21.840 --> 0:08:22.760
就到这里为止

275
0:08:22.760 --> 0:08:23.360
我们今天呢

276
0:08:23.360 --> 0:08:25.680
主要是了解了一些基础的参数的概念

277
0:08:25.680 --> 0:08:27.560
然后以英伟达T4作为例子

278
0:08:27.560 --> 0:08:29.800
再看了一些具体的这些参数

279
0:08:29.800 --> 0:08:31.440
对我们怎么去看芯片

280
0:08:31.440 --> 0:08:35.320
对我们的模型小型化推进行有什么作用

281
0:08:36.120 --> 0:08:36.960
卷的不行了

282
0:08:36.960 --> 0:08:37.800
卷的不行了

283
0:08:37.800 --> 0:08:39.600
记得一键三连加关注哦

284
0:08:39.600 --> 0:08:41.000
所有的内容都会开源

285
0:08:41.040 --> 0:08:42.680
在下面这条链接里面

286
0:08:43.280 --> 0:08:44.040
拜拜

