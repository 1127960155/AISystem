0
0:00:00.000 --> 0:00:11.200
Hello大家好,我是一直坚持分享AI系统,但是关注的人并不是很多的ZOMIE

1
0:00:11.200 --> 0:00:16.000
今天我们来到推理系统里面的模型小型化

2
0:00:16.000 --> 0:00:21.400
今天我要给大家去分享的一个内容就是CNN网络模型的小型化

3
0:00:21.400 --> 0:00:25.400
为啥会单独分开CNN跟Transformer模型的小型化呢?

4
0:00:25.400 --> 0:00:30.800
就有两个内容了,是因为其实我们现在Transformer非常非常的火

5
0:00:30.800 --> 0:00:35.400
火到一大壶通,现在你不发Transformer的模型呢,说实话它的精度很难上去

6
0:00:35.400 --> 0:00:40.600
而且Transformer确实很outstanding,可以发很多新的一些paper

7
0:00:40.600 --> 0:00:44.800
但是呢,Transformer根据我们在对客户的一些交付

8
0:00:44.800 --> 0:00:48.400
它的落地并不是说非常的理想,而现在用的更多

9
0:00:48.400 --> 0:00:52.400
或者在落地场景里面做模型的小型化或者结构的小型化

10
0:00:52.400 --> 0:00:54.600
更多的是以CNN为主

11
0:00:58.200 --> 0:01:02.600
那可以看到其实我们的网络模型的精度越高

12
0:01:02.600 --> 0:01:05.200
确实我们的模型的参数量越大

13
0:01:05.200 --> 0:01:10.000
好像DanceNet, SDNet,还有MobileNet都是一些相同精度

14
0:01:10.000 --> 0:01:14.000
但是模型量确实小了非常多的模型推出来了

15
0:01:15.800 --> 0:01:21.200
今天我主要是聚焦这种CNN模型的小型化的一些结构给大家去分享的

16
0:01:21.200 --> 0:01:24.600
在分享的过程当中,我们以一些系列作为例子

17
0:01:24.600 --> 0:01:29.000
那我们看一看简单的去了解一下今天主要分享的内容有哪些

18
0:01:29.000 --> 0:01:33.400
首先呢,我们会去按时间的序号去排一排

19
0:01:33.400 --> 0:01:37.200
我们会了解一下SquizNet,Servanet,MobileNet

20
0:01:37.200 --> 0:01:39.200
了解完这一套系列之后呢

21
0:01:39.200 --> 0:01:44.200
我们再去简单的去看一看ESPNet,FBNet,EfficientNet

22
0:01:44.200 --> 0:01:47.200
最后还有华为洛亚的GhostNet

23
0:01:47.200 --> 0:01:49.600
那了解完这些内容之后呢

24
0:01:49.600 --> 0:01:52.600
我们再会去做一个具体的总结

25
0:01:52.600 --> 0:01:56.400
现在我们一起去看看具体的算法

26
0:01:56.400 --> 0:02:01.000
去了解一下轻量级网络模型具体是怎幺设计的

27
0:02:01.000 --> 0:02:06.200
它跟传统的CNN卷积一层一层的堆下来这种WestNet有哪些区别

28
0:02:08.400 --> 0:02:10.600
在轻量化主干网络模型里面呢

29
0:02:10.600 --> 0:02:13.600
第一个比较著名的是SquizNet

30
0:02:13.600 --> 0:02:16.600
跟ESPNet相比有了50倍的模型的压缩

31
0:02:16.600 --> 0:02:22.000
那下面呢我们看看SquizNet具体有哪些不一样的网络模型结构的设计

32
0:02:22.000 --> 0:02:22.800
看到这个图呢

33
0:02:22.800 --> 0:02:25.200
其实SquizNet提出一个最重要的model

34
0:02:25.200 --> 0:02:27.900
就是它的一个FileModel

35
0:02:27.900 --> 0:02:30.300
这个FileModel有两个模块去组成的

36
0:02:30.300 --> 0:02:31.800
一个是Squiz模块

37
0:02:31.800 --> 0:02:34.000
一个是ESPN模块

38
0:02:34.000 --> 0:02:35.300
Squiz这个模块呢

39
0:02:35.300 --> 0:02:39.200
主要是由一系列连续的一层一的卷积进行组成

40
0:02:39.200 --> 0:02:43.200
它最重要的主要是对一些FeatureMap的信道数进行改变

41
0:02:43.200 --> 0:02:45.900
减少了整个信道数

42
0:02:45.900 --> 0:02:47.000
ESPN的模块呢

43
0:02:47.000 --> 0:02:48.700
主要是由一层一的卷积盒

44
0:02:48.700 --> 0:02:51.200
还有三层三的卷积盒进行组成的

45
0:02:51.200 --> 0:02:51.900
它们之间呢

46
0:02:51.900 --> 0:02:54.600
通过Concat进行汇聚起来

47
0:02:54.600 --> 0:02:55.300
下面呢

48
0:02:55.300 --> 0:03:00.500
我们往下浏览去看看SquizNet的一个网络模型的结构

49
0:03:00.500 --> 0:03:02.900
可以看到SquizNet的网络模型啊

50
0:03:02.900 --> 0:03:05.400
会比ESNet的网络模型更加深

51
0:03:05.400 --> 0:03:08.200
而且会有很多个FileModel进行组成

52
0:03:08.200 --> 0:03:09.600
不同的组织模块呢

53
0:03:09.600 --> 0:03:12.300
或者不同的网络模型的组成了又有不同的精度

54
0:03:13.700 --> 0:03:15.000
那对应来说啊

55
0:03:15.000 --> 0:03:17.200
SquizNet V1这篇文章呢

56
0:03:17.200 --> 0:03:19.400
是发表在ICLR 2017

57
0:03:19.400 --> 0:03:22.700
而它真正的是在2016的时候发表出来了

58
0:03:22.700 --> 0:03:23.600
我们现在呢

59
0:03:23.600 --> 0:03:26.000
看看SquizNet V2这个版本

60
0:03:28.600 --> 0:03:32.500
下面我们看一下SquizNet系列里面的第二篇SquizNet

61
0:03:32.500 --> 0:03:34.600
我们也叫做SquizNet V2

62
0:03:34.600 --> 0:03:35.900
那这篇文章呢

63
0:03:35.900 --> 0:03:37.600
对比那个Vesnet50呢

64
0:03:37.600 --> 0:03:39.000
它有了相同的精度

65
0:03:39.000 --> 0:03:41.500
但是提升了100倍的模型压缩

66
0:03:41.500 --> 0:03:42.200
左边这个呢

67
0:03:42.200 --> 0:03:44.600
就是Vesnet最经典的一个Block

68
0:03:44.600 --> 0:03:45.500
像SquizNet呢

69
0:03:45.500 --> 0:03:48.500
就提出了右边的两个结构

70
0:03:48.500 --> 0:03:51.600
中间这个就是SquizNet V1的一个Block

71
0:03:51.600 --> 0:03:54.000
由Squiz跟Expand两层进行组成

72
0:03:54.000 --> 0:03:54.500
Squiz呢

73
0:03:54.500 --> 0:03:56.500
就是1x1的卷集合

74
0:03:56.500 --> 0:03:57.300
Expand层呢

75
0:03:57.300 --> 0:04:00.700
就是1x1 Concat 3x3这种方式

76
0:04:00.700 --> 0:04:01.700
那在右边呢

77
0:04:01.700 --> 0:04:03.600
就是SquizNet的一个Block

78
0:04:03.600 --> 0:04:07.400
就是SquizNet V2的一种Block的一个网络模型结构

79
0:04:07.400 --> 0:04:08.400
那幺可以看到啊

80
0:04:08.400 --> 0:04:09.300
其实这里面呢

81
0:04:09.300 --> 0:04:10.500
有一种比较有意思

82
0:04:10.500 --> 0:04:12.100
就是它主要呢

83
0:04:12.100 --> 0:04:15.300
是采用了Vesnet的这种长叉的结构方式

84
0:04:15.300 --> 0:04:16.000
但是呢

85
0:04:16.000 --> 0:04:19.100
它做了一个就是把3x3的卷集呢

86
0:04:19.100 --> 0:04:20.600
拆分成3x1

87
0:04:20.600 --> 0:04:23.300
然后1x3两层的卷集

88
0:04:23.300 --> 0:04:24.200
这样的方式呢

89
0:04:24.200 --> 0:04:25.800
有点类似于D字分解

90
0:04:25.800 --> 0:04:28.400
最后再接1x1的卷集

91
0:04:28.400 --> 0:04:29.800
网络模型的参数量呢

92
0:04:29.800 --> 0:04:32.200
从k的平方变成2k

93
0:04:32.200 --> 0:04:33.100
而另外一点呢

94
0:04:33.100 --> 0:04:35.200
需要注意的就是网络模型的数呢

95
0:04:35.200 --> 0:04:37.000
是有128的信道

96
0:04:37.000 --> 0:04:38.100
那通过这里面呢

97
0:04:38.100 --> 0:04:40.400
64 32两个1x1的卷集呢

98
0:04:40.500 --> 0:04:41.500
把信道数呢

99
0:04:41.500 --> 0:04:44.100
从128减到64再减到32

100
0:04:44.100 --> 0:04:44.600
然后呢

101
0:04:44.600 --> 0:04:46.100
通过刚才的D字分解呢

102
0:04:46.100 --> 0:04:48.400
慢慢的把信道数恢复上来

103
0:04:48.400 --> 0:04:51.700
最后再通过一个1x1的卷集恢复到128

104
0:04:51.700 --> 0:04:53.500
然后再往下传递

105
0:04:53.500 --> 0:04:54.700
那通过这种方式呢

106
0:04:54.700 --> 0:04:57.300
其实代替掉了原来的SquizNet

107
0:04:57.300 --> 0:04:58.200
那原来的数呢

108
0:04:58.200 --> 0:04:59.000
是128

109
0:04:59.000 --> 0:05:00.900
中间是两层64

110
0:05:00.900 --> 0:05:03.600
最后输出的是128

111
0:05:03.600 --> 0:05:04.700
那这种方式呢

112
0:05:04.700 --> 0:05:06.300
更多的是采用了

113
0:05:06.300 --> 0:05:10.100
或者借鉴了Vesnet50的这种长叉的结构的方式

114
0:05:10.100 --> 0:05:12.700
对原来的SquizNet V1进行改进

115
0:05:14.300 --> 0:05:17.400
下面我们看看第二个模型系列

116
0:05:17.400 --> 0:05:18.900
Sephernet

117
0:05:18.900 --> 0:05:19.500
在这里面呢

118
0:05:19.500 --> 0:05:22.100
我更多的是把他们之间最重要的特点

119
0:05:22.100 --> 0:05:23.900
给大家简单的过一遍

120
0:05:23.900 --> 0:05:25.700
我希望其实更多的开发者

121
0:05:25.700 --> 0:05:28.000
或者更多的有兴趣的读者呢

122
0:05:28.000 --> 0:05:30.800
能够深入的去自己去看看相关的论文

123
0:05:32.900 --> 0:05:34.100
Sephernet这篇文章呢

124
0:05:34.100 --> 0:05:36.300
最早是发布于2017年的

125
0:05:36.300 --> 0:05:38.500
我们现在看看它具体的一个

126
0:05:38.500 --> 0:05:41.100
网络模型的结构

127
0:05:41.100 --> 0:05:43.700
如果对算法比较了解的同学可以知道啊

128
0:05:43.700 --> 0:05:44.500
其实卷机呢

129
0:05:44.500 --> 0:05:46.000
有很多种有Device卷机

130
0:05:46.000 --> 0:05:46.700
有Group卷机

131
0:05:46.700 --> 0:05:48.000
还有普通的卷机

132
0:05:48.000 --> 0:05:48.500
当然了

133
0:05:48.500 --> 0:05:51.500
一层一跟三层三跟七层七的卷机的具体实现方式

134
0:05:51.500 --> 0:05:52.700
也是不一样的

135
0:05:52.700 --> 0:05:55.500
而且他们的运算速度也是不一样

136
0:05:55.500 --> 0:05:56.700
这里面作者就提到

137
0:05:56.700 --> 0:05:57.700
一层一的卷机呢

138
0:05:57.700 --> 0:05:59.700
会消耗大量的计算资源

139
0:05:59.700 --> 0:06:01.100
而像Group卷机这种

140
0:06:01.100 --> 0:06:03.000
可以降低我们计算量的卷机呢

141
0:06:03.000 --> 0:06:04.700
其实很难对不同Group

142
0:06:04.700 --> 0:06:06.600
之间的一些信息进行共享

143
0:06:06.600 --> 0:06:07.200
所以呢

144
0:06:07.200 --> 0:06:09.700
他就提出了一个使用Group卷机

145
0:06:09.700 --> 0:06:10.400
但是呢

146
0:06:10.400 --> 0:06:13.400
后面去采用一个Channel Surfer的操作

147
0:06:13.400 --> 0:06:15.700
然后去代替传统的Group卷机

148
0:06:15.700 --> 0:06:18.200
直接得到一个Feature Map的这种方式

149
0:06:18.200 --> 0:06:22.000
那我们可以看一下具体的网络模型结构

150
0:06:22.000 --> 0:06:23.100
那左边的这个呢

151
0:06:23.100 --> 0:06:25.200
就是最原始的这种

152
0:06:25.200 --> 0:06:27.300
WebSnap50的网络模型结构

153
0:06:27.300 --> 0:06:27.900
这里面呢

154
0:06:27.900 --> 0:06:30.200
DW就是Device卷机

155
0:06:30.200 --> 0:06:31.600
而右边的这两个图呢

156
0:06:31.600 --> 0:06:33.800
就是刚才提出的Surfnet的一个思想

157
0:06:33.800 --> 0:06:34.300
里面呢

158
0:06:34.300 --> 0:06:36.800
就使用了一个Channel Surfer

159
0:06:36.800 --> 0:06:37.500
而上面呢

160
0:06:37.500 --> 0:06:39.800
把普通卷机变成一个Group卷机

161
0:06:39.800 --> 0:06:40.300
接着呢

162
0:06:40.300 --> 0:06:42.000
去接一个Device的卷机

163
0:06:42.000 --> 0:06:43.600
最后再接一个Group卷机

164
0:06:43.600 --> 0:06:46.900
就是魔改了WebSnap50的一种结构的方式

165
0:06:46.900 --> 0:06:49.200
好处就是能够减少我们的网络模型的大小

166
0:06:49.200 --> 0:06:50.300
保持相同的精度

167
0:06:50.300 --> 0:06:51.600
模型更小了

168
0:06:51.600 --> 0:06:53.300
但是有一个比较大的缺点

169
0:06:53.300 --> 0:06:54.500
就是Channel Surfer呢

170
0:06:54.500 --> 0:06:56.000
是沿用去定义的

171
0:06:56.000 --> 0:06:57.000
那这里面的规则呢

172
0:06:57.000 --> 0:06:58.000
就比较复杂

173
0:07:00.100 --> 0:07:00.600
下面呢

174
0:07:00.600 --> 0:07:03.300
我们来看看Surfnet V2的一个网络模型结构

175
0:07:03.300 --> 0:07:04.000
那A呢

176
0:07:04.000 --> 0:07:06.200
就是Surfnet V1的网络模型结构

177
0:07:06.200 --> 0:07:06.900
那C呢

178
0:07:06.900 --> 0:07:10.100
就是对应的Surfnet V2的网络模型结构

179
0:07:10.100 --> 0:07:10.700
可以看到啊

180
0:07:10.700 --> 0:07:11.900
在模型输入的时候呢

181
0:07:11.900 --> 0:07:13.700
加了一个Channel Speed的工作

182
0:07:13.700 --> 0:07:15.300
把我们的网络模型的Channel呢

183
0:07:15.300 --> 0:07:17.800
直接分开成为两半

184
0:07:17.800 --> 0:07:18.400
在这里面呢

185
0:07:18.400 --> 0:07:22.000
作者就表明在网络模型的输入输出大小相同的时候呢

186
0:07:22.000 --> 0:07:22.600
这种方式呢

187
0:07:22.600 --> 0:07:25.000
能够有效的提升网络模型的

188
0:07:25.000 --> 0:07:27.200
或者我们计算的MAC

189
0:07:27.200 --> 0:07:28.500
第二个优化的工作呢

190
0:07:28.500 --> 0:07:31.200
就取消了1x1的Group卷机

191
0:07:31.200 --> 0:07:33.500
就直接使用了1x1的卷机去代替

192
0:07:34.500 --> 0:07:36.500
作者在经过大量的实验里面呢

193
0:07:36.500 --> 0:07:38.900
去表示其实过多的Group卷机呢

194
0:07:38.900 --> 0:07:41.300
会提升我们整个计算的MAC

195
0:07:42.000 --> 0:07:43.000
那MAC是什幺

196
0:07:43.000 --> 0:07:45.600
我们其实在上一节课里面给大家去普及过

197
0:07:45.600 --> 0:07:47.800
相关的一些参数的概念

198
0:07:48.700 --> 0:07:49.800
第三点优化呢

199
0:07:49.800 --> 0:07:51.200
就是Channel Surfer

200
0:07:51.200 --> 0:07:53.000
可以看到蓝色的这个模块呢

201
0:07:53.000 --> 0:07:54.900
其实挪到最后面

202
0:07:54.900 --> 0:07:56.200
就是Concate之后

203
0:07:56.200 --> 0:07:57.300
因为我们可以看到

204
0:07:57.300 --> 0:07:58.900
我们改变了这个Group卷机

205
0:07:58.900 --> 0:08:01.100
后面再加个Channel Surfer是没有意义的

206
0:08:01.500 --> 0:08:04.100
我们把Channel Surfer往后面放

207
0:08:04.100 --> 0:08:07.100
把不同信道之间的一些信息的传递

208
0:08:07.100 --> 0:08:08.100
放到最后面

209
0:08:08.100 --> 0:08:11.500
有效的减少我们网络模型的虽不化的程度

210
0:08:11.500 --> 0:08:13.000
那第四个改变呢

211
0:08:13.000 --> 0:08:15.400
就是Concate这个操作

212
0:08:15.400 --> 0:08:17.100
可以看到在SurferLive 1里面呢

213
0:08:17.100 --> 0:08:19.000
用的是一个Add的操作

214
0:08:19.000 --> 0:08:21.000
而在SurferLive 1里面呢

215
0:08:21.000 --> 0:08:23.600
用的是一个Concate的这种方式

216
0:08:23.600 --> 0:08:24.200
其实呢

217
0:08:24.200 --> 0:08:26.200
Add它是一个MOS的一种操作

218
0:08:26.200 --> 0:08:27.200
而Concate呢

219
0:08:27.200 --> 0:08:30.200
其实更加有效的提升我们计算的Flops

220
0:08:32.100 --> 0:08:32.800
接下来呢

221
0:08:32.800 --> 0:08:34.800
我们看一个最重要的系列

222
0:08:34.800 --> 0:08:36.400
就是MobileNet系列

223
0:08:36.400 --> 0:08:39.500
那MobileNet系列是由谷歌去提出来的

224
0:08:39.500 --> 0:08:40.000
里面呢

225
0:08:40.000 --> 0:08:41.600
就推出了V1 V2 V3

226
0:08:41.600 --> 0:08:42.900
三个不同的系列

227
0:08:45.900 --> 0:08:48.200
首先我们看看MobileNet V1

228
0:08:48.200 --> 0:08:51.400
这个系列到底有什幺最大的特点呢

229
0:08:51.400 --> 0:08:52.300
MobileNet V1呢

230
0:08:52.300 --> 0:08:55.500
最重要的就提出了一个新的计算的方式

231
0:08:55.500 --> 0:08:56.900
或者新的计算的结构

232
0:08:56.900 --> 0:08:58.300
那上面这个图A呢

233
0:08:58.300 --> 0:09:01.000
就是Standard卷机的一种滤波器啊

234
0:09:01.000 --> 0:09:03.700
可以看到大部分的我们假设两个DK呢

235
0:09:03.700 --> 0:09:05.700
就是我们Kernels的大小

236
0:09:05.700 --> 0:09:06.500
那一般来说呢

237
0:09:06.500 --> 0:09:09.400
我们会使用三乘三的这种卷机的方式

238
0:09:09.400 --> 0:09:10.500
那结果呢

239
0:09:10.500 --> 0:09:13.000
作者就把这种三乘三的卷机方式呢

240
0:09:13.000 --> 0:09:14.800
就替换成为了两个结构

241
0:09:14.800 --> 0:09:16.500
一个呢就是Deadwise Convolution

242
0:09:16.500 --> 0:09:18.400
另外一个是Pointwise Convolution

243
0:09:18.400 --> 0:09:20.100
可以看到Deadwise Convolution呢

244
0:09:20.100 --> 0:09:24.100
里面就使用了跟Standard卷机是一样的DK

245
0:09:24.100 --> 0:09:24.700
但是呢

246
0:09:24.700 --> 0:09:25.700
在Pointwise里面呢

247
0:09:25.700 --> 0:09:29.900
就使用了1乘以1卷机的方式去替换掉

248
0:09:29.900 --> 0:09:31.100
那举一个具体的例子

249
0:09:31.100 --> 0:09:33.600
假设原始的网络模型的卷机呢

250
0:09:33.600 --> 0:09:35.400
使用了一个三乘三的卷机

251
0:09:35.400 --> 0:09:36.100
那这里面呢

252
0:09:36.100 --> 0:09:38.100
先使用一个Deadwise三乘三

253
0:09:38.100 --> 0:09:40.600
再执行一个1乘1的Pointwise的卷机

254
0:09:40.600 --> 0:09:42.100
那通过这种方式呢

255
0:09:42.100 --> 0:09:44.800
去代替掉原来的卷机的方式

256
0:09:44.800 --> 0:09:49.600
有了一个比较大的网络模型的参数量的降低

257
0:09:49.600 --> 0:09:50.300
这种方式呢

258
0:09:50.300 --> 0:09:54.100
有效的增加了我们整个网络模型执行的Flops

259
0:09:54.100 --> 0:09:55.400
模型的参数量呢

260
0:09:55.400 --> 0:09:58.200
也是急剧的减少了非常非常的多

261
0:10:00.600 --> 0:10:04.600
接下来我们看看MobileNet VR这个网络模型

262
0:10:04.600 --> 0:10:05.900
那这个网络模型呢

263
0:10:05.900 --> 0:10:08.500
确实是非常非常的经典了

264
0:10:08.500 --> 0:10:09.000
这里面呢

265
0:10:09.000 --> 0:10:10.700
就提出了两个概念

266
0:10:10.700 --> 0:10:11.100
第一个呢

267
0:10:11.100 --> 0:10:13.000
就是Invert Vestu

268
0:10:13.000 --> 0:10:15.400
第二个就是Linear Bottleneck

269
0:10:15.400 --> 0:10:17.700
那我们看看两个具体的概念

270
0:10:17.700 --> 0:10:22.200
对我们整个模型压缩和轻量化有什幺不一样的工作

271
0:10:22.200 --> 0:10:23.000
第一个概念呢

272
0:10:23.000 --> 0:10:24.600
就是Invert Vestu这个Block

273
0:10:24.600 --> 0:10:25.400
可以看到啊

274
0:10:25.400 --> 0:10:27.600
左边这个就是传统的Vestu的Block

275
0:10:27.800 --> 0:10:28.900
传统Vestu的Block呢

276
0:10:28.900 --> 0:10:31.400
就是输入的一个Invert Channel之后呢

277
0:10:31.400 --> 0:10:33.100
我进行一个1x1的卷积

278
0:10:33.100 --> 0:10:34.700
再进行3x3的卷积

279
0:10:34.700 --> 0:10:36.600
最后再进行1x1的卷积

280
0:10:36.600 --> 0:10:38.300
然后把残插中间那条线呢

281
0:10:38.300 --> 0:10:40.400
就是残插的结构输出

282
0:10:40.400 --> 0:10:41.900
那可以看到基本上输入呢

283
0:10:41.900 --> 0:10:45.200
是一个128或者256的一个Channel的大小

284
0:10:45.200 --> 0:10:45.700
接着呢

285
0:10:45.700 --> 0:10:47.400
Channel的大小会减少

286
0:10:47.400 --> 0:10:48.800
最后再恢复过来

287
0:10:48.800 --> 0:10:49.600
那作者呢

288
0:10:49.600 --> 0:10:50.800
就觉得像这种方式呢

289
0:10:50.800 --> 0:10:53.700
其实是破坏了Fetch Map的完整性

290
0:10:53.700 --> 0:10:57.000
然后没办法去获取很多有效的Fetch Map

291
0:10:57.600 --> 0:10:58.000
于是呢

292
0:10:58.000 --> 0:11:01.500
右边的这边的作者就加入了一个Invert Vestu Block

293
0:11:01.500 --> 0:11:03.300
可以看到输入的时候呢

294
0:11:03.300 --> 0:11:04.800
假设是128

295
0:11:04.800 --> 0:11:05.400
但是呢

296
0:11:05.400 --> 0:11:07.200
里面的Channel的大小呢

297
0:11:07.200 --> 0:11:09.000
就变得非常的多了

298
0:11:09.000 --> 0:11:10.700
就是他先做一个扩展

299
0:11:10.700 --> 0:11:13.200
然后相关的卷积也是1x1 3x3

300
0:11:13.200 --> 0:11:14.900
再做1x1的输出

301
0:11:14.900 --> 0:11:15.900
可以看到这里面呢

302
0:11:15.900 --> 0:11:18.600
就是跟Vestu的一个Channel的数呢

303
0:11:18.600 --> 0:11:20.000
是刚好是相反的

304
0:11:20.000 --> 0:11:20.800
一个是压缩

305
0:11:20.800 --> 0:11:21.700
一个是增大

306
0:11:21.700 --> 0:11:22.600
那这种方式呢

307
0:11:22.600 --> 0:11:26.200
有效的去提升了我们网络模型的精度

308
0:11:26.200 --> 0:11:28.400
沿用了MobileNet V1的前提下呢

309
0:11:28.400 --> 0:11:30.600
有效的提升了我们网络模型的性能

310
0:11:33.500 --> 0:11:37.800
而第二个比较重要的工作就是Niner ButtonLab

311
0:11:37.800 --> 0:11:38.500
那这个呢

312
0:11:38.500 --> 0:11:42.500
就使用了一个V6代替了传统的V6

313
0:11:42.500 --> 0:11:44.000
而且在Dell Vest之前呢

314
0:11:44.000 --> 0:11:46.400
增加了一个1x1的卷积

315
0:11:46.400 --> 0:11:47.900
那通过这两种方式呢

316
0:11:47.900 --> 0:11:51.000
有效的去提升了模型的精度

317
0:11:51.000 --> 0:11:54.800
而且在保持相同的一个网络模型的压缩比的前提下呢

318
0:11:54.800 --> 0:11:57.200
确实MobileNet V2这个模型呢

319
0:11:57.200 --> 0:11:58.900
就已经非常的收场

320
0:11:58.900 --> 0:11:59.600
现在呢

321
0:11:59.600 --> 0:12:01.500
很多全数码的网络模型结构啊

322
0:12:01.500 --> 0:12:03.200
也是去引用了

323
0:12:03.200 --> 0:12:04.700
或者加入了MobileNet V2

324
0:12:04.700 --> 0:12:05.800
或者把MobileNet V2呢

325
0:12:05.800 --> 0:12:07.300
当成它的一个Benchmark

326
0:12:08.700 --> 0:12:09.400
最后呢

327
0:12:09.400 --> 0:12:11.800
我们看一下MobileNet V3这个系列

328
0:12:11.800 --> 0:12:13.100
其实MobileNet V3这个系呢

329
0:12:13.100 --> 0:12:15.600
我觉得可能谈的内容不太多

330
0:12:15.600 --> 0:12:16.700
因为MobileNet V3呢

331
0:12:16.700 --> 0:12:17.200
主要呢

332
0:12:17.200 --> 0:12:18.900
是针对谷歌的TPU进行优化的

333
0:12:18.900 --> 0:12:21.400
其实我们在实践的过程当中发现

334
0:12:21.400 --> 0:12:22.200
MobileNet V3呢

335
0:12:22.300 --> 0:12:26.500
并不是对很多大部分的设备能够有一个很好的性能的优化

336
0:12:26.500 --> 0:12:28.200
或者模型的进一步的降低

337
0:12:28.200 --> 0:12:30.600
但是保持相同的一个性能

338
0:12:30.600 --> 0:12:32.400
那在这篇文章里面呢

339
0:12:32.400 --> 0:12:34.400
更多的是使用了Search

340
0:12:34.400 --> 0:12:36.400
就是一些NAS的搜索方法

341
0:12:36.400 --> 0:12:39.400
然后找到了一个比较好的网络模型的结构

342
0:12:39.400 --> 0:12:40.300
那这篇文章呢

343
0:12:40.300 --> 0:12:41.700
是在19年发布的

344
0:12:41.700 --> 0:12:42.200
这边呢

345
0:12:42.200 --> 0:12:45.200
我就不再对MobileNet V3进行详细的展开

346
0:12:45.200 --> 0:12:46.700
更多的大家MobileNet系列呢

347
0:12:46.700 --> 0:12:49.100
可以聚焦于MobileNet V1和V2

348
0:12:49.100 --> 0:12:49.900
卷的不行了

349
0:12:49.900 --> 0:12:50.800
卷的不行了

350
0:12:50.900 --> 0:12:52.700
记得一键三连加关注哦

351
0:12:52.700 --> 0:12:56.300
所有的内容都会开源在下面这条链接里面

352
0:12:56.300 --> 0:12:56.900
拜了个拜

