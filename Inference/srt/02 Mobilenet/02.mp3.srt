0
0:00:00.000 --> 0:00:04.360


1
0:00:05.880 --> 0:00:08.580
哈喽大家好我是一直坚持分享AI系统

2
0:00:08.580 --> 0:00:11.220
但是关注的人并不是很多的ZOMI

3
0:00:11.220 --> 0:00:16.000
那今天呢我们来到推理系统里面的模型小型化

4
0:00:16.000 --> 0:00:18.020
今天我要跟大家去分享的一个内容

5
0:00:18.020 --> 0:00:21.460
就是 CNN网络模型的小型化

6
0:00:21.460 --> 0:00:23.300
为啥会单独分开CNN

7
0:00:23.300 --> 0:00:25.540
跟传媒网模型的小型化呢

8
0:00:25.540 --> 0:00:26.980
就有两个内容了

9
0:00:26.980 --> 0:00:29.200
是因为其实啊我们现在传媒网

10
0:00:29.200 --> 0:00:31.960
非常非常的火火到一大户头

11
0:00:31.960 --> 0:00:35.520
现在你不发全松吗的模型呢说实话它的精度很难上去

12
0:00:35.520 --> 0:00:40.720
而且全松嘛确实很奥斯丁丁可以发很多新的一些撇步

13
0:00:40.720 --> 0:00:44.880
但是呢全松嘛根据我们在对客户的一些交付

14
0:00:45.160 --> 0:00:47.320
它的落地并不是说非常的理想

15
0:00:47.320 --> 0:00:50.120
而现在用的更多或者在落地场景里面呢

16
0:00:50.120 --> 0:00:54.960
做模型的小型化或者结构的小型化更多的是以CNN为主

17
0:00:54.960 --> 0:01:02.640
那可以看到啊其实我们的网络模型的精度越高

18
0:01:02.640 --> 0:01:05.120
确实我们的模型的参数量越大

19
0:01:05.120 --> 0:01:10.080
好像丹森娜娜艾斯妮娜娜还有那个摩巴娜娜都是一些相同精度

20
0:01:10.080 --> 0:01:14.240
但是模型量啊确实小得非常多的模型推出来了

21
0:01:15.760 --> 0:01:21.200
今天我主要是聚焦这种CNN模型的小型化的一些结构给大家去分享的

22
0:01:21.200 --> 0:01:24.640
在分享的过程当中呢我们以一些系列作为例子

23
0:01:24.640 --> 0:01:28.960
那我们看一看简单地去了解一下今天主要分享的内容有哪些

24
0:01:29.520 --> 0:01:33.520
首先呢我们会去按时间的序号啊去排一排

25
0:01:33.520 --> 0:01:37.120
我们会了解一下SQL Net Server Net Mobile Net

26
0:01:37.120 --> 0:01:41.680
了解完这一套系列之后呢我们再去简单地去看看

27
0:01:41.680 --> 0:01:47.040
ESP Net FB Net还有Efficient Net最后还有华为洛亚的Ghost Net

28
0:01:47.040 --> 0:01:52.640
那了解完这些内容之后呢我们再会去做一个具体的总结

29
0:01:52.640 --> 0:01:56.400
那现在我们一起去看看具体的算法

30
0:01:56.400 --> 0:02:00.960
去了解一下轻量级网络模型具体是怎么实际的

31
0:02:00.960 --> 0:02:06.400
它跟传统的CNN卷积一层一层地堆下来这种OS Net有哪些区别

32
0:02:08.400 --> 0:02:13.600
在轻量化主干网络模型里面呢第一个比较著名的是SQL Net

33
0:02:13.600 --> 0:02:16.480
跟OS Net相比呢有了五十倍的模型的压缩

34
0:02:16.480 --> 0:02:21.920
那下面呢我们看看SQL Net具体有哪些不一样的网络模型结构的实际

35
0:02:21.920 --> 0:02:27.840
看到这个图呢其实SQL Net呢提出一个最重要的model就是它的一个file model

36
0:02:27.840 --> 0:02:34.080
这个file model呢由两个模块去组成的一个是sqs模块一个是sqs模块

37
0:02:34.080 --> 0:02:39.200
sqs这个模块呢主要是由一系列连续的一层一的卷积进行组成

38
0:02:39.200 --> 0:02:45.840
它最重要的主要是对一些feature map的通道数啊进行改变减少了整个通道数

39
0:02:45.840 --> 0:02:51.200
sqs模块呢主要是由一层一的卷积和还有三成三的卷积和进行组成的

40
0:02:51.200 --> 0:02:54.560
它们之间呢通过看开进行汇聚起来

41
0:02:54.560 --> 0:03:00.480
下面呢我们往下浏览去看看sqs Net的一个网络模型的结构

42
0:03:00.480 --> 0:03:05.280
可以看到sqs Net的网络模型啊会比s Net的网络模型更加深

43
0:03:05.280 --> 0:03:13.600
而且会有很多个file model进行组成不同的组织模块呢或者不同的网络模型的组成呢又有不同的精度

44
0:03:13.600 --> 0:03:19.360
那对应来说啊sqs Net V1这篇文章呢是发表在ICLR二零一七

45
0:03:19.360 --> 0:03:22.640
而它真正的是在二零一六的时候发表出来了

46
0:03:22.640 --> 0:03:26.480
我们现在呢看看sqs Net V2这个版本

47
0:03:28.480 --> 0:03:34.560
下面我们看一下sqs Next系列里面的第二篇sqs Next我们也叫做sqs Net V2

48
0:03:34.560 --> 0:03:41.440
那这篇文章呢对比那个s Net五零呢它有了相同的精度但是提升了一百倍的模型压缩

49
0:03:41.440 --> 0:03:44.560
左边这个呢就是s Net最经典的一个block

50
0:03:44.560 --> 0:03:48.480
像sqs Net呢就提出了右边的两个结构

51
0:03:48.480 --> 0:03:54.080
中间这个就是sqs Net V1的一个block由sqs跟s band两层进行组成

52
0:03:54.080 --> 0:04:00.720
sqs呢就是一层一的卷骑和s band层呢就是一层一concate三层三这种方式

53
0:04:00.720 --> 0:04:07.360
那在右边呢就是sqs Next的一个bloc就是sqs V二的一种bloc的一个网络模型结构

54
0:04:07.360 --> 0:04:15.280
那么可以看到其实这里面呢有一种比较有意思就是它主要呢是采用了s Net的这种残差的结构方式

55
0:04:15.280 --> 0:04:25.840
但是呢它做了一个就是把三乘三的卷骑呢拆分成三乘一然后一乘三两乘的卷骑这样的方式呢有点类似于低资分解

56
0:04:25.840 --> 0:04:32.080
最后再接一乘一乘一的卷骑网络模型的参数量呢从k的平方变成二k

57
0:04:32.080 --> 0:04:40.400
而另外一点呢需要注意的就是网络模型的数呢是有一百二十八的通道那通过这里面的六十四三十二两个一乘一的卷骑呢

58
0:04:40.400 --> 0:04:48.320
把通道数呢从一百二十八减到六十四再减到三十二然后呢通过刚才的低资分解呢慢慢地把通道数恢复上来

59
0:04:48.320 --> 0:04:57.200
最后再通过一个一乘一的卷骑恢复到一百二十八然后再往下传递那通过这种方式呢其实代替掉了原来的sqs Net

60
0:04:57.200 --> 0:05:10.000
那原来的数呢是一百二十八中间剩两层六十四最后输出的是一百二十八那这种方式呢更多的是采用了或者借鉴了s Net五零的这种残差的结构的方式

61
0:05:10.000 --> 0:05:18.880
对原来的sqs Net唯一进行改进下面我们看看第二个模型系列suffer net

62
0:05:18.880 --> 0:05:32.840
在这里面呢我更多的是把它们之间最重要的特点给大家简单地过一遍我希望其实更多的开发者或者更多的有兴趣的读者呢能够深入地去自己去看看相关的论文

63
0:05:32.840 --> 0:05:41.160
suffer net这篇文章呢最早是发布于二零一七年的我们先来看看它具体的一个网络模型的结构

64
0:05:41.160 --> 0:05:48.080
如果对算法比较了解的同学可以知道啊其实卷机呢有很多种有device卷机有google卷机还有普通的卷机

65
0:05:48.080 --> 0:05:55.520
但原来一乘一跟三乘三跟七乘七的卷机的具体实现方式也是不一样的而且它们的运算速度也是不一样

66
0:05:55.520 --> 0:06:06.680
这里面作者就提到一乘一的卷机呢会消耗大量的计算资源而像google卷机这种可以降低我们计算量的卷机呢其实很难对不同group之间的一些信息进行共享

67
0:06:06.680 --> 0:06:18.200
所以呢他就提出了一个使用google卷机但是呢后面去采用一个全能serve的操作然后去代替传统的google卷机直接得到一个feature map这种方式

68
0:06:18.200 --> 0:06:22.040
那我们可以看一下具体的网络模型结构

69
0:06:22.040 --> 0:06:30.160
那左边的这个呢就是最原始的这种webnet五零的网络模型结构这里面的dw就是device卷机

70
0:06:30.160 --> 0:06:39.880
而右边的这两个图呢就是刚才提出的surfernet的一个思想里面呢就使用了一个全能serve而上面呢把普通卷机变成一个group卷机

71
0:06:39.880 --> 0:06:51.640
接着呢去借一个device的卷机最后再借一个group卷机就是磨改了webnet五零的一种结构的方式好处就是能够减少我们的网络模型的大小保持相同的精度模型更小了

72
0:06:51.640 --> 0:07:00.040
但是有一个比较大的缺点就是channel server呢是员工去定义的那这里面的规则呢就比较复杂

73
0:07:00.040 --> 0:07:17.760
下面呢我们看看surfernet v二的一个网络模型结构那a呢就是surfernet v一的网络模型结构那c呢就是对应的surfernet v二的网络模型结构可以看到啊在模型树的时候呢加了一个channel speed的工作把我们的网络模型的channel呢直接分开成为两半

74
0:07:17.760 --> 0:07:33.640
在这里面的作者就表明在网络模型的输出大小相同的时候呢这种方式呢能够有效的提升网络模型的或者我们计算的mac第二个优化的工作呢就取消了一乘一的group卷机就直接使用了一乘一的卷机去代替

75
0:07:35.000 --> 0:07:41.680
作者在经过大量的实验里面呢去表示其实过多的group卷机呢会提升我们整个计算的mac

76
0:07:42.400 --> 0:08:04.160
那mac是怎么我们其实在上一节课里面给大家去普及过相关的一些参数的概念第三点优化呢就是channel server可以看到蓝色的这个模块呢其实挪到最后面就是concat之后因为我们可以看到我们改变了这个group卷机后面再加个channel server是没有意义的我们把channel server往后面放

77
0:08:04.560 --> 0:08:30.120
把不同通道之间的一些信息的传递放到最后面有效的减少我们网络模型的虽化的程度那第四个改变呢就是concat这个操作可以看到在surface一里面的用的是一个add的操作而在surface一里面的用的是一个concat的这种方式其实呢add它是一个mwise的一种操作而concat呢其实更加有效的提升我们计算的flow次

78
0:08:31.120 --> 0:08:43.120
接下来了我们看一个最重要的系列就是摩拜纳系列那摩拜系列是由谷歌去提出来的里面呢就推出了v一v二v三三个不同的系列

79
0:08:45.120 --> 0:08:51.120
首先我们看看摩拜的v一这个系列到底有什么最大的特点了

80
0:08:51.120 --> 0:08:56.120
摩拜的v一呢最重要的就提出了一个新的计算的方式或者新的计算的结构

81
0:08:56.120 --> 0:09:05.120
那上面这个图a呢就是standard卷机的一种濾波器啊可以看到大部分呢我们假设两个dk呢就是我们kernels的大小

82
0:09:05.120 --> 0:09:09.120
那一般来说呢我们会使用三乘三的这种卷机的方式

83
0:09:09.120 --> 0:09:18.120
那结果呢作者就把这种三乘三的卷机方式呢就替换成为了两个结构一个呢就是depthwise convolution另外一个是pointwise convolution

84
0:09:18.120 --> 0:09:24.120
可以看到depthwise convolution呢里面就使用了跟standard卷机是一样的dk

85
0:09:24.120 --> 0:09:35.120
但是呢在pointwise里面呢就使用了一乘以一卷机的方式去替换掉那举一个具体的例子假设原始的网络模型的卷机呢使用了一个三乘三的卷机

86
0:09:35.120 --> 0:09:49.120
那这里面呢先使用一个depthwise三乘三再执行一个一乘一的pointwise的卷机那通过这种方式呢去代替掉原来的卷机的方式有了一个比较大的网络模型的参数量的降低

87
0:09:49.120 --> 0:10:00.120
这种方式呢有效地增加了我们整个网络模型执行的flops模型的参数量呢也是急剧地减少了非常非常的多

88
0:10:00.120 --> 0:10:10.120
接下来我们看看摩拜的v二这个网络模型那这个网络模型呢确实是非常非常的经典了这里面呢就提出了两个概念

89
0:10:10.120 --> 0:10:15.120
第一个呢就是invert invest第二个就是liner bottleneck

90
0:10:15.120 --> 0:10:22.120
那我们看看两个具体的概念对我们整个模型压缩或者轻量化有什么不一样的工作

91
0:10:22.120 --> 0:10:27.120
第一个概念呢就是invert invest这个block可以看到啊左边这个就是传统的invest的block

92
0:10:27.120 --> 0:10:40.120
那传统的invest的block呢就是输入的一个input channel之后呢我进行一个一乘一的卷机再进行三乘三的卷机最后再进行一乘一的卷机然后把残差中间这条线呢就是残差的结构输出

93
0:10:40.120 --> 0:10:57.120
那可以看到基本上输入的是一个一二八或者二五六的一个channel的大小接着呢channel的大小会减少最后再恢复过来那作者呢就觉得像这种方式呢其实是破坏了feature map的完整性然后没办法去获取很多有效的feature map

94
0:10:57.120 --> 0:11:09.120
于是呢右边的这边的作者就加入一个invert invest九宝可以看到输入的时候呢假设是一二八但是呢里面的channels的大小呢就变得非常的多了

95
0:11:09.120 --> 0:11:31.120
他先做一个扩展然后相关的卷机也是一乘一三乘三再做一乘一的输出可以看到这里面的就是跟invest九的一个channel的数呢是刚好是相反的一个是压缩一个是增大那这种方式呢有效地去提升了我们网络模型的精度沿用了mobile v一的前提下呢有效地提升了我们网络模型的性能

96
0:11:31.120 --> 0:11:37.120
而第二个比较重要的工作就是line botnet

97
0:11:37.120 --> 0:12:07.080
那这个呢就使用了一个v六六呢代替了传统的v六而且在delvice之前呢增加了一个一乘一的卷机那通过这两种方式呢有效地去提升了模型的精度而且在保持相同的一个网络模型的压缩比的前提下呢确实mobile v二这个模型呢就已经非常的收堂现在呢很多全数码的网络模型结构啊也是去引用了或者加入了mobile v二或者把mobile v二呢当成它的一个版权

98
0:12:07.080 --> 0:12:08.080
最后呢我们看一下mobile v三这个系列其实mobile v三这个系呢我觉得呃可能谈的内容不太多因为mobile v三呢主要呢是针对谷歌的tpu进行优化的其实我们在实践的过程当中发现mobile v三呢并不是对很多大部分的设备能够有一个很好的性能的优化或者模型的进一步降低但是保持相同的一个性能那在这篇文章里面呢更多的是使用了search就是一些nice的搜索方法

99
0:12:37.080 --> 0:12:57.540
来找到了一个比较好的网络模型的结构那这篇文章呢是在一九年发布的这边呢我就不再对mobile v三进行详细的展开更多的大家mobile系列呢可以聚焦于Mobile v1和v2
