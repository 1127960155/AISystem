1
00:00:00,000 --> 00:00:04,325
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:05,050 --> 00:00:11,200
Hello大家好,我是一直坚持分享AI系统,但是关注的人并不是很多的ZOMI

3
00:00:11,200 --> 00:00:16,000
今天我们来到推理系统里面的模型小型化

4
00:00:16,000 --> 00:00:21,400
今天我要给大家去分享的一个内容就是CNN网络模型的小型化

5
00:00:21,400 --> 00:00:25,400
为啥会单独分开CNN跟Transformer模型的小型化呢?

6
00:00:25,400 --> 00:00:30,800
就有两个内容了,是因为其实我们现在Transformer非常非常的火

7
00:00:30,800 --> 00:00:35,400
火到一大壶通,现在你不发Transformer的模型呢,说实话它的精度很难上去

8
00:00:35,400 --> 00:00:40,600
而且Transformer确实很outstanding,可以发很多新的一些paper

9
00:00:40,600 --> 00:00:44,800
但是呢,Transformer根据我们在对客户的一些交付

10
00:00:44,800 --> 00:00:48,400
它的落地并不是说非常的理想,而现在用的更多 

11
00:00:48,400 --> 00:00:52,400
或者在落地场景里面做模型的小型化或者结构的小型化

12
00:00:52,400 --> 00:00:54,600
更多的是以CNN为主

13
00:00:58,200 --> 00:01:02,600
那可以看到其实我们的网络模型的精度越高

14
00:01:02,600 --> 00:01:05,200
确实我们的模型的参数量越大

15
00:01:05,200 --> 00:01:10,000
好像DenseNet, SENet,还有MobileNet都是一些相同精度

16
00:01:10,000 --> 00:01:14,000
但是模型量确实小了非常多的模型推出来了

17
00:01:15,800 --> 00:01:21,200
今天我主要是聚焦这种CNN模型的小型化的一些结构给大家去分享的

18
00:01:21,200 --> 00:01:24,600
在分享的过程当中,我们以一些系列作为例子

19
00:01:24,600 --> 00:01:29,000
那我们看一看简单的去了解一下今天主要分享的内容有哪些

20
00:01:29,000 --> 00:01:33,400
首先呢,我们会去按时间的序号去排一排

21
00:01:33,400 --> 00:01:37,200
我们会了解一下SqueezeNet,ShuffleNet,MobileNet

22
00:01:37,200 --> 00:01:39,200
了解完这一套系列之后呢

23
00:01:39,200 --> 00:01:44,200
我们再去简单的去看一看ESPNet,FBNet,EfficientNet

24
00:01:44,200 --> 00:01:47,200
最后还有华为诺亚的GhostNet

25
00:01:47,200 --> 00:01:49,600
那了解完这些内容之后呢

26
00:01:49,600 --> 00:01:52,600
我们再会去做一个具体的总结

27
00:01:52,600 --> 00:01:56,400
现在我们一起去看看具体的算法

28
00:01:56,400 --> 00:02:01,000
去了解一下轻量级网络模型具体是怎么设计的

29
00:02:01,000 --> 00:02:06,200
它跟传统的CNN卷积一层一层的堆下来这种RestNet有哪些区别

30
00:02:08,400 --> 00:02:10,600
在轻量化主干网络模型里面呢

31
00:02:10,600 --> 00:02:13,600
第一个比较著名的是SqueezeNet

32
00:02:13,600 --> 00:02:16,600
跟AlexNet相比有了50倍的模型的压缩

33
00:02:16,600 --> 00:02:22,000
那下面呢我们看看SqueezeNet具体有哪些不一样的网络模型结构的设计

34
00:02:22,000 --> 00:02:22,800
看到这个图呢

35
00:02:22,800 --> 00:02:25,200
其实SqueezeNet提出一个最重要的model

36
00:02:25,200 --> 00:02:27,900
就是它的一个FireModel

37
00:02:27,900 --> 00:02:30,300
这个FireModel有两个模块去组成的

38
00:02:30,300 --> 00:02:31,800
一个是Squeeze模块

39
00:02:31,800 --> 00:02:34,000
一个是ESPN模块

40
00:02:34,000 --> 00:02:35,300
Squeeze这个模块呢

41
00:02:35,300 --> 00:02:39,200
主要是由一系列连续的一层一的卷积进行组成

42
00:02:39,200 --> 00:02:43,200
它最重要的主要是对一些FeatureMap的通道数进行改变

43
00:02:43,200 --> 00:02:45,900
减少了整个通道数

44
00:02:45,900 --> 00:02:47,000
Expand 的模块呢

45
00:02:47,000 --> 00:02:48,700
主要是由一乘一的卷积核心
46
00:02:48,700 --> 00:02:51,200
还有三乘三的卷积核进行组成的

47
00:02:51,200 --> 00:02:51,900
它们之间呢

48
00:02:51,900 --> 00:02:54,600
通过Concat进行汇聚起来

49
00:02:54,600 --> 00:02:55,300
下面呢

50
00:02:55,300 --> 00:03:00,500
我们往下浏览去看看SqueezeNet的一个网络模型的结构

51
00:03:00,500 --> 00:03:02,900
可以看到SqueezeNet的网络模型啊

52
00:03:02,900 --> 00:03:05,400
会比AlexNet的网络模型更加深

53
00:03:05,400 --> 00:03:08,200
而且会有很多个FireModel进行组成

54
00:03:08,200 --> 00:03:09,600
不同的组织模块呢

55
00:03:09,600 --> 00:03:12,300
或者不同的网络模型的组成了又有不同的精度

56
00:03:13,700 --> 00:03:15,000
那对应来说啊

57
00:03:15,000 --> 00:03:17,200
SqueezeNet V1这篇文章呢

58
00:03:17,200 --> 00:03:19,400
是发表在ICLR 2017

59
00:03:19,400 --> 00:03:22,700
而它真正的是在2016的时候发表出来了

60
00:03:22,700 --> 00:03:23,600
我们现在呢

61
00:03:23,600 --> 00:03:26,000
看看SqueezeNet V2这个版本

62
00:03:28,600 --> 00:03:32,500
下面我们看一下SqueezeNet系列里面的第二篇SqueezeNet

63
00:03:32,500 --> 00:03:34,600
我们也叫做SqueezeNet V2

64
00:03:34,600 --> 00:03:35,900
那这篇文章呢

65
00:03:35,900 --> 00:03:37,600
对比那个ResNet50呢

66
00:03:37,600 --> 00:03:39,000
它有了相同的精度

67
00:03:39,000 --> 00:03:41,500
但是提升了100倍的模型压缩

68
00:03:41,500 --> 00:03:42,200
左边这个呢

69
00:03:42,200 --> 00:03:44,600
就是ResNet最经典的一个Block

70
00:03:44,600 --> 00:03:45,500
像SqueezeNet呢

71
00:03:45,500 --> 00:03:48,500
就提出了右边的两个结构

72
00:03:48,500 --> 00:03:51,600
中间这个就是SqueezeNet V1的一个Block

73
00:03:51,600 --> 00:03:54,000
由Squeeze跟Expand两层进行组成

74
00:03:54,000 --> 00:03:54,500
Squeeze呢

75
00:03:54,500 --> 00:03:56,500
就是1x1的卷积合

76
00:03:56,500 --> 00:03:57,300
Expand层呢

77
00:03:57,300 --> 00:04:00,700
就是1x1 Concat 3x3这种方式

78
00:04:00,700 --> 00:04:01,700
那在右边呢

79
00:04:01,700 --> 00:04:03,600
就是SqueezeNet的一个Block

80
00:04:03,600 --> 00:04:07,400
就是SqueezeNet V2的一种Block的一个网络模型结构

81
00:04:07,400 --> 00:04:08,400
那么可以看到啊

82
00:04:08,400 --> 00:04:09,300
其实这里面呢

83
00:04:09,300 --> 00:04:10,500
有一种比较有意思

84
00:04:10,500 --> 00:04:12,100
就是它主要呢

85
00:04:12,100 --> 00:04:15,300
是采用了ResNet的这种残差的结构方式

86
00:04:15,300 --> 00:04:16,000
但是呢

87
00:04:16,000 --> 00:04:19,100
它做了一个就是把3x3的卷积呢

88
00:04:19,100 --> 00:04:20,600
拆分成3x1

89
00:04:20,600 --> 00:04:23,300
然后1x3两层的卷积

90
00:04:23,300 --> 00:04:24,200
这样的方式呢

91
00:04:24,200 --> 00:04:25,800
有点类似于D字分解

92
00:04:25,800 --> 00:04:28,400
最后再接1x1的卷积

93
00:04:28,400 --> 00:04:29,800
网络模型的参数量呢

94
00:04:29,800 --> 00:04:32,200
从k的平方变成2k

95
00:04:32,200 --> 00:04:33,100
而另外一点呢

96
00:04:33,100 --> 00:04:35,200
需要注意的就是网络模型的数呢

97
00:04:35,200 --> 00:04:37,000
是有128的信道

98
00:04:37,000 --> 00:04:38,100
那通过这里面呢

99
00:04:38,100 --> 00:04:40,400
64 32两个1x1的卷积呢

100
00:04:40,500 --> 00:04:41,500
把信道数呢

101
00:04:41,500 --> 00:04:44,100
从128减到64再减到32

102
00:04:44,100 --> 00:04:44,600
然后呢

103
00:04:44,600 --> 00:04:46,100
通过刚才的D字分解呢

104
00:04:46,100 --> 00:04:48,400
慢慢的把信道数恢复上来

105
00:04:48,400 --> 00:04:51,700
最后再通过一个1x1的卷积恢复到128

106
00:04:51,700 --> 00:04:53,500
然后再往下传递

107
00:04:53,500 --> 00:04:54,700
那通过这种方式呢

108
00:04:54,700 --> 00:04:57,300
其实代替掉了原来的SqueezeNet

109
00:04:57,300 --> 00:04:58,200
那原来的数呢

110
00:04:58,200 --> 00:04:59,000
是128

111
00:04:59,000 --> 00:05:00,900
中间是两层64

112
00:05:00,900 --> 00:05:03,600
最后输出的是128

113
00:05:03,600 --> 00:05:04,700
那这种方式呢

114
00:05:04,700 --> 00:05:06,300
更多的是采用了

115
00:05:06,300 --> 00:05:10,100
或者借鉴了ResNet50的这种长叉的结构的方式

116
00:05:10,100 --> 00:05:12,700
对原来的SqueezeNet V1进行改进

117
00:05:14,300 --> 00:05:17,400
下面我们看看第二个模型系列

118
00:05:17,400 --> 00:05:18,900
ShuffleNet

119
00:05:18,900 --> 00:05:19,500
在这里面呢

120
00:05:19,500 --> 00:05:22,100
我更多的是把他们之间最重要的特点

121
00:05:22,100 --> 00:05:23,900
给大家简单的过一遍

122
00:05:23,900 --> 00:05:25,700
我希望其实更多的开发者

123
00:05:25,700 --> 00:05:28,000
或者更多的有兴趣的读者呢

124
00:05:28,000 --> 00:05:30,800
能够深入的去自己去看看相关的论文

125
00:05:32,900 --> 00:05:34,100
ShuffleNet这篇文章呢

126
00:05:34,100 --> 00:05:36,300
最早是发布于2017年的

127
00:05:36,300 --> 00:05:38,500
我们现在看看它具体的一个

128
00:05:38,500 --> 00:05:41,100
网络模型的结构

129
00:05:41,100 --> 00:05:43,700
如果对算法比较了解的同学可以知道啊

130
00:05:43,700 --> 00:05:44,500
其实卷积呢

131
00:05:44,500 --> 00:05:46,000
有很多种有Depth-wise卷积

132
00:05:46,000 --> 00:05:46,700
有Group卷积

133
00:05:46,700 --> 00:05:48,000
还有普通的卷积

134
00:05:48,000 --> 00:05:48,500
当然了

135
00:05:48,500 --> 00:05:51,500
一层一跟三层三跟七层七的卷积的具体实现方式

136
00:05:51,500 --> 00:05:52,700
也是不一样的

137
00:05:52,700 --> 00:05:55,500
而且他们的运算速度也是不一样

138
00:05:55,500 --> 00:05:56,700
这里面作者就提到

139
00:05:56,700 --> 00:05:57,700
一层一的卷积呢

140
00:05:57,700 --> 00:05:59,700
会消耗大量的计算资源

141
00:05:59,700 --> 00:06:01,100
而像Group卷积这种

142
00:06:01,100 --> 00:06:03,000
可以降低我们计算量的卷积呢

143
00:06:03,000 --> 00:06:04,700
其实很难对不同Group

144
00:06:04,700 --> 00:06:06,600
之间的一些信息进行共享

145
00:06:06,600 --> 00:06:07,200
所以呢

146
00:06:07,200 --> 00:06:09,700
他就提出了一个使用Group卷积

147
00:06:09,700 --> 00:06:10,400
但是呢

148
00:06:10,400 --> 00:06:13,400
后面去采用一个Channel Shuffle的操作

149
00:06:13,400 --> 00:06:15,700
然后去代替传统的Group卷积

150
00:06:15,700 --> 00:06:18,200
直接得到一个Feature Map的这种方式

151
00:06:18,200 --> 00:06:22,000
那我们可以看一下具体的网络模型结构

152
00:06:22,000 --> 00:06:23,100
那左边的这个呢

153
00:06:23,100 --> 00:06:25,200
就是最原始的这种

154
00:06:25,200 --> 00:06:27,300
ResNet50的网络模型结构

155
00:06:27,300 --> 00:06:27,900
这里面呢

156
00:06:27,900 --> 00:06:30,200
DW就是Depth-wise卷积

157
00:06:30,200 --> 00:06:31,600
而右边的这两个图呢

158
00:06:31,600 --> 00:06:33,800
就是刚才提出的ShuffleNet的一个思想

159
00:06:33,800 --> 00:06:34,300
里面呢

160
00:06:34,300 --> 00:06:36,800
就使用了一个Channel Shuffle

161
00:06:36,800 --> 00:06:37,500
而上面呢

162
00:06:37,500 --> 00:06:39,800
把普通卷积变成一个Group卷积

163
00:06:39,800 --> 00:06:40,300
接着呢

164
00:06:40,300 --> 00:06:42,000
去接一个Depth-wise的卷积

165
00:06:42,000 --> 00:06:43,600
最后再接一个Group卷积

166
00:06:43,600 --> 00:06:46,900
就是魔改了ResNet50的一种结构的方式

167
00:06:46,900 --> 00:06:49,200
好处就是能够减少我们的网络模型的大小

168
00:06:49,200 --> 00:06:50,300
保持相同的精度

169
00:06:50,300 --> 00:06:51,600
模型更小了

170
00:06:51,600 --> 00:06:53,300
但是有一个比较大的缺点

171
00:06:53,300 --> 00:06:54,500
就是Channel Shuffle呢

172
00:06:54,500 --> 00:06:56,000
是人工去定义的

173
00:06:56,000 --> 00:06:57,000
那这里面的规则呢

174
00:06:57,000 --> 00:06:58,000
就比较复杂

175
00:07:00,100 --> 00:07:00,600
下面呢

176
00:07:00,600 --> 00:07:03,300
我们来看看ShuffleNet V2的一个网络模型结构

177
00:07:03,300 --> 00:07:04,000
那A呢

178
00:07:04,000 --> 00:07:06,200
就是ShuffleNet V1的网络模型结构

179
00:07:06,200 --> 00:07:06,900
那C呢

180
00:07:06,900 --> 00:07:10,100
就是对应的ShuffleNet V2的网络模型结构

181
00:07:10,100 --> 00:07:10,700
可以看到啊

182
00:07:10,700 --> 00:07:11,900
在模型输入的时候呢

183
00:07:11,900 --> 00:07:13,700
加了一个Channel Split的工作

184
00:07:13,700 --> 00:07:15,300
把我们的网络模型的Channel呢

185
00:07:15,300 --> 00:07:17,800
直接分开成为两半

186
00:07:17,800 --> 00:07:18,400
在这里面呢

187
00:07:18,400 --> 00:07:22,000
作者就表明在网络模型的输入输出大小相同的时候呢

188
00:07:22,000 --> 00:07:22,600
这种方式呢

189
00:07:22,600 --> 00:07:25,000
能够有效的提升网络模型的

190
00:07:25,000 --> 00:07:27,200
或者我们计算的MAC

191
00:07:27,200 --> 00:07:28,500
第二个优化的工作呢

192
00:07:28,500 --> 00:07:31,200
就取消了1x1的Group卷积

193
00:07:31,200 --> 00:07:33,500
就直接使用了1x1的卷积去代替

194
00:07:34,500 --> 00:07:36,500
作者在经过大量的实验里面呢

195
00:07:36,500 --> 00:07:38,900
去表示其实过多的Group卷积呢

196
00:07:38,900 --> 00:07:41,300
会提升我们整个计算的MAC

197
00:07:42,000 --> 00:07:43,000
那MAC是什么

198
00:07:43,000 --> 00:07:45,600
我们其实在上一节课里面给大家去普及过

199
00:07:45,600 --> 00:07:47,800
相关的一些参数的概念

200
00:07:48,700 --> 00:07:49,800
第三点优化呢

201
00:07:49,800 --> 00:07:51,200
就是Channel Shuffle

202
00:07:51,200 --> 00:07:53,000
可以看到蓝色的这个模块呢

203
00:07:53,000 --> 00:07:54,900
其实挪到最后面

204
00:07:54,900 --> 00:07:56,200
就是Concate之后

205
00:07:56,200 --> 00:07:57,300
因为我们可以看到

206
00:07:57,300 --> 00:07:58,900
我们改变了这个Group卷积

207
00:07:58,900 --> 00:08:01,100
后面再加个Channel Shuffle是没有意义的

208
00:08:01,500 --> 00:08:04,100
我们把Channel Shuffle往后面放

209
00:08:04,100 --> 00:08:07,100
把不同通道之间的一些信息的传递

210
00:08:07,100 --> 00:08:08,100
放到最后面

211
00:08:08,100 --> 00:08:11,500
有效的减少我们网络模型的碎片化的程度

212
00:08:11,500 --> 00:08:13,000
那第四个改变呢

213
00:08:13,000 --> 00:08:15,400
就是Concate这个操作

214
00:08:15,400 --> 00:08:17,100
可以看到在ShuffleNet V1里面呢

215
00:08:17,100 --> 00:08:19,000
用的是一个Add的操作

216
00:08:19,000 --> 00:08:21,000
而在ShuffleNet V1里面呢

217
00:08:21,000 --> 00:08:23,600
用的是一个Concate的这种方式

218
00:08:23,600 --> 00:08:24,200
其实呢

219
00:08:24,200 --> 00:08:26,200
Add它是一个ElementWise的一种操作

220
00:08:26,200 --> 00:08:27,200
而Concate呢

221
00:08:27,200 --> 00:08:30,200
其实更加有效的提升我们计算的Flops

222
00:08:32,100 --> 00:08:32,800
接下来呢

223
00:08:32,800 --> 00:08:34,800
我们看一个最重要的系列

224
00:08:34,800 --> 00:08:36,400
就是MobileNet系列

225
00:08:36,400 --> 00:08:39,500
那MobileNet系列是由谷歌去提出来的

226
00:08:39,500 --> 00:08:40,000
里面呢

227
00:08:40,000 --> 00:08:41,600
就推出了V1 V2 V3

228
00:08:41,600 --> 00:08:42,900
三个不同的系列

229
00:08:45,900 --> 00:08:48,200
首先我们看看MobileNet V1

230
00:08:48,200 --> 00:08:51,400
这个系列到底有什么最大的特点呢

231
00:08:51,400 --> 00:08:52,300
MobileNet V1呢

232
00:08:52,300 --> 00:08:55,500
最重要的就提出了一个新的计算的方式

233
00:08:55,500 --> 00:08:56,900
或者新的计算的结构

234
00:08:56,900 --> 00:08:58,300
那上面这个图A呢

235
00:08:58,300 --> 00:09:01,000
就是Standard卷积的一种滤波器啊

236
00:09:01,000 --> 00:09:03,700
可以看到大部分的我们假设两个DK呢

237
00:09:03,700 --> 00:09:05,700
就是我们Kernels的大小

238
00:09:05,700 --> 00:09:06,500
那一般来说呢

239
00:09:06,500 --> 00:09:09,400
我们会使用三乘三的这种卷积的方式

240
00:09:09,400 --> 00:09:10,500
那结果呢

241
00:09:10,500 --> 00:09:13,000
作者就把这种三乘三的卷积方式呢

242
00:09:13,000 --> 00:09:14,800
就替换成为了两个结构

243
00:09:14,800 --> 00:09:16,500
一个呢就是Depth-wise Convolution

244
00:09:16,500 --> 00:09:18,400
另外一个是Pointwise Convolution

245
00:09:18,400 --> 00:09:20,100
可以看到Deadwise Convolution呢

246
00:09:20,100 --> 00:09:24,100
里面就使用了跟Standard卷积是一样的DK

247
00:09:24,100 --> 00:09:24,700
但是呢

248
00:09:24,700 --> 00:09:25,700
在Pointwise里面呢

249
00:09:25,700 --> 00:09:29,900
就使用了1乘以1卷积的方式去替换掉

250
00:09:29,900 --> 00:09:31,100
那举一个具体的例子

251
00:09:31,100 --> 00:09:33,600
假设原始的网络模型的卷积呢

252
00:09:33,600 --> 00:09:35,400
使用了一个三乘三的卷积

253
00:09:35,400 --> 00:09:36,100
那这里面呢

254
00:09:36,100 --> 00:09:38,100
先使用一个Depth-wise三乘三

255
00:09:38,100 --> 00:09:40,600
再执行一个1乘1的Pointwise的卷积

256
00:09:40,600 --> 00:09:42,100
那通过这种方式呢

257
00:09:42,100 --> 00:09:44,800
去代替掉原来的卷积的方式

258
00:09:44,800 --> 00:09:49,600
有了一个比较大的网络模型的参数量的降低

259
00:09:49,600 --> 00:09:50,300
这种方式呢

260
00:09:50,300 --> 00:09:54,100
有效的增加了我们整个网络模型执行的Flops

261
00:09:54,100 --> 00:09:55,400
模型的参数量呢

262
00:09:55,400 --> 00:09:58,200
也是急剧的减少了非常非常的多

263
00:10:00,600 --> 00:10:04,600
接下来我们看看MobileNet V2这个网络模型

264
00:10:04,600 --> 00:10:05,900
那这个网络模型呢

265
00:10:05,900 --> 00:10:08,500
确实是非常非常的经典了

266
00:10:08,500 --> 00:10:09,000
这里面呢

267
00:10:09,000 --> 00:10:10,700
就提出了两个概念

268
00:10:10,700 --> 00:10:11,100
第一个呢

269
00:10:11,100 --> 00:10:13,000
就是Inverted Residuals

270
00:10:13,000 --> 00:10:15,400
第二个就是Linear Bottlenecks

271
00:10:15,400 --> 00:10:17,700
那我们看看两个具体的概念

272
00:10:17,700 --> 00:10:22,200
对我们整个模型压缩和轻量化有什么不一样的工作

273
00:10:22,200 --> 00:10:23,000
第一个概念呢

274
00:10:23,000 --> 00:10:24,600
就是Invert Residual这个Block

275
00:10:24,600 --> 00:10:25,400
可以看到啊

276
00:10:25,400 --> 00:10:27,600
左边这个就是传统的Residual的Block

277
00:10:27,800 --> 00:10:28,900
传统Residual的Block呢

278
00:10:28,900 --> 00:10:31,400
就是输入的一个Invert Channel之后呢

279
00:10:31,400 --> 00:10:33,100
我进行一个1x1的卷积

280
00:10:33,100 --> 00:10:34,700
再进行3x3的卷积

281
00:10:34,700 --> 00:10:36,600
最后再进行1x1的卷积

282
00:10:36,600 --> 00:10:38,300
然后把残差中间那条线呢

283
00:10:38,300 --> 00:10:40,400
就是残差的结构输出

284
00:10:40,400 --> 00:10:41,900
那可以看到基本上输入呢

285
00:10:41,900 --> 00:10:45,200
是一个128或者256的一个Channel的大小

286
00:10:45,200 --> 00:10:45,700
接着呢

287
00:10:45,700 --> 00:10:47,400
Channel的大小会减少

288
00:10:47,400 --> 00:10:48,800
最后再恢复过来

289
00:10:48,800 --> 00:10:49,600
那作者呢

290
00:10:49,600 --> 00:10:50,800
就觉得像这种方式呢

291
00:10:50,800 --> 00:10:53,700
其实是破坏了Feature Map的完整性

292
00:10:53,700 --> 00:10:57,000
然后没办法去获取很多有效的Feature Map

293
00:10:57,600 --> 00:10:58,000
于是呢

294
00:10:58,000 --> 00:11:01,500
右边的这边的作者就加入了一个Invert Residual Block

295
00:11:01,500 --> 00:11:03,300
可以看到输入的时候呢

296
00:11:03,300 --> 00:11:04,800
假设是128

297
00:11:04,800 --> 00:11:05,400
但是呢

298
00:11:05,400 --> 00:11:07,200
里面的Channel的大小呢

299
00:11:07,200 --> 00:11:09,000
就变得非常的多了

300
00:11:09,000 --> 00:11:10,700
就是他先做一个扩展

301
00:11:10,700 --> 00:11:13,200
然后相关的卷积也是1x1 3x3

302
00:11:13,200 --> 00:11:14,900
再做1x1的输出

303
00:11:14,900 --> 00:11:15,900
可以看到这里面呢

304
00:11:15,900 --> 00:11:18,600
就是跟Residual的一个Channel的数呢

305
00:11:18,600 --> 00:11:20,000
是刚好是相反的

306
00:11:20,000 --> 00:11:20,800
一个是压缩

307
00:11:20,800 --> 00:11:21,700
一个是增大

308
00:11:21,700 --> 00:11:22,600
那这种方式呢

309
00:11:22,600 --> 00:11:26,200
有效的去提升了我们网络模型的精度

310
00:11:26,200 --> 00:11:28,400
沿用了MobileNet V1的前提下呢

311
00:11:28,400 --> 00:11:30,600
有效的提升了我们网络模型的性能

312
00:11:33,500 --> 00:11:37,800
而第二个比较重要的工作就是Linear Bottleneck

313
00:11:37,800 --> 00:11:38,500
那这个呢

314
00:11:38,500 --> 00:11:42,500
就使用了一个ReLU6代替了传统的ReLU

315
00:11:42,500 --> 00:11:44,000
而且在Depth-wise之前呢

316
00:11:44,000 --> 00:11:46,400
增加了一个1x1的卷积

317
00:11:46,400 --> 00:11:47,900
那通过这两种方式呢

318
00:11:47,900 --> 00:11:51,000
有效的去提升了模型的精度

319
00:11:51,000 --> 00:11:54,800
而且在保持相同的一个网络模型的压缩比的前提下呢

320
00:11:54,800 --> 00:11:57,200
确实MobileNet V2这个模型呢

321
00:11:57,200 --> 00:11:58,900
就已经非常的SOTA

322
00:11:58,900 --> 00:11:59,600
现在呢

323
00:11:59,600 --> 00:12:01,500
很多Transformer的网络模型结构啊

324
00:12:01,500 --> 00:12:03,200
也是去引用了

325
00:12:03,200 --> 00:12:04,700
或者加入了MobileNet V2

326
00:12:04,700 --> 00:12:05,800
或者把MobileNet V2呢

327
00:12:05,800 --> 00:12:07,300
当成它的一个Benchmark

328
00:12:08,700 --> 00:12:09,400
最后呢

329
00:12:09,400 --> 00:12:11,800
我们看一下MobileNet V3这个系列

330
00:12:11,800 --> 00:12:13,100
其实MobileNet V3这个系呢

331
00:12:13,100 --> 00:12:15,600
我觉得可能谈的内容不太多

332
00:12:15,600 --> 00:12:16,700
因为MobileNet V3呢

333
00:12:16,700 --> 00:12:17,200
主要呢

334
00:12:17,200 --> 00:12:18,900
是针对谷歌的TPU进行优化的

335
00:12:18,900 --> 00:12:21,400
其实我们在实践的过程当中发现

336
00:12:21,400 --> 00:12:22,200
MobileNet V3呢

337
00:12:22,300 --> 00:12:26,500
并不是对很多大部分的设备能够有一个很好的性能的优化

338
00:12:26,500 --> 00:12:28,200
或者模型的进一步的降低

339
00:12:28,200 --> 00:12:30,600
但是保持相同的一个性能

340
00:12:30,600 --> 00:12:32,400
那在这篇文章里面呢

341
00:12:32,400 --> 00:12:34,400
更多的是使用了Searching

342
00:12:34,400 --> 00:12:36,400
就是一些NAS的搜索方法

343
00:12:36,400 --> 00:12:39,400
然后找到了一个比较好的网络模型的结构

344
00:12:39,400 --> 00:12:40,300
那这篇文章呢

345
00:12:40,300 --> 00:12:41,700
是在19年发布的

346
00:12:41,700 --> 00:12:42,200
这边呢

347
00:12:42,200 --> 00:12:45,200
我就不再对MobileNet V3进行详细的展开

348
00:12:45,200 --> 00:12:46,700
更多的大家MobileNet系列呢

349
00:12:46,700 --> 00:12:49,100
可以聚焦于MobileNet V1和V2

350
00:12:49,100 --> 00:12:49,900
卷的不行了

351
00:12:49,900 --> 00:12:50,800
卷的不行了

352
00:12:50,900 --> 00:12:52,700
记得一键三连加关注哦

353
00:12:52,700 --> 00:12:56,300
所有的内容都会开源在下面这条链接里面

354
00:12:56,300 --> 00:12:56,900
拜拜

