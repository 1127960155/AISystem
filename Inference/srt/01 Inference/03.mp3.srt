0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴巴

1
0:00:05.920 --> 0:00:08.120
我是那个改革春风吹满地

2
0:00:08.120 --> 0:00:11.120
新的力言要增气的中米

3
0:00:11.120 --> 0:00:14.520
其实我今天要给大家去分享的内容

4
0:00:14.520 --> 0:00:17.320
还是集中在我们的推理系统里面

5
0:00:17.320 --> 0:00:20.720
而去看看整个推理的流程的全景

6
0:00:20.720 --> 0:00:23.800
这里面总要分开三个内容给大家去汇报的

7
0:00:23.800 --> 0:00:25.240
第一个就是部署态

8
0:00:25.240 --> 0:00:27.360
跟我们之前的一个训练态的区别

9
0:00:27.480 --> 0:00:30.520
接着我们去看看云测推理的整体的流程

10
0:00:30.520 --> 0:00:32.520
还有边缘部署的方式

11
0:00:32.520 --> 0:00:37.640
这里面我们去汇区别云测和边缘的一个具体的不同

12
0:00:38.840 --> 0:00:41.160
而聊到这里其实我想插一个话题

13
0:00:41.160 --> 0:00:43.600
就是前几天的林特找我谈话说

14
0:00:43.600 --> 0:00:46.240
我可能只懂AI训练框架

15
0:00:46.240 --> 0:00:48.520
我要更多的去了解一下其他

16
0:00:48.520 --> 0:00:51.200
这个时候其实我是想反驳

17
0:00:51.200 --> 0:00:52.160
但是也不想说啥

18
0:00:52.160 --> 0:00:54.880
因为整个AI系统或者AI全在

19
0:00:54.880 --> 0:00:59.800
在整个SE团队里面算是比较资深

20
0:01:03.520 --> 0:01:04.560
不再吐槽和抱怨

21
0:01:04.560 --> 0:01:06.720
我们来到回到我们的真正的内容

22
0:01:06.720 --> 0:01:09.320
部署态的一个真正的区别

23
0:01:09.320 --> 0:01:12.080
首先我们看看什么叫做真正的部署态

24
0:01:12.080 --> 0:01:14.760
就AI系统一般都会部署在云端

25
0:01:14.760 --> 0:01:17.240
或者我们的一个边缘的设备上面

26
0:01:17.240 --> 0:01:20.400
而云端更多的是集中在一些web的服务

27
0:01:20.400 --> 0:01:24.120
而在边缘更多的可能是一些边缘的服务器

28
0:01:24.120 --> 0:01:25.160
边缘的设备基站

29
0:01:25.160 --> 0:01:27.000
还有一些边缘的设备

30
0:01:27.000 --> 0:01:29.520
我们最熟悉的手机手环耳机

31
0:01:29.640 --> 0:01:31.240
这些就是我们的IoT设备

32
0:01:32.240 --> 0:01:34.600
当然了部署态其实有另外一个

33
0:01:34.600 --> 0:01:35.840
就是我们的云策

34
0:01:35.840 --> 0:01:37.480
大家不要觉得谈到推理的

35
0:01:37.480 --> 0:01:39.920
大部分都是在我们的IoT上面去运行的

36
0:01:39.920 --> 0:01:41.040
其实我们的云策

37
0:01:41.360 --> 0:01:42.680
就是我们整个推理服务

38
0:01:42.880 --> 0:01:44.880
是承载了非常多的服务的

39
0:01:44.880 --> 0:01:47.440
因为云策就是我们整个云策服务器

40
0:01:47.560 --> 0:01:49.360
它有非常多的算力内存

41
0:01:49.360 --> 0:01:51.240
而且电力非常充足

42
0:01:51.240 --> 0:01:52.920
我们只需要通过一个网线

43
0:01:52.920 --> 0:01:56.560
或者RPC或者HTTP的一些客户的请求和响应

44
0:01:56.680 --> 0:01:59.640
就能够完成我们整体的一个服务了

45
0:01:59.800 --> 0:02:03.040
这个时候我们提供的更多的是一个推理的服务

46
0:02:03.040 --> 0:02:04.480
或者云策的服务

47
0:02:06.120 --> 0:02:08.440
现在我们看一下在部署态里面

48
0:02:08.440 --> 0:02:11.120
特别是在云端的一个部署态的特点

49
0:02:11.480 --> 0:02:13.960
我简单的给大家练一下

50
0:02:13.960 --> 0:02:15.520
可能语速有点快没关系

51
0:02:15.520 --> 0:02:18.200
现在的内容还是更多的是概念性的问题

52
0:02:18.560 --> 0:02:20.040
第一个就是对功耗温度

53
0:02:20.160 --> 0:02:21.320
还有我们的模型大小

54
0:02:21.440 --> 0:02:23.000
是没有太严格的限制的

55
0:02:23.000 --> 0:02:24.080
因为云策

56
0:02:24.280 --> 0:02:26.960
我们的云端的资源还是非常的丰富的

57
0:02:27.280 --> 0:02:28.120
接着我们看一下

58
0:02:28.120 --> 0:02:30.800
其实云策它不仅是用推理

59
0:02:30.800 --> 0:02:33.440
其实我们推理和训练都是有的

60
0:02:33.440 --> 0:02:35.720
只是在服务的时候

61
0:02:35.720 --> 0:02:38.120
我们更多的是集中在推理

62
0:02:38.360 --> 0:02:39.640
接着我们在云策

63
0:02:39.640 --> 0:02:42.240
其实有比较集中的数据管理的

64
0:02:42.520 --> 0:02:46.360
在云策我们有非常集中的一个数据的管理

65
0:02:46.360 --> 0:02:49.480
因为数据的管理是非常严峻的一个挑战

66
0:02:49.520 --> 0:02:51.680
我们的数据不仅是数据库的数据

67
0:02:51.680 --> 0:02:55.000
而且更多的是一些非结构化的数据

68
0:02:55.320 --> 0:02:56.480
接着我们可以看到

69
0:02:56.600 --> 0:02:59.480
其实模型在云端更好的去保护

70
0:02:59.480 --> 0:03:01.840
我们现在对模型的保护其实是比较弱的

71
0:03:01.840 --> 0:03:04.520
我们的算法的侵入其实是比较简单的

72
0:03:04.520 --> 0:03:07.640
这块又衍生了一个AI安全的问题

73
0:03:08.080 --> 0:03:09.080
大家听完就算了

74
0:03:09.080 --> 0:03:13.000
接下来我们看一下云端也会遇到一些问题

75
0:03:13.240 --> 0:03:17.520
云端的问题就是人工智能的服务成本非常高昂

76
0:03:17.720 --> 0:03:19.840
因为我们需要云端的服务

77
0:03:19.840 --> 0:03:21.440
你就需要大量的机器

78
0:03:21.440 --> 0:03:22.440
大量的云灾

79
0:03:22.440 --> 0:03:24.480
大量的各种设备维护

80
0:03:24.480 --> 0:03:25.400
机房

81
0:03:25.400 --> 0:03:27.560
所以它整体的成本是很高的

82
0:03:27.560 --> 0:03:29.480
然后我们看一下云端的服务

83
0:03:29.480 --> 0:03:31.840
对网络的依赖是非常高的

84
0:03:31.840 --> 0:03:32.840
如果断网了

85
0:03:32.840 --> 0:03:34.960
基本上你的推理服务就断了

86
0:03:34.960 --> 0:03:36.760
我们做推荐的时候滑着滑着

87
0:03:37.040 --> 0:03:38.680
没有网络了你就滑不动了

88
0:03:38.960 --> 0:03:41.920
我经常晚上刷抖音的时候会遇到这种问题

89
0:03:42.120 --> 0:03:43.640
就自己断网了

90
0:03:43.920 --> 0:03:47.480
接着我们看一下数据隐私是个很大的问题

91
0:03:47.880 --> 0:03:49.680
用户的数据如果不出端

92
0:03:49.960 --> 0:03:53.400
这时候我们推理服务就怎么去保护用户

93
0:03:53.400 --> 0:03:55.680
数据的隐私是个很大的挑战

94
0:03:55.680 --> 0:03:58.920
接着数据的传输也是个很大的成本

95
0:03:59.360 --> 0:04:02.160
我们现在传一张图片比较简单

96
0:04:02.720 --> 0:04:05.080
假设是我们的端测的摄像头

97
0:04:05.560 --> 0:04:07.720
我们要传大量的视频数据的时候

98
0:04:07.920 --> 0:04:11.440
这个时候对我们的数据传输压力或者安全隐私

99
0:04:11.440 --> 0:04:12.680
是个很大的挑战

100
0:04:12.960 --> 0:04:17.200
最后云端基本上很少会做一些定制化的模型

101
0:04:17.360 --> 0:04:19.200
一般都会做一些通用的模型

102
0:04:19.440 --> 0:04:21.360
因为定制化的模型是非常难的

103
0:04:21.360 --> 0:04:22.720
就千人千面这个问题

104
0:04:22.840 --> 0:04:25.840
还是一直是我们努力去解决的问题

105
0:04:27.120 --> 0:04:30.440
接下来我们看一下部署态的另外一面

106
0:04:30.440 --> 0:04:32.440
就是我们的Edge端测

107
0:04:34.200 --> 0:04:38.400
端测我们端测的设备和资源是非常紧张的

108
0:04:38.400 --> 0:04:40.400
特别是手机或者IoT的设备

109
0:04:40.640 --> 0:04:42.960
我们里面的布景是因为靠一块

110
0:04:42.960 --> 0:04:46.080
像手指夹那么大小的一块芯片去支撑

111
0:04:46.240 --> 0:04:48.400
我们也只能靠一块电池

112
0:04:49.000 --> 0:04:51.840
如果我们的算法把所有里面的计算资源都用起来了

113
0:04:51.840 --> 0:04:54.800
我们的耗电量是很大的

114
0:04:56.160 --> 0:04:58.000
不过有个好处就是它的响应

115
0:04:58.200 --> 0:05:00.360
在设备里面就自我完成了

116
0:05:00.360 --> 0:05:01.280
就自闭环了

117
0:05:01.840 --> 0:05:04.840
就不需要去消耗云端的一些资源

118
0:05:06.200 --> 0:05:09.120
我们看一下端测的一些挑战

119
0:05:09.480 --> 0:05:12.840
端测的挑战就是会有非常严格的一个功耗

120
0:05:12.840 --> 0:05:15.240
热量模型大小所受限

121
0:05:15.560 --> 0:05:20.120
因为我们之前说的我们的内存只有两个G或者4个G

122
0:05:20.120 --> 0:05:21.960
大家所有的设备去共用的

123
0:05:21.960 --> 0:05:25.560
我们的功耗或者我们的一块手机里面

124
0:05:25.920 --> 0:05:28.160
就400毫安完全不够用

125
0:05:28.160 --> 0:05:30.920
在这个时候就有非常多的约束

126
0:05:30.920 --> 0:05:34.600
而且硬件的算力对于推理来说是远远不够的

127
0:05:34.600 --> 0:05:38.560
我们对于算力的需求是非常的多的

128
0:05:38.560 --> 0:05:40.680
而且数据非常分散

129
0:05:40.680 --> 0:05:44.280
就我之前第一份工作的时候是做情景感知

130
0:05:44.480 --> 0:05:48.880
我们就会去收集非常多的手机的一个传感器的信息

131
0:05:48.880 --> 0:05:53.160
一台手机我们就可以获取接近30多种传感器的设备了

132
0:05:53.160 --> 0:05:55.240
所以数据是非常分散的

133
0:05:55.240 --> 0:05:58.320
而且模型在边缘更容易受到攻击

134
0:05:58.320 --> 0:06:01.000
就我去攻击你的模型是很简单的

135
0:06:01.000 --> 0:06:04.800
而且这个时候我们的平台非常多样化

136
0:06:04.800 --> 0:06:07.720
不同的设备可能会有自己不同的平台

137
0:06:08.200 --> 0:06:11.800
包括我们有一款推力引擎部署在我们的端测

138
0:06:12.080 --> 0:06:16.840
这个时候我们端测跟我们在手机上面用的又是不同的一款

139
0:06:16.840 --> 0:06:18.240
这款我们叫做Michael

140
0:06:18.240 --> 0:06:20.880
这款我们叫做Lite推理的引擎

141
0:06:20.880 --> 0:06:23.360
所以说它基本上没法做到一个通用

142
0:06:24.880 --> 0:06:27.000
下面我们看一下端测

143
0:06:27.000 --> 0:06:30.920
就是我们Edge端的一些要解决的一些技术点

144
0:06:31.040 --> 0:06:33.000
第一个就是应用的算法优化

145
0:06:33.000 --> 0:06:34.480
我们需要对算法优化

146
0:06:34.480 --> 0:06:36.400
就是训练的时候跟推理的时候

147
0:06:36.400 --> 0:06:38.040
我们希望越小越好

148
0:06:38.040 --> 0:06:39.680
但是精度越高越好

149
0:06:39.680 --> 0:06:41.160
其实这就是个矛盾

150
0:06:41.280 --> 0:06:44.440
接着我们去看看高效率的模型的设计

151
0:06:44.440 --> 0:06:46.600
这个时候我们希望模型越小越好

152
0:06:46.600 --> 0:06:49.080
因为不管是在手机上面跑起来小

153
0:06:49.080 --> 0:06:51.440
我们网络传输的时候也很小

154
0:06:51.720 --> 0:06:53.000
我们装一个APP

155
0:06:53.000 --> 0:06:55.120
APP里面有非常多的算法

156
0:06:55.120 --> 0:06:57.280
这些算法不可能都放在APP的

157
0:06:57.280 --> 0:07:00.560
而是在运行的时候从网络加载进来的

158
0:07:00.560 --> 0:07:04.760
这个时候对我们的模型的尺寸要求是非常高的

159
0:07:04.760 --> 0:07:07.480
最后我们端测是有非常多的推力引擎的

160
0:07:07.480 --> 0:07:10.840
我们自己也是推出了一个Maspot Lite推力引擎

161
0:07:10.880 --> 0:07:13.520
这个是HMX Core里面去跑的一个后端

162
0:07:13.520 --> 0:07:17.840
另外我们还会要去对接很多不同的芯片

163
0:07:17.840 --> 0:07:20.360
这些就是它的一个主要的特点

164
0:07:22.200 --> 0:07:27.120
下面更多是在云测部署和端测部署的一个具体的区别

165
0:07:27.120 --> 0:07:30.560
我们分开算力、时间、网络能耗很多的方面去对比

166
0:07:30.560 --> 0:07:31.760
大家看一下就好了

167
0:07:31.760 --> 0:07:34.080
我就不再给大家去慢慢的练了

168
0:07:35.760 --> 0:07:38.440
接下来我去给大家汇报另外一个内容

169
0:07:38.440 --> 0:07:42.200
就是云测部署和推力的方式

170
0:07:42.200 --> 0:07:46.400
云测部署其实我们之前比较系统的给大家汇报过

171
0:07:46.400 --> 0:07:50.440
整个推力系统我们在云测可能会做很多的请求

172
0:07:50.440 --> 0:07:52.960
监控、调度还有推力引擎

173
0:07:52.960 --> 0:07:55.800
更多的是模型的管理这些相关的工作

174
0:07:55.800 --> 0:07:58.400
这块这里面就不详细的展开

175
0:07:59.800 --> 0:08:02.040
去到另外一个内容

176
0:08:02.040 --> 0:08:04.920
就是我们的边缘部署的推力的方式

177
0:08:05.360 --> 0:08:07.080
这里面有5种不同的

178
0:08:07.120 --> 0:08:08.360
都是我刚画完的图案

179
0:08:08.360 --> 0:08:10.320
画这个图案还挺复杂的

180
0:08:10.320 --> 0:08:12.480
希望大家可以给我一个赞

181
0:08:14.440 --> 0:08:18.720
首先第一种就是纯粹在边缘里面去做一个推力的

182
0:08:18.720 --> 0:08:21.960
包括在我们的手机、耳机还有手环上面

183
0:08:21.960 --> 0:08:23.920
去做一个简单的推力

184
0:08:23.920 --> 0:08:26.920
这个时候对我们的实验和对我们的模型

185
0:08:26.920 --> 0:08:28.520
就我们这个只能是小模型

186
0:08:28.520 --> 0:08:30.280
对我们的模型要求是非常高的

187
0:08:31.560 --> 0:08:33.240
第二种方式就是我的模型

188
0:08:33.240 --> 0:08:35.640
我的算法是跑在我的云端的

189
0:08:35.840 --> 0:08:39.520
这种方式更多是在我们的推荐的场景

190
0:08:39.520 --> 0:08:43.640
我们会把边缘设备的一些功能去做一个加密

191
0:08:43.640 --> 0:08:45.240
加密完之后丢给我们的云端

192
0:08:45.240 --> 0:08:48.080
云端算完之后传给我们的边缘的设备

193
0:08:48.960 --> 0:08:54.120
第三种方式就是边缘设备跟云端服务器做一个联动的

194
0:08:54.120 --> 0:08:56.920
首先边缘设备更多是跑一个小模型

195
0:08:56.920 --> 0:08:59.680
跑一个小模型得到一个简单的结果

196
0:08:59.680 --> 0:09:03.560
这个简单的结果就会做一个简单的预测

197
0:09:03.680 --> 0:09:06.360
但是我们可能有一些部分的数据

198
0:09:06.360 --> 0:09:09.520
部分的结果是没有办法去在手机端上面去跑的

199
0:09:09.520 --> 0:09:13.240
这个时候就会把这些数据传给我们的边缘的服务器

200
0:09:13.240 --> 0:09:15.640
用较大的模型去跑出一个结果

201
0:09:15.640 --> 0:09:18.760
这种就是我们的小模型跟大模型的联动

202
0:09:18.760 --> 0:09:21.880
我们在边缘的设备里面做一些简单的预测

203
0:09:23.520 --> 0:09:25.920
为啥要有这种这么奇怪的方式

204
0:09:26.080 --> 0:09:29.240
是因为其实我们有一些数据是不出端的

205
0:09:29.240 --> 0:09:31.640
用户对这些数据的隐私的保护

206
0:09:31.640 --> 0:09:32.680
要求非常严格的

207
0:09:32.680 --> 0:09:34.600
例如举个很简单的例子

208
0:09:34.600 --> 0:09:36.840
就是我们相册的推荐

209
0:09:37.600 --> 0:09:41.400
我们点开华为相册的发现就会看到

210
0:09:41.640 --> 0:09:45.000
其实我们对人像地点事物拍照的方式

211
0:09:45.280 --> 0:09:48.160
还有我们的一些美食做了一个归类的

212
0:09:48.400 --> 0:09:51.000
像这种归类这些数据其实是不出端的

213
0:09:51.000 --> 0:09:53.880
用户对这些数据的隐私要求非常高

214
0:09:53.880 --> 0:09:55.960
这个时候我们会做一个小模型

215
0:09:55.960 --> 0:09:57.720
在这里面做一个简单的决策

216
0:09:57.720 --> 0:09:59.240
但是这个小模型怎么来

217
0:09:59.400 --> 0:10:00.880
可能会通过我们的大模型

218
0:10:00.920 --> 0:10:02.920
对大量的数据进行训练

219
0:10:02.920 --> 0:10:04.520
训练完之后再推过来

220
0:10:04.520 --> 0:10:07.080
要变成一个小模型做一个简单的决策的

221
0:10:07.080 --> 0:10:09.640
这个就是我们的第三种方式

222
0:10:09.640 --> 0:10:11.680
边缘跟云端联动

223
0:10:13.360 --> 0:10:16.200
第四个方式就是分布式计算的

224
0:10:16.480 --> 0:10:17.560
我们看一下这个图

225
0:10:17.680 --> 0:10:19.120
这个字我们就不念了

226
0:10:19.120 --> 0:10:22.240
然后这个图就是我有非常多的边缘的设备

227
0:10:22.240 --> 0:10:23.400
有IoT的设备

228
0:10:23.400 --> 0:10:24.720
还有边缘的服务器

229
0:10:25.000 --> 0:10:28.360
这个时候我通过网络去把这些数据

230
0:10:28.360 --> 0:10:29.600
去把这些基站

231
0:10:29.640 --> 0:10:32.040
去把这些边缘的设备把它串通起来

232
0:10:32.040 --> 0:10:35.760
这种方式就是充分的去利用了我的不同的边

233
0:10:35.760 --> 0:10:38.960
不同的IoT的设备的一个算力

234
0:10:39.200 --> 0:10:42.720
这种方式用的更多的是我们的联邦学习

235
0:10:42.720 --> 0:10:44.400
这是一个非常大的

236
0:10:44.400 --> 0:10:47.400
又是另外一个谷歌提出来的一个idea

237
0:10:47.400 --> 0:10:48.520
他当时候提出来了

238
0:10:48.520 --> 0:10:50.560
就希望大量的谷歌的设备

239
0:10:50.920 --> 0:10:53.720
想把它的一些算力给用起来

240
0:10:53.720 --> 0:10:55.760
在我们睡觉和充了电的时候

241
0:10:56.040 --> 0:10:58.960
能不能把我们的手机上面的算力用起来

242
0:10:58.960 --> 0:11:00.160
去做一些计算

243
0:11:00.160 --> 0:11:00.960
做一些训练

244
0:11:00.960 --> 0:11:01.640
做一些学习

245
0:11:02.200 --> 0:11:04.800
这个时候就会用到分布式计算的功能了

246
0:11:07.880 --> 0:11:09.920
最后一个就是我们的方式5

247
0:11:09.920 --> 0:11:12.240
跨设备的一个offloading

248
0:11:12.480 --> 0:11:13.840
上面字我也不念了

249
0:11:13.840 --> 0:11:15.600
我们看一下下面的图

250
0:11:15.800 --> 0:11:19.160
这种方式其实应用我是没有想太懂的

251
0:11:19.160 --> 0:11:21.240
但是确实有这种方式存在

252
0:11:21.240 --> 0:11:23.600
第一个就是我们的边缘设备

253
0:11:23.880 --> 0:11:25.840
去跑模型的一部分

254
0:11:25.840 --> 0:11:26.760
就是模型的切片

255
0:11:27.280 --> 0:11:28.960
然后我们的边缘服务器

256
0:11:29.200 --> 0:11:31.000
因为我们不可能说

257
0:11:31.000 --> 0:11:33.520
每一台边缘的设备直接连到云端的

258
0:11:33.520 --> 0:11:36.080
有可能中间是通过边缘服务器

259
0:11:36.080 --> 0:11:40.080
去做一个缓存或者做一个交接的工作

260
0:11:40.320 --> 0:11:43.400
这个时候这个部分就跑一些模型的切片

261
0:11:43.680 --> 0:11:47.400
云端又去跑另外一部分模型的切片

262
0:11:48.120 --> 0:11:51.320
我们可以把这一个上面的边缘设备

263
0:11:51.320 --> 0:11:51.960
边缘服务器

264
0:11:52.040 --> 0:11:53.320
还有云端数据中心

265
0:11:53.440 --> 0:11:54.520
看成一个整体

266
0:11:54.520 --> 0:11:57.040
它去一起去训练一个模型

267
0:11:57.280 --> 0:11:59.880
这种就是花设备的一种offloading

268
0:12:00.400 --> 0:12:02.320
花设备的一个边缘推理的方式

269
0:12:04.080 --> 0:12:05.680
好了我们来回顾一下

270
0:12:05.680 --> 0:12:07.280
今天我们去看了一下

271
0:12:07.280 --> 0:12:10.400
部署态的一个跟训练态的具体的区别

272
0:12:10.400 --> 0:12:13.000
特别是部署它云侧跟边缘的一个

273
0:12:13.000 --> 0:12:13.920
有什么不一样

274
0:12:13.920 --> 0:12:15.680
它们之间的一个特点

275
0:12:15.680 --> 0:12:17.200
我们了解完这个之后

276
0:12:17.520 --> 0:12:18.920
就给大家去汇报

277
0:12:18.920 --> 0:12:22.120
云侧推理流程到底有哪些功能

278
0:12:23.120 --> 0:12:25.160
最后我们去看了一下

279
0:12:25.160 --> 0:12:27.280
边缘部署的方式有5种

280
0:12:27.280 --> 0:12:30.480
每种方式都有对应的应用场景

281
0:12:30.480 --> 0:12:35.120
这个时候我们就衍生了非常多新的技术点了

282
0:12:35.120 --> 0:12:36.840
这个也是对推理系统

283
0:12:36.840 --> 0:12:39.120
有一个比较大的压力的挑战

284
0:12:39.120 --> 0:12:42.280
它需要去适配很多不同的部署的方式

285
0:12:42.280 --> 0:12:44.200
今天的内容就到这里为止

286
0:12:44.200 --> 0:12:45.400
好了谢谢各位

287
0:12:45.400 --> 0:12:47.560
卷的不行了

288
0:12:47.560 --> 0:12:49.000
记得一键三连加关注

289
0:12:49.320 --> 0:12:51.040
所有的内容都会开源在

290
0:12:51.080 --> 0:12:52.600
下面这条链接里面

291
0:12:53.040 --> 0:12:53.920
掰了 掰掰

