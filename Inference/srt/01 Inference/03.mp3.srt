0
0:00:00.000 --> 0:00:11.420


1
0:00:30.000 --> 0:00:32.000
还有边缘部署的方式

2
0:00:32.000 --> 0:00:38.000
那这里面呢我们去会区别云策和边缘的一个具体的不同

3
0:00:38.000 --> 0:00:41.000
而聊到这里呢其实我想插一个话题就是

4
0:00:41.000 --> 0:00:48.000
前几天的领导找我谈话说我可能只懂AI训练方向我要更多的去了解一下其他

5
0:00:48.000 --> 0:00:52.000
这个时候呢其实我是想反驳但是也不想说啥

6
0:00:52.000 --> 0:00:54.000
因为整个AI系统或者AI权在哪儿

7
0:00:54.000 --> 0:00:59.000
在整个SE团队里面呢算是算是比较资深

8
0:00:59.000 --> 0:01:00.000
可

9
0:01:03.000 --> 0:01:09.000
不再吐槽抱怨我们来到回到我们的真正内容部署态的一个真正的区别

10
0:01:09.000 --> 0:01:12.000
首先呢我们看看什么叫做真正的部署态

11
0:01:12.000 --> 0:01:17.000
就AI系统呢一般都会部署在云端或者我们的一个边缘的设备上面嘛

12
0:01:17.000 --> 0:01:27.000
而云端的更多是集中在一些web的服务啊而在边缘呢更多的可能是一些呃边缘的服务器啊边缘的设备基站呢还有一些边缘的设备

13
0:01:27.000 --> 0:01:32.000
我们最熟悉的手机手环耳机啊这些就是我们的IoT设备

14
0:01:32.000 --> 0:01:36.000
但呢部署态呢其实有另外一个就是我们的云策

15
0:01:36.000 --> 0:01:40.000
大家不要觉得谈到推理的大部分都是在我们的IoT上面去执行的

16
0:01:40.000 --> 0:01:45.000
其实我们的云策啊其实我们整个推理服务呢是承载了非常多的服务的

17
0:01:45.000 --> 0:01:51.000
因为呢云策就是我们整个云策服务器呢它有非常多的算力内存而且电力呢非常充足

18
0:01:51.000 --> 0:01:59.000
我们只需要通过一个网线或者那个RPC或者http的一些客户的请求和响应呢就能够完成我们整体的一个服务了

19
0:01:59.000 --> 0:02:05.000
那这个时候呢我们提供的更多的是一个推理的服务或者云策的服务

20
0:02:06.000 --> 0:02:11.000
现在呢我们看一下在部署态里面特别是在云端的一个部署态的特点

21
0:02:11.000 --> 0:02:18.000
那我简单的给大家练习一下可能与速点快没关系现在的内容呢还是更多的是概念性的问题

22
0:02:18.000 --> 0:02:23.000
那第一个呢就是对功耗温度啊还有我们的模型大小呢是没有太严格的限制的

23
0:02:23.000 --> 0:02:27.000
因为云策嘛我们的云端的资源还是非常的丰富的

24
0:02:27.000 --> 0:02:38.000
那接着我们看一下其实云策啊它不仅使用推理其实我们推理和训练呢都是有的只是在服务的时候我们更多的是集中在推理

25
0:02:38.000 --> 0:02:42.000
那接着呢我们在云策其实有比较集中的数据管理的

26
0:02:42.000 --> 0:02:49.000
在云策呢我们有非常集中的一个数据的一个管理因为数据的管理是非常严峻的一个挑战

27
0:02:49.000 --> 0:02:55.000
我们的数据呢不仅是数据化的数据而且更多的是一些非结构化的数据

28
0:02:55.000 --> 0:03:08.000
那接着呢我们可以看到啊其实模型在云端呢更好地去保护我们现在对模型的保护其实比较弱的我们的算法的侵入其实是比较简单的这一块呢又衍生了一个AI安全的问题

29
0:03:08.000 --> 0:03:38.000
大家听完就算了接下来我们看一下云端也会遇到一些问题云端的问题就是人工智能的服务的成本非常高昂因为我们需要云端的服务你就要需要大量的机器大量的拥摘大量的各种设备维护机房所以它整体的成本是很高的然后呢我们看一下云端的服务对网络的依赖是非常高的如果断网了基本上你的推理服务了就断了就我们做推荐的时候滑着滑着没网络了

30
0:03:38.000 --> 0:04:08.000
滑不动了我经常晚上刷抖音的时候呢会遇到这种问题呀就就自己断网了那接着我们看一下数据隐私是个很大的问题就是用户的数据如果不出端那这时候呢我们推理服务呢就怎么去保护用用户数据的隐私呢是个很大的挑战接着呢数据的传输呢也是个很大的成本而我们现在的传一张图片比较简单角色是我们的端侧的摄像头我们要传大量的视频数据的时候

31
0:04:08.000 --> 0:04:38.000
这个时候对我们的数据传输压力或者那个安全隐私是个很大的挑战最后呢就是云端的基本上很少会做一些定字化的模型一般都会做一些通用的模型因为定字化的模型是非常难的就千人千面这个问题呢还是一直是我们努力去解决的问题那接下来呢我们看一下部署态的另外一面就是我们的一点端端侧端侧呢我们端侧的设备和资源呢是非常紧贴

32
0:04:38.000 --> 0:05:08.000
紧张的特别是手机啊或者ios的设备我们里面的部件是因为靠一块像手指夹那么大小的一块芯片去支撑我们也只能靠一块电池如果我们的算法把所有里面的计算资源都用起来了那我们的耗电量是很大的不过有个好处就是它的响应呢在设备里面就自我完成了就自闭环了就不需要去消耗云端的一些资源那我们看一下端侧的

33
0:05:08.000 --> 0:05:38.000
一些挑战那端侧的挑战呢就是会有非常严格的一个功耗啊热量啊模型大小所受限嘛因为我们之前说的我们的内存呢只有两个G或者四个G大家所有的设备去共用的我们的功耗或者我们的一块那个手机里面呢就四百毫安而完全不够用在这个时候呢就有非常多的约束而且硬件的算力呢对于推理来说是远远远不够的就我们对于算力的需求是非常的

34
0:05:38.000 --> 0:06:08.000
多的而且数据呢非常分散就我之前第一份工作时是做那个情景感知嘛我们就会去收集非常多的手机的一个传感器的信息一台手机我们就可以获取接近三十多种传感器的设备了所以数据呢是非常分散的而且模型呢在边缘更容易受到攻击就我去攻击你的模型是很简单的而且这个时候呢我们的平台非常多样化不同的设备呢可能会有自己不同的平台

35
0:06:08.000 --> 0:06:38.000
包括我们有一款那个推理引擎啊部署在我们的端侧那这个时候呢我们端侧跟我们在手机上面用的又是不同的一款那这款我们叫做Macro这款我们叫做Lite的推理的引擎所以说它基本上没法做到一个通用下面呢我们看一下端侧就是我们Edge端的一些要解决的一些技术点那第一个呢就是应用的算法优化我们需要对算法优化就是训练的时候跟推理的时候我们希望啊越小越好

36
0:06:38.000 --> 0:06:39.680
但是精度越高越好

37
0:06:39.680 --> 0:06:41.200
那其实这是个矛盾

38
0:06:41.200 --> 0:06:44.480
接着呢我们去看看高效率的模型的设计

39
0:06:44.480 --> 0:06:46.600
那这个时候我们希望模型越小越好

40
0:06:46.600 --> 0:06:49.120
因为不管是在手机上面跑起来小

41
0:06:49.120 --> 0:06:51.720
我们网络传输的时候也很小嘛

42
0:06:51.720 --> 0:06:55.240
我们装一个APP那APP里面有非常多的算法

43
0:06:55.240 --> 0:06:57.200
这些算法不可能都放在APP的

44
0:06:57.200 --> 0:07:00.560
而是在运行的时候从网络加载进来的

45
0:07:00.560 --> 0:07:04.880
这个时候呢对我们的模型的尺寸啊要求是非常高的

46
0:07:04.880 --> 0:07:07.480
最后呢我们端侧是有非常多的推理引擎的

47
0:07:07.480 --> 0:07:10.880
我们自己也是推出了一个马斯波赖的一个推理引擎

48
0:07:10.880 --> 0:07:13.600
这个呢是HMX core里面去跑的一个后端

49
0:07:13.600 --> 0:07:17.840
那另外呢我们还会要去对接很多不同的芯片

50
0:07:17.840 --> 0:07:20.320
那这些呢就是它的一个主要的特点

51
0:07:22.280 --> 0:07:27.160
下面的更多是在云侧部署和端侧部署的一个具体的区别

52
0:07:27.160 --> 0:07:30.640
我们分开算力时间网络能耗很多的方面去对比

53
0:07:30.640 --> 0:07:34.080
大家看一下就好了我就不再给大家去慢慢地练了

54
0:07:35.080 --> 0:07:42.200
接下来呢我去给大家汇报另外一个内容就是云侧部署和推理的方式

55
0:07:42.200 --> 0:07:47.640
那云侧部署其实我们之前比较系统地给大家汇报过整个推理系统了

56
0:07:47.640 --> 0:07:52.920
我们在云侧可能会做很多的请求啊监控啊调度啊还有推理引擎

57
0:07:52.920 --> 0:07:55.840
而更多的是模型的管理这些相关的工作

58
0:07:55.840 --> 0:07:58.440
那这块呢这里面就不详细地展开

59
0:07:59.440 --> 0:08:05.200
而去到另外一个内容就是我们的边缘部署的推理的方式

60
0:08:05.200 --> 0:08:12.640
这里面呢有五种啊不同的都是我刚画完的图啊画图啊画这个图还挺复杂的希望大家可以那个给我一个赞

61
0:08:12.640 --> 0:08:23.880
首先第一种呢就是纯粹在边缘里面去做一个推理的就包括在我们的手机啊耳机啊还有手环上面去做一个简单的推理

62
0:08:23.880 --> 0:08:30.280
那这个时候呢对我们的食盐和对我们的模型就我们这个只能是小模型对我们的模型要求是非常高的

63
0:08:31.560 --> 0:08:48.120
第二种方式就是我的模型我的算法呢是跑在我的云端的这种场这种方式呢更多是在我们的推荐的场景我们会把边缘设备的一些功能呢去做一个加密加密完之后呢丢给我们的云端云端算完之后呢传传给我们的边缘的设备

64
0:08:48.120 --> 0:08:48.760
那第三种方式呢就是边缘设备跟云端服务器做一个联动的首先呢边缘设备呢更多是跑一个小模型跑一个小模型呢得到一个简单的结果那这个简单的结果呢就会做一个简单的预测但是呢我们可能有一些部分的数据部分的结果是没有办法去在手机端上面去跑的这个时候呢就会把这些数据呢传给我们的边缘的服务器用较大的模型去跑出一个结果那这种呢就是我们的小模型跟大模型

65
0:09:18.120 --> 0:09:48.120
联动我们在边缘的设备里面呢做一些简单的预测为啥要有这种这么奇怪的方式呢是因为其实我们有一些数据啊是不出端的用户对这些数据的隐私的保护是要求非常严格的例如举个很简单的例子就是我们相册的推荐我们点开华为相册的发现呢就会看到啊其实我们对人像啊地点啊事物啊拍照的方式啊还有我们的一些美食呢做了一个归类的

66
0:09:48.120 --> 0:10:18.120
那像这种归类这些数据啊其实是不出端的用户对这些数据的隐私要求非常高这个时候呢我们会做一个小模型在这里面做一个简单的决策但是这个小模型怎么来了可能会通过我们的大模型对大量的数据进行训练训练完之后呢再推过来要变成一个小模型做一个简单的决策的这个呢就是我们的第三种方式边缘跟云端联动那第四个方式呢就是分布式计算的我们看一下这个图啊

67
0:10:18.120 --> 0:10:48.120
这个我们就不练了然后呢这个图呢就是我有非常多的边缘的设备有IoT的设备还有边缘的服务器那这个时候呢我通过网络呀去把这些数据去把这些基站去把这些边缘的设备把它串通起来这种方式呢就是充分地去利用了我的不同的边不同的IoT的设备的一个算力那这种方式用的更多的是我们的联邦学习这是一个非常大的又是另外一个谷歌提出来的一个idea

68
0:10:48.120 --> 0:11:18.120
提出来了就希望大量的谷歌的设备呢想把它的一些算力给用起来在我们睡觉和充着电的时候呢能不能把我们的手机上面的算力用起来去做一些计算做一些训练做一些学习呢那这个时候呢就会用到分布式计算的功能了最后一个呢就是我们的方式五跨设备的一个offload上面字呢我也不念了我们看一下下面的这个图那这种方式呢其实应用的我是没有

69
0:11:18.120 --> 0:11:48.120
想太懂的但是呢确实有这种方式存在第二呢就是我们的边缘设备呢去跑模型的一部分就是模型的切片嘛然后呢我们的边缘服务器呢因为我们不可能说每一台边缘的设备直接连到云端的有可能中间的是通过边缘服务器去做一个缓存或者做一个嗯交接的工作那这个时候呢这个部分就跑一些模型的切片那云端呢又去跑另外一部分模型的切片

70
0:11:48.120 --> 0:12:18.120
我们可以把这一个上面的这个边缘设备边缘服务器还有云云端数据中心呢看成一个整体它去一起去训练一个模型那这种呢就是花设备的一种offloading花设备的一个边缘推进的方式好了我们来回顾一下今天呢我们去看了一下部属态的一个跟训练态的一个具体的区别特别是部属态云策跟边缘的一个有什么不一样它们之间的一个特点我们了解完这个之后呢

71
0:12:18.120 --> 0:12:22.120
要去汇报云策推理流程到底有哪些功能

72
0:12:23.760 --> 0:12:27.400
最后呢我们去看了一下边缘部属的方式有五种

73
0:12:27.400 --> 0:12:30.600
每种方式呢都有对应的应用场景

74
0:12:30.600 --> 0:12:35.240
那这个时候呢我们就衍生了非常多新的技术点了

75
0:12:35.240 --> 0:12:39.200
这个也是对推理系统有一个比较大的一个压力的挑战

76
0:12:39.200 --> 0:12:42.360
它需要去适配很多不同的部属的方式
