1
00:00:00,000 --> 00:00:08,200
Hello 大家好,我是ZOMI

2
00:00:08,200 --> 00:00:10,600
今天我们来到推理系统系列里面

3
00:00:10,600 --> 00:00:13,080
去真正的了解一下什么是推理系统

4
00:00:13,080 --> 00:00:15,640
那推理系统它不是推理引擎

5
00:00:15,640 --> 00:00:17,400
所以我们可以区分下来

6
00:00:17,400 --> 00:00:21,640
我们接下来的内容主要是聚焦于推理系统的整体的介绍

7
00:00:21,640 --> 00:00:25,120
今天我要去给大家分享的内容可能会稍微多了一点

8
00:00:25,120 --> 00:00:29,240
但是我尽量的简洁一点去给大家汇报一下相关的内容

9
00:00:30,880 --> 00:00:34,520
第一个就是我们去了解一下训练和推理到底有什么区别

10
00:00:34,520 --> 00:00:37,200
接着我们看看什么是推理系统

11
00:00:37,200 --> 00:00:41,160
第三点就是去看一下推理系统的优化目标和约束

12
00:00:41,160 --> 00:00:43,960
这些更多的是一些宏观上的概念

13
00:00:43,960 --> 00:00:48,600
最后我们看看推理系统和推理引擎到底要研究哪些内容

14
00:00:48,600 --> 00:00:53,520
其实推理系统对于ZOMI来说应该是最熟悉的一部分了

15
00:00:53,520 --> 00:00:56,280
这个也是我之前在刚进入华为的时候

16
00:00:56,280 --> 00:01:01,560
做了大量非常底层相关的一些代码的研发或者代码的开发工作

17
00:01:03,560 --> 00:01:06,080
下面我们看一下整体的生命周期

18
00:01:06,080 --> 00:01:07,680
首先我们很简单

19
00:01:07,680 --> 00:01:11,640
其实大家应该搞 AI 或者做算法的人特别熟

20
00:01:11,640 --> 00:01:14,880
可能做系统的人不是说非常熟

21
00:01:14,880 --> 00:01:17,720
所以我这里面简单的去给大家汇报一下

22
00:01:17,720 --> 00:01:21,320
首先我们可能会收集非常多的一些训练的数据

23
00:01:21,320 --> 00:01:22,560
公开的数据集也好

24
00:01:22,560 --> 00:01:23,840
自有的数据集也好

25
00:01:23,880 --> 00:01:27,600
然后我们就会对我们的神经网络模型进行训练

26
00:01:27,600 --> 00:01:31,320
训练完之后我们就得到一个固定化的网络模型

27
00:01:31,320 --> 00:01:34,600
这个模型我们就真正的在部署的阶段了

28
00:01:34,600 --> 00:01:36,800
我们就会在云端做一些服务的请求

29
00:01:36,800 --> 00:01:38,440
还有服务的响应

30
00:01:38,440 --> 00:01:39,840
而在我们的推理引擎

31
00:01:39,840 --> 00:01:43,320
更多是聚焦于我们的这一个深度学习的模型

32
00:01:43,320 --> 00:01:44,800
怎么跑起来更快

33
00:01:44,800 --> 00:01:50,160
而推理系统整个系统需要把我们端到端的流程串起来

34
00:01:50,160 --> 00:01:52,040
把我们的服务化把它做好

35
00:01:52,160 --> 00:01:53,680
这个就是最大的区别

36
00:01:54,680 --> 00:01:58,880
现在我们去区分一下训练任务和推理任务之间的一个最大的区别

37
00:01:58,880 --> 00:02:02,040
训练任务更多的是我们在训练的时候

38
00:02:02,040 --> 00:02:04,960
我们会采用中心化的一个训练

39
00:02:04,960 --> 00:02:08,400
一般来说我们训练就需要非常多的时间了

40
00:02:08,400 --> 00:02:11,520
就把我们的数据去跟我们的算法匹配起来

41
00:02:11,520 --> 00:02:13,280
而且需要比较大的数据

42
00:02:13,280 --> 00:02:18,960
然后这个时候对我们中心服务器的吞吐量还是比较要求高的

43
00:02:19,000 --> 00:02:22,920
而我们训练的模型的精度和准确率也是比较高的

44
00:02:22,920 --> 00:02:26,120
就尽可能模型的精度和性能比较好

45
00:02:27,880 --> 00:02:30,360
而推理基本上我去想想

46
00:02:31,360 --> 00:02:34,240
HMS Core里面的推理引擎用的是MindSpore

47
00:02:34,240 --> 00:02:37,040
然后它基本上就是因为我们要服务全球

48
00:02:37,040 --> 00:02:40,160
所以我们会7×24小时的去服务

49
00:02:40,160 --> 00:02:44,760
而且我们每天的调用量可能就超过5亿次

50
00:02:44,760 --> 00:02:48,560
每天调用量超过5亿次的服务响应请求

51
00:02:48,560 --> 00:02:52,880
这个时候我们整个推理的任务的要求是非常高的

52
00:02:52,880 --> 00:02:56,960
而且模型一般稳定收敛的情况下不会重新训练

53
00:02:56,960 --> 00:02:59,760
而且推一个新的模型是非常的谨慎的

54
00:02:59,760 --> 00:03:02,000
因为服务压力请求是非常大的

55
00:03:02,000 --> 00:03:07,040
这个就是深度学习里面生命周期训练和推理的最大的区别

56
00:03:08,800 --> 00:03:12,240
现在我们看看他们具体的遇到的一些挑战

57
00:03:12,240 --> 00:03:15,320
或者他们的不同的特点

58
00:03:15,360 --> 00:03:17,720
上面这个就是训练场景的

59
00:03:17,720 --> 00:03:19,800
下面就是推理场景的

60
00:03:20,840 --> 00:03:23,560
训练场景我们会用非常多的数据

61
00:03:23,560 --> 00:03:24,920
就我们的big BatchSize

62
00:03:24,920 --> 00:03:27,360
然后在我们的云服务器上面去跑

63
00:03:27,360 --> 00:03:30,160
这个时候我们会做一些前向的推理

64
00:03:30,160 --> 00:03:31,040
反向的传播

65
00:03:31,040 --> 00:03:32,200
然后去更新

66
00:03:32,200 --> 00:03:34,720
不断的去训练我们整个网络模型

67
00:03:34,720 --> 00:03:38,040
使得我们网络模型的精度肯定是越高越好

68
00:03:38,800 --> 00:03:39,960
在训练完之后

69
00:03:40,120 --> 00:03:43,000
我们就会把网络模型固定化下来

70
00:03:43,520 --> 00:03:45,960
给到我们的IoT设备或者Web Service

71
00:03:45,960 --> 00:03:47,640
去做一个推理服务的

72
00:03:48,040 --> 00:03:50,360
而推理服务我们需要的数据量

73
00:03:50,880 --> 00:03:52,880
不会像我们训练这么大

74
00:03:52,880 --> 00:03:56,880
这个时候使用的更多的是真实的数据场景

75
00:03:56,880 --> 00:03:58,480
然后去部署起来

76
00:03:58,480 --> 00:04:01,520
真正的做一个简单的前向的推理

77
00:04:01,520 --> 00:04:03,000
然后预测到结果

78
00:04:03,200 --> 00:04:05,520
这个结果就会返回给我们的用户

79
00:04:05,520 --> 00:04:08,000
或者返回给我们的Web服务器请求

80
00:04:08,000 --> 00:04:10,160
做一些传统的工作

81
00:04:10,360 --> 00:04:13,280
这个就是他们最大的一些区别

82
00:04:14,480 --> 00:04:16,480
现在我们看一些遇到的一些挑战

83
00:04:16,480 --> 00:04:18,920
就是我们的模型在推理的时候

84
00:04:18,920 --> 00:04:20,440
需要长期的运行

85
00:04:20,440 --> 00:04:23,600
而且推理会有更加苛刻的资源的要求

86
00:04:23,600 --> 00:04:24,760
因为我们IoT设备

87
00:04:25,160 --> 00:04:27,360
它的计算资源是非常有限的

88
00:04:27,360 --> 00:04:30,080
而且还是不能做一个反向传播

89
00:04:30,080 --> 00:04:32,040
就基本上我们不用学习

90
00:04:32,040 --> 00:04:33,440
做个推理就好了

91
00:04:33,440 --> 00:04:35,480
而且部署的型号非常多

92
00:04:36,440 --> 00:04:38,280
可能对于我一个应用来说

93
00:04:38,280 --> 00:04:39,640
用户看到的是一个应用

94
00:04:39,640 --> 00:04:40,880
但是在厂商来说

95
00:04:40,880 --> 00:04:44,440
我看到有100台不同的手机

96
00:04:44,440 --> 00:04:47,320
100台不同的设备去做一个服务的

97
00:04:47,320 --> 00:04:49,240
所以对于要求还是非常高的

98
00:04:51,840 --> 00:04:54,160
现在我们来看一下推理系统

99
00:04:54,160 --> 00:04:55,200
就是整个推理系统

100
00:04:55,200 --> 00:04:56,440
我们要做哪些内容呢

101
00:04:56,440 --> 00:04:58,000
就是首先我们推理系统

102
00:04:58,000 --> 00:05:01,240
最重要的就是管理我们的网络模型

103
00:05:01,240 --> 00:05:02,360
因为我们训练出来了

104
00:05:02,360 --> 00:05:04,240
会有非常多的网络模型

105
00:05:04,240 --> 00:05:06,840
而这些模型就是我们对应的算法

106
00:05:06,840 --> 00:05:08,880
我们要真正的部署给用户的

107
00:05:08,920 --> 00:05:10,760
而整个推理的服务系统里面

108
00:05:10,840 --> 00:05:11,800
我们做很多工作

109
00:05:12,000 --> 00:05:13,720
第一个就是模型的加载了

110
00:05:13,720 --> 00:05:15,160
模型的版本管理了

111
00:05:15,160 --> 00:05:16,360
还有数据的管理了

112
00:05:16,360 --> 00:05:17,880
还有服务的接口

113
00:05:17,880 --> 00:05:20,920
所以基本上它跟AI训练

114
00:05:20,920 --> 00:05:22,600
或者AI算法不相关

115
00:05:22,600 --> 00:05:25,000
更多的是平台性的工作

116
00:05:25,000 --> 00:05:26,840
然后最后我们会去除一些

117
00:05:27,200 --> 00:05:30,000
服务器端和客户端的一些请求和响应

118
00:05:30,000 --> 00:05:32,720
完成整个端到端的功能

119
00:05:32,720 --> 00:05:34,880
这个是推理系统要做的工作

120
00:05:36,560 --> 00:05:37,840
而所谓的推理系统

121
00:05:38,000 --> 00:05:40,920
其实不仅仅是以数据中心

122
00:05:40,920 --> 00:05:42,120
为一个服务

123
00:05:42,120 --> 00:05:44,080
作为一个主要的方式

124
00:05:44,080 --> 00:05:45,640
而且它要兼顾边缘的

125
00:05:45,640 --> 00:05:47,000
移动设备的场景

126
00:05:48,000 --> 00:05:51,120
而我们提到的整个推理服务的策略

127
00:05:52,560 --> 00:05:53,760
不仅仅需要考虑到

128
00:05:53,760 --> 00:05:55,080
数据中心的一个推理

129
00:05:55,080 --> 00:05:57,200
还要考虑到边缘设备的推理

130
00:05:57,200 --> 00:05:59,240
所以推理系统还是很复杂的

131
00:05:59,240 --> 00:06:01,160
它是一个非常复杂的系统工程

132
00:06:01,160 --> 00:06:03,400
下面我们提几个问题

133
00:06:04,240 --> 00:06:06,720
就是深度学习的推理系统

134
00:06:06,800 --> 00:06:08,480
我们要做设计的时候

135
00:06:08,480 --> 00:06:11,160
一般要考虑哪些问题

136
00:06:12,320 --> 00:06:13,680
因为考虑的这些问题

137
00:06:13,680 --> 00:06:16,200
可能会影响我们后面的架构的设计

138
00:06:16,200 --> 00:06:18,040
还有我们的技术的方案

139
00:06:19,080 --> 00:06:21,640
第二点就是推理系统

140
00:06:21,640 --> 00:06:23,680
跟传统的服务系统

141
00:06:23,680 --> 00:06:25,640
有哪些新的挑战

142
00:06:26,640 --> 00:06:28,040
大家可以一起回顾一下

143
00:06:28,040 --> 00:06:32,080
我们刚才所讨论所探讨的一些内容

144
00:06:33,480 --> 00:06:35,280
最后一个就是云侧

145
00:06:35,280 --> 00:06:36,640
就是我们的中心服务器

146
00:06:36,640 --> 00:06:38,720
还有端侧到我们的手机

147
00:06:38,720 --> 00:06:40,120
或者IoT设备上面

148
00:06:40,120 --> 00:06:44,000
整个推理系统的服务有什么不同

149
00:06:44,000 --> 00:06:46,080
有什么各自的侧重点

150
00:06:46,080 --> 00:06:48,600
我觉得大家可以停下来几分钟

151
00:06:48,600 --> 00:06:51,200
去思考一下具体是怎么做的

152
00:06:51,200 --> 00:06:53,200
因为这个对我们的架构的设计

153
00:06:53,200 --> 00:06:55,000
有非常大的一个挑战

154
00:06:55,000 --> 00:06:56,720
和不同的技术选行

155
00:06:59,640 --> 00:07:02,120
接下来我会去给大家汇报一下

156
00:07:02,120 --> 00:07:04,800
推理系统的一个优化的目标和约束

157
00:07:04,920 --> 00:07:06,840
现在这个更多的是一些

158
00:07:07,720 --> 00:07:08,720
宏观的概念

159
00:07:08,840 --> 00:07:11,320
大家其实可以不用太在乎

160
00:07:11,320 --> 00:07:12,640
或者听一听就完了

161
00:07:14,440 --> 00:07:17,680
我们现在还是以淘宝作为例子

162
00:07:17,680 --> 00:07:19,760
就是包括我们的在线新闻

163
00:07:19,760 --> 00:07:20,560
推理这些

164
00:07:20,560 --> 00:07:22,200
都是推荐一些

165
00:07:22,200 --> 00:07:24,040
我们比较喜欢的一些服务

166
00:07:24,040 --> 00:07:25,320
包括抖音它的推荐

167
00:07:25,320 --> 00:07:26,680
其实也是相同的

168
00:07:26,680 --> 00:07:28,480
我们可能要考虑到低延迟

169
00:07:28,680 --> 00:07:30,120
还有一个高吞吐

170
00:07:30,120 --> 00:07:31,000
还有扩展性

171
00:07:31,000 --> 00:07:32,320
还有准确性的问题

172
00:07:32,320 --> 00:07:33,960
就我们提一点

173
00:07:34,080 --> 00:07:35,040
就是低延迟

174
00:07:35,040 --> 00:07:37,400
我们网络上的文章的推荐

175
00:07:37,400 --> 00:07:39,720
或者我们的产品的推荐

176
00:07:39,720 --> 00:07:41,480
我们希望延迟越少越好

177
00:07:41,480 --> 00:07:43,320
因为我不断的刷刷刷

178
00:07:43,520 --> 00:07:44,840
这个刷刷刷的过程

179
00:07:45,160 --> 00:07:47,960
就要求我们系统不断的响应

180
00:07:48,120 --> 00:07:49,040
有时候我淘宝

181
00:07:49,040 --> 00:07:50,120
或者我经常刷抖音

182
00:07:50,440 --> 00:07:52,040
也是经常刷刷刷

183
00:07:52,040 --> 00:07:53,360
然后看他不满意的

184
00:07:53,360 --> 00:07:54,600
就下一个

185
00:07:55,280 --> 00:07:56,160
它的推荐过程

186
00:07:56,360 --> 00:07:58,640
是在后面不断的去计算

187
00:07:58,640 --> 00:07:59,960
我哪个视频喜欢

188
00:07:59,960 --> 00:08:02,040
哪个视频是不喜欢的

189
00:08:02,080 --> 00:08:04,000
系统为了算法要考虑很多问题

190
00:08:04,240 --> 00:08:05,440
第二个就是

191
00:08:05,480 --> 00:08:06,720
除了我们刚才讲的

192
00:08:06,720 --> 00:08:07,880
一些业务上的问题

193
00:08:07,880 --> 00:08:10,560
我们回到我们真正跟AI内容相关的

194
00:08:10,560 --> 00:08:11,080
就是

195
00:08:11,400 --> 00:08:12,480
我整个推理系统

196
00:08:12,480 --> 00:08:13,840
假设这个是推理系统

197
00:08:13,840 --> 00:08:14,720
我们要考虑到

198
00:08:14,720 --> 00:08:16,080
对接很多不同的

199
00:08:16,080 --> 00:08:18,680
AI框架训练出来的模型

200
00:08:18,720 --> 00:08:20,960
接着我们可能还会考虑到

201
00:08:20,960 --> 00:08:22,600
我要部署在非常多的

202
00:08:22,600 --> 00:08:24,120
不同的硬件上面

203
00:08:24,120 --> 00:08:25,160
而整个推理系统

204
00:08:25,280 --> 00:08:27,120
它有很多模块去组成

205
00:08:27,120 --> 00:08:27,840
不同的模块

206
00:08:28,000 --> 00:08:30,640
又有不同的一个具体的功能

207
00:08:30,680 --> 00:08:32,000
所以说整个推理系统

208
00:08:32,000 --> 00:08:33,880
它是要考虑很多的问题

209
00:08:35,120 --> 00:08:36,160
现在我们看一下

210
00:08:36,160 --> 00:08:38,440
我们在设计推理系统时候

211
00:08:38,440 --> 00:08:40,240
需要考虑的几个问题

212
00:08:40,240 --> 00:08:42,560
也就是刚才我们的一个提问

213
00:08:42,560 --> 00:08:44,000
所对应的回答

214
00:08:44,000 --> 00:08:45,200
第一个就是

215
00:08:45,200 --> 00:08:45,920
低延迟

216
00:08:45,920 --> 00:08:46,680
高吞吐

217
00:08:46,680 --> 00:08:47,280
高效率

218
00:08:47,280 --> 00:08:47,840
灵活性

219
00:08:47,840 --> 00:08:48,440
扩展性

220
00:08:48,440 --> 00:08:51,560
下面我简单的去展开一下

221
00:08:51,560 --> 00:08:52,640
对应的内容

222
00:08:52,640 --> 00:08:54,120
第一个就是灵活性

223
00:08:54,120 --> 00:08:55,480
就是AI部署

224
00:08:55,840 --> 00:08:57,320
AI服务的部署

225
00:08:57,520 --> 00:08:59,840
其实对于优化和系统维护来说

226
00:08:59,840 --> 00:09:01,840
是比较困难的

227
00:09:01,840 --> 00:09:04,200
因为我们需要对接非常多的框架

228
00:09:04,360 --> 00:09:05,040
我们可以看到

229
00:09:05,040 --> 00:09:06,840
AI框架有非常多

230
00:09:06,840 --> 00:09:08,800
而且我们对应的硬件系统

231
00:09:08,800 --> 00:09:10,040
也是非常的复杂

232
00:09:10,040 --> 00:09:12,560
所以我们要求整个我们的AI系统

233
00:09:12,560 --> 00:09:15,800
是要求它整个灵活性要比较高

234
00:09:15,800 --> 00:09:17,680
这个是对于我们系统开发工程师

235
00:09:17,680 --> 00:09:19,200
或者一些厂商来说

236
00:09:19,200 --> 00:09:22,120
第二个就是整体的灵活性

237
00:09:22,760 --> 00:09:23,800
系统的灵活性

238
00:09:23,920 --> 00:09:25,640
就需要可以支持非常多的

239
00:09:25,640 --> 00:09:27,000
不同的AI的模型

240
00:09:27,240 --> 00:09:29,200
而且AI的框架的版本迭代

241
00:09:29,320 --> 00:09:31,800
这个是对我们的挑战是非常高的

242
00:09:31,800 --> 00:09:34,200
我们要维护非常多的版本

243
00:09:34,200 --> 00:09:36,880
第三个就是跟不同的语言的对接

244
00:09:36,880 --> 00:09:38,920
还有不同的逻辑的应用的结合

245
00:09:38,920 --> 00:09:40,760
因为应用五花八门

246
00:09:40,760 --> 00:09:43,280
还有语言也是非常的多

247
00:09:43,280 --> 00:09:44,760
包括我们部署在iOS

248
00:09:44,760 --> 00:09:46,320
部署在网页上面

249
00:09:46,320 --> 00:09:48,960
部署在我们的安卓手机

250
00:09:48,960 --> 00:09:51,280
都是不同的语言的API的接口

251
00:09:51,280 --> 00:09:52,600
所以说整体来说

252
00:09:53,680 --> 00:09:56,840
可能会需要去做一个开放性的协议

253
00:09:56,840 --> 00:09:58,400
例如onnx这种转换

254
00:09:58,400 --> 00:10:00,560
另外我们接口可能需要进行

255
00:10:00,560 --> 00:10:01,840
一系列的抽象

256
00:10:01,840 --> 00:10:04,600
可能还会做一些容器的一些封装

257
00:10:04,600 --> 00:10:06,280
例如我们经常用的docker

258
00:10:06,280 --> 00:10:08,560
最后可能还会需要一些IPC

259
00:10:08,560 --> 00:10:11,640
就跨语言跨进程的一些通讯的协议

260
00:10:13,360 --> 00:10:15,520
接着我们看一下延迟

261
00:10:15,520 --> 00:10:17,920
延迟其实是对于在线系统来说

262
00:10:17,920 --> 00:10:20,000
是非常高的要求

263
00:10:20,000 --> 00:10:21,520
就我们刚才举的一个例子

264
00:10:21,520 --> 00:10:22,560
刷抖音的时候

265
00:10:22,560 --> 00:10:24,680
我们需要希望尽可能的

266
00:10:24,680 --> 00:10:26,640
去给我提供一个低延迟

267
00:10:26,760 --> 00:10:29,000
而且需要满足到我们有限的

268
00:10:29,000 --> 00:10:30,040
长尾的用户

269
00:10:30,040 --> 00:10:31,040
长尾的应用

270
00:10:31,880 --> 00:10:33,040
因为每个用户

271
00:10:33,280 --> 00:10:35,400
千人千面在推进系统的时候

272
00:10:35,400 --> 00:10:37,600
所以很多一些长尾的问题

273
00:10:37,600 --> 00:10:39,160
我们需要去解决的

274
00:10:39,160 --> 00:10:42,480
那最后我们还有一些吞吐量的问题

275
00:10:42,480 --> 00:10:44,680
就是我们面向一些传统的

276
00:10:44,680 --> 00:10:45,480
并发的问题

277
00:10:45,480 --> 00:10:46,600
并发的请求

278
00:10:46,600 --> 00:10:49,200
那这个时候怎么去解决吞吐量

279
00:10:51,080 --> 00:10:52,880
最后还有一个效率

280
00:10:52,880 --> 00:10:55,440
效率其实是对于系统来说

281
00:10:55,440 --> 00:10:56,440
不是说非常重要

282
00:10:56,480 --> 00:10:58,960
但是对于推理引擎来说

283
00:10:59,160 --> 00:11:00,360
就特别的重要了

284
00:11:00,800 --> 00:11:04,440
因为我们的端侧的资源是有限的

285
00:11:05,200 --> 00:11:08,560
云侧的运算的预算也是有限的

286
00:11:08,560 --> 00:11:10,760
所以我们经常会对网络模型

287
00:11:10,960 --> 00:11:11,920
去做一些压缩

288
00:11:12,320 --> 00:11:14,760
去使用一些高效的AI的推理芯片

289
00:11:15,160 --> 00:11:18,000
而这一块就催生了非常多的一些

290
00:11:18,000 --> 00:11:19,160
第三方的厂商

291
00:11:19,160 --> 00:11:22,480
去做很多推出新的推理芯片

292
00:11:23,960 --> 00:11:26,240
那最后一个还有扩展性

293
00:11:26,360 --> 00:11:29,160
那扩展性就是应对用户的请求

294
00:11:29,160 --> 00:11:30,120
五花八门

295
00:11:30,120 --> 00:11:32,680
然后整个推理系统的吞吐量

296
00:11:32,680 --> 00:11:34,280
是非常的复杂的

297
00:11:35,000 --> 00:11:36,520
这要求我们整个推理系统

298
00:11:36,680 --> 00:11:39,440
需要提供非常强大的推理的吞吐

299
00:11:39,560 --> 00:11:41,560
还有让我们整个的推理系统

300
00:11:41,720 --> 00:11:43,240
更加的可靠

301
00:11:44,440 --> 00:11:44,880
另外的话

302
00:11:44,880 --> 00:11:47,560
我们可能还会做一些平台化的部署

303
00:11:47,560 --> 00:11:48,400
例如Kubernetes

304
00:11:48,400 --> 00:11:50,200
还有Docker相关的部署

305
00:11:50,200 --> 00:11:52,640
使得我们整个系统更加的自动化

306
00:11:52,640 --> 00:11:55,160
而不是员工的去参与很多的维护

307
00:11:55,280 --> 00:11:58,280
因为里面的配置是非常的复杂

308
00:11:58,760 --> 00:12:00,320
我们一个APP里面

309
00:12:00,320 --> 00:12:02,400
就涉及到非常多的算法

310
00:12:02,400 --> 00:12:04,800
例如我们在淘宝页面看到的推荐

311
00:12:04,800 --> 00:12:06,920
跟我们淘宝子页面看到的推荐

312
00:12:06,920 --> 00:12:07,760
是不一样的

313
00:12:07,760 --> 00:12:09,480
你个人搜了一个内容

314
00:12:09,480 --> 00:12:11,760
里面的推荐的内容也是不一样

315
00:12:11,760 --> 00:12:14,000
用到的推荐算法也是不一样

316
00:12:14,000 --> 00:12:14,720
所以这里面

317
00:12:14,800 --> 00:12:16,720
我们就维护了非常多不同的算法

318
00:12:16,720 --> 00:12:18,560
非常多不同的副本

319
00:12:18,560 --> 00:12:21,000
而且很多用户大量涌入一个入口的时候

320
00:12:21,240 --> 00:12:23,760
我们怎么去解决这些负载的问题

321
00:12:23,800 --> 00:12:26,040
我们怎么解决我们后台服务器的问题

322
00:12:26,040 --> 00:12:28,720
这些都要求我们有非常高的扩展性

323
00:12:32,160 --> 00:12:34,480
接下来我们更多的是去理解一下

324
00:12:34,480 --> 00:12:37,560
或者了解一下推理系统和推理引擎的区别

325
00:12:38,160 --> 00:12:41,400
下面推理系统的一个简图

326
00:12:41,560 --> 00:12:42,160
丑了一点

327
00:12:42,160 --> 00:12:43,280
大家不要介意

328
00:12:43,280 --> 00:12:45,160
推理系统我们第一个

329
00:12:45,160 --> 00:12:50,880
很多时候需要去响应和请求一些服务的处理

330
00:12:50,880 --> 00:12:52,440
接着有了这些处理之后

331
00:12:52,560 --> 00:12:55,520
我们在系统里面需要做一些调度的队列

332
00:12:55,520 --> 00:12:57,000
和调度的排布

333
00:12:57,680 --> 00:12:58,800
有了调度的排布之后

334
00:12:58,920 --> 00:13:00,040
真正在推理了

335
00:13:00,040 --> 00:13:01,800
我们需要去执行我们的算法

336
00:13:01,800 --> 00:13:04,440
我们会有一个推理引擎去执行的

337
00:13:04,440 --> 00:13:06,720
而推理引擎它需要有另外一个输入

338
00:13:06,720 --> 00:13:08,080
就是我们的模型

339
00:13:08,080 --> 00:13:09,760
而我们推理系统很重要的

340
00:13:09,760 --> 00:13:11,080
就是对我们的模型的系统

341
00:13:11,160 --> 00:13:11,880
模型的版本

342
00:13:12,240 --> 00:13:14,720
模型的算法进行管理

343
00:13:14,720 --> 00:13:18,240
而这个模型更多是来自于我们训练的一个流水线

344
00:13:18,240 --> 00:13:20,520
和模型库里面去获取的

345
00:13:20,720 --> 00:13:23,480
另外一个在最后或者比较全面的

346
00:13:23,480 --> 00:13:27,760
我们需要对整个过程进行监控和调度

347
00:13:27,760 --> 00:13:28,840
还有分发

348
00:13:28,840 --> 00:13:32,600
这个时候监控模块就显得非常的重要了

349
00:13:32,600 --> 00:13:34,400
这个就是整个推理系统了

350
00:13:34,400 --> 00:13:36,600
而推理系统可能会跑在一些GPU

351
00:13:36,600 --> 00:13:39,520
CPU、NPU各种不同的PU上面

352
00:13:41,120 --> 00:13:43,680
而推理系统我们需要考虑那些问题

353
00:13:43,680 --> 00:13:45,760
考虑的问题可能还真有点多

354
00:13:45,760 --> 00:13:47,560
第一个就是我们刚才提到的

355
00:13:47,560 --> 00:13:49,760
怎么实现低延迟高吞吐

356
00:13:50,280 --> 00:13:52,440
怎么分配我们的调度请求

357
00:13:52,720 --> 00:13:53,760
我们的调度算法

358
00:13:54,200 --> 00:13:56,720
怎么去管理我们整个AI的生命周期

359
00:13:57,200 --> 00:14:00,280
我们的模型怎么去管理的各种版本

360
00:14:02,200 --> 00:14:06,160
接下来我们看一下推理引擎的一个架构图

361
00:14:06,280 --> 00:14:10,600
这个架构图我综合了非常多的一些推理框架

362
00:14:10,600 --> 00:14:12,880
去或者推理引擎去汇总的

363
00:14:12,880 --> 00:14:15,880
现在看上去有点那个花花绿绿的

364
00:14:15,880 --> 00:14:16,760
大家不要介意

365
00:14:16,880 --> 00:14:20,080
就我们将就着来看我们需要做哪些内容

366
00:14:20,080 --> 00:14:22,960
首先推理引擎我们有一些API的接口

367
00:14:22,960 --> 00:14:25,040
面向用户会提供API的接口

368
00:14:25,040 --> 00:14:28,480
接着我们需要把不同的AI框架去做一个转换

369
00:14:28,480 --> 00:14:30,680
转换成我们AI引擎

370
00:14:30,680 --> 00:14:31,880
就我们上一步

371
00:14:32,720 --> 00:14:35,680
在推理系统我们推理引擎能够去执行的

372
00:14:35,680 --> 00:14:38,360
具体的模型就自己的screen

373
00:14:38,360 --> 00:14:40,440
然后我们去做一个runtime

374
00:14:40,440 --> 00:14:44,200
那runtime就真正的跑起来的一个调度的服务

375
00:14:44,200 --> 00:14:45,400
真正跑起来之后

376
00:14:45,800 --> 00:14:47,840
它其实是依赖于我们的kernel

377
00:14:47,840 --> 00:14:50,960
就是我们的算子层去实现的

378
00:14:50,960 --> 00:14:55,080
而算子层其实是跑在不同的一些具体的设备上面了

379
00:14:55,080 --> 00:14:58,120
那这个时候我们推理系统需要考虑哪些问题呢

380
00:14:59,360 --> 00:15:02,800
第一个就是我们怎么去保持我们在训练的时候

381
00:15:02,800 --> 00:15:04,360
训练的时候精度越高越好

382
00:15:04,840 --> 00:15:07,840
但是我们在推理的时候希望减少模型的尺寸

383
00:15:07,840 --> 00:15:10,080
那模型越小肯定是越好

384
00:15:10,080 --> 00:15:11,880
但是精度怎么维持呢

385
00:15:11,880 --> 00:15:14,600
第二个就是怎么对不同的AI框架

386
00:15:14,600 --> 00:15:18,440
训练出来的结果或者训练的模型进行转换呢

387
00:15:19,000 --> 00:15:21,800
如何加快我们整体的调度和执行

388
00:15:21,800 --> 00:15:23,400
因为当我们的AI系统

389
00:15:23,400 --> 00:15:26,720
我们需要对很多资源内存进行调度的

390
00:15:26,720 --> 00:15:29,000
特别是在一台手机设备上面

391
00:15:29,000 --> 00:15:32,320
一台SoC里面就包括DPU NPU GPU

392
00:15:32,320 --> 00:15:35,760
还有我们的SPU或者NPU各种PU了

393
00:15:35,760 --> 00:15:37,400
就集成一块SoC

394
00:15:37,400 --> 00:15:39,200
这个时候面对这么多资源

395
00:15:39,200 --> 00:15:42,000
这么多PU我们怎么加快我们的调度执行

396
00:15:42,600 --> 00:15:45,920
那还有就是我们怎么去提高我们算子的性能

397
00:15:45,920 --> 00:15:48,520
因为在手机设备呢

398
00:15:48,520 --> 00:15:51,600
例如那个ARM可能会提供一些NEO的指令集

399
00:15:51,600 --> 00:15:54,600
在X86可能会提供一些AVX的指令集

400
00:15:54,600 --> 00:15:58,760
那一台手机设备上面面向不同的设备

401
00:15:58,760 --> 00:16:01,240
我们具有不同的那个Vulkan Metal

402
00:16:01,240 --> 00:16:03,400
或者OpenCL的使用的方式

403
00:16:03,400 --> 00:16:04,840
可能面向专用的设备

404
00:16:04,840 --> 00:16:07,520
我们有TIC TVM不同的Kernel

405
00:16:07,520 --> 00:16:09,200
或者我们的编程的体系

406
00:16:09,200 --> 00:16:13,400
另外我们可能会有一些单独提供的高性能的算子库

407
00:16:13,400 --> 00:16:15,640
那这个时候面向这么多不同的算子

408
00:16:15,640 --> 00:16:20,720
我们什么是提高整个算子的性能和综合的一个压力呢

409
00:16:22,240 --> 00:16:25,160
今天我们主要给大家去汇报了一下

410
00:16:25,160 --> 00:16:26,560
整个推理引擎

411
00:16:26,560 --> 00:16:29,960
我们有哪些服务为什么要去做推理系统或者推理引擎

412
00:16:29,960 --> 00:16:33,160
里面是因为我们遇到了大量的AI的算法

413
00:16:33,160 --> 00:16:34,160
新的服务

414
00:16:34,160 --> 00:16:37,960
现在很多传统的算法都转为AI的算法去代替了

415
00:16:38,000 --> 00:16:41,000
接着我们去探讨了一个训练和推理服务

416
00:16:41,000 --> 00:16:42,120
有什么不一样

417
00:16:42,120 --> 00:16:44,640
到底训练和推理侧重点是啥

418
00:16:44,640 --> 00:16:47,200
接着我们去系统的回顾了一下

419
00:16:47,200 --> 00:16:48,640
什么是推理系统

420
00:16:48,640 --> 00:16:53,120
推理系统的一个具体的约束和优化的目标到底是什么

421
00:16:53,120 --> 00:16:54,200
卷的不行了

422
00:16:54,200 --> 00:16:55,080
卷的不行了

423
00:16:55,080 --> 00:16:56,880
记得一键三连加关注哦

424
00:16:56,880 --> 00:17:00,520
所有的内容都会开源在下面这条链接里面

425
00:17:00,520 --> 00:17:01,240
拜了个拜

