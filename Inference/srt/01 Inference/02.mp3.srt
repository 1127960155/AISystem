0
0:00:00.000 --> 0:00:08.200
Hello 大家好,我是周米

1
0:00:08.200 --> 0:00:10.600
今天我们来到推理系统系列里面

2
0:00:10.600 --> 0:00:13.080
去真正的了解一下什幺是推理系统

3
0:00:13.080 --> 0:00:15.640
那推理系统它不是推理引擎

4
0:00:15.640 --> 0:00:17.400
所以我们可以区分下来

5
0:00:17.400 --> 0:00:21.640
我们接下来的内容主要是聚焦于推理系统的整体的介绍

6
0:00:21.640 --> 0:00:25.120
今天我要去给大家分享的内容可能会稍微多了一点

7
0:00:25.120 --> 0:00:29.240
但是我尽量的简洁一点去给大家汇报一下相关的内容

8
0:00:30.880 --> 0:00:34.520
第一个就是我们去了解一下训练和推理到底有什幺区别

9
0:00:34.520 --> 0:00:37.200
接着我们看看什幺是推理系统

10
0:00:37.200 --> 0:00:41.160
第三点就是去看一下推理系统的优化目标和约束

11
0:00:41.160 --> 0:00:43.960
这些更多的是一些宏观上的概念

12
0:00:43.960 --> 0:00:48.600
最后我们看看推理系统和推理引擎到底要研究哪些内容

13
0:00:48.600 --> 0:00:53.520
其实推理系统对于周米来说应该是最熟悉的一部分了

14
0:00:53.520 --> 0:00:56.280
这个也是我之前在刚进入华为的时候

15
0:00:56.280 --> 0:01:01.560
做了大量非常底层相关的一些代码的研发或者代码的开发工作

16
0:01:03.560 --> 0:01:06.080
下面我们看一下整体的声明周期

17
0:01:06.080 --> 0:01:07.680
首先我们很简单

18
0:01:07.680 --> 0:01:11.640
其实大家应该搞 AI 或者做算法的人特别熟

19
0:01:11.640 --> 0:01:14.880
可能做系统的人不是说非常熟

20
0:01:14.880 --> 0:01:17.720
所以我这里面简单的去给大家汇报一下

21
0:01:17.720 --> 0:01:21.320
首先我们可能会收集非常多的一些训练的数据

22
0:01:21.320 --> 0:01:22.560
公开的数据机也好

23
0:01:22.560 --> 0:01:23.840
自有的数据机也好

24
0:01:23.880 --> 0:01:27.600
然后我们就会对我们的神经网络模型进行训练

25
0:01:27.600 --> 0:01:31.320
训练完之后我们就得到一个固定化的网络模型

26
0:01:31.320 --> 0:01:34.600
这个模型我们就真正的在部署的阶段了

27
0:01:34.600 --> 0:01:36.800
我们就会在云端做一些服务的请求

28
0:01:36.800 --> 0:01:38.440
还有服务的响应

29
0:01:38.440 --> 0:01:39.840
而在我们的推理引擎

30
0:01:39.840 --> 0:01:43.320
更多是聚焦于我们的这一个深度学习的模型

31
0:01:43.320 --> 0:01:44.800
怎幺跑起来更快

32
0:01:44.800 --> 0:01:50.160
而推理系统整个系统需要把我们端到端的流程串起来

33
0:01:50.160 --> 0:01:52.040
把我们的服务化把它做好

34
0:01:52.160 --> 0:01:53.680
这个就是最大的区别

35
0:01:54.680 --> 0:01:58.880
现在我们去区分一下训练任务和推理任务之间的一个最大的区别

36
0:01:58.880 --> 0:02:02.040
训练任务更多的是我们在训练的时候

37
0:02:02.040 --> 0:02:04.960
我们会采用中心化的一个训练

38
0:02:04.960 --> 0:02:08.400
一般来说我们训练就需要非常多的时间了

39
0:02:08.400 --> 0:02:11.520
就把我们的数据去跟我们的算法匹配起来

40
0:02:11.520 --> 0:02:13.280
而且需要比较大的数据

41
0:02:13.280 --> 0:02:18.960
然后这个时候对我们中心服务器的吞吐量还是比较要求高的

42
0:02:19.000 --> 0:02:22.920
而我们训练的模型的精度和准确率也是比较高的

43
0:02:22.920 --> 0:02:26.120
就尽可能模型的精度和性能比较好

44
0:02:27.880 --> 0:02:30.360
而推理基本上我去想想

45
0:02:31.360 --> 0:02:34.240
HMS Core里面的推理引擎用的是Mathball

46
0:02:34.240 --> 0:02:37.040
然后它基本上就是因为我们要服务全球

47
0:02:37.040 --> 0:02:40.160
所以我们会7×24小时的去服务

48
0:02:40.160 --> 0:02:44.760
而且我们每天的调用量可能就超过5亿次

49
0:02:44.760 --> 0:02:48.560
每天调用量超过5亿次的服务响应请求

50
0:02:48.560 --> 0:02:52.880
这个时候我们整个推理的任务的要求是非常高的

51
0:02:52.880 --> 0:02:56.960
而且模型一般稳定收敛的情况下不会重新训练

52
0:02:56.960 --> 0:02:59.760
而且推一个新的模型是非常的谨慎的

53
0:02:59.760 --> 0:03:02.000
因为服务压力请求是非常大的

54
0:03:02.000 --> 0:03:07.040
这个就是深度学习里面生命周期训练和推理的最大的区别

55
0:03:08.800 --> 0:03:12.240
现在我们看看他们具体的遇到的一些挑战

56
0:03:12.240 --> 0:03:15.320
或者他们的不同的特点

57
0:03:15.360 --> 0:03:17.720
上面这个就是训练场景的

58
0:03:17.720 --> 0:03:19.800
下面就是推理场景的

59
0:03:20.840 --> 0:03:23.560
训练场景我们会用非常多的数据

60
0:03:23.560 --> 0:03:24.920
就我们的BitBatchSize

61
0:03:24.920 --> 0:03:27.360
然后在我们的云服务器上面去跑

62
0:03:27.360 --> 0:03:30.160
这个时候我们会做一些前向的推理

63
0:03:30.160 --> 0:03:31.040
反向的传播

64
0:03:31.040 --> 0:03:32.200
然后去更新

65
0:03:32.200 --> 0:03:34.720
不断的去训练我们整个网络模型

66
0:03:34.720 --> 0:03:38.040
使得我们网络模型的精度肯定是越高越好

67
0:03:38.800 --> 0:03:39.960
在训练完之后

68
0:03:40.120 --> 0:03:43.000
我们就会把网络模型固定化下来

69
0:03:43.520 --> 0:03:45.960
给到我们的IoT设备或者Web Service

70
0:03:45.960 --> 0:03:47.640
去做一个推理服务的

71
0:03:48.040 --> 0:03:50.360
而推理服务我们需要的数据量

72
0:03:50.880 --> 0:03:52.880
不会像我们训练这幺大

73
0:03:52.880 --> 0:03:56.880
这个时候使用的更多的是真实的数据场景

74
0:03:56.880 --> 0:03:58.480
然后去部署起来

75
0:03:58.480 --> 0:04:01.520
真正的做一个简单的前向的推理

76
0:04:01.520 --> 0:04:03.000
然后预测到结果

77
0:04:03.200 --> 0:04:05.520
这个结果就会返回给我们的用户

78
0:04:05.520 --> 0:04:08.000
或者返回给我们的Web服务器请求

79
0:04:08.000 --> 0:04:10.160
做一些传统的工作

80
0:04:10.360 --> 0:04:13.280
这个就是他们最大的一些区别

81
0:04:14.480 --> 0:04:16.480
现在我们看一些遇到的一些挑战

82
0:04:16.480 --> 0:04:18.920
就是我们的模型在推理的时候

83
0:04:18.920 --> 0:04:20.440
需要长期的运行

84
0:04:20.440 --> 0:04:23.600
而且推理会有更加苛刻的资源的要求

85
0:04:23.600 --> 0:04:24.760
因为我们IoT设备

86
0:04:25.160 --> 0:04:27.360
它的计算资源是非常有限的

87
0:04:27.360 --> 0:04:30.080
而且还是不能做一个反向传播

88
0:04:30.080 --> 0:04:32.040
就基本上我们不用学习

89
0:04:32.040 --> 0:04:33.440
做个推理就好了

90
0:04:33.440 --> 0:04:35.480
而且部署的型号非常多

91
0:04:36.440 --> 0:04:38.280
可能对于我一个应用来说

92
0:04:38.280 --> 0:04:39.640
用户看到的是一个应用

93
0:04:39.640 --> 0:04:40.880
但是在厂商来说

94
0:04:40.880 --> 0:04:44.440
我看到有100台不同的手机

95
0:04:44.440 --> 0:04:47.320
100台不同的设备去做一个服务的

96
0:04:47.320 --> 0:04:49.240
所以对于要求还是非常高的

97
0:04:51.840 --> 0:04:54.160
现在我们来看一下推理系统

98
0:04:54.160 --> 0:04:55.200
就是整个推理系统

99
0:04:55.200 --> 0:04:56.440
我们要做哪些内容呢

100
0:04:56.440 --> 0:04:58.000
就是首先我们推理系统

101
0:04:58.000 --> 0:05:01.240
最重要的就是管理我们的网络模型

102
0:05:01.240 --> 0:05:02.360
因为我们训练出来了

103
0:05:02.360 --> 0:05:04.240
会有非常多的网络模型

104
0:05:04.240 --> 0:05:06.840
而这些模型就是我们对应的算法

105
0:05:06.840 --> 0:05:08.880
我们要真正的部署给用户的

106
0:05:08.920 --> 0:05:10.760
而整个推理的服务系统里面

107
0:05:10.840 --> 0:05:11.800
我们做很多任务作

108
0:05:12.000 --> 0:05:13.720
第一个就是模型的加载了

109
0:05:13.720 --> 0:05:15.160
模型的版本管理了

110
0:05:15.160 --> 0:05:16.360
还有数据的管理了

111
0:05:16.360 --> 0:05:17.880
还有服务的接口

112
0:05:17.880 --> 0:05:20.920
所以基本上它跟AI训练

113
0:05:20.920 --> 0:05:22.600
或者AI算法不相关

114
0:05:22.600 --> 0:05:25.000
更多的是平台性的工作

115
0:05:25.000 --> 0:05:26.840
然后最后我们会去除一些

116
0:05:27.200 --> 0:05:30.000
服务器端和客户端的一些请求和响应

117
0:05:30.000 --> 0:05:32.720
完成整个端到端的功能

118
0:05:32.720 --> 0:05:34.880
这个是推理系统要做的工作

119
0:05:36.560 --> 0:05:37.840
而所谓的推理系统

120
0:05:38.000 --> 0:05:40.920
其实不仅仅是以数据中心

121
0:05:40.920 --> 0:05:42.120
为一个服务

122
0:05:42.120 --> 0:05:44.080
作为一个主要的方式

123
0:05:44.080 --> 0:05:45.640
而且它要兼顾边缘的

124
0:05:45.640 --> 0:05:47.000
移动设备的场景

125
0:05:48.000 --> 0:05:51.120
而我们提到的整个推理服务的策略

126
0:05:52.560 --> 0:05:53.760
不仅仅需要考虑到

127
0:05:53.760 --> 0:05:55.080
数据中心的一个推理

128
0:05:55.080 --> 0:05:57.200
还要考虑到边缘设备的推理

129
0:05:57.200 --> 0:05:59.240
所以推理系统还是很复杂的

130
0:05:59.240 --> 0:06:01.160
它是一个非常复杂的系统工程

131
0:06:01.160 --> 0:06:03.400
下面我们提几个问题

132
0:06:04.240 --> 0:06:06.720
就是深度学习的推理系统

133
0:06:06.800 --> 0:06:08.480
我们要做设计的时候

134
0:06:08.480 --> 0:06:11.160
一般要考虑哪些问题

135
0:06:12.320 --> 0:06:13.680
因为考虑的这些问题

136
0:06:13.680 --> 0:06:16.200
可能会影响我们后面的架构的设计

137
0:06:16.200 --> 0:06:18.040
还有我们的技术的方案

138
0:06:19.080 --> 0:06:21.640
第二点就是推理系统

139
0:06:21.640 --> 0:06:23.680
跟传统的服务系统

140
0:06:23.680 --> 0:06:25.640
有哪些新的挑战

141
0:06:26.640 --> 0:06:28.040
大家可以一起回顾一下

142
0:06:28.040 --> 0:06:32.080
我们刚才所讨论所探讨的一些内容

143
0:06:33.480 --> 0:06:35.280
最后一个就是云测

144
0:06:35.280 --> 0:06:36.640
就是我们的中心服务器

145
0:06:36.640 --> 0:06:38.720
还有端测到我们的手机

146
0:06:38.720 --> 0:06:40.120
或者IoT设备上面

147
0:06:40.120 --> 0:06:44.000
整个推理系统的服务有什幺不同

148
0:06:44.000 --> 0:06:46.080
有什幺各自的侧重点

149
0:06:46.080 --> 0:06:48.600
我觉得大家可以停下来几分钟

150
0:06:48.600 --> 0:06:51.200
去思考一下具体是怎幺做的

151
0:06:51.200 --> 0:06:53.200
因为这个对我们的架构的设计

152
0:06:53.200 --> 0:06:55.000
有非常大的一个挑战

153
0:06:55.000 --> 0:06:56.720
和不同的技术选行

154
0:06:59.640 --> 0:07:02.120
接下来我会去给大家汇报一下

155
0:07:02.120 --> 0:07:04.800
推理系统的一个优化的目标和约束

156
0:07:04.920 --> 0:07:06.840
现在这个更多的是一些

157
0:07:07.720 --> 0:07:08.720
宏观的概念

158
0:07:08.840 --> 0:07:11.320
大家其实可以不用太在乎

159
0:07:11.320 --> 0:07:12.640
或者听一听就完了

160
0:07:14.440 --> 0:07:17.680
我们现在还是以淘宝作为例子

161
0:07:17.680 --> 0:07:19.760
就是包括我们的在线新闻

162
0:07:19.760 --> 0:07:20.560
推理这些

163
0:07:20.560 --> 0:07:22.200
都是推荐一些

164
0:07:22.200 --> 0:07:24.040
我们比较喜欢的一些服务

165
0:07:24.040 --> 0:07:25.320
包括抖音它的推荐

166
0:07:25.320 --> 0:07:26.680
其实也是相同的

167
0:07:26.680 --> 0:07:28.480
我们可能要考虑到低延迟

168
0:07:28.680 --> 0:07:30.120
还有一个高吞吐

169
0:07:30.120 --> 0:07:31.000
还有扩展性

170
0:07:31.000 --> 0:07:32.320
还有准确性的问题

171
0:07:32.320 --> 0:07:33.960
就我们提一点

172
0:07:34.080 --> 0:07:35.040
就是低延迟

173
0:07:35.040 --> 0:07:37.400
我们网络上的文章的推荐

174
0:07:37.400 --> 0:07:39.720
或者我们的产品的推荐

175
0:07:39.720 --> 0:07:41.480
我们希望延迟越少越好

176
0:07:41.480 --> 0:07:43.320
因为我不断的刷刷刷

177
0:07:43.520 --> 0:07:44.840
这个刷刷刷的过程

178
0:07:45.160 --> 0:07:47.960
就要求我们系统不断的响应

179
0:07:48.120 --> 0:07:49.040
有时候我淘宝

180
0:07:49.040 --> 0:07:50.120
或者我经常刷抖音

181
0:07:50.440 --> 0:07:52.040
也是经常刷刷刷

182
0:07:52.040 --> 0:07:53.360
然后看他不满意的

183
0:07:53.360 --> 0:07:54.600
就下一个

184
0:07:55.280 --> 0:07:56.160
它的推荐过程

185
0:07:56.360 --> 0:07:58.640
是在后面不断的去计算

186
0:07:58.640 --> 0:07:59.960
我哪个视频喜欢

187
0:07:59.960 --> 0:08:02.040
哪个视频是不喜欢的

188
0:08:02.080 --> 0:08:04.000
系统为了散发要考虑很多问题

189
0:08:04.240 --> 0:08:05.440
第二个就是

190
0:08:05.480 --> 0:08:06.720
除了我们刚才讲的

191
0:08:06.720 --> 0:08:07.880
一些业务上的问题

192
0:08:07.880 --> 0:08:10.560
我们回到我们真正跟AI内容相关的

193
0:08:10.560 --> 0:08:11.080
就是

194
0:08:11.400 --> 0:08:12.480
我整个推理系统

195
0:08:12.480 --> 0:08:13.840
假设这个是推理系统

196
0:08:13.840 --> 0:08:14.720
我们要考虑到

197
0:08:14.720 --> 0:08:16.080
对接很多不同的

198
0:08:16.080 --> 0:08:18.680
AI框架训练出来的模型

199
0:08:18.720 --> 0:08:20.960
接着我们可能还会考虑到

200
0:08:20.960 --> 0:08:22.600
我要部署在非常多的

201
0:08:22.600 --> 0:08:24.120
不同的硬件上面

202
0:08:24.120 --> 0:08:25.160
而整个推理系统

203
0:08:25.280 --> 0:08:27.120
它有很多模块去组成

204
0:08:27.120 --> 0:08:27.840
不同的模块

205
0:08:28.000 --> 0:08:30.640
又有不同的一个具体的功能

206
0:08:30.680 --> 0:08:32.000
所以说整个推理系统

207
0:08:32.000 --> 0:08:33.880
它是要考虑很多的问题

208
0:08:35.120 --> 0:08:36.160
现在我们看一下

209
0:08:36.160 --> 0:08:38.440
我们在设计推理系统时候

210
0:08:38.440 --> 0:08:40.240
需要考虑的几个问题

211
0:08:40.240 --> 0:08:42.560
也就是刚才我们的一个提问

212
0:08:42.560 --> 0:08:44.000
所对应的回答

213
0:08:44.000 --> 0:08:45.200
第一个就是

214
0:08:45.200 --> 0:08:45.920
低延迟

215
0:08:45.920 --> 0:08:46.680
高存储

216
0:08:46.680 --> 0:08:47.280
高效率

217
0:08:47.280 --> 0:08:47.840
灵活性

218
0:08:47.840 --> 0:08:48.440
扩展性

219
0:08:48.440 --> 0:08:51.560
下面我简单的去展开一下

220
0:08:51.560 --> 0:08:52.640
对应的内容

221
0:08:52.640 --> 0:08:54.120
第一个就是灵活性

222
0:08:54.120 --> 0:08:55.480
就是AI部署

223
0:08:55.840 --> 0:08:57.320
AI服务的部署

224
0:08:57.520 --> 0:08:59.840
其实对于优化和系统维护来说

225
0:08:59.840 --> 0:09:01.840
是比较困难的

226
0:09:01.840 --> 0:09:04.200
因为我们需要对接非常多的框架

227
0:09:04.360 --> 0:09:05.040
我们可以看到

228
0:09:05.040 --> 0:09:06.840
AI框架有非常多

229
0:09:06.840 --> 0:09:08.800
而且我们对应的硬件系统

230
0:09:08.800 --> 0:09:10.040
也是非常的复杂

231
0:09:10.040 --> 0:09:12.560
所以我们要求整个我们的AI系统

232
0:09:12.560 --> 0:09:15.800
是要求它整个灵活性要比较高

233
0:09:15.800 --> 0:09:17.680
这个是对于我们系统开发工程师

234
0:09:17.680 --> 0:09:19.200
或者一些厂商来说

235
0:09:19.200 --> 0:09:22.120
第二个就是整体的灵活性

236
0:09:22.760 --> 0:09:23.800
系统的灵活性

237
0:09:23.920 --> 0:09:25.640
就需要可以支持非常多的

238
0:09:25.640 --> 0:09:27.000
不同的AI的模型

239
0:09:27.240 --> 0:09:29.200
而且AI的框架的版本迭代

240
0:09:29.320 --> 0:09:31.800
这个是对我们的挑战是非常高的

241
0:09:31.800 --> 0:09:34.200
我们要维护非常多的版本

242
0:09:34.200 --> 0:09:36.880
第三个就是跟不同的语言的对接

243
0:09:36.880 --> 0:09:38.920
还有不同的逻辑的应用的结合

244
0:09:38.920 --> 0:09:40.760
因为应用五花八门

245
0:09:40.760 --> 0:09:43.280
还有语言也是非常的多

246
0:09:43.280 --> 0:09:44.760
包括我们部署在iOS

247
0:09:44.760 --> 0:09:46.320
部署在网页上面

248
0:09:46.320 --> 0:09:48.960
部署在我们的安卓手机

249
0:09:48.960 --> 0:09:51.280
都是不同的语言的API的接口

250
0:09:51.280 --> 0:09:52.600
所以说整体来说

251
0:09:53.680 --> 0:09:56.840
可能会需要去做一个开放性的协议

252
0:09:56.840 --> 0:09:58.400
例如onlix这种转换

253
0:09:58.400 --> 0:10:00.560
另外我们接口可能需要进行

254
0:10:00.560 --> 0:10:01.840
一系列的抽象

255
0:10:01.840 --> 0:10:04.600
可能还会做一些容器的一些封装

256
0:10:04.600 --> 0:10:06.280
例如我们经常用的docker

257
0:10:06.280 --> 0:10:08.560
最后可能还会需要一些IPC

258
0:10:08.560 --> 0:10:11.640
就化语言化进程的一些通讯的协议

259
0:10:13.360 --> 0:10:15.520
接着我们看一下延迟

260
0:10:15.520 --> 0:10:17.920
延迟其实是对于在线系统来说

261
0:10:17.920 --> 0:10:20.000
是非常高的要求

262
0:10:20.000 --> 0:10:21.520
就我们刚才举的一个例子

263
0:10:21.520 --> 0:10:22.560
刷抖音的时候

264
0:10:22.560 --> 0:10:24.680
我们需要希望尽可能的

265
0:10:24.680 --> 0:10:26.640
去给我提供一个低延迟

266
0:10:26.760 --> 0:10:29.000
而且需要满足到我们有限的

267
0:10:29.000 --> 0:10:30.040
常委的用户

268
0:10:30.040 --> 0:10:31.040
常委的应用

269
0:10:31.880 --> 0:10:33.040
因为每个用户

270
0:10:33.280 --> 0:10:35.400
千人千面在推进系统的时候

271
0:10:35.400 --> 0:10:37.600
所以很多一些常委的问题

272
0:10:37.600 --> 0:10:39.160
我们需要去解决的

273
0:10:39.160 --> 0:10:42.480
那最后我们还有一些吞吐量的问题

274
0:10:42.480 --> 0:10:44.680
就是我们面向一些传统的

275
0:10:44.680 --> 0:10:45.480
并发的问题

276
0:10:45.480 --> 0:10:46.600
并发的请求

277
0:10:46.600 --> 0:10:49.200
那这个时候怎幺去解决吞吐量

278
0:10:51.080 --> 0:10:52.880
最后还有一个效率

279
0:10:52.880 --> 0:10:55.440
效率其实是对于系统来说

280
0:10:55.440 --> 0:10:56.440
不是说非常重要

281
0:10:56.480 --> 0:10:58.960
但是对于推理引擎来说

282
0:10:59.160 --> 0:11:00.360
就特别的重要了

283
0:11:00.800 --> 0:11:04.440
因为我们的端测的资源是有限的

284
0:11:05.200 --> 0:11:08.560
云测的运算的预算也是有限的

285
0:11:08.560 --> 0:11:10.760
所以我们经常会对网络模型

286
0:11:10.960 --> 0:11:11.920
去做一些压缩

287
0:11:12.320 --> 0:11:14.760
去使用一些高效的AI的推理芯片

288
0:11:15.160 --> 0:11:18.000
而这一块就催生了非常多的一些

289
0:11:18.000 --> 0:11:19.160
第三方的厂商

290
0:11:19.160 --> 0:11:22.480
去做很多推出新的推理芯片

291
0:11:23.960 --> 0:11:26.240
那最后一个还有扩展性

292
0:11:26.360 --> 0:11:29.160
那扩展性就是应对用户的请求

293
0:11:29.160 --> 0:11:30.120
五花八门

294
0:11:30.120 --> 0:11:32.680
然后整个推理系统的吞吐量

295
0:11:32.680 --> 0:11:34.280
是非常的复杂的

296
0:11:35.000 --> 0:11:36.520
这要求我们整个推理系统

297
0:11:36.680 --> 0:11:39.440
需要提供非常强大的推理的吞吐

298
0:11:39.560 --> 0:11:41.560
还有让我们整个的推理系统

299
0:11:41.720 --> 0:11:43.240
更加的可靠

300
0:11:44.440 --> 0:11:44.880
另外的话

301
0:11:44.880 --> 0:11:47.560
我们可能还会做一些平台化的部署

302
0:11:47.560 --> 0:11:48.400
例如Kubernetes

303
0:11:48.400 --> 0:11:50.200
还有Docker相关的部署

304
0:11:50.200 --> 0:11:52.640
使得我们整个系统更加的自动化

305
0:11:52.640 --> 0:11:55.160
而不是员工的去参与很多的维护

306
0:11:55.280 --> 0:11:58.280
因为里面的配置是非常的复杂

307
0:11:58.760 --> 0:12:00.320
我们一个APP里面

308
0:12:00.320 --> 0:12:02.400
就涉及到非常多的算法

309
0:12:02.400 --> 0:12:04.800
例如我们在淘宝页面看到的推荐

310
0:12:04.800 --> 0:12:06.920
跟我们淘宝子页面看到的推荐

311
0:12:06.920 --> 0:12:07.760
是不一样的

312
0:12:07.760 --> 0:12:09.480
你个人搜了一个内容

313
0:12:09.480 --> 0:12:11.760
里面的推荐的内容也是不一样

314
0:12:11.760 --> 0:12:14.000
用到的推荐算法也是不一样

315
0:12:14.000 --> 0:12:14.720
所以这里面

316
0:12:14.800 --> 0:12:16.720
我们就维护了非常多不同的算法

317
0:12:16.720 --> 0:12:18.560
非常多不同的副本

318
0:12:18.560 --> 0:12:21.000
而且很多用户大量涌入一个入口的时候

319
0:12:21.240 --> 0:12:23.760
我们怎幺去解决这些负载的问题

320
0:12:23.800 --> 0:12:26.040
我们怎幺解决我们后台服务器的问题

321
0:12:26.040 --> 0:12:28.720
这些都要求我们有非常高的扩展性

322
0:12:32.160 --> 0:12:34.480
接下来我们更多的是去理解一下

323
0:12:34.480 --> 0:12:37.560
或者了解一下推理系统和推理引擎的区别

324
0:12:38.160 --> 0:12:41.400
下面推理系统的一个简图

325
0:12:41.560 --> 0:12:42.160
丑了一点

326
0:12:42.160 --> 0:12:43.280
大家不要介意

327
0:12:43.280 --> 0:12:45.160
推理系统我们第一个

328
0:12:45.160 --> 0:12:50.880
很多时候需要去响应和请求一些服务的处理

329
0:12:50.880 --> 0:12:52.440
接着有了这些处理之后

330
0:12:52.560 --> 0:12:55.520
我们在系统里面需要做一些调度的对练

331
0:12:55.520 --> 0:12:57.000
和调度的排布

332
0:12:57.680 --> 0:12:58.800
有了调度的排布之后

333
0:12:58.920 --> 0:13:00.040
真正在推理了

334
0:13:00.040 --> 0:13:01.800
我们需要去执行我们的算法

335
0:13:01.800 --> 0:13:04.440
我们会有一个推理引擎去执行的

336
0:13:04.440 --> 0:13:06.720
而推理引擎它需要有另外一个输入

337
0:13:06.720 --> 0:13:08.080
就是我们的模型

338
0:13:08.080 --> 0:13:09.760
而我们推理系统很重要的

339
0:13:09.760 --> 0:13:11.080
就是对我们的模型的系统

340
0:13:11.160 --> 0:13:11.880
模型的版本

341
0:13:12.240 --> 0:13:14.720
模型的算法进行管理

342
0:13:14.720 --> 0:13:18.240
而这个模型更多是来自于我们训练的一个流水线

343
0:13:18.240 --> 0:13:20.520
和模型库里面去获取的

344
0:13:20.720 --> 0:13:23.480
另外一个在最后或者比较全面的

345
0:13:23.480 --> 0:13:27.760
我们需要对整个过程进行监控和调度

346
0:13:27.760 --> 0:13:28.840
还有分发

347
0:13:28.840 --> 0:13:32.600
这个时候监控模块就显得非常的重要了

348
0:13:32.600 --> 0:13:34.400
这个就是整个推理系统了

349
0:13:34.400 --> 0:13:36.600
而推理系统可能会跑在一些GPU

350
0:13:36.600 --> 0:13:39.520
CPU、NPU各种不同的PU上面

351
0:13:41.120 --> 0:13:43.680
而推理系统我们需要考虑那些问题

352
0:13:43.680 --> 0:13:45.760
考虑的问题可能还真有点多

353
0:13:45.760 --> 0:13:47.560
第一个就是我们刚才提到的

354
0:13:47.560 --> 0:13:49.760
怎幺实现低延迟高吞吐

355
0:13:50.280 --> 0:13:52.440
怎幺分配我们的调度请求

356
0:13:52.720 --> 0:13:53.760
我们的调度算法

357
0:13:54.200 --> 0:13:56.720
怎幺去管理我们整个AI的生命周期

358
0:13:57.200 --> 0:14:00.280
我们的模型怎幺去管理的各种版本

359
0:14:02.200 --> 0:14:06.160
接下来我们看一下推理引擎的一个架构图

360
0:14:06.280 --> 0:14:10.600
这个架构图我综合了非常多的一些推理框架

361
0:14:10.600 --> 0:14:12.880
去或者推理引擎去汇总的

362
0:14:12.880 --> 0:14:15.880
现在看上去有点那个花花绿绿的

363
0:14:15.880 --> 0:14:16.760
大家不要介意

364
0:14:16.880 --> 0:14:20.080
就我们就着来看我们需要做哪些内容

365
0:14:20.080 --> 0:14:22.960
首先推理引擎我们有一些API的接口

366
0:14:22.960 --> 0:14:25.040
面向用户会提供API的接口

367
0:14:25.040 --> 0:14:28.480
接着我们需要把不同的AI框架去做一个转换

368
0:14:28.480 --> 0:14:30.680
转换成我们AI引擎

369
0:14:30.680 --> 0:14:31.880
就我们上一步

370
0:14:32.720 --> 0:14:35.680
在推理系统我们推理引擎能够去执行的

371
0:14:35.680 --> 0:14:38.360
具体的模型就自己的screen

372
0:14:38.360 --> 0:14:40.440
然后我们去做一个one time

373
0:14:40.440 --> 0:14:44.200
那one time就真正的跑起来的一个调度的服务

374
0:14:44.200 --> 0:14:45.400
真正跑起来之后

375
0:14:45.800 --> 0:14:47.840
它其实是依赖于我们的kernel

376
0:14:47.840 --> 0:14:50.960
就是我们的算子层去实现的

377
0:14:50.960 --> 0:14:55.080
而算子层其实是跑在不同的一些具体的设备上面了

378
0:14:55.080 --> 0:14:58.120
那这个时候我们推理系统需要考虑哪些问题呢

379
0:14:59.360 --> 0:15:02.800
第一个就是我们怎幺去保持我们在训练的时候

380
0:15:02.800 --> 0:15:04.360
训练的时候精度越高越好

381
0:15:04.840 --> 0:15:07.840
但是我们在推理的时候希望减少模型的尺寸

382
0:15:07.840 --> 0:15:10.080
那模型越小肯定是越好

383
0:15:10.080 --> 0:15:11.880
但是精度怎幺维持呢

384
0:15:11.880 --> 0:15:14.600
第二个就是怎幺对不同的AI框架

385
0:15:14.600 --> 0:15:18.440
训练出来的结果或者训练的模型进行转换呢

386
0:15:19.000 --> 0:15:21.800
如何加快我们整体的调度和执行

387
0:15:21.800 --> 0:15:23.400
因为当我们的AI系统

388
0:15:23.400 --> 0:15:26.720
我们需要对很多资源内存进行调度的

389
0:15:26.720 --> 0:15:29.000
特别是在一台手机设备上面

390
0:15:29.000 --> 0:15:32.320
一台SoC里面就包括DPU NPU GPU

391
0:15:32.320 --> 0:15:35.760
还有我们的SPU或者NPU各种PU了

392
0:15:35.760 --> 0:15:37.400
就集成一块SoC

393
0:15:37.400 --> 0:15:39.200
这个时候面对这幺多资源

394
0:15:39.200 --> 0:15:42.000
这幺多PU我们怎幺加快我们的调度执行

395
0:15:42.600 --> 0:15:45.920
那还有就是我们怎幺去提高我们算子的性能

396
0:15:45.920 --> 0:15:48.520
因为在手机设备呢

397
0:15:48.520 --> 0:15:51.600
例如那个ARM可能会提供一些NEO的指令集

398
0:15:51.600 --> 0:15:54.600
在X86可能会提供一些AVX的指令集

399
0:15:54.600 --> 0:15:58.760
那一台手机设备上面面向不同的设备

400
0:15:58.760 --> 0:16:01.240
我们具有不同的那个Win卡Metal

401
0:16:01.240 --> 0:16:03.400
或者OpenCL的使用的方式

402
0:16:03.400 --> 0:16:04.840
可能面向专用的设备

403
0:16:04.840 --> 0:16:07.520
我们有TIC TVM不同的Kernel

404
0:16:07.520 --> 0:16:09.200
或者我们的编程的体系

405
0:16:09.200 --> 0:16:13.400
另外我们可能会有一些单独提供的高性能的算子库

406
0:16:13.400 --> 0:16:15.640
那这个时候面向这幺多不同的算子

407
0:16:15.640 --> 0:16:20.720
我们什幺是提高整个算子的性能和综合的一个压力呢

408
0:16:22.240 --> 0:16:25.160
今天我们主要给大家去汇报了一下

409
0:16:25.160 --> 0:16:26.560
整个推理引擎

410
0:16:26.560 --> 0:16:29.960
我们有哪些服务为什幺要去做推理系统或者推理引擎

411
0:16:29.960 --> 0:16:33.160
里面是因为我们遇到了大量的AI的算法

412
0:16:33.160 --> 0:16:34.160
新的服务

413
0:16:34.160 --> 0:16:37.960
现在很多传统的算法都转为AI的算法去代替了

414
0:16:38.000 --> 0:16:41.000
接着我们去探讨了一个训练和推理服务

415
0:16:41.000 --> 0:16:42.120
有什幺不一样

416
0:16:42.120 --> 0:16:44.640
到底训练和推理侧重点是啥

417
0:16:44.640 --> 0:16:47.200
接着我们去系统的回顾了一下

418
0:16:47.200 --> 0:16:48.640
什幺是推理系统

419
0:16:48.640 --> 0:16:53.120
推理系统的一个具体的约束和优化的目标到底是什幺

420
0:16:53.120 --> 0:16:54.200
卷的不行了

421
0:16:54.200 --> 0:16:55.080
卷的不行了

422
0:16:55.080 --> 0:16:56.880
记得一键三连加关注哦

423
0:16:56.880 --> 0:17:00.520
所有的内容都会开源在下面这条链接里面

424
0:17:00.520 --> 0:17:01.240
摆了个摆

