0
0:00:00.000 --> 0:00:04.680


1
0:00:06.120 --> 0:00:08.040
哈喽大家好我是ZOMI

2
0:00:08.040 --> 0:00:13.040
今天的我们来到推理系统系列里面去真正地了解一下什么是推理系统

3
0:00:13.040 --> 0:00:15.520
那推理系统它不是推定型

4
0:00:15.520 --> 0:00:17.320
所以我们可以区分下来

5
0:00:17.320 --> 0:00:21.680
我们接下来的内容主要是聚焦于推理系统的整体的介绍

6
0:00:21.680 --> 0:00:25.080
今天我要去给大家分享的内容可能会稍微多了一点

7
0:00:25.080 --> 0:00:29.160
但是呢我尽量的简洁一点去给大家汇报一下相关的内容

8
0:00:30.000 --> 0:00:34.540
第一个呢就是我们去了解一下训练和推理到底有什么区别

9
0:00:34.540 --> 0:00:37.100
接着我们看看什么是推理系统

10
0:00:37.100 --> 0:00:41.100
那第三点呢就是去看一下推理系统的优化目标和约束

11
0:00:41.100 --> 0:00:43.900
那这些呢更多的是一些宏观上的概念

12
0:00:43.900 --> 0:00:48.500
最后我们看看推理系统和推理引擎到底要研究哪些内容

13
0:00:48.500 --> 0:00:53.500
那其实推理系统呢嗯对于终米来说应该是最熟悉的一部分了

14
0:00:53.500 --> 0:00:59.900
这个也是我之前在刚进入华为的时候做了大量非常底层相关的一些代码的研究

15
0:00:59.900 --> 0:01:01.600
研发块代码的开发工作

16
0:01:03.600 --> 0:01:11.700
那下面我们看一下整体的生命周期首先呢我们很简单啦其实这个大家应该搞AI筷子做算法的人特别熟

17
0:01:11.700 --> 0:01:23.900
可能做系统的人呢嗯不是说非常熟所以我这里面呢简单地去给大家汇报一下首先我们可能会收集非常多的一些训练的数据公开的数据集也好自有的数据集也好

18
0:01:23.900 --> 0:01:29.800
然后呢我们就会对我们的神经网络模型进行训练训练完之后呢我们就得到一个固定化的训练

19
0:01:29.800 --> 0:01:38.500
训练化的一个网络模型那这个模型呢我们就真正的在部署的阶段呢我们就会在云端做一些服务的请求还有服务的响应

20
0:01:38.500 --> 0:01:50.300
而在我们的推理引擎呢更多是聚焦于我们的这一个深度学习的模型怎么跑起来更快而推理系统呢整个系统需要把我们端到端的流程串起来

21
0:01:50.300 --> 0:01:54.700
把我们的服务化把它做好那这个就是最大的区别

22
0:01:54.700 --> 0:01:59.100
那现在呢我们去区分一下训练任务和推理任务之间的一个最大的区别

23
0:01:59.100 --> 0:02:05.100
训练任务呢更多的是我们在训练的时候呢我们会采用中心化的一个训练

24
0:02:05.100 --> 0:02:19.000
那一般来说我们训练呢就需要非常多的时间了就把我们的数据去跟我们的算法匹配起来而且需要比较大的数据然后这个时候呢对我们中心服务器的吞吐量呢还是比较要求高的

25
0:02:19.000 --> 0:02:26.300
而我们训练的模型的精度和准确率呢也是比较高了就尽可能模型的精度和性能比较好

26
0:02:27.300 --> 0:02:30.300
而推理呢基本上啊我去想想啊

27
0:02:31.300 --> 0:02:59.700
HMS call里面的推理引擎呢用的是麦斯波然后它基本上就是因为我们要服务全球嘛所以我们会七乘二十四小时的去服务而且我们每天的调用量可能就超过五亿次每天每天调用量超过五亿次的服务响应请求那这个时候呢我们整个推理的任务的要求是非常高的而且模型呢一般稳定收敛的情况下呢不会重新训练而且推一个新的模型呢是非常的谨慎的

28
0:02:59.700 --> 0:03:07.100
因为服务压力请求是非常大的那这个呢就是深度学习里面生命周期训练和推理的最大的区别

29
0:03:08.900 --> 0:03:19.900
现在我们看看他们具体的遇到的一些挑战或者他们的嗯不同的特点那上面这个呢就是训练场景的下面的就是推理场景的

30
0:03:20.900 --> 0:03:27.400
训练场景呢我们会用非常多的数据就我们big batch size然后在我们的云服务器上面去跑

31
0:03:27.400 --> 0:03:38.000
那这个时候呢我我们会做一些前向的推理反向的传播然后去更新不断地去训练我们整个网络模型使得我们网络模型的精度呢肯定是越高越好

32
0:03:38.900 --> 0:03:56.900
在训练完之后呢我们就会把这个网络模型的固定化下来给到我们的IoT设备或者web service去做一个推理服务喔而推理服务呢我们需要的数据量呢不会像我们训练这么大这个时候呢使用的更多的是真实的数据场景

33
0:03:56.900 --> 0:04:13.200
然后呢去部署起来真正地做一个简单的前向的推理然后呢预测到结果那这个结果呢就会返回给我们的用户或者返回给我们的web服务器请求做一些传统的工作那这个呢就是它们最大的一些区别

34
0:04:14.400 --> 0:04:26.800
现在我们看一些遇到的一些挑战就是我们的模型呢在推理的时候需要长期的运行而且推理会有更加苛刻的资源的要求因为我们爱多T设备嘛它的计算资源是非常有用的

35
0:04:27.400 --> 0:04:49.200
而且还是不能做一个反向传播就就基本上我们不用学习做个推理就好了而且部署的型号非常多可能对于我一个应用来说用户看到的是一个应用但是在厂商来说我看到有一百个一百台不同的手机一百台不同的设备去做一个服务的所以对于要求还是非常高的

36
0:04:51.800 --> 0:04:56.400
现在我们来看一下推理系统就是整个推理系统我们要做哪些内容呢

37
0:04:56.400 --> 0:05:25.000
就首先呢我们推理系统最重要的就是管理我们的网络模型因为我们训练出来了会有非常多的网络模型而这些模型呢就是我们对应的算法我们要真正的部署给用户的而整个推理的服务系统里面呢我们做很多工作啊第一个就是模型的加载了模型的版本管理了还有数据的管理了还有服务的接口所以基本上它跟嗯AI训练或者AI算法不相关更多的是平台性的工作

38
0:05:25.000 --> 0:05:34.800
而最后呢我们会去除一些服务器端和客户端的一些请求和响应完成整个端到端的功能这个是推理系统要做的工作

39
0:05:36.600 --> 0:05:47.000
而所谓的推理系统呢其实嗯不仅仅是以数据中心为一个服务作为一个主要的方式而且它要兼顾边缘的移动设备的场景

40
0:05:48.000 --> 0:05:51.600
而我们提到的整个推理服务的策略呢

41
0:05:52.600 --> 0:06:03.200
不仅仅需要考虑到数据中心的一个推理还要考虑到边缘设备的推理所以推理系统还是很复杂的它是一个非常复杂的系统工程下面呢我们提几个问题

42
0:06:04.300 --> 0:06:10.900
就是深度学习的推理系统我们要做设计的时候呢一般要考虑哪些问题

43
0:06:12.300 --> 0:06:17.900
因为考虑的这些问题可能会影响我们后面的架构的设计还有我们的技术的方案

44
0:06:18.900 --> 0:06:25.400
那第二点呢就是推理系统呢跟传统的服务系统呢有哪些新的挑战

45
0:06:26.600 --> 0:06:31.800
大家可以一起回顾一下我们刚才嗯所讨论所探讨的一些内容

46
0:06:33.400 --> 0:06:45.800
最后一个呢就是云测就我们的中心服务器还有端测到我们的手机或者 IoT设备上面整个推理系统的服务有什么不同有什么各自的侧重点

47
0:06:46.200 --> 0:06:51.200
我觉得大家可以停下来几分钟去思考一下具体是怎么做的

48
0:06:51.300 --> 0:06:56.600
因为这个对我们的架构的设计有非常大的一个挑战和不同的技术选行

49
0:06:59.700 --> 0:07:04.500
接下来呢我会去给大家汇报一下推理系统的一个优化的目标和约束

50
0:07:04.900 --> 0:07:12.600
那现在呢这个更多的是一些宏观的概念呢大家其实可以不用不用太在乎或者听听就完了

51
0:07:12.600 --> 0:07:41.500
我们现在呢还是以淘宝这个作为例子就是包括我们的在线新闻啊推理这些都是推荐一些我们比较喜欢的一些服务包括抖音它的推荐其实也是相同的我们可能要考虑到低延迟啦还有一个高吞吐还有扩展性还有准确性的问题就我们提一点吧就是低延迟我们网络上的文章的推荐或者我们的产品的推荐我们希望延迟越少越好

52
0:07:41.500 --> 0:08:11.500
因为我不断地刷刷刷那这个刷刷刷的过程呢就要求我们系统不断地响应了有时候我淘宝或者我经常刷抖音啊也是经常刷刷刷然后看的不满意的就就下一个它的推荐过程呢是在后面不断地去计算我哪个视频喜欢哪个系视频是不喜欢的系统为了散发要考虑很多问题那第二个呢就是除了我们刚才讲的一些业务上的问题我们回到我们真正跟AI类型相关的就是

53
0:08:11.500 --> 0:08:41.500
我整个推理系统假设这个是推理系统我们要考虑到对接很多不同的AI框架训练出来的模型接着呢我们可能还会考虑到我要部署在非常多的不同的硬件上面而整个推理系统呢它有很多模块去组成不同的模块呢拥有不同的一个具体的功能所以说整个推理系统它是要考虑很多的问题现在我们看一下我们在设计推理系统时候需要考虑的几个问题也就是刚才我们的一个

54
0:08:41.500 --> 0:09:11.500
提问所对应的回答这个就是低延迟高存储高效率灵活性扩展性下面呢我简单地去展开一下对应的内容这个就是灵活性就是AI部署AI服务的部署呢其实对于优化和系统维护来说是比较困难的因为我们需要对接非常多的框架嘛我们可以看到AI框架有非常多而且我们对应的硬件系统也是非常的复杂所以我们要求整个我们

55
0:09:11.500 --> 0:09:41.500
的AI系统是要求它整个灵活性要比较高这是对于我们系统裁判工程师或者一些厂商来说第二个呢就是整体的灵活性系统的灵活性呢就需要可以支持非常多的不同的AI的模型啦而且AI的框架的版本迭代这个是对我们的挑战是非常高的我们要维护维护非常多的版本第三个呢就是跟不同的语言的对接还有不同的逻辑的应用的结合因为应用五花八门还有语言

56
0:09:41.500 --> 0:10:11.500
也是非常的多包括我们部署在ios部署在网页上面部署在我们的安卓手机都是不同的语言的API的接口所以说整体来说呢可能会需要去做一个开放性的协议例如only这种转换另外呢我们接口可能需要进行一系列的抽象可能还会做一些容器的一些封装例如我们经常用的docker最后呢可能还会需要一些IPC旧化语言化进程的一些通讯的协议

57
0:10:11.500 --> 0:10:41.500
那接着呢我们看一下延迟延迟这个其实对于在线系统来说是非常高的要求就我们刚才举的一个例子刷抖音的时候我们需要希望就是尽可能地去给我提供一个低延迟而且需要满足到我们有限的常委的用户常委的应用因为每每个用户嘛千人千面在推进系统的时候所以很多一些常委的问题我们需要去解决的那最后呢我们还有一些吞吐

58
0:10:41.500 --> 0:11:11.500
量的问题就是我们面向一些传统的并发的问题并发的请求那这个时候呢怎么去解决吞吐量最后还有一个效率效率其实是对于系统来说不是说非常重要但是对于推理引擎来说呢就特别的重要了因为我们的端测的资源是有限的云测的运算的预算呢也是有限的所以我们经常会对网络模型呢去做一些

59
0:11:11.500 --> 0:11:41.500
压缩呢去使用一些高效的AI的推理芯片呢而这一块呢就催生了非常多的一些第三方的厂商去做很多推出新的推理芯片那最后一个还有扩展性那扩展性就是应应对用户的请求五花八门然后整个推理系统的吞吐量是非常的复杂的这要求我们整个推理系统呢需要提供非常强大的推理的吞吐量还有让我们整个的推理系统

60
0:11:41.500 --> 0:12:11.500
呢更加的可靠另外的话我们可能还会做一些平台化的部署例如科本的还有那个多卡相关的部署使得我们整个系统更加的自动化而不是人工的去参与很多的维护因为里面的配置是非常的复杂我们一个APP里面呢就涉及到非常多的算法例如我们在淘宝页面看到的推荐跟我们淘宝子页面看到的推荐是不一样的你个人搜了一个内容里面的推荐的内容

61
0:12:11.500 --> 0:12:41.500
是不一样用到的推荐算法也是不一样所以这里面呢我们就维护着非常多不同的算法非常多不同的副本而且很多用户大量涌入一个入口的时候呢我们怎么去解决这些负载的问题我们怎么解决我们后台服务器的问题这些呢都要求我们有非常高的扩展性那接下来了我们更要是去理解一下或者了解一下推理系统和推理引擎的区别那下面这个呢就是推理系统的一个简图

62
0:12:41.500 --> 0:13:11.500
丑了一点大家不要介意那推理系统呢我们第一个很多时候需要去响应和请求一些服务的处理接着呢有了这些处理之后呢我们在系统里面需要做一些调度的对列和调度的排布有了调度的排布之后呢真正在推理了我们需要去执行我们的算法我们会有一个推理引擎去执行的而推理引擎呢它需要有另外一个输入就是我们的模型而我们推理系统呢很重要的就是对我们的模型的系统而模型的

63
0:13:11.500 --> 0:13:41.500
版本呀模型的算法进行管理而这个模型呢更多是来自于我们训练的一个流水线和模型库里面去获取的另外一个在最后或者比较全面的我们需要对整个过程进行监控和调度还有分发那这个时候呢这个监控模块就显得非常的重要了这个呢就是整个推理系统了而推理系统呢可能会跑在一些GPU CPU NPU各种不同的PU上面

64
0:13:41.500 --> 0:14:11.500
推理系统呢我们需要考虑哪些问题考虑的问题可能还真有点多那第一个呢就是我们刚才提到的怎么实现低延迟高吞吐呢怎么分配我们的调度请求呢我们的调度算法呢怎么去管理我们整个AI的生命周期呢我们的模型怎么去管理的各种版本接下来呢我们看一下推理引擎的一个架构图啊这个架构图呢我综合了非常多的一些呃推理框架去

65
0:14:11.500 --> 0:14:41.500
引擎去汇总的现在看上去有点那个花花绿绿的大家不要介意就我们呃就着来看我们需要做哪些内容首先呢推理引擎呢我们有一些API的接口面向用户会提供API的接口接着呢我们需要把不同的AI框架呢去做一个转换转换成我们AI引擎就我们上一步在推理系统我们推理引擎能够去执行的具体的模型就自自己的screen嘛然后呢我们去做一个温泰那温泰呢就真正

66
0:14:41.500 --> 0:15:11.500
地跑起来的一个嗯调度的服务真正跑起来之后呢它其实是依赖于我们的kernel就是我们的算子层去实现的而算子层其实是跑在不同的一些具体的设备上面了那这个时候呢我们推理系统需要考虑哪些问题呢第一个就是我们怎么去保持我们在训练的时候训练的时候精度越高越好嘛但是我们在推理的时候希望减少模型的尺寸那模型越小肯定是越好但是精度怎么维持

67
0:15:11.500 --> 0:15:41.500
呢第二个呢就是怎么对不同的AI框架训练出来的结果或者训练的模型进行转换呢如何加快我们整体的调度和执行因为到我们的AI系统我们需要对很多资源内存进行调度的特别是在一台手机设备上面一台soc里面就包括dpu npu gpu还有我们的spu或npu各种各种pu了就集成一块soc这个时候面对这么多资源这么多pu我们怎么加快我们的调度

68
0:15:41.500 --> 0:16:11.500
执行那还有就是我们怎么去提高我们算子的性能因为在手机设备呢例如那个arm可能会提供一些neo的指令集在插拔六可能会提供一些avx的指令集那一台手机设备上面面向不同的设备我们具有不同的那个温卡啊meta啊或者opencl的使用的方式可能面向专用的设备我们有tiktvm不同的kernel或者我们的编程的体系另外了我们可能会有一些单独的

69
0:16:11.500 --> 0:16:20.780
提供的高性能的算子库那这个时候面向这么多不同的算子我们什么是提高整个算子的性能和综合的一个压力呢

70
0:16:20.780 --> 0:16:21.780
今天呢我们主要给大家去汇报了一下整个推理引擎呢我们有哪些服务为什么要去做推理系统和推理引擎里面是因为我们遇到了大量的AI的算法新的服务现在很多传统的算法呢都转为AI的算法去代替了接着呢我们去探讨了一个训练和推理服务有什么不一样到底训练和推理侧重点是啥接着呢我们去系统地回顾了一下什么是推理系统推理系统的一个具体的约束
