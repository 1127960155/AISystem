0
0:00:00.000 --> 0:00:04.620


1
0:00:06.780 --> 0:00:09.660
哈喽大家好我就是ZOMI

2
0:00:09.660 --> 0:00:13.260
上今天的班睡昨天的觉的ZOMi

3
0:00:13.260 --> 0:00:16.740
那新官之后呢其实我的更新频率变得慢了很多

4
0:00:16.740 --> 0:00:19.520
是因为真的是最近特别的累

5
0:00:20.000 --> 0:00:23.620
我们现在呢来到推理系统系列里面的推理引擎

6
0:00:23.620 --> 0:00:25.740
也就是我觉得比较重要的一块

7
0:00:25.740 --> 0:00:28.360
那在推理引擎之里面呢

8
0:00:28.360 --> 0:00:32.920
我主要是介绍整体的架构和推力引擎的工作流程

9
0:00:35.320 --> 0:00:39.840
下面呢我们一起来了解一下整体的架构Architecture

10
0:00:40.240 --> 0:00:43.120
虽然我的英文不太准但是我还是喜欢练两句

11
0:00:43.240 --> 0:00:45.360
大家就就着来听吧

12
0:00:46.760 --> 0:00:52.120
首先呢我们看一下这个呢右边这个图呢就是我们整个推力引擎的整体的架构

13
0:00:52.200 --> 0:00:54.920
那可能每一个推力引擎呢都有不一样的样子

14
0:00:55.040 --> 0:00:57.480
但是呢这里面呢我做了一个整体的抽一下

15
0:00:57.480 --> 0:00:58.880
要去给大家汇报的

16
0:01:00.920 --> 0:01:04.200
在推力引擎架构里面呢则分开两大块啊

17
0:01:04.760 --> 0:01:12.400
第一块呢就是上面呢就我这条黄色网上的蓝色的这一块我们叫做优化的阶段

18
0:01:12.600 --> 0:01:18.840
那在优化阶段呢我们会做非常多的不同的工作第一个呢就是模型的转换呢模型的压缩呢

19
0:01:19.040 --> 0:01:24.520
对外提供一些API的接口呢还可能会提供一些恩benchmark啊APP的demo呢

20
0:01:24.640 --> 0:01:37.880
那后面我们会展开网下呢是我们的运行的阶段就是我们整体的推力引擎真正能干活的一些功能点了包括我们的one time还有kernel两层是最重要的

21
0:01:39.840 --> 0:01:54.360
下面呢我们逐层的去打开看一下第一个呢就是还没到这个我先介绍一下网上的API层不管我们哪个AI框架包括训练也好推理也好我们面向开发者

22
0:01:54.520 --> 0:02:22.160
都需要提供一些API的接口那可能呢我们为了做一些端群统一的时候我们会提供一个派森的API的接口我们对一些云服务呢会提供一些沟的接口对集成一些在相关硬件上面会提供一些C++的接口可能对一些网约上面的服务呢会提供一些js的接口所以API的接口呢是非常的重要每一个推力引擎呢都会有对应的API接口只是多和少的问题

23
0:02:24.520 --> 0:02:54.520
那我们来到正式的第一个内容就是模型转换工具模型转换工具呢就是左边的这一块呀这里面的最重要的就是我们的模型的格式转换把不同的框架的一些格式呢转换到我们自己这个推力引擎的一个IR或者格式我们下面有一层金黄色或者死黄色的一层IR就中间表达或者对应的spring嘛就是为了去对接到自己的一个

24
0:02:54.520 --> 0:03:02.520
格式那另外了我们在转换的时候呢可能会有一些编译器的工作或者编译的功能

25
0:03:02.520 --> 0:03:19.320
我们在之前大家可以翻一下我的那些历史的视频在之前的分享了我们会讲了很多AI编辑里面的一些图优化的工作或者在AI框架里面呢我们会有很多图优化的工作那这些工作呢其实是交叉相互融合的

26
0:03:19.320 --> 0:03:23.320
接着呢我们来到下一个模块就是模型压缩

27
0:03:23.320 --> 0:03:53.320
那模型压缩我们其实讲了它有个很大的问题就是我们的网络模型呢我们希望它的精度呢是越高越好但是模型的大小呢我们希望它越小越好这是不可调和的矛盾于是呢就提供了模型压缩的一个模块这个模块呢会对我们的模型做一些低比德的量化呢对我们的网络模型做一些丢人和踢犬的增留呢然后可能我们会做一些系数的减值最后一个呢是二次换那二次换呢可能会对一些比较学术的前年的研究

28
0:03:53.320 --> 0:03:59.320
做一些相关的工作变成零和一的一些相关的模型

29
0:03:59.320 --> 0:04:29.320
下面呢我们看另外一个网友看看这个时候呢有一些端侧学习那端侧学习呢主要是由两种学习的方式第一种呢就是联邦学习另外一种是增量学习那联邦学习也好增量学习也好我们在系统第三节课推理的方式或者推理的工作流程里面呢去讲的一个具体的学习的方式但是呢其实这两块呀是一个非常大或者非常新的一个前沿的研究它又衍生了自己很多相关的工作

30
0:04:29.320 --> 0:04:59.320
要支持这些相关的工作我们在做推理引擎的时候呢其实虹市这条趋向框啊有相关的工作第一个呢我们可能会提供一些数据的处理呢我们会提供一些trainer呢我们还会提供一些简单的optimizer就是我们的优化企业损失函数去支持我们在端侧做联邦学习和做增量学习的工作而联邦学习呢又分为纵向联邦和横向联邦它们不同的联邦的学习的方式不同的增量学习的方式呢需求的模块是不同的

31
0:04:59.320 --> 0:05:29.320
这里面只是简单地讲了一下增量学习可能我们现在需要一些数据的处理啊trainer啊opt和loss的模块那在往后呢我们就是其他模块了那在其他模块呢我觉得我们现在来说呢生腾里面做得不一定说最好或者我觉得甚至有点缺失的就是我们现在其实很多时候我们做一个推理框架我觉得很重要的就是要有benchmark你要有性能的对比你要有模型的对比

32
0:05:29.320 --> 0:05:59.320
用户或者开发者要知道你到底针对每一个模块到底做得怎么样了你的性能你的效果到底好还是不好大家一看呢就会有一个清晰的认知如果你不做到底是你懒得没做还是因为你的性能不过边好呢第二个呢就是很多端侧的推理引擎呢都会有的针对ios呢可能会提供相关的demo去告诉用户哎这些API呢是怎么去对接的怎么去应用的可能安卓呢也会有

33
0:05:59.320 --> 0:06:29.320
对应的APP的demo那有这些demo了包括现在的tierflight了还是pytorch mobile呢都会有对应的一个demo我觉得是非常好的非常方便我们直接去集成和学习的那接着我们往下看看往下看看呢就是中间表达中间表达了我们为啥会一条使环式的线呢横穿在我们各个模块呢

34
0:06:29.320 --> 0:06:59.320
是因为不管是哪个框架或哪种方式去对接我们在下面正式跑的时候必须要有自己的一个格式和一个序列化的表达那这个时候呢每个推理框架其实都有自己的定义为了提供自己的定义包括我们的图优化我基于哪个图优化呀肯定是基于这个ir自己的一个定义的ir我做压缩减资我在哪个图上面去做呀肯定有自己的定义包括我的端侧学习我在什么一个图上面去做一个

35
0:06:59.320 --> 0:07:29.320
应用呢这些都需要有自己的计算图自己的哎呀自己的中间表达自己的规码去做一个统一的表述才能够更好的进行一些优化转化呀所有的功能那在下面的就是我们真正在整个推理引擎去运行的推理引擎的最重要的功能就是温泰真正的一个执行的模块那执行模块其实我们有很多需要注意的技术点第一个我觉得比较重要的就是动态的

36
0:07:29.320 --> 0:07:59.320
办取还有依购的执行内存的分配可能在那个安上面呢我们会经常强调大小和的调度因为安呢它有分开大和和小和那大小和我们可能会就着来用那大和可能会跑一些主要的APP那有可能主要的APP已经在跑了这个时候呢就占满它大和的一个计算资源可能我们就会做一些小和的调度就真正的把我们的AI呢跑在小和上面它不一定要具体根据我们具体的业务去分配的当然呢

37
0:07:59.320 --> 0:08:29.320
还会有一些多副本的并且还有装箱的功能这些我们在后面都会一一展开最后的一个模块啦我们就会有一些很多高性能的蒜子库或者蒜子区出现这里面呢我们要做很多工作第一个呢就是蒜子六块蒜子执行还有蒜子的调度非常多相关的工作而可能啊我们最后面呢会给大家展示一下我以恩恩为例子啊里面恩恩的代码有一半

38
0:08:29.320 --> 0:08:31.620
都是科诺有一半都是蒜子

39
0:08:31.620 --> 0:08:35.020
而上面这些功能的代码量啊其实占不到一半

40
0:08:35.020 --> 0:08:37.620
就科诺层的代码就占的非常多

41
0:08:37.620 --> 0:08:39.820
为啥会占这么多呢

42
0:08:39.820 --> 0:08:44.120
那其实原因很简单啊我们针对无垦后端可能会提供

43
0:08:44.120 --> 0:08:45.820
一百个蒜子是无垦的

44
0:08:45.820 --> 0:08:50.620
然后呢可能针对OpenCL了我们又会提供一百多个蒜子是OpenCL的

45
0:08:50.620 --> 0:08:54.820
针对Meta呢我们可能还会提供一百多个蒜子是Meta的

46
0:08:54.820 --> 0:08:58.220
就是每个后端我们都会提供一些对应的蒜子

47
0:08:58.220 --> 0:09:01.920
我们可能拟有指令集我们又会提供对应的蒜子

48
0:09:01.920 --> 0:09:05.920
就跑在不同的设备上面跑在手机上面我跑提供一百个蒜子

49
0:09:05.920 --> 0:09:09.320
跑在这个上面我跑着提供一百个蒜子就合起来了

50
0:09:09.320 --> 0:09:11.920
就变得我们的蒜子层就极度的爆炸了

51
0:09:11.920 --> 0:09:15.420
那这个呢其实呃这个矛盾一直是不可调和的

52
0:09:15.420 --> 0:09:18.520
更多的是针对不同的设备我们提供更多的蒜子

53
0:09:18.520 --> 0:09:22.720
那可能呢我们有一些在GPU里面呢可能OpenCL提供几个

54
0:09:22.720 --> 0:09:28.120
无垦提供几个分别的去依购的去调度我们的蒜子也有这种方式

55
0:09:29.120 --> 0:09:33.620
最后呢最后呢最后了就我没有在下一赛里面呈现的

56
0:09:33.620 --> 0:09:40.820
就是里面会跑的非常多的不同的设备我们的推进引擎呢要在不同的设备去执行

57
0:09:42.520 --> 0:09:48.220
那下面我们看看具体的工作流程就我们的推进引擎的工作流程工作流程比较简单

58
0:09:48.220 --> 0:09:53.620
首先呢我们会把各种AI框架漫斯波特搜狐拍到去拍到拍到也好

59
0:09:53.620 --> 0:09:59.120
都会把这些AI框架训练得到的模型呢丢给我们的模型转换工具

60
0:09:59.120 --> 0:10:03.820
然后去做一个转换成为自己的一个推进引擎的格式

61
0:10:03.820 --> 0:10:08.620
那这个推进引擎的格式呢我们叫做推进模型这可能还是个离线模型

62
0:10:08.620 --> 0:10:15.120
接着呢我们会对这个离线模型呢进行一个压缩一般都会压缩不压缩的情况下其实很少的

63
0:10:15.120 --> 0:10:23.220
然后呢压缩完之后呢再做一个环境的准备环境的准备呢我们会做好多大量的配置包括我们大小核的调度啊

64
0:10:23.220 --> 0:10:32.620
模型文件模型文件从哪里拉拉取啊这些我们都是环境的准备准备完这些环境之后呢我们就会做开发和编译推理的程序

65
0:10:32.620 --> 0:10:38.520
就是这个推理程序是真正开发出来的我们的推理引擎呢是在真正执行的时候用的

66
0:10:38.520 --> 0:10:48.720
那推理引擎刚才提供的API呢是在这个阶段去使用的就给用户提供那些API做一些它对应的模块和对应的任务的开发

67
0:10:48.720 --> 0:10:51.920
那这个时候呢开发工程师大部分都是按照这个流程来走

68
0:10:51.920 --> 0:11:00.920
接着呢开发完自己的一个对应的任务之后呢就做一个推理的引擎推理的执行让它真正在温碳里面去跑起来

69
0:11:00.920 --> 0:11:05.120
跑小的时候呢就依赖于输入和输出的结果了

70
0:11:05.120 --> 0:11:17.920
那这个就是在线执行就是我们刚才往下面分开两部分一部分是离线的优化一部分是在线的优化对于我们架构图也是比较清晰的对应下来

71
0:11:17.920 --> 0:11:26.520
接下来我们看一下开发一个推理程序需要执行或者需要做哪些步骤这一面呢我就不一一念了我给大家去看看这个图

72
0:11:26.520 --> 0:11:47.880
一般来说我们会做哪些工作呢开发一个推理程序首先我们会把其他模型转换过来转换到自己的一个screen嘛或者自己的爱啊通过converter就我们的离线转换工具转换过来接着呢转换过来那转换过程当中压缩或者图优化的工作那这里面呢就不再提了转换完之后呢我们就去做一些推理

73
0:11:47.880 --> 0:12:17.880
的配置的管理那这个时候我们会把模型我的模型权重在哪里拉取啊是线上拉取的还是本地托管的我们运行的克诺是大小和的哪个呢我的配置优化的选项就刚才我们问他其实有很多优化的一些方式那这些配置呢我们都通过config和通过批递或样文件的去写写好之后呢我们就需要去调用我们的推理引擎把它做一个具体的对象这个就是推理引擎提供的API接着第四步

74
0:12:17.880 --> 0:12:47.880
我们需要去处理我们的数据我们以安卓手机人脸检测或人脸验mark作为一个例子那这个时候呢我们需要从派破烂就是那个ISP的派破烂我们的摄像头的派破烂去捕获一张图片然后做很多的预处理这些很多工作呢其实不在我们的推理引擎里面我们做完这些工作之后呢就拿到我们的数据推理引擎呢就提供了一个API接口图tensor把我们这些数据把人脸的数据啊或者把从派破烂里面得到的万元的数据呢

75
0:12:47.880 --> 0:13:17.880
注入进去我们的推理引擎的API接着呢推理引擎去执行我们的问就把真正的去调起来问了之后呢我们真正的去执行了最后呢我们会有推理结果推理结果呢可能就会把它转换成为对应的tensor或者转换成为内存可识别的模块最后做一些后处理那后处理呢就是交给我们的APP或者具体的任务然后做一些显示的工作呀然后打点的工作呀或者真正推给我们的服务器去处理那这些很多工作也不在我们的推理引擎

76
0:13:17.880 --> 0:13:47.880
里面这个呢就整体的开发的程序了好了那今天呢我们主要是讲了我们推理引擎的整体的架构主要分开五个部分第一个就是我们的API转换模块压缩模块还有真正的运行时候的优化温探还有我们的空头层有了整个架构的了解之后呢有多少个模块我们就会去看看这些模块之间呢是怎么配合工作的就去看了一下

77
0:13:47.880 --> 0:14:14.210
整个推理引擎的工作流程有了工作流程之后呢我们又带着大家去汇报了一下针对工作流程我们要开发一个具体的应用它应该有哪些步骤它应该怎么去用我们的推理引擎

