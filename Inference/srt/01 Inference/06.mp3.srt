0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴

1
0:00:06.840 --> 0:00:09.640
哈喽大家好,我就是ZOMMIE

2
0:00:09.640 --> 0:00:13.160
上今天的班,睡昨天的觉的ZOMMIE

3
0:00:13.160 --> 0:00:16.760
新官之后呢,其实我的更新频率变得慢了很多

4
0:00:16.760 --> 0:00:19.480
是因为真的是最近特别的累

5
0:00:20.040 --> 0:00:23.680
我们现在来到推理系统系列里面的推理引擎

6
0:00:23.680 --> 0:00:25.800
也就是我觉得比较重要的一块

7
0:00:25.800 --> 0:00:28.320
那在推理引擎这里面呢

8
0:00:28.320 --> 0:00:33.000
我主要是介绍整体的架构和推理引擎的工作流程

9
0:00:35.280 --> 0:00:38.560
下面我们一起来了解一下整体的架构

10
0:00:38.560 --> 0:00:40.040
Architecture

11
0:00:40.040 --> 0:00:41.440
虽然我的英文不太准

12
0:00:41.440 --> 0:00:43.120
但是我还是喜欢练两句

13
0:00:43.120 --> 0:00:45.440
大家就这来听吧

14
0:00:46.680 --> 0:00:49.000
首先我们看一下右边这个图

15
0:00:49.000 --> 0:00:52.160
就是我们整个推理引擎的整体的架构

16
0:00:52.160 --> 0:00:54.920
可能每一个推理引擎都有不一样的样子

17
0:00:54.920 --> 0:00:57.400
但是这里面我做了一个整体的抽象

18
0:00:57.440 --> 0:00:58.840
然后去给大家汇报的

19
0:01:00.920 --> 0:01:03.960
在推理引擎架构里面主要分开两大块

20
0:01:04.760 --> 0:01:06.360
第一块就是上面

21
0:01:06.360 --> 0:01:09.720
就我这条黄线往上的蓝色的这一块

22
0:01:09.720 --> 0:01:12.520
我们叫做优化的阶段

23
0:01:12.520 --> 0:01:16.080
那在优化阶段我们会做非常多的不同的工作

24
0:01:16.080 --> 0:01:17.680
第一个就是模型的转化

25
0:01:17.680 --> 0:01:18.960
模型的压缩

26
0:01:18.960 --> 0:01:20.600
对外提供一些API的接口

27
0:01:20.600 --> 0:01:24.600
还可能会提供一些Benchmark APP的Demo

28
0:01:24.640 --> 0:01:26.160
那后面我们会展开

29
0:01:26.160 --> 0:01:29.760
往下是我们的运行的阶段

30
0:01:29.760 --> 0:01:31.280
就是我们整体的推理引擎

31
0:01:31.280 --> 0:01:33.880
真正能干活的一些功能点

32
0:01:33.880 --> 0:01:34.960
包括我们的OneTime

33
0:01:34.960 --> 0:01:37.880
还有Kernel两层是最重要的

34
0:01:39.760 --> 0:01:42.760
下面我们逐层的去打开看一下

35
0:01:42.760 --> 0:01:45.280
第一个就是还没到这个

36
0:01:45.280 --> 0:01:47.960
我先介绍一下网上的API层

37
0:01:47.960 --> 0:01:50.480
不管我们哪个AI框架

38
0:01:50.480 --> 0:01:51.800
包括训练也好

39
0:01:51.800 --> 0:01:52.600
推理也好

40
0:01:52.720 --> 0:01:54.400
我们面向开发者

41
0:01:54.400 --> 0:01:57.400
都需要提供一些API的接口

42
0:01:57.400 --> 0:02:00.320
那可能我们为了做一些端云统一的时候

43
0:02:00.320 --> 0:02:03.280
我们会提供一个Python的API的接口

44
0:02:03.280 --> 0:02:04.760
我们对于一些云服务

45
0:02:04.760 --> 0:02:06.640
会提供一些Go的接口

46
0:02:06.640 --> 0:02:09.120
对于集成一些在相关硬件上面

47
0:02:09.120 --> 0:02:10.880
会提供一些C++的接口

48
0:02:10.880 --> 0:02:13.120
可能对于一些网页上面的服务

49
0:02:13.120 --> 0:02:14.960
会提供一些JS的接口

50
0:02:14.960 --> 0:02:17.040
所以API的接口是非常的重要

51
0:02:17.040 --> 0:02:18.200
每一个推理引擎

52
0:02:18.200 --> 0:02:20.560
都会有对应的API接口

53
0:02:20.560 --> 0:02:22.240
只是多和少的问题

54
0:02:23.600 --> 0:02:26.760
我们来到正式的第一个内容

55
0:02:26.760 --> 0:02:29.880
就是模型转换工具

56
0:02:30.960 --> 0:02:32.000
模型转换工具

57
0:02:32.000 --> 0:02:33.400
就是左边的这一块

58
0:02:33.560 --> 0:02:34.560
这里面最重要的

59
0:02:34.560 --> 0:02:36.760
就是我们的模型的格式转换

60
0:02:36.760 --> 0:02:38.640
把不同的框架的一些格式

61
0:02:38.840 --> 0:02:40.520
转换到我们自己

62
0:02:40.520 --> 0:02:45.000
推理引擎的一个IR或者格式

63
0:02:45.440 --> 0:02:47.680
就是我们下面有一层金黄色

64
0:02:47.680 --> 0:02:49.600
或者使黄色的一层IR

65
0:02:49.600 --> 0:02:51.480
就中间表达过了对应的Spring

66
0:02:51.680 --> 0:02:55.560
就是为了去对接到自己的一个格式

67
0:02:55.560 --> 0:02:57.760
另外我们在转换的时候

68
0:02:57.880 --> 0:03:00.360
可能会有一些编译器的工作

69
0:03:00.360 --> 0:03:02.040
或者编译的功能

70
0:03:02.600 --> 0:03:03.880
我们在之前

71
0:03:03.880 --> 0:03:04.680
大家可以翻一下

72
0:03:04.680 --> 0:03:06.280
我的那些历史的视频

73
0:03:06.280 --> 0:03:07.160
在之前的分享

74
0:03:07.280 --> 0:03:09.480
我们会讲了很多AI编辑器里面的

75
0:03:09.480 --> 0:03:11.120
一些图优化的工作

76
0:03:11.120 --> 0:03:12.360
或者在AI框架里面

77
0:03:12.440 --> 0:03:14.760
我们会有很多图优化的工作

78
0:03:14.960 --> 0:03:17.640
这些工作其实是交叉相互融合的

79
0:03:18.640 --> 0:03:21.840
接着我们来到下一个模块

80
0:03:21.840 --> 0:03:23.200
就是模型压缩

81
0:03:23.600 --> 0:03:24.920
模型压缩我们其实讲了

82
0:03:24.920 --> 0:03:25.880
它有一个很大的问题

83
0:03:25.880 --> 0:03:26.960
就是我们的网络模型

84
0:03:27.040 --> 0:03:29.320
我们希望它的精度是越高越好

85
0:03:29.320 --> 0:03:30.440
但是模型的大小

86
0:03:30.560 --> 0:03:32.040
我又希望它越小越好

87
0:03:32.040 --> 0:03:33.560
这是不可调和的矛盾

88
0:03:33.560 --> 0:03:36.520
于是就提供了模型压缩的一个模块

89
0:03:36.520 --> 0:03:38.000
这个模块会对我们的模型

90
0:03:38.000 --> 0:03:39.120
做一些dbit的量化

91
0:03:39.480 --> 0:03:40.360
对我们的网络模型

92
0:03:40.360 --> 0:03:42.160
做一些student和teacher的增留

93
0:03:42.560 --> 0:03:45.160
然后可能我们会做一些系数的减值

94
0:03:46.120 --> 0:03:47.920
最后一个是二次化

95
0:03:47.920 --> 0:03:50.600
二次化可能会对应一些

96
0:03:50.600 --> 0:03:53.200
比较学术的前沿的研究

97
0:03:53.200 --> 0:03:54.880
做一些相关的工作

98
0:03:54.880 --> 0:03:57.160
变成0和1的一些相关的模型

99
0:03:59.440 --> 0:04:01.960
下面我们看另外一个网友

100
0:04:01.960 --> 0:04:02.640
看一看

101
0:04:02.640 --> 0:04:05.280
这个时候有一些端测学习

102
0:04:05.480 --> 0:04:08.000
端测学习主要是有两种学习的方式

103
0:04:08.000 --> 0:04:09.800
第一种就是联邦学习

104
0:04:09.800 --> 0:04:11.240
另外一种是增量学习

105
0:04:11.480 --> 0:04:12.680
联邦学习还也好

106
0:04:12.680 --> 0:04:13.760
增量学习也好

107
0:04:13.880 --> 0:04:16.840
我们在AI系统第三节课推理的方式

108
0:04:16.840 --> 0:04:18.160
或者推理的工作流程里面

109
0:04:18.280 --> 0:04:21.320
去讲的一个具体的学习的方式

110
0:04:21.320 --> 0:04:23.960
但是其实这两块是一个非常大

111
0:04:23.960 --> 0:04:26.000
或者非常新的一个前沿的研究

112
0:04:26.000 --> 0:04:28.640
它又衍生了自己很多相关的工作

113
0:04:28.640 --> 0:04:30.600
而要支持这些相关的工作

114
0:04:30.600 --> 0:04:32.080
我们在做推理引擎的时候

115
0:04:32.400 --> 0:04:34.440
其实同是这条虚像框

116
0:04:34.840 --> 0:04:35.840
有相关的工作

117
0:04:35.840 --> 0:04:37.240
第一个我们可能会提供一些

118
0:04:37.240 --> 0:04:37.960
数据的处理

119
0:04:38.280 --> 0:04:39.640
我们会提供一些trainer

120
0:04:39.960 --> 0:04:42.200
我们还会提供一些简单的

121
0:04:42.200 --> 0:04:43.000
optimizer

122
0:04:43.040 --> 0:04:44.600
就是我们的优化器和损失函数

123
0:04:44.600 --> 0:04:46.320
去支持我们在端测

124
0:04:46.320 --> 0:04:49.720
做联邦学习和做增量学习的工作

125
0:04:49.720 --> 0:04:51.160
而联邦学习又分为

126
0:04:51.160 --> 0:04:53.000
纵向联邦和横向联邦

127
0:04:53.000 --> 0:04:55.560
它们不同的联邦的学习的方式

128
0:04:55.560 --> 0:04:57.080
不同的增量学习的方式

129
0:04:57.360 --> 0:04:59.280
需求的模块是不同的

130
0:04:59.280 --> 0:05:01.280
这里面只是简单的讲了一下

131
0:05:01.280 --> 0:05:02.160
增量学习

132
0:05:02.160 --> 0:05:04.440
可能我们现在需要一些数据的处理

133
0:05:04.560 --> 0:05:07.280
trainer opt和loss的模块

134
0:05:07.880 --> 0:05:11.160
再往后我们就是其他模块了

135
0:05:13.600 --> 0:05:14.360
在其他模块

136
0:05:14.480 --> 0:05:16.080
我觉得我们现在来说

137
0:05:16.320 --> 0:05:18.880
生腾里面做的不一定说最好

138
0:05:18.880 --> 0:05:21.040
或者我觉得甚至有点缺失的

139
0:05:21.440 --> 0:05:22.640
我们现在其实很多时候

140
0:05:22.640 --> 0:05:24.040
我们做一个推理框架

141
0:05:24.040 --> 0:05:25.120
我觉得很重要的

142
0:05:25.120 --> 0:05:26.720
就是要有benchmark

143
0:05:26.720 --> 0:05:28.320
你要有性能的对比

144
0:05:28.320 --> 0:05:29.640
你要有模型的对比

145
0:05:29.640 --> 0:05:31.880
用户或者开发者要知道

146
0:05:31.880 --> 0:05:33.720
你到底针对每一个模块

147
0:05:33.720 --> 0:05:35.720
到底做的怎幺样了

148
0:05:35.720 --> 0:05:36.760
你的性能

149
0:05:36.760 --> 0:05:39.000
你的效果到底好还是不好

150
0:05:39.280 --> 0:05:41.440
大家一看就会有一个清晰的认知

151
0:05:41.440 --> 0:05:42.440
如果你不做

152
0:05:42.640 --> 0:05:44.200
到底是你懒得没做

153
0:05:44.200 --> 0:05:46.040
还是因为你的性能不够变好

154
0:05:47.320 --> 0:05:49.880
第二个就是很多端测的推理引擎

155
0:05:50.000 --> 0:05:50.800
都会有的

156
0:05:50.800 --> 0:05:53.800
针对iOS可能会提供相关的demo

157
0:05:53.800 --> 0:05:54.760
去告诉用户

158
0:05:55.120 --> 0:05:56.920
这些API是怎幺去对接的

159
0:05:56.920 --> 0:05:58.080
怎幺去应用的

160
0:05:58.080 --> 0:06:00.760
可能安卓也会有对应的APP的demo

161
0:06:00.760 --> 0:06:01.760
有这些demo

162
0:06:01.840 --> 0:06:03.080
包括现在的TF Lite

163
0:06:03.240 --> 0:06:04.440
还是Pytorch Mobile

164
0:06:04.840 --> 0:06:06.840
都会有对应的一个demo

165
0:06:06.840 --> 0:06:08.040
我觉得是非常好的

166
0:06:09.000 --> 0:06:11.560
非常方便我们直接去集成和学习的

167
0:06:13.720 --> 0:06:16.080
接着我们往下看看

168
0:06:16.440 --> 0:06:18.120
往下看看就是中间表达

169
0:06:18.120 --> 0:06:18.760
中间表达了

170
0:06:18.760 --> 0:06:21.000
我们为啥会一条使唤式的线

171
0:06:21.440 --> 0:06:23.480
横穿在我们各个模块

172
0:06:29.720 --> 0:06:32.240
是因为不管是哪个框架

173
0:06:32.240 --> 0:06:34.000
或者哪种方式去对接

174
0:06:34.000 --> 0:06:36.440
我们在下面正式跑的时候

175
0:06:36.560 --> 0:06:38.280
必须要有自己的格式

176
0:06:38.280 --> 0:06:40.040
和一个串行化的表达

177
0:06:40.840 --> 0:06:42.720
这个时候每个推理框架

178
0:06:42.720 --> 0:06:44.240
其实都有自己的定义

179
0:06:44.240 --> 0:06:45.880
为了提供自己的定义

180
0:06:45.880 --> 0:06:47.080
包括我们的图优化

181
0:06:47.080 --> 0:06:48.120
我基于哪个图优化

182
0:06:48.400 --> 0:06:49.880
肯定是基于IR

183
0:06:49.880 --> 0:06:51.320
自己的定义的IR

184
0:06:51.320 --> 0:06:52.320
我做压缩剪辞

185
0:06:52.320 --> 0:06:53.840
我在哪一个图上面去做

186
0:06:54.120 --> 0:06:55.560
肯定有自己的定义

187
0:06:55.560 --> 0:06:56.600
包括我的端测学习

188
0:06:56.600 --> 0:06:58.280
我在什幺一个图上面

189
0:06:58.280 --> 0:06:59.720
去做一个应用

190
0:07:00.480 --> 0:07:02.360
这些都需要有自己的计算图

191
0:07:02.360 --> 0:07:03.000
自己的IR

192
0:07:03.000 --> 0:07:03.920
自己的中间表达

193
0:07:03.920 --> 0:07:04.840
自己的Screenmark

194
0:07:04.960 --> 0:07:07.480
去做一个统一的表述

195
0:07:07.480 --> 0:07:10.480
才能够更好的进行一些优化转化

196
0:07:10.880 --> 0:07:12.080
所有的功能

197
0:07:14.960 --> 0:07:16.680
在下面就是我们真正

198
0:07:16.680 --> 0:07:18.760
在整个推理引擎去运行的

199
0:07:18.760 --> 0:07:20.720
推理引擎最重要的功能

200
0:07:20.720 --> 0:07:21.640
就是OneTime

201
0:07:21.640 --> 0:07:23.720
真正的一个执行的模块

202
0:07:23.720 --> 0:07:24.440
那执行模块

203
0:07:24.560 --> 0:07:25.720
其实我们有很多

204
0:07:25.720 --> 0:07:27.160
需要注意的技术点

205
0:07:27.160 --> 0:07:28.640
第一个我觉得比较重要的

206
0:07:28.640 --> 0:07:29.680
就是动态的Batch

207
0:07:29.680 --> 0:07:31.440
还有Ego的执行内存的分配

208
0:07:31.960 --> 0:07:33.760
可能在ARM上面

209
0:07:33.880 --> 0:07:35.360
我们会经常强调

210
0:07:35.360 --> 0:07:37.040
大小核的调度

211
0:07:37.040 --> 0:07:39.600
因为ARM它有分开大核和小核

212
0:07:40.120 --> 0:07:42.400
大小核我们可能会就着来用

213
0:07:42.560 --> 0:07:44.480
大可能会跑一些主要的APP

214
0:07:44.680 --> 0:07:47.080
有可能主要的APP已经在跑了

215
0:07:47.080 --> 0:07:49.440
这个时候就占满它大核的一个

216
0:07:49.720 --> 0:07:50.480
计算资源

217
0:07:50.480 --> 0:07:52.640
可能我们就会做一些小核的调度

218
0:07:52.640 --> 0:07:53.960
就真正的把我们的AI

219
0:07:54.040 --> 0:07:55.080
跑在小核上面

220
0:07:55.080 --> 0:07:56.040
它不一定要具体

221
0:07:56.040 --> 0:07:58.240
根据我们具体的业务去分配的

222
0:07:58.560 --> 0:08:00.720
当然我们还会有一些多副本的

223
0:08:00.720 --> 0:08:02.400
并且还有装箱的功能

224
0:08:02.480 --> 0:08:04.920
这些我们在后面都会一一展开

225
0:08:06.920 --> 0:08:08.800
最后的一个模块

226
0:08:09.080 --> 0:08:12.200
我们就会有一些很多高性能的

227
0:08:12.200 --> 0:08:15.320
算子库或者算子去出现

228
0:08:15.520 --> 0:08:16.960
这里面我们要做很多任务作

229
0:08:16.960 --> 0:08:18.040
第一个就是算子流化

230
0:08:18.040 --> 0:08:18.560
算子的执行

231
0:08:18.560 --> 0:08:20.000
还有算子的调度

232
0:08:20.240 --> 0:08:21.440
非常多相关的工作

233
0:08:21.440 --> 0:08:23.720
而可能我们最后面

234
0:08:23.800 --> 0:08:25.360
会给大家展示一下

235
0:08:25.680 --> 0:08:26.960
我以MMN为例子

236
0:08:27.120 --> 0:08:29.800
里面MMN的代码有一半都是

237
0:08:29.800 --> 0:08:31.400
Kernel有一半都是算子

238
0:08:31.680 --> 0:08:33.640
而上面这些功能的代码量

239
0:08:33.760 --> 0:08:34.960
其实占不到一半

240
0:08:35.080 --> 0:08:37.440
就Kernel层的代码就占了非常多

241
0:08:37.960 --> 0:08:39.240
为啥会占这幺多

242
0:08:40.360 --> 0:08:41.680
其实原因很简单

243
0:08:41.800 --> 0:08:43.120
我们针对无卡后端

244
0:08:43.120 --> 0:08:45.800
可能会提供100个算子是无卡的

245
0:08:45.920 --> 0:08:47.920
然后可能针对OpenCL

246
0:08:48.000 --> 0:08:49.680
我们又会提供100多个算子

247
0:08:49.680 --> 0:08:50.600
是OpenCL的

248
0:08:50.720 --> 0:08:51.800
针对Meta

249
0:08:51.960 --> 0:08:53.840
我们可能还会提供100多个算子

250
0:08:53.840 --> 0:08:54.760
是Meta的

251
0:08:55.360 --> 0:08:56.000
每个后端

252
0:08:56.000 --> 0:08:58.280
我们都会提供一些对应的算子

253
0:08:58.280 --> 0:08:59.760
我们可能NEO指令集

254
0:08:59.800 --> 0:09:01.880
我们又会提供对应的算子

255
0:09:01.880 --> 0:09:03.320
就跑在不同的设备上面

256
0:09:03.320 --> 0:09:04.320
跑在手机上面

257
0:09:04.320 --> 0:09:05.920
我提供100个算子

258
0:09:05.920 --> 0:09:06.600
跑在这个上面

259
0:09:06.600 --> 0:09:08.120
我跑的提供100个算子

260
0:09:08.120 --> 0:09:09.200
就合起来了

261
0:09:09.320 --> 0:09:10.400
就变得我们的算子层

262
0:09:10.400 --> 0:09:11.880
就极度的爆炸了

263
0:09:12.080 --> 0:09:13.960
这个其实这个矛盾

264
0:09:13.960 --> 0:09:15.360
一直都是不可调和的

265
0:09:15.360 --> 0:09:16.800
更多的是针对不同的设备

266
0:09:16.800 --> 0:09:18.480
我们提供更多的算子

267
0:09:18.640 --> 0:09:21.240
可能我们有一些在GPU里面

268
0:09:21.360 --> 0:09:22.720
可能OpenCL提供几个

269
0:09:22.720 --> 0:09:23.720
无卡提供几个

270
0:09:23.720 --> 0:09:26.440
分别的去异构的去调度我们的算子

271
0:09:26.440 --> 0:09:28.000
也有这种方式

272
0:09:29.000 --> 0:09:33.520
最后就我没有在下一个slide里面呈现的

273
0:09:33.520 --> 0:09:36.920
就是里面会跑的非常多的不同的设备

274
0:09:36.920 --> 0:09:38.000
我们的推理引擎

275
0:09:38.280 --> 0:09:40.800
要在不同的设备去执行

276
0:09:42.680 --> 0:09:44.920
下面我们来看看具体的工作流程

277
0:09:44.920 --> 0:09:46.760
就我们的推理引擎的工作流程

278
0:09:46.760 --> 0:09:48.240
工作流程比较简单

279
0:09:48.240 --> 0:09:50.680
首先我们会把各种AI框架

280
0:09:50.680 --> 0:09:53.520
MousePod and Soho Paddle也好

281
0:09:53.520 --> 0:09:56.280
都会把这些AI框架训练得到的模型

282
0:09:56.480 --> 0:09:59.000
就给我们的模型转换工具

283
0:09:59.000 --> 0:10:00.600
然后去做一个转换

284
0:10:00.600 --> 0:10:03.680
成为自己的一个推理引擎的格式

285
0:10:04.000 --> 0:10:05.560
这个推理引擎的格式

286
0:10:05.640 --> 0:10:06.680
我们叫做推理模型

287
0:10:06.680 --> 0:10:08.480
这可能还是个离线模型

288
0:10:08.480 --> 0:10:12.160
接着我们会对离线模型进行一个压缩

289
0:10:12.160 --> 0:10:13.120
一般都会压缩

290
0:10:13.120 --> 0:10:15.000
不压缩的情况下其实很少的

291
0:10:15.000 --> 0:10:16.360
然后压缩完之后

292
0:10:16.560 --> 0:10:18.520
再做一个环境的准备

293
0:10:18.520 --> 0:10:19.280
环境的准备

294
0:10:19.360 --> 0:10:21.200
我们会做好多大量的配置

295
0:10:21.200 --> 0:10:22.520
包括我们大小和的调度

296
0:10:23.160 --> 0:10:25.680
模型文档从哪里拉取

297
0:10:25.800 --> 0:10:28.000
这些我们都是环境的准备

298
0:10:28.000 --> 0:10:29.160
准备完这些环境之后

299
0:10:29.320 --> 0:10:32.560
我们就会做开发和编译推理的进程

300
0:10:32.560 --> 0:10:34.920
就是这个推理进程是真正开发出来的

301
0:10:34.920 --> 0:10:36.080
我们的推理引擎

302
0:10:36.240 --> 0:10:38.520
是在真正执行的时候用的

303
0:10:38.720 --> 0:10:40.840
推理引擎刚才提供的API

304
0:10:41.160 --> 0:10:43.320
是在这个阶段去使用的

305
0:10:43.320 --> 0:10:45.200
就给用户提供那些API

306
0:10:45.200 --> 0:10:47.040
做一些它对应的模块

307
0:10:47.040 --> 0:10:48.680
或者对应的任务的开发

308
0:10:48.880 --> 0:10:50.080
这个时候开发工程师

309
0:10:50.080 --> 0:10:51.840
大部分都是按照这个流程来走

310
0:10:52.000 --> 0:10:54.720
接着开发完自己的一个对应的任务之后

311
0:10:54.880 --> 0:10:57.560
就做一个推理的引擎推理的执行

312
0:10:57.560 --> 0:11:00.040
让它真正在温炭里面去跑起来

313
0:11:00.880 --> 0:11:01.560
跑起来的时候

314
0:11:01.720 --> 0:11:04.640
就依赖于输入和输出的结果了

315
0:11:05.320 --> 0:11:07.400
这个就是在线执行

316
0:11:07.400 --> 0:11:10.520
就是我们刚才往下面分开两部分

317
0:11:10.520 --> 0:11:12.120
一部分是离线的优化

318
0:11:12.120 --> 0:11:13.600
一部分是在线的优化

319
0:11:13.600 --> 0:11:15.600
对于我们架构图也是比较清晰的

320
0:11:15.600 --> 0:11:16.440
对应下来

321
0:11:17.920 --> 0:11:20.360
接下来我们看一下开发一个推理进程

322
0:11:20.360 --> 0:11:22.680
需要执行或者需要做哪些步骤

323
0:11:22.800 --> 0:11:24.760
这一面我就不一一念了

324
0:11:24.760 --> 0:11:26.480
我给大家去看看这个图

325
0:11:26.480 --> 0:11:28.360
一般来说我们会做哪些工作

326
0:11:28.480 --> 0:11:29.480
开发一个推理进程

327
0:11:29.480 --> 0:11:32.840
首先我们会把其他模型转换过来

328
0:11:32.840 --> 0:11:34.200
转换到自己的一个screen

329
0:11:34.200 --> 0:11:35.320
或者自己的IR

330
0:11:35.320 --> 0:11:36.720
通过converter

331
0:11:36.720 --> 0:11:39.000
就是我们的离线转换工具转换过来

332
0:11:39.280 --> 0:11:40.360
接着转换过来

333
0:11:40.480 --> 0:11:43.240
转换过程当中压缩或者图优化的工作

334
0:11:43.440 --> 0:11:44.880
这里面就不再提了

335
0:11:44.880 --> 0:11:45.840
转换完之后

336
0:11:46.040 --> 0:11:49.520
我们就去做一些推理的配置的管理

337
0:11:49.720 --> 0:11:51.280
这个时候我们会把模型

338
0:11:51.400 --> 0:11:53.160
我的模型权重在哪里拉取

339
0:11:53.280 --> 0:11:55.080
是线上拉取还是本地托管

340
0:11:55.480 --> 0:11:57.640
我们运行的kernel是大小或者哪个

341
0:11:58.000 --> 0:11:59.480
我的配置优化的选项

342
0:11:59.480 --> 0:12:00.440
就刚才我们问他

343
0:12:00.440 --> 0:12:02.960
其实有很多优化的一些方式

344
0:12:03.160 --> 0:12:03.840
这些配置

345
0:12:04.160 --> 0:12:05.760
我们都通过config

346
0:12:05.760 --> 0:12:08.360
或者通过pb或者样文档的去写

347
0:12:08.360 --> 0:12:09.120
写好之后

348
0:12:09.480 --> 0:12:12.000
我们就需要去调用我们的推理引擎

349
0:12:12.000 --> 0:12:14.280
把它作为一个具体的对象

350
0:12:14.440 --> 0:12:16.440
这个就是推理引擎提供的API

351
0:12:16.440 --> 0:12:17.600
接着第4步

352
0:12:17.600 --> 0:12:19.800
我们需要去处理我们的数据

353
0:12:20.400 --> 0:12:21.280
我们以安卓手机

354
0:12:21.280 --> 0:12:23.080
盐点检测块盐点链mark

355
0:12:23.080 --> 0:12:24.320
作为一个例子

356
0:12:24.600 --> 0:12:26.760
这个时候我们需要从pipeline

357
0:12:26.760 --> 0:12:28.720
就是asp的pipeline

358
0:12:28.720 --> 0:12:30.200
我们的摄像头的pipeline

359
0:12:30.200 --> 0:12:32.000
去捕获一张图片

360
0:12:32.000 --> 0:12:33.880
然后做很多的预处理

361
0:12:34.280 --> 0:12:35.040
这些很多任务作

362
0:12:35.160 --> 0:12:37.000
其实不在我们的推理引擎里面

363
0:12:37.000 --> 0:12:38.320
我们做完这些工作之后

364
0:12:38.440 --> 0:12:39.680
就拿到我们的数据

365
0:12:39.680 --> 0:12:41.480
推理引擎就提供了一个API接口

366
0:12:41.480 --> 0:12:42.120
to tensor

367
0:12:42.120 --> 0:12:43.200
把我们这些数据

368
0:12:43.200 --> 0:12:44.200
把盐点的数据

369
0:12:44.600 --> 0:12:46.440
或者把从pipeline里面得到的

370
0:12:46.440 --> 0:12:47.280
y1的数据

371
0:12:47.480 --> 0:12:50.240
去输入进去我们的推理引擎的API

372
0:12:50.240 --> 0:12:52.440
接着推理引擎去执行我们的问

373
0:12:52.440 --> 0:12:54.160
就把真正的去调起来

374
0:12:54.160 --> 0:12:54.720
问了之后

375
0:12:55.000 --> 0:12:56.680
我们真正的去执行了

376
0:12:56.680 --> 0:12:58.120
最后我们会有推理结果

377
0:12:58.120 --> 0:12:58.640
推理结果

378
0:12:58.760 --> 0:13:01.800
可能就会把它转换成为对应的tensor

379
0:13:01.800 --> 0:13:04.080
或者转换成为内存可识别的模块

380
0:13:04.080 --> 0:13:05.360
最后做一些后处理

381
0:13:05.360 --> 0:13:07.360
那后处理就是交给我们的APP

382
0:13:07.360 --> 0:13:08.760
或者具体的任务

383
0:13:08.760 --> 0:13:10.480
然后做一些显示的工作

384
0:13:10.760 --> 0:13:12.120
然后打点的工作

385
0:13:12.240 --> 0:13:15.560
或者真正推给我们的服务器去处理

386
0:13:15.760 --> 0:13:16.600
这些很多任务作

387
0:13:16.920 --> 0:13:18.520
不在我们的推理引擎里面

388
0:13:18.520 --> 0:13:21.240
这个就是整体的开发的进程了

389
0:13:22.320 --> 0:13:22.920
好了

390
0:13:22.920 --> 0:13:24.920
那今天我们主要是讲了

391
0:13:24.920 --> 0:13:27.280
我们推理引擎的整体的架构

392
0:13:27.280 --> 0:13:29.200
主要分开5个部分

393
0:13:29.200 --> 0:13:30.800
第一个就是我们的API

394
0:13:30.800 --> 0:13:32.280
转换模块压缩模块

395
0:13:32.280 --> 0:13:35.040
还有真正的运行时的优化

396
0:13:35.040 --> 0:13:35.640
one time

397
0:13:35.640 --> 0:13:36.880
还有我们的console层

398
0:13:39.280 --> 0:13:41.600
有了整个架构的了解之后

399
0:13:41.760 --> 0:13:42.840
有多少个模块

400
0:13:42.840 --> 0:13:44.560
我们就会去看看

401
0:13:44.680 --> 0:13:46.840
这些模块之间是怎幺配合工作的

402
0:13:46.840 --> 0:13:47.880
就去看了一下

403
0:13:47.880 --> 0:13:49.840
整个推理引擎的工作流程

404
0:13:49.840 --> 0:13:51.080
有了工作流程之后

405
0:13:51.440 --> 0:13:54.120
我们又带着大家去汇报了一下

406
0:13:54.120 --> 0:13:55.120
针对工作流程

407
0:13:55.120 --> 0:13:57.360
我们要开发一个具体的应用

408
0:13:57.360 --> 0:13:58.920
它应该有哪些步骤

409
0:13:58.920 --> 0:14:02.040
它应该怎幺去用我们的推理引擎

410
0:14:02.040 --> 0:14:04.240
那今天的内容就到这里为止

411
0:14:04.240 --> 0:14:04.800
好了

412
0:14:04.800 --> 0:14:05.520
谢谢各位

413
0:14:05.520 --> 0:14:07.480
卷的不行了

414
0:14:07.480 --> 0:14:08.880
记得一键三连加关注

415
0:14:09.240 --> 0:14:10.640
所有的内容都会开源

416
0:14:10.640 --> 0:14:12.440
在下面这条链接里面

417
0:14:12.840 --> 0:14:13.760
摆了个掰

