0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴

1
0:00:06.360 --> 0:00:07.480
哈喽大家好

2
0:00:07.480 --> 0:00:10.040
我是那个赚钱的时候千辛万苦

3
0:00:10.040 --> 0:00:12.800
花钱的时候稀里糊涂的ZOMBIE

4
0:00:12.800 --> 0:00:15.680
那今天呢我们来到一个比较

5
0:00:15.680 --> 0:00:17.000
比较重要的内容吧

6
0:00:17.000 --> 0:00:18.480
也不能说很重要啊

7
0:00:18.480 --> 0:00:21.520
就是整个推理系统的一个架构

8
0:00:21.520 --> 0:00:25.200
其实我觉得最重要的是我们推理引擎的一个架构

9
0:00:25.200 --> 0:00:26.400
就围绕着这一块

10
0:00:26.400 --> 0:00:28.520
可能是比较多的技术含量的

11
0:00:28.560 --> 0:00:32.720
那推理系统呢更多的是一些传统的一些功能点

12
0:00:32.720 --> 0:00:35.120
然后把AI的一些特性加进去

13
0:00:35.120 --> 0:00:37.680
那没关系我们继续来深入的去看一下

14
0:00:37.680 --> 0:00:39.840
今天呢我们主要是去讲一讲

15
0:00:39.840 --> 0:00:42.440
整个推理系统的具体的架构

16
0:00:42.440 --> 0:00:43.560
那在讲架构之前呢

17
0:00:43.560 --> 0:00:45.040
我们会来讲另外一个内容

18
0:00:45.040 --> 0:00:47.320
就是推理、部署、服务

19
0:00:47.320 --> 0:00:49.520
它三者到底有啥区别呢

20
0:00:49.520 --> 0:00:51.560
它之间是啥关系呢

21
0:00:51.560 --> 0:00:55.640
那接着我们去看一下推理系统的整体的架构

22
0:00:55.640 --> 0:00:56.720
如果有时间

23
0:00:56.760 --> 0:00:58.600
因为我希望能够在每个视频呢

24
0:00:58.600 --> 0:01:00.080
控制在十来分钟

25
0:01:00.080 --> 0:01:02.040
就大家看的不累我讲的不累

26
0:01:02.040 --> 0:01:04.560
然后呢如果有时间或者有剩下的时间呢

27
0:01:04.560 --> 0:01:08.480
我去想去讲讲模型的生命周期的管理

28
0:01:08.480 --> 0:01:10.360
因为模型生命周期的管理呢

29
0:01:10.360 --> 0:01:12.360
是AI属性比较强的

30
0:01:12.360 --> 0:01:14.080
在整个推理系统里面

31
0:01:14.080 --> 0:01:17.000
而推理系统更多的是一些服务的响应啊

32
0:01:17.000 --> 0:01:18.120
技术的编排啊

33
0:01:18.120 --> 0:01:19.800
系统的调度啊这些问题

34
0:01:19.800 --> 0:01:23.600
模型的生命周期呢是跟AI属性比较强相关

35
0:01:23.600 --> 0:01:25.360
所以我想给大家后面呢

36
0:01:25.400 --> 0:01:27.160
有时间去汇报一下的

37
0:01:29.120 --> 0:01:33.280
接下来呢我们回顾一下整个模型的生命周期

38
0:01:33.280 --> 0:01:36.800
我们首先呢需要去收集我们训练的模型

39
0:01:36.800 --> 0:01:37.680
设计完模型之后呢

40
0:01:37.680 --> 0:01:39.680
去给我们深度学习的这个模型呢

41
0:01:39.680 --> 0:01:41.520
去给我们的算法呢去训练

42
0:01:41.520 --> 0:01:45.320
训练完之后呢得到一个具体的AI模型

43
0:01:45.320 --> 0:01:49.360
AI模型呢就可以真正的去用来做服务的请求和服务的响应

44
0:01:49.360 --> 0:01:51.160
而对服务的请求和服务的响应

45
0:01:51.160 --> 0:01:53.040
还有真正的推理和部署呢

46
0:01:53.080 --> 0:01:55.520
这一块就是我们整个AI

47
0:01:55.520 --> 0:01:59.040
AI推理系统呢需要去解决的一些问题

48
0:01:59.040 --> 0:02:00.440
或者AI推理系统呢

49
0:02:00.440 --> 0:02:03.320
它主要关注的点就在这一块了

50
0:02:05.200 --> 0:02:07.640
现在呢我想给大家提一个问题

51
0:02:07.640 --> 0:02:09.560
引起大家去思考了

52
0:02:09.560 --> 0:02:10.840
就是

53
0:02:10.840 --> 0:02:15.120
AI应用部署需要考虑哪些方面呢

54
0:02:15.120 --> 0:02:17.560
我们需要考虑哪些技术点呢

55
0:02:18.960 --> 0:02:20.680
诶 这个问题问得非常好

56
0:02:20.720 --> 0:02:23.120
因为只要我们考虑到这些点

57
0:02:23.120 --> 0:02:26.920
我们才能够很好的去设计我们的整个框架

58
0:02:26.920 --> 0:02:29.000
去设计我们的推理系统

59
0:02:29.000 --> 0:02:30.920
需要应该有哪些功能

60
0:02:32.800 --> 0:02:34.960
那下面呢我们去了解第一个内容

61
0:02:34.960 --> 0:02:38.640
就是我们的推理部署服务化三者之间的区别

62
0:02:38.640 --> 0:02:39.880
那在了解之前呢

63
0:02:39.880 --> 0:02:42.320
我想给大家再提几个问题

64
0:02:42.320 --> 0:02:45.440
第一个呢就是什幺是模型推理

65
0:02:45.440 --> 0:02:47.600
什幺是叫推理服务化

66
0:02:47.600 --> 0:02:49.080
服务化到底叫啥呢

67
0:02:49.080 --> 0:02:50.280
到底有什幺作用

68
0:02:50.280 --> 0:02:51.640
它是啥意思呢

69
0:02:53.120 --> 0:02:53.680
接着呢

70
0:02:53.680 --> 0:02:55.080
我想给大家提个问题

71
0:02:55.080 --> 0:02:56.640
不知道大家了解过没有

72
0:02:56.640 --> 0:03:00.920
我们平时常见的有哪些推理的服务的框架

73
0:03:00.920 --> 0:03:02.040
大家都叫框架

74
0:03:02.040 --> 0:03:05.840
这个推理服务框架跟AI框架不是一个概念哦

75
0:03:05.840 --> 0:03:08.440
这个Triton就是什幺东西呢

76
0:03:08.440 --> 0:03:10.240
我好像见了好几个Triton

77
0:03:10.240 --> 0:03:13.200
有些Triton呢是指那个推理的服务框架

78
0:03:13.200 --> 0:03:14.800
有个Triton呢

79
0:03:14.800 --> 0:03:18.720
就好像是指用Python去写GPU的一些算字

80
0:03:18.760 --> 0:03:20.680
到底这个Triton到底是个啥玩意

81
0:03:21.760 --> 0:03:22.240
好了

82
0:03:22.240 --> 0:03:22.840
带着这个

83
0:03:24.800 --> 0:03:26.120
那带着这个疑问呢

84
0:03:26.120 --> 0:03:28.320
我们往下去看三个概念

85
0:03:28.320 --> 0:03:29.720
第一个呢就是Inference

86
0:03:29.720 --> 0:03:30.920
我们的推理

87
0:03:30.920 --> 0:03:32.000
第二个就是部署

88
0:03:32.000 --> 0:03:33.080
Development

89
0:03:33.080 --> 0:03:34.440
第三个就是服务化

90
0:03:34.440 --> 0:03:35.360
Serving

91
0:03:35.360 --> 0:03:37.560
我们逐个的来去澄清一下

92
0:03:39.280 --> 0:03:41.880
首先第一个就是推理Inference的

93
0:03:41.880 --> 0:03:44.360
我们既然有训练肯定会有推理嘛

94
0:03:44.360 --> 0:03:46.800
推理就是一个前项的一个计算

95
0:03:46.840 --> 0:03:49.160
那接着呢我们看看部署

96
0:03:49.160 --> 0:03:50.840
那部署其实是我们训练的时候

97
0:03:50.840 --> 0:03:52.480
不是得到一个模型吗

98
0:03:52.480 --> 0:03:54.040
我们希望把这个模型呢

99
0:03:54.040 --> 0:03:56.400
真正部署在我们的硬件上面

100
0:03:56.400 --> 0:03:58.000
那这个就是部署的概念

101
0:03:58.000 --> 0:03:59.600
部署在硬件上面之后呢

102
0:03:59.600 --> 0:04:01.560
我们才做推理的工作

103
0:04:01.560 --> 0:04:04.520
那部署呢就会涉及到包括移植压缩加速

104
0:04:04.520 --> 0:04:06.080
还有我们的推理引擎

105
0:04:06.080 --> 0:04:08.720
这整一块的相关的工作叫做部署

106
0:04:10.840 --> 0:04:11.960
所谓的服务化呢

107
0:04:11.960 --> 0:04:15.160
其实相对应对我们的部署的方式

108
0:04:15.200 --> 0:04:17.080
进行一个抽象的

109
0:04:17.080 --> 0:04:18.040
我们服务化的时候呢

110
0:04:18.040 --> 0:04:19.800
会把我们的AI的算法呢

111
0:04:19.800 --> 0:04:23.080
我们会封装成一个SDK来集中在相关的APP上面

112
0:04:23.080 --> 0:04:26.320
也有可能封装成一个对应的web服务

113
0:04:26.320 --> 0:04:28.600
对外呢去暴露一些HTTP

114
0:04:28.600 --> 0:04:32.240
还有RPC等CIE的一些功能

115
0:04:32.240 --> 0:04:34.800
那这个呢就是三者的区别

116
0:04:34.800 --> 0:04:36.200
有了这三者的区别呢

117
0:04:36.200 --> 0:04:39.680
我们看一下具体的一些服务化的一些框架

118
0:04:39.680 --> 0:04:41.400
那好像TF-SPHIN呢

119
0:04:41.400 --> 0:04:44.960
就是最早的推出的一个服务化的功能

120
0:04:45.000 --> 0:04:46.600
谷歌呢其实是围绕着TensorFlow

121
0:04:46.600 --> 0:04:48.520
做了非常大量相关的工作

122
0:04:48.520 --> 0:04:51.480
包括TF-SPHIN、TF-Lite了很多任务作

123
0:04:51.480 --> 0:04:52.280
那TF-SPHIN呢

124
0:04:52.280 --> 0:04:54.760
就是在谷歌2016年的时候

125
0:04:54.760 --> 0:04:57.480
推出的一个服务化的框架

126
0:04:57.480 --> 0:04:58.760
通过网络请求呢

127
0:04:58.760 --> 0:05:00.680
去获取用户的一些数据

128
0:05:00.680 --> 0:05:01.560
然后进行处理

129
0:05:01.560 --> 0:05:03.520
最后呢返回相关的结果

130
0:05:03.520 --> 0:05:05.000
那我们现在来看看

131
0:05:05.000 --> 0:05:07.480
常见的一些服务化的框架

132
0:05:07.480 --> 0:05:08.400
那可以看到啊

133
0:05:08.400 --> 0:05:11.720
其实从那个16年到20年呢

134
0:05:11.720 --> 0:05:12.720
各大厂商呢

135
0:05:12.720 --> 0:05:16.200
推出了非常多的一些服务化的框架

136
0:05:16.200 --> 0:05:19.240
用的比较多的有Cubanflow呢

137
0:05:19.240 --> 0:05:20.600
是Cubanus里面推出的

138
0:05:20.600 --> 0:05:23.000
还有英伟达推出的Triton呢

139
0:05:23.000 --> 0:05:25.480
现在我们以Triton作为一个具体的例子

140
0:05:25.480 --> 0:05:28.040
来看看常见的服务化的框架的

141
0:05:28.040 --> 0:05:29.920
一个具体的架构

142
0:05:29.920 --> 0:05:31.280
那Triton呢

143
0:05:31.280 --> 0:05:32.440
它的一个全名呢

144
0:05:32.440 --> 0:05:35.200
叫做Triton推理服务器

145
0:05:35.200 --> 0:05:38.720
NVIDIA Triton Inference Service

146
0:05:39.720 --> 0:05:42.920
给用户去提供一个部署在云端

147
0:05:42.920 --> 0:05:45.800
或者边缘侧的一个具体的解决方案

148
0:05:45.800 --> 0:05:46.800
对接的呢

149
0:05:46.800 --> 0:05:48.520
有客户的云端的服务器

150
0:05:48.520 --> 0:05:50.560
还有云端的服务的请求

151
0:05:50.560 --> 0:05:51.400
然后对应到呢

152
0:05:51.400 --> 0:05:53.520
对AI模型的一个管理

153
0:05:53.520 --> 0:05:54.520
然后这里面呢

154
0:05:54.520 --> 0:05:57.320
就是我们具体的一些服务化的功能了

155
0:05:58.520 --> 0:05:59.720
那下面我们来看一下

156
0:05:59.720 --> 0:06:01.560
它的一个具体的架构

157
0:06:01.560 --> 0:06:02.160
右边呢

158
0:06:02.160 --> 0:06:03.840
就是它的整体的一个架构

159
0:06:03.840 --> 0:06:04.480
我们现在呢

160
0:06:04.480 --> 0:06:05.960
逐个模块的去展开

161
0:06:05.960 --> 0:06:07.320
这里面有非常多的模块

162
0:06:07.320 --> 0:06:08.000
那第一个呢

163
0:06:08.000 --> 0:06:10.040
就是它的一个介入层

164
0:06:10.040 --> 0:06:11.480
我们叫做Interface

165
0:06:11.480 --> 0:06:12.400
那这个介入层呢

166
0:06:12.400 --> 0:06:14.520
主要是指这里面的这个模块

167
0:06:14.520 --> 0:06:15.960
那提供了一个HTTP

168
0:06:15.960 --> 0:06:17.720
还有GRPC

169
0:06:17.720 --> 0:06:20.280
那这个RPC就是谷歌的RPC的协议

170
0:06:20.280 --> 0:06:20.640
然后呢

171
0:06:20.640 --> 0:06:21.320
除此之外呢

172
0:06:21.320 --> 0:06:23.040
它还提供了一个C++的

173
0:06:23.040 --> 0:06:24.960
或者CE的一个API的接口

174
0:06:24.960 --> 0:06:27.200
去给到我们一个用户

175
0:06:27.200 --> 0:06:28.880
做一个服务访问请求的

176
0:06:28.880 --> 0:06:31.040
另外它还支持一个共享内存

177
0:06:31.040 --> 0:06:33.520
Share Memory的一个IPC的通讯

178
0:06:35.200 --> 0:06:35.680
那现在呢

179
0:06:35.680 --> 0:06:36.880
我们看看另外

180
0:06:36.880 --> 0:06:40.160
又往右边数一数的一个功能点

181
0:06:40.160 --> 0:06:41.080
那这个功能点呢

182
0:06:41.080 --> 0:06:43.320
叫做Triton的模型仓库

183
0:06:43.320 --> 0:06:44.800
我们叫做Model Manager

184
0:06:45.960 --> 0:06:47.200
这个模型仓库呢

185
0:06:47.200 --> 0:06:49.560
可以接入到各种的云服务

186
0:06:49.560 --> 0:06:51.360
而且可以支持多模型呢

187
0:06:51.360 --> 0:06:53.040
也支持模型的编排

188
0:06:53.040 --> 0:06:55.360
也支持模型的刺激化的存储

189
0:06:55.360 --> 0:06:56.920
那这个就是具体的功能

190
0:06:56.920 --> 0:06:59.560
下面我们来看一下左下角

191
0:06:59.560 --> 0:07:00.360
那左下角呢

192
0:07:00.360 --> 0:07:04.000
这里面有一个Print Model Scheduler

193
0:07:04.040 --> 0:07:06.440
就是我们的模型的预编排

194
0:07:06.440 --> 0:07:07.400
我们可以刚才呀

195
0:07:07.400 --> 0:07:09.480
其实大量的去强调

196
0:07:09.480 --> 0:07:12.040
在我们的AI系统里面的模型的管理

197
0:07:12.040 --> 0:07:14.680
是比较特别比较内核的

198
0:07:14.680 --> 0:07:16.120
那这里面的主要的工作呢

199
0:07:16.120 --> 0:07:18.560
就是解析整个URL的请求

200
0:07:18.560 --> 0:07:20.880
就我们上面对于URL的请求

201
0:07:20.880 --> 0:07:21.600
请求完之后呢

202
0:07:21.600 --> 0:07:23.600
就会从我们的模型库里面

203
0:07:23.600 --> 0:07:25.800
去做一个具体的模型的编排

204
0:07:25.800 --> 0:07:27.040
去选择哪些模型

205
0:07:27.040 --> 0:07:30.520
对于哪些任务应该用什幺调度的方式

206
0:07:30.520 --> 0:07:30.880
这个呢

207
0:07:30.880 --> 0:07:33.520
就是我们模型预编排所做的工作

208
0:07:35.000 --> 0:07:36.320
那往下呢

209
0:07:36.320 --> 0:07:37.880
我们再看看下个功能

210
0:07:37.880 --> 0:07:38.440
这个呢

211
0:07:38.440 --> 0:07:40.120
叫做Backend

212
0:07:40.120 --> 0:07:42.120
在我们AI推理系统里面的Backend

213
0:07:42.120 --> 0:07:44.080
其实是对应的推理引擎

214
0:07:44.080 --> 0:07:45.080
那这个推理引擎呢

215
0:07:45.080 --> 0:07:46.000
在Tryton里面呢

216
0:07:46.000 --> 0:07:47.000
支持非常多的

217
0:07:47.000 --> 0:07:47.520
TensorFlow

218
0:07:47.520 --> 0:07:47.920
Linux

219
0:07:47.920 --> 0:07:48.720
PyTorch

220
0:07:48.720 --> 0:07:50.880
还有自定义的也可以

221
0:07:52.160 --> 0:07:53.760
在Tryton启动的时候呢

222
0:07:53.760 --> 0:07:55.160
我们的模型仓库的模型呢

223
0:07:55.160 --> 0:07:56.440
已经被加载进来

224
0:07:56.440 --> 0:07:58.080
就是右边的这条线

225
0:07:58.080 --> 0:07:59.120
加载进来之后呢

226
0:07:59.120 --> 0:08:01.960
就在我们的后端的服务化的推理引擎上面

227
0:08:01.960 --> 0:08:02.920
去执行的

228
0:08:05.000 --> 0:08:05.800
最后呢

229
0:08:05.800 --> 0:08:08.560
我们看一个最底下的功能

230
0:08:08.560 --> 0:08:09.200
那这个呢

231
0:08:09.200 --> 0:08:11.280
就分开两个把它合在一起了

232
0:08:11.880 --> 0:08:12.480
第一个呢

233
0:08:12.480 --> 0:08:14.840
就是服务的返回

234
0:08:14.840 --> 0:08:17.040
通过HTTP或者GRPC呢

235
0:08:17.040 --> 0:08:20.560
对我们推理引擎的一个结果进行返回

236
0:08:21.640 --> 0:08:22.280
第二个呢

237
0:08:22.280 --> 0:08:23.360
就是对我们的state

238
0:08:23.360 --> 0:08:24.400
就我们训练的时候

239
0:08:24.400 --> 0:08:27.400
整个AI系统的一些状态

240
0:08:27.400 --> 0:08:28.520
健康状况呢

241
0:08:28.520 --> 0:08:29.800
进行一个监控

242
0:08:29.800 --> 0:08:31.960
然后返回给我们的HTTP请求

243
0:08:31.960 --> 0:08:32.720
然后这个呢

244
0:08:32.720 --> 0:08:34.080
就对应我们HTTP

245
0:08:34.080 --> 0:08:37.120
就应我们的整个服务器的管理的后台

246
0:08:37.120 --> 0:08:39.640
那这个就是监控的功能了

247
0:08:39.640 --> 0:08:41.080
所以说整体的框架呢

248
0:08:41.080 --> 0:08:42.640
就分开这好几块功能

249
0:08:42.640 --> 0:08:43.720
第一块第二块

250
0:08:43.720 --> 0:08:45.520
模型的服务模型的编排

251
0:08:45.520 --> 0:08:46.520
推理引擎

252
0:08:46.520 --> 0:08:48.600
还有健康的管理

253
0:08:48.600 --> 0:08:49.600
这几大内容

254
0:08:51.480 --> 0:08:52.080
那下面呢

255
0:08:52.080 --> 0:08:55.680
就是整个Triton集成推理引擎的一个流程

256
0:08:55.680 --> 0:08:56.120
这个呢

257
0:08:56.120 --> 0:08:56.720
是咫呼

258
0:08:56.720 --> 0:08:58.200
我是小baby

259
0:08:58.200 --> 0:08:58.760
哈哈

260
0:08:58.760 --> 0:09:01.920
那这位同学里面去截取的一个图

261
0:09:01.920 --> 0:09:02.560
这位同学呢

262
0:09:02.560 --> 0:09:03.800
在Triton的一个对接呢

263
0:09:03.800 --> 0:09:05.520
做了非常大量的工作

264
0:09:05.520 --> 0:09:09.600
那我引用了里面其中的一些相关的一个图

265
0:09:09.600 --> 0:09:11.240
那其实我们可以看到啊

266
0:09:11.240 --> 0:09:12.120
Triton呢

267
0:09:12.120 --> 0:09:16.160
主要是帮我们集成了很多的网络的请求和模型的编排的工作

268
0:09:16.160 --> 0:09:17.400
而服务化的功能呢

269
0:09:17.400 --> 0:09:18.520
它基本上做好了

270
0:09:18.520 --> 0:09:20.280
我们只需要把我们的后端

271
0:09:20.280 --> 0:09:21.280
把我们的模型

272
0:09:21.280 --> 0:09:23.800
把我们的一些推理的引擎

273
0:09:23.800 --> 0:09:25.840
对接到整个Triton的后端

274
0:09:25.840 --> 0:09:27.160
就可以了

275
0:09:27.160 --> 0:09:27.560
这个呢

276
0:09:27.560 --> 0:09:30.200
是Triton给我们做了一个非常多的相关的工作

277
0:09:30.200 --> 0:09:31.080
而我们华为呢

278
0:09:31.080 --> 0:09:35.280
我理解这个对应的产品就是Mate X里面的Mate DL

279
0:09:35.280 --> 0:09:36.840
它会做很多相关的服务器

280
0:09:36.840 --> 0:09:37.960
企划服务的部署啊

281
0:09:38.960 --> 0:09:40.720
还有推理系统相关的工作

282
0:09:40.720 --> 0:09:41.280
那其实呢

283
0:09:41.280 --> 0:09:42.200
大部分的架构呢

284
0:09:42.200 --> 0:09:44.040
就是我们刚才所讲的

285
0:09:44.040 --> 0:09:45.600
就那几个模块

286
0:09:45.600 --> 0:09:46.520
那最后呢

287
0:09:46.520 --> 0:09:48.440
我们还有点时间呢

288
0:09:48.440 --> 0:09:51.320
就这个视频可能会稍微有点长

289
0:09:51.320 --> 0:09:54.120
我们来看看模型的生命周期管理

290
0:10:01.600 --> 0:10:02.160
首先呢

291
0:10:02.160 --> 0:10:06.680
我们了解一下为什幺要对模型的版本进行管理

292
0:10:06.680 --> 0:10:07.280
其实呢

293
0:10:07.280 --> 0:10:09.200
我们在应该是我现在啊

294
0:10:09.200 --> 0:10:10.600
遇到一个比较大的挑战

295
0:10:10.600 --> 0:10:12.720
就是我PyTorch一个对应的模型

296
0:10:12.720 --> 0:10:14.040
或者MineSport对应的模型

297
0:10:14.040 --> 0:10:15.760
MineSport的版本升级了

298
0:10:15.760 --> 0:10:17.280
它的API接口变了

299
0:10:17.280 --> 0:10:19.000
这个模型就跑不通了

300
0:10:19.000 --> 0:10:20.560
我们需要重新的适配

301
0:10:20.560 --> 0:10:21.840
那这个情况呢

302
0:10:21.840 --> 0:10:23.480
我们会对我们的模型呢

303
0:10:23.480 --> 0:10:24.720
重新上线

304
0:10:24.720 --> 0:10:25.400
那第二个呢

305
0:10:25.400 --> 0:10:27.920
就是我的模型的精度

306
0:10:27.920 --> 0:10:28.920
对于我同一个任务

307
0:10:28.920 --> 0:10:29.760
我是检测的

308
0:10:29.800 --> 0:10:31.880
我之前用的是YOLOv3

309
0:10:31.880 --> 0:10:32.400
这个时候呢

310
0:10:32.400 --> 0:10:33.800
模型的精度

311
0:10:33.800 --> 0:10:34.920
可能我们的MAP呢

312
0:10:34.920 --> 0:10:36.120
只有50%多

313
0:10:36.120 --> 0:10:36.840
结果呢

314
0:10:36.840 --> 0:10:39.920
YOLOv4、V5、V6、V7都出来了

315
0:10:39.920 --> 0:10:40.600
这个时候呢

316
0:10:40.600 --> 0:10:41.320
我的MAP呢

317
0:10:41.320 --> 0:10:42.600
已经上到70%多了

318
0:10:43.640 --> 0:10:45.240
因为新的算法

319
0:10:45.240 --> 0:10:46.640
所以我们会对模型呢

320
0:10:46.640 --> 0:10:48.280
进行新的升级

321
0:10:48.280 --> 0:10:49.200
都是用YOLO

322
0:10:49.200 --> 0:10:50.400
都是做检测

323
0:10:50.400 --> 0:10:51.920
用户不会关心你用YOLO

324
0:10:51.920 --> 0:10:53.480
还是用YOLOv3、V2

325
0:10:53.480 --> 0:10:55.400
用户只关心我的检测模型

326
0:10:55.400 --> 0:10:57.840
我的检测算法到底准还是不准

327
0:10:57.840 --> 0:10:59.640
因为最后的模型的出口

328
0:10:59.640 --> 0:11:00.520
或者最后的任务呢

329
0:11:00.520 --> 0:11:01.200
只有一个

330
0:11:01.200 --> 0:11:02.960
那这些情况呢

331
0:11:02.960 --> 0:11:05.720
我们也是需要对模型进行管理的

332
0:11:05.720 --> 0:11:06.920
另外有可能

333
0:11:06.920 --> 0:11:09.040
我们的模型出现问题呢

334
0:11:09.040 --> 0:11:10.360
我们的模型被攻击

335
0:11:10.360 --> 0:11:11.440
或者我们的模型

336
0:11:11.440 --> 0:11:13.640
随着我们的时间管理的问题呢

337
0:11:13.640 --> 0:11:15.080
它有些数据丢失了

338
0:11:15.080 --> 0:11:16.240
那它的精度掉了

339
0:11:16.240 --> 0:11:16.840
这个时候呢

340
0:11:16.840 --> 0:11:19.840
我们也需要进行缺陷的检测

341
0:11:19.840 --> 0:11:20.960
和回顾的

342
0:11:20.960 --> 0:11:21.600
这个时候呢

343
0:11:21.600 --> 0:11:23.120
因为种种这些原因呢

344
0:11:23.120 --> 0:11:24.840
我们需要对模型的生命周期

345
0:11:24.840 --> 0:11:25.720
进行管理

346
0:11:25.720 --> 0:11:26.360
那这个呢

347
0:11:26.360 --> 0:11:27.560
就是TensorFlow Surfing

348
0:11:27.560 --> 0:11:29.280
这篇文章里面去讲到

349
0:11:29.280 --> 0:11:30.760
它怎幺去做的

350
0:11:30.760 --> 0:11:33.080
也非常欢迎大家去看看这篇文章

351
0:11:33.080 --> 0:11:34.080
所以说TF呢

352
0:11:34.080 --> 0:11:36.080
其实在AI系统里面呢

353
0:11:36.080 --> 0:11:39.480
做了非常多相关的工作和创新的

354
0:11:41.280 --> 0:11:42.640
而模型生命周期管理呢

355
0:11:42.640 --> 0:11:43.720
主要有两个

356
0:11:43.720 --> 0:11:44.280
第一个呢

357
0:11:44.280 --> 0:11:46.680
就是Canary的策略

358
0:11:46.680 --> 0:11:47.360
那第二个呢

359
0:11:47.360 --> 0:11:49.560
就是Wallpack的策略

360
0:11:49.560 --> 0:11:52.120
那我们看一下Canary的策略呢

361
0:11:52.120 --> 0:11:54.080
它主要是有几个方面

362
0:11:55.160 --> 0:11:57.200
如果有一个新的模型的版本呢

363
0:11:57.200 --> 0:12:00.200
用户可以同时选择保留这两个版本

364
0:12:00.200 --> 0:12:01.680
就新旧版本你可以随便用

365
0:12:02.680 --> 0:12:03.080
然后呢

366
0:12:03.080 --> 0:12:04.920
我们会对流量的做一个ABTEX

367
0:12:04.920 --> 0:12:06.520
或者做一个分流

368
0:12:06.520 --> 0:12:08.360
分别推送到两个版本里面

369
0:12:08.360 --> 0:12:10.240
去比较对应的效果

370
0:12:10.240 --> 0:12:10.760
一旦呢

371
0:12:10.760 --> 0:12:14.120
发现最新的版本效果比较好的

372
0:12:14.120 --> 0:12:17.680
那用户就会直接切换到对应的最新的版本

373
0:12:17.680 --> 0:12:18.880
但是这种方式呢

374
0:12:18.880 --> 0:12:22.600
更多的是在互联网厂商里面用的比较多

375
0:12:22.600 --> 0:12:23.120
因为呢

376
0:12:23.120 --> 0:12:25.840
它需要非常多的一个高峰的资源

377
0:12:25.840 --> 0:12:28.640
就非常多的一个流量

378
0:12:28.640 --> 0:12:29.200
避免呢

379
0:12:29.200 --> 0:12:30.400
将大部分的用户呢

380
0:12:30.400 --> 0:12:32.320
都暴露于我们的一些缺陷模型

381
0:12:32.320 --> 0:12:34.120
或者有问题的模型

382
0:12:34.120 --> 0:12:35.680
导致我们的服务中断

383
0:12:36.680 --> 0:12:37.400
那接着呢

384
0:12:37.400 --> 0:12:39.400
第二个就是不管三七二十一

385
0:12:39.400 --> 0:12:41.720
我们都要做一个hold back

386
0:12:41.720 --> 0:12:43.360
或者回馆的一个策略

387
0:12:44.440 --> 0:12:47.440
就假设我现在在当前的主要的服务器的版本上面呢

388
0:12:47.440 --> 0:12:49.560
去检测到我的模型是有问题的

389
0:12:49.560 --> 0:12:50.960
或者可能会有问题

390
0:12:50.960 --> 0:12:52.040
或者我升级了

391
0:12:52.040 --> 0:12:52.680
这个时候呢

392
0:12:52.680 --> 0:12:55.520
用户可以请求切换到比较低的版本

393
0:12:55.560 --> 0:12:58.480
所以我们会同时维护非常多不同的版本

394
0:12:58.480 --> 0:12:59.680
方便我们去回顾的

395
0:13:01.680 --> 0:13:02.720
然后第二个特点呢

396
0:13:02.720 --> 0:13:07.040
就是我的现在安装了它的整体的配置的顺序

397
0:13:07.040 --> 0:13:08.440
我的现在安装的顺序呢

398
0:13:08.440 --> 0:13:09.440
还有我们的模型呢

399
0:13:09.440 --> 0:13:11.120
都是可以配置的

400
0:13:11.120 --> 0:13:12.600
方便我们解决问题之后呢

401
0:13:12.600 --> 0:13:13.680
我们推到新的版本

402
0:13:13.680 --> 0:13:15.640
我们可以结束回馆

403
0:13:15.640 --> 0:13:16.720
那这种方式呢

404
0:13:16.720 --> 0:13:19.320
主要是对版本的维护

405
0:13:19.320 --> 0:13:20.760
我现在接触到的工作呢

406
0:13:20.760 --> 0:13:23.640
更多的是采用这种回馆的策略

407
0:13:26.000 --> 0:13:28.440
好了我们来总结一下

408
0:13:28.440 --> 0:13:29.000
那今天呢

409
0:13:29.000 --> 0:13:31.960
我们看了一下推理系统的整体的架构

410
0:13:31.960 --> 0:13:34.240
希望大家能够对推理系统呢

411
0:13:34.240 --> 0:13:37.320
有个比较清晰或者全面的认识

412
0:13:37.320 --> 0:13:38.000
接着呢

413
0:13:38.000 --> 0:13:39.120
我们往下看一下

414
0:13:39.120 --> 0:13:40.800
今天我们给大家汇报了一下

415
0:13:40.800 --> 0:13:44.600
Info、Develop、Surface的一个相关的概念

416
0:13:44.600 --> 0:13:46.000
搞清楚这个概念之后呢

417
0:13:46.000 --> 0:13:48.840
我们就比较自然而然的去引入到了

418
0:13:48.840 --> 0:13:52.040
我们整个推理系统的整体的架构

419
0:13:52.040 --> 0:13:53.040
聊起来架构之后呢

420
0:13:53.040 --> 0:13:54.600
跟AI比较相关的

421
0:13:54.600 --> 0:13:56.760
因为这大部分都是平台性的工作

422
0:13:56.760 --> 0:13:59.720
利于我们的Triton的服务化的工作

423
0:13:59.720 --> 0:14:01.200
对于AI比较相关的

424
0:14:01.200 --> 0:14:03.360
就是我们的模型的生命周期的管理

425
0:14:03.360 --> 0:14:04.000
那其实呢

426
0:14:04.000 --> 0:14:05.360
这个模型生命周期管理呢

427
0:14:05.360 --> 0:14:06.840
是一个非常大的学问

428
0:14:06.840 --> 0:14:08.680
我们有不同的一个策略

429
0:14:08.680 --> 0:14:09.920
针对不同的业务呢

430
0:14:09.920 --> 0:14:11.440
可能会采用不同的策略

431
0:14:11.440 --> 0:14:12.760
而不是一条论的

432
0:14:12.760 --> 0:14:13.720
那今天的内容呢

433
0:14:13.720 --> 0:14:14.600
到此为止

434
0:14:14.600 --> 0:14:15.080
好了

435
0:14:15.080 --> 0:14:16.080
谢谢各位

436
0:14:16.080 --> 0:14:16.880
卷的不行了

437
0:14:16.880 --> 0:14:17.760
卷的不行了

438
0:14:17.760 --> 0:14:19.560
记得一键三连加关注哦

439
0:14:19.560 --> 0:14:21.160
所有的内容都会开源在

440
0:14:21.160 --> 0:14:23.160
下面这条链接里面

441
0:14:23.160 --> 0:14:24.120
拜拜

