0
0:00:00.000 --> 0:00:04.580


1
0:00:06.260 --> 0:00:07.420
哈喽大家好

2
0:00:07.420 --> 0:00:12.780
我是那个赚钱的时候千辛万苦花钱的时候稀里糊涂的ZOMI

3
0:00:12.860 --> 0:00:16.980
那今天呢我们来到一个比较比较重要的内容吧

4
0:00:16.980 --> 0:00:18.500
也不能说很重要

5
0:00:18.500 --> 0:00:21.500
就是整个推理系统的一个架构

6
0:00:21.500 --> 0:00:25.180
其实我觉得最重要的是我们推理引擎的一个架构

7
0:00:25.180 --> 0:00:28.520
就围绕的这一块可能是比较多的技术含量的

8
0:00:28.520 --> 0:00:32.800
那推理系统呢更多的是一些传统的一些功能点

9
0:00:32.800 --> 0:00:35.120
然后把AI的一些特性加进去

10
0:00:35.120 --> 0:00:37.720
那没关系我们继续来深入地去看一下

11
0:00:37.720 --> 0:00:42.480
今天呢我们主要是去讲讲整个推理系统的具体的架构

12
0:00:42.480 --> 0:00:47.160
那在讲架构之前呢我们会来讲另外一个内容就是推理部署服务

13
0:00:47.160 --> 0:00:49.320
那三者到底有啥区别咧

14
0:00:49.320 --> 0:00:51.520
它之前是啥关系呢

15
0:00:51.520 --> 0:00:55.560
那接着我们去看一下推理系统的整体的架构

16
0:00:55.560 --> 0:01:00.040
如果有时间因为我希望能够在每个视频呢控制在十来分钟

17
0:01:00.040 --> 0:01:01.920
就大家看着不累我讲着不累

18
0:01:01.920 --> 0:01:05.920
然后呢如果有时间或者有剩下的时间呢我去想去讲讲

19
0:01:05.920 --> 0:01:08.400
模型的生命周期的管理

20
0:01:08.400 --> 0:01:13.960
因为模型生命周期的管理呢是AI属性比较强的在整个推理系统里面

21
0:01:13.960 --> 0:01:19.760
而推理系统更多的是一些服务的响应啊技术的编排啊系统的调度啊这些问题

22
0:01:19.760 --> 0:01:23.600
模型的生命周期呢是跟AI属性比较强相关

23
0:01:23.600 --> 0:01:27.200
所以我想给大家后面呢有时间去回报一下的

24
0:01:29.080 --> 0:01:33.280
接下来呢我们回顾一下整个模型的生命周期

25
0:01:33.280 --> 0:01:36.800
我们首先呢需要去收集我们训练的模型

26
0:01:36.800 --> 0:01:41.520
设计完模型之后呢去给我们深度学习的这个模型呢去给我们的算法来去训练

27
0:01:41.520 --> 0:01:45.320
训练完之后呢得到一个具体的AI模型

28
0:01:45.320 --> 0:01:49.360
AI模型呢就可以真正地去用来做服务的请求和服务的响应

29
0:01:49.360 --> 0:01:53.040
而对服务的请求服务的响应还有真正的推理和部署呢

30
0:01:53.040 --> 0:01:55.960
最快就是我们整个AI

31
0:01:55.960 --> 0:02:00.440
AI推理系统呢需要去解决的一些问题或者AI推理系统呢

32
0:02:00.440 --> 0:02:03.280
它主要关注的点就在这一块了

33
0:02:05.200 --> 0:02:09.640
现在呢我想给大家提一个问题引起大家去思考了

34
0:02:09.640 --> 0:02:15.200
就是AI应用部署需要考虑哪些方面呢

35
0:02:15.200 --> 0:02:17.520
我们需要考虑哪些技术点呢

36
0:02:18.920 --> 0:02:20.680
哎这个问题问得非常好

37
0:02:20.760 --> 0:02:26.960
因为只要我们考虑到这些点我们才能够很好的去设计我们的整个框架

38
0:02:26.960 --> 0:02:30.960
去设计我们的推理系统需要应该有哪些功能

39
0:02:32.800 --> 0:02:38.640
那下面呢我们去了解第一个内容就是我们的推理部署服务化三者之间的区别

40
0:02:38.640 --> 0:02:42.320
那在了解之前呢我想给大家再提几个问题

41
0:02:42.320 --> 0:02:47.680
第一个呢就是什么是模型推理什么是叫推理服务化

42
0:02:47.680 --> 0:02:50.320
服务化到底叫啥呢到底有什么作用

43
0:02:50.320 --> 0:02:51.720
它是啥意思呢

44
0:02:53.120 --> 0:02:56.720
接着呢我想给大家提个问题不知道大家了解过没有

45
0:02:56.720 --> 0:03:02.120
我们平时常见的有哪些推理的服务的框架大家都叫框架

46
0:03:02.120 --> 0:03:05.880
这个推理服务框架跟AI框架不是一个概念哦

47
0:03:05.880 --> 0:03:10.280
这个tryton就是什么东西呢我好像见了好几个tryton

48
0:03:10.280 --> 0:03:13.240
有些tryton呢是指那个推理的服务框架

49
0:03:13.240 --> 0:03:18.760
有个tryton呢就好像是指用python去写GPU的一些算子

50
0:03:18.760 --> 0:03:20.760
到底这个tryton到底是个啥玩意儿

51
0:03:21.760 --> 0:03:22.960
好了带着这个

52
0:03:24.760 --> 0:03:28.360
那带着这个疑问呢我们往下去看三个概念

53
0:03:28.360 --> 0:03:33.080
第一个呢就是inference我们的推理第二个就是部署给volume

54
0:03:33.080 --> 0:03:37.640
第三个就是服务化设备我们足够的来去澄清一下

55
0:03:39.280 --> 0:03:46.760
首先第一个就是推理inference的我们既然有训练肯定会有推理嘛推理就是一个前向的一个计算

56
0:03:46.760 --> 0:03:49.120
那接着了我们看看部署

57
0:03:49.120 --> 0:03:52.480
那部署其实是我们训练的时候不是得到一个模型吗

58
0:03:52.480 --> 0:03:56.400
我们希望把这个模型呢真正部署在我们的硬件上面

59
0:03:56.400 --> 0:04:01.560
那这个就是部署的概念部署在硬件上面之后呢我们才做推理的工作

60
0:04:01.560 --> 0:04:08.840
那部署呢就会涉及到包括移子压缩加速还有我们的推进引擎这整一块的相关的工作叫做部署

61
0:04:08.840 --> 0:04:16.960
所谓的服务化呢其实相对应对我们的部署的方式进行一个抽象的

62
0:04:16.960 --> 0:04:23.000
我们服务化的时候呢会把我们的AI的算法呢我们会封装成一个SDK来集中在相关的APP上面

63
0:04:23.000 --> 0:04:32.160
也有可能封装成一个对应的web服务对外呢需暴露一些HTTP还有RPC等随意的一些功能

64
0:04:32.160 --> 0:04:38.800
那这个呢就是三者的区别有了这三者的区别呢我们看一下具体的一些服务化的一些功能

65
0:04:38.800 --> 0:04:45.040
的框架那好像贴粉丝呢就是最早的推出的一个服务化的功能

66
0:04:45.040 --> 0:04:51.560
谷歌呢其实围绕着探索部做了非常大量相关的工作包括贴粉丝呢贴粉丝呢很多工作

67
0:04:51.560 --> 0:04:57.560
那贴粉丝呢就是在谷歌二零一六年的时候推出的一个服务化的框架

68
0:04:57.560 --> 0:05:03.560
通过网络请求呢去获取用户的一些数据然后进行处理最后呢反馈相关的结果

69
0:05:03.600 --> 0:05:11.720
那我们现在来看看常见的一些服务化的框架那可以看到啊其实从那个一六年到二零年的

70
0:05:11.720 --> 0:05:16.320
各大厂商呢推出了非常多的一些服务化的框架

71
0:05:17.080 --> 0:05:23.000
用的比较多的有科帕弗洛的是科帕诺斯面推出的还有英伟达推出的川腾呢

72
0:05:23.000 --> 0:05:29.800
而现在我们以川腾作为一个具体的例子来看看常见的服务化的框架的一个具体的架构

73
0:05:29.800 --> 0:05:38.720
那川腾呢他的一个全名呢叫做川腾推理服务器呃诺威迪尔川腾影分子服务

74
0:05:40.240 --> 0:05:46.160
给用户去提供一个部署在云端或者边缘侧的一个具体的解决方案

75
0:05:46.160 --> 0:05:53.600
对接的呢有客户的云端的服务器的还有云端的服务的请求然后对应到了对AI模型的一个管理

76
0:05:53.600 --> 0:05:57.240
然后这里面的就是我们具体的一些服务化的功能了

77
0:05:58.240 --> 0:06:07.240
那下面我们看一下它的一个具体的架构右边的就是它的整体的一个架构我们现在的足够模块的去展开这里面有非常多的模块

78
0:06:07.240 --> 0:06:11.400
那第一个呢就是它的一个接入层我们叫做interface

79
0:06:11.400 --> 0:06:17.680
那这个接入层呢主要是指这里面的这个模块那提供了一个http还有g RPC

80
0:06:17.680 --> 0:06:24.920
那这个RPC就是谷歌的RPC的协议然后呢除此之外呢它还提供了一个C++的或者C的一个API的接口

81
0:06:25.040 --> 0:06:33.480
去给到我们一个用户做一个服务访问请求的另外它还支持一个共享内存虽然memory的一个IPC的通讯

82
0:06:35.200 --> 0:06:39.880
那现在呢我们看看另外右往右边数一数的一个功能点

83
0:06:39.880 --> 0:06:44.800
那这个功能点呢叫做Triton的模型仓库我们叫做model manager

84
0:06:46.000 --> 0:06:54.880
这个模型仓库呢可以接入到各种的云服务而且可以支持多模型呢也支持模型的编排也支持模型的持久化的存储

85
0:06:55.400 --> 0:07:01.240
那这个就是具体的功能下面我们来看一下左下角那左下角呢这里面有一个

86
0:07:01.920 --> 0:07:14.560
premodel scheduler就是我们的模型的预编排我们可以刚才呀其实大量地去强强调在我们的AI系统里面的模型的管理是比较特别比较核心的

87
0:07:14.760 --> 0:07:24.840
那这里面的主要的工作呢就是解析整个摇摇的请求就我们上面对于摇摇的请求请求完之后呢就会从我们的模型库里面去做一个具体的模型

88
0:07:24.880 --> 0:07:33.520
模型的编排去选择哪些模型对于哪些任务应该用什么调度的方式这个呢就是我们模型预编排所做的工作

89
0:07:35.480 --> 0:07:39.440
那往下呢我们再看看下个功能这个呢叫做backend

90
0:07:40.200 --> 0:07:46.960
在我们AI推系统里面的backend呢其实是对硬的推力引擎那这个推力引擎呢在Triton里面的知识非常多的

91
0:07:47.000 --> 0:07:50.840
探索夫的恩利斯洛派特许了还有自定义的也可以

92
0:07:52.200 --> 0:08:02.960
在Triton启动的时候呢我们的模型仓库的模型呢已经被加载进来就是右边的这条线加进来之后呢就在我们的后端的服务化的推力引擎上面去执行的

93
0:08:05.360 --> 0:08:11.280
最后呢我们看一个最底下的功能那这个呢就分开两个把它合在一起了

94
0:08:12.040 --> 0:08:14.720
第一个呢就是服务的反馈

95
0:08:14.720 --> 0:08:20.280
通过HTTP或者具IPC呢对我们推力引擎的一个结果进行反馈

96
0:08:21.760 --> 0:08:31.800
第二个呢就是对我们的state就我们训练的时候整个AI系统的一些状态健康状况呢进行一个监控然后反馈给我们的http请求

97
0:08:32.000 --> 0:08:39.400
然后这个呢就对应我们http就应我们的整个服务器的管理的后台那这个就是监控的功能了

98
0:08:39.680 --> 0:08:49.520
所以说整体的框架呢就分开这好几块功能第一块第二块模型的服务模型的编排推力引擎还有健康的管理这几大内容

99
0:08:51.520 --> 0:09:01.560
那下面呢就是整个川腾集成推力引擎的一个流程这个呢是自呼我是小baby哈哈的这位同学里面取截取的一个图

100
0:09:01.960 --> 0:09:09.280
这位同学呢在川腾的一个队界呢做了非常大量的工作让我引用了里面其中的一些相关的一个图

101
0:09:09.400 --> 0:09:38.000
那其实我们可以看到啊而川腾呢主要是帮我们集成了很多的网络的请求和模型的编排的工作而服务化的功能呢他基本上做好了我们只需要把我们的后端把我们的模型把我们的一些推力的引擎对接到整个川腾的后端就可以了这个呢是川腾给我们做了一个非常多的相关的工作而我们华为呢我理解这个对应的产品就是麦德埃克里面的麦德迪奥他会做很多相关的服务请求啊服务的部署啊

102
0:09:38.920 --> 0:09:48.720
好推系统相关的工作那其实呢大部分的架构呢就是我们刚才想所讲的就那几个模块那最后呢我们还有点时间那就

103
0:09:49.800 --> 0:09:54.080
这个视频可能会稍微有点长我们来看看模型的生命周期管理

104
0:10:01.680 --> 0:10:06.280
首先呢我们了解一下为什么要对模型的版本进行管理

105
0:10:06.280 --> 0:10:24.680
其实呢我们在应该是我现在啊遇到一个比较大的挑战就是我拍套取一个对应的模型或者麦斯珀对应的模型麦斯珀的版本升级了他的API接口变了这个模型就跑不通了我们需要重新的适配那这个情况呢我们会对我们的模型呢重新上线

106
0:10:24.680 --> 0:10:36.000
那第二个呢就是我的模型的精度对于我同一个任务我是检测的我之前用的是优露威三这个时候呢模型的精度可能我们的MAP呢只有百分之五十多

107
0:10:36.000 --> 0:10:42.600
结果呢优露威四威五威六威七都出来了这个时候呢我的MAP呢已经上到百分之七十多了

108
0:10:43.560 --> 0:11:05.600
因为新的算法所以我们会对模型呢进行新的升级都是用优露都是做检测用户不会关心你用优露还是用优露威三不要用户只关心我的检测模型我的检测算法到底准还是不准因为最后的模型的出口或者最后的任务呢只有一个那这些情况呢我们也是需要对模型进行管理的

109
0:11:05.600 --> 0:11:33.120
另外有可能我们的模型出现问题呢我们的模型被攻击或者我们的模型随着我们的实行管理的问题呢它有些数据丢失了那它的精度掉了这个时候呢我们也需要进行缺陷的检测和回顾的这个时候呢因为种种这些原因呢我们需要对模型的生命周期进行管理那这个呢就是天特斯福这篇文章里面去讲到它怎么去做的也非常欢迎大家去看看这篇文章

110
0:11:33.120 --> 0:11:39.400
所以说贴幅呢其实在AI系统里面呢做了非常多相关的工作和创新的

111
0:11:41.280 --> 0:11:54.000
而模型生命周期管理呢主要有两个第一个呢就是肯雷里的策略那第二个呢就是沃贝克的策略那我们看一下肯雷里的策略呢它主要是有几个方面

112
0:11:55.200 --> 0:12:01.720
如果有一个新的模型的版本呢用户可以同时选择保留这两个版本就新旧版本你可以随便用

113
0:12:02.680 --> 0:12:16.840
然后呢我们会对流量呢做一个APTX或者做一个分流分别推送到两个版本里面去比较对应的效果一旦呢发现最新的版本呃效果比较好的那用户就会直接切换到对应的最新的版本

114
0:12:17.720 --> 0:12:31.680
但是这种方式呢更多的是在互联网厂商里面用的比较多因为呢它需要非常多的一个高峰的资源就非常多的一个流量避免呢将大部分的用户呢都暴露于我们的一切

115
0:12:31.720 --> 0:12:35.720
缺陷模型或者有问题的模型导致我们的服务中断

116
0:12:36.720 --> 0:12:43.360
那接着呢第二个就是不管三七二十一我们都要做一个回管的或者回管的一个策略

117
0:12:44.480 --> 0:12:55.360
就假设我现在在当前的主要的服务器的版本上面呢去检测到我的模型是有问题的或者可能会有问题或者我升级了这个时候呢用户可以请求切换到比较低的版本

118
0:12:55.600 --> 0:12:59.800
所以我们会同时维护非常多不同的版本方面我们去回顾的

119
0:13:01.720 --> 0:13:06.560
然后第二个特点呢就是我的卸载安装了它的整体的配置的顺序

120
0:13:07.080 --> 0:13:10.680
我的卸载安装的顺序呢还有我们的模型呢都是可以配置的

121
0:13:11.280 --> 0:13:18.720
方便我们解决问题之后呢我们推到新的版本我们可以结束回管那这种方式呢主要是对版本的维护

122
0:13:19.360 --> 0:13:23.600
我现在接触到的工作呢更多的是采用这种回管的策略

123
0:13:23.960 --> 0:13:50.480
好了我们来总结一下那今天呢我们看了一下推理系统的整体的架构希望大家呢能够对推理系统呢有个比较清晰或者全面的认识接着呢我们往下看一下今天我们给大家汇报了一下info developer还有surfing的一个相关的概念搞清楚这个概念之后呢我们就比较自然而然地去引入到了我们整个推理系统的整体的架构

124
0:13:51.440 --> 0:13:59.000
聊一聊架构之后呢跟AI比较相关的因为最大部分都是平台性的工作利于我们的揣敞的我们的服务化的工作啊

125
0:13:59.800 --> 0:14:12.560
对于AI比较相关的就是我们的模型的生命周期的管理那其实呢这个模型生命周期管理呢是一个非常大的学问我们有不同的一个策略针对不同的业务呢可能会采用不同的策略而不是一条论的
