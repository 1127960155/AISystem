0
0:00:00.000 --> 0:00:04.560
巴巴巴巴巴巴巴巴巴巴

1
0:00:06.560 --> 0:00:07.480
哈喽大家好

2
0:00:07.480 --> 0:00:11.640
我是那个饭后摆步走两腿疼一宿的ZOMI

3
0:00:12.080 --> 0:00:13.800
每次跟着那群架构师吃饭

4
0:00:14.080 --> 0:00:18.760
他们吃完晚饭之后都要散步快半小时到一个小时的

5
0:00:18.760 --> 0:00:19.680
我实在受不了了

6
0:00:19.680 --> 0:00:22.840
所以以后的吃饭我就不跟着他们去散步了

7
0:00:23.120 --> 0:00:27.600
今天我们主要是来到了推理系统这个系列里面的

8
0:00:27.600 --> 0:00:29.600
我个人觉得比较重要的一个章节

9
0:00:29.920 --> 0:00:31.960
就是推理引擎

10
0:00:31.960 --> 0:00:34.240
那我们会在后面的模型小进化

11
0:00:34.240 --> 0:00:35.760
离线优化压缩

12
0:00:35.760 --> 0:00:37.280
部署和运行优化

13
0:00:37.280 --> 0:00:42.920
这些内容都是围绕着我们整个推理引擎去展开的

14
0:00:42.920 --> 0:00:46.040
而推理系统我们不会再深入的去介绍了

15
0:00:46.040 --> 0:00:49.000
因为推理系统更多的是平台相关的工作

16
0:00:49.880 --> 0:00:51.840
这在传统的云服务服务器

17
0:00:52.040 --> 0:00:53.760
还有相关的一些工作

18
0:00:53.920 --> 0:00:57.240
其实已经有非常多的书籍视频

19
0:00:57.680 --> 0:01:01.000
而我更多的是聚焦于AI系统

20
0:01:01.000 --> 0:01:05.720
这个AI系统里面推理引擎是其中占的比较大的一块

21
0:01:05.720 --> 0:01:10.240
我们今天主要是给大家分开两个内容来去汇报的

22
0:01:10.240 --> 0:01:13.280
第一个就是去看看推理引擎的一个主要的特点

23
0:01:13.280 --> 0:01:14.400
有了这些特点

24
0:01:14.480 --> 0:01:17.240
我们看一下推理引擎的一个技术的挑战

25
0:01:17.240 --> 0:01:19.440
这些纯粹是吹牛逼的工作

26
0:01:19.440 --> 0:01:22.520
让大家了解一下具体的概念就好了

27
0:01:22.520 --> 0:01:25.320
就你可能会比较系统的去了解一下

28
0:01:25.520 --> 0:01:28.040
接着我们会在下个内容里面

29
0:01:28.040 --> 0:01:30.840
去给大家汇报一个整体的架构

30
0:01:30.840 --> 0:01:33.400
因为我们知道推理引擎有哪些模块

31
0:01:33.400 --> 0:01:34.200
有哪些架构

32
0:01:34.200 --> 0:01:35.640
它应该注意哪些功能

33
0:01:36.640 --> 0:01:38.960
我们就可以站在一个比较宏观的概念

34
0:01:38.960 --> 0:01:42.640
去了解整个推理引擎应该长成什幺样子

35
0:01:42.640 --> 0:01:44.880
面对我们自身自己的业务

36
0:01:44.880 --> 0:01:47.040
我应该设计一个怎幺样的推理引擎

37
0:01:47.040 --> 0:01:49.320
最后我们去简单的去看看

38
0:01:49.320 --> 0:01:51.240
整个推理引擎的工作流程

39
0:01:51.240 --> 0:01:52.520
它到底是怎幺样的

40
0:01:52.640 --> 0:01:54.880
这两个工作应该比较重要的

41
0:01:54.880 --> 0:01:57.800
或者内容我会去在下一节给大家汇报

42
0:01:57.800 --> 0:01:59.560
今天我们来吹吹牛逼

43
0:01:59.560 --> 0:02:02.160
去看看推理引擎的特点和技术挑战

44
0:02:05.720 --> 0:02:07.480
下面我们看看推理引擎

45
0:02:07.480 --> 0:02:11.040
我总结的有四个比较重要的特点

46
0:02:11.040 --> 0:02:12.640
第一个就是轻量化

47
0:02:12.640 --> 0:02:14.320
第二个你要做到通用

48
0:02:14.320 --> 0:02:17.320
第三个你除了通用你还要易用

49
0:02:17.320 --> 0:02:19.480
因为推理引擎其实易用性

50
0:02:19.480 --> 0:02:21.240
很多人去忽略掉的

51
0:02:21.280 --> 0:02:24.800
也是我们一开始去追求极致的高效

52
0:02:24.800 --> 0:02:27.320
就是高性能的时候去忽略掉的

53
0:02:27.320 --> 0:02:29.760
所以说在这幺多特点里面

54
0:02:29.880 --> 0:02:32.000
我觉得最重要的就是高效

55
0:02:32.000 --> 0:02:34.160
还有易用两个点

56
0:02:35.160 --> 0:02:38.600
接下来我们每一个点去打开

57
0:02:41.760 --> 0:02:45.440
首先第一个就是高性能就高效的问题

58
0:02:47.040 --> 0:02:49.440
推理引擎其实要适配非常多的

59
0:02:49.480 --> 0:02:51.680
不同的架构和操作系统

60
0:02:51.680 --> 0:02:53.760
我们希望在单线层下面

61
0:02:53.760 --> 0:02:56.040
不管我们到时候跑的是一个并行还是什幺

62
0:02:56.040 --> 0:02:58.120
希望尽可能的在单线层下面

63
0:02:58.480 --> 0:03:01.000
整个模型的运行效率

64
0:03:01.000 --> 0:03:04.600
是占满我们整个设备的计算的峰值

65
0:03:04.600 --> 0:03:06.240
不管是理论峰值还是实际峰值

66
0:03:06.240 --> 0:03:08.840
我们希望跑得越快越好

67
0:03:09.360 --> 0:03:11.800
第二个点就是针对对应的一个加速芯片

68
0:03:11.920 --> 0:03:14.400
我们希望能够做到一个深度的调油

69
0:03:14.400 --> 0:03:17.200
做一个极致性能的优化

70
0:03:17.320 --> 0:03:19.360
我们可能会在OpenCL或者Win卡上面

71
0:03:19.720 --> 0:03:22.000
去做不同的一些写不同的算子

72
0:03:22.000 --> 0:03:23.160
写不同的kernel

73
0:03:23.160 --> 0:03:26.480
可能甚至我们会编写一些会议编的代码

74
0:03:26.480 --> 0:03:28.600
或者SIMT的一些代码

75
0:03:28.600 --> 0:03:30.960
充分的去发挥我们的算力

76
0:03:31.320 --> 0:03:33.520
说白了就是越快越好

77
0:03:33.520 --> 0:03:35.360
不管我用什幺方式去实现

78
0:03:35.360 --> 0:03:38.040
反正你得给我实现的越快越好

79
0:03:39.000 --> 0:03:42.760
最后我们需要支持不同精度的一个计算

80
0:03:42.760 --> 0:03:45.760
在不同的架构上面去进行一个适配的

81
0:03:45.800 --> 0:03:48.920
这个就是高性能的一个具体的特点

82
0:03:49.240 --> 0:03:52.320
可以看到其实我们不同不管是哪个框架

83
0:03:53.200 --> 0:03:55.080
这些都是一些推理的框架

84
0:03:55.080 --> 0:03:56.800
大家可以简单的去了解一下

85
0:03:56.800 --> 0:03:58.480
有NCAN腾讯的

86
0:03:58.480 --> 0:04:00.720
还有MAC小米的

87
0:04:00.960 --> 0:04:02.120
TFLite谷歌的

88
0:04:02.120 --> 0:04:03.040
还有QML

89
0:04:03.040 --> 0:04:04.600
苹果的MMN的

90
0:04:04.600 --> 0:04:05.480
阿里的单压

91
0:04:05.600 --> 0:04:07.680
还有华为的Mathbot Lite

92
0:04:08.760 --> 0:04:10.520
华为的产品其实我也不知道为啥

93
0:04:10.520 --> 0:04:13.120
大家很少把它做一个对标的例子

94
0:04:13.520 --> 0:04:15.720
可以看到了不管是哪个情况

95
0:04:15.760 --> 0:04:18.840
我们都会在不同的设备上面去做一个比较

96
0:04:18.840 --> 0:04:20.680
这些比较就是比较性能

97
0:04:21.480 --> 0:04:23.320
性能才是第一位的

98
0:04:23.320 --> 0:04:25.000
性能才是我们推理引擎

99
0:04:25.000 --> 0:04:27.160
所聚焦的最重要的一个点

100
0:04:29.800 --> 0:04:32.160
接着我们看一下轻量化

101
0:04:33.160 --> 0:04:35.240
因为我们的推理引擎

102
0:04:35.360 --> 0:04:38.520
要部署到不同的硬件设备上面

103
0:04:38.520 --> 0:04:40.240
这个就是华为

104
0:04:40.840 --> 0:04:45.240
从2012年到2019年的相关旗舰手机

105
0:04:45.360 --> 0:04:46.400
我们的推理引擎

106
0:04:46.480 --> 0:04:49.000
要部署在不同的一个手机上面

107
0:04:49.000 --> 0:04:50.080
不同的设备上面

108
0:04:50.080 --> 0:04:51.200
对我们的要求

109
0:04:51.200 --> 0:04:53.080
轻量化的要求是非常高的

110
0:04:53.080 --> 0:04:56.560
第二个可能我们还会部署在一些手表

111
0:04:56.560 --> 0:04:57.960
手机耳环上面

112
0:04:58.480 --> 0:05:01.080
这个时候这些包括我们的手机

113
0:05:01.280 --> 0:05:03.560
包括我们的耳机降噪功能也好

114
0:05:03.560 --> 0:05:06.800
我们都会有对应的一些AI的系统

115
0:05:06.800 --> 0:05:07.560
AI的设备

116
0:05:07.560 --> 0:05:09.080
推理的引擎

117
0:05:09.280 --> 0:05:11.120
这个时候就需要我们进一步的

118
0:05:11.120 --> 0:05:13.560
去降低我们整个包的大小

119
0:05:14.440 --> 0:05:15.520
简单的来说

120
0:05:15.720 --> 0:05:18.200
我们在手机上面去部署的推理引擎

121
0:05:18.200 --> 0:05:19.840
可能在我们手表

122
0:05:19.840 --> 0:05:22.520
还有耳机上面部署的推理引擎

123
0:05:22.520 --> 0:05:23.520
是不同的

124
0:05:23.720 --> 0:05:25.400
我们举个具体的例子

125
0:05:25.400 --> 0:05:27.160
我们现在华为手机上面

126
0:05:27.280 --> 0:05:28.760
部署的是Mathball Lite

127
0:05:28.960 --> 0:05:29.880
可能在手表上面

128
0:05:29.960 --> 0:05:31.480
我们部署的是Mathball的

129
0:05:31.480 --> 0:05:32.920
Macro的版本

130
0:05:32.920 --> 0:05:34.640
所以说它不同的版本

131
0:05:34.640 --> 0:05:36.640
对轻量化的要求是不一样的

132
0:05:36.640 --> 0:05:37.440
不同的设备

133
0:05:37.480 --> 0:05:39.440
对轻量化的要求也不一样

134
0:05:41.360 --> 0:05:45.160
接着我们看一个通用性的问题

135
0:05:45.160 --> 0:05:46.160
作为一个推理引擎

136
0:05:46.280 --> 0:05:48.400
我需要支持非常多不同的框架

137
0:05:48.400 --> 0:05:50.200
训练出来的一个格式

138
0:05:50.240 --> 0:05:52.920
而且我还要支持很多不同的

139
0:05:52.920 --> 0:05:54.720
主流的网络模型结构

140
0:05:54.720 --> 0:05:57.480
所以说为什幺做系统的人要懂算法

141
0:05:57.480 --> 0:05:59.400
我们现在其实有很多系统的

142
0:05:59.400 --> 0:06:00.960
工程师是不懂算法的

143
0:06:00.960 --> 0:06:02.240
或者从其他产品线

144
0:06:02.240 --> 0:06:04.920
或者传统的一些优先过来的

145
0:06:05.360 --> 0:06:07.040
懂算法很重要

146
0:06:07.040 --> 0:06:09.560
懂业务也很重要

147
0:06:09.800 --> 0:06:10.760
接着我们可以看到

148
0:06:10.800 --> 0:06:12.080
其实我们通用性

149
0:06:12.080 --> 0:06:14.120
会遇到很大的一个挑战

150
0:06:14.120 --> 0:06:16.280
这个也是会在后面去讲

151
0:06:16.280 --> 0:06:17.440
有什幺解决方案的

152
0:06:17.440 --> 0:06:19.880
第一个就是支持多输入的多输出

153
0:06:19.880 --> 0:06:22.080
还有任意维度的输入输出

154
0:06:22.080 --> 0:06:24.560
可能还会有动态的batch

155
0:06:24.560 --> 0:06:27.800
另外可能还会支持带空滋流的模型

156
0:06:27.800 --> 0:06:31.520
这些都是非常大的一些挑战和特点

157
0:06:32.640 --> 0:06:34.120
而在中米的眼中

158
0:06:34.320 --> 0:06:35.720
可能动态的输入

159
0:06:35.880 --> 0:06:37.800
是比较大的一个挑战

160
0:06:37.800 --> 0:06:39.880
对我们的一个推进引擎来说

161
0:06:39.880 --> 0:06:42.160
因为我们在处理一些NLP的样术

162
0:06:42.280 --> 0:06:43.160
例如BERT的时候

163
0:06:43.160 --> 0:06:45.400
我们的输入是一个变长的串行

164
0:06:45.400 --> 0:06:48.000
变长的串行就需要支持动态的输入

165
0:06:48.000 --> 0:06:49.960
对我们的一个引擎来说

166
0:06:50.080 --> 0:06:52.080
推进引擎是一个很大的挑战

167
0:06:52.080 --> 0:06:53.080
接着我们看一下

168
0:06:53.080 --> 0:06:54.480
可能我们还要支持服务器

169
0:06:54.600 --> 0:06:56.720
跟电脑还有不同的操作系统

170
0:06:56.720 --> 0:06:58.640
可以看到我讲的是举个例子

171
0:06:58.640 --> 0:07:00.080
现在华为的设备

172
0:07:00.200 --> 0:07:01.920
我都是来用华为作为例子

173
0:07:02.440 --> 0:07:04.160
虽然我不在终端产品线

174
0:07:04.840 --> 0:07:06.920
像华为它自己就有非常多的

175
0:07:06.920 --> 0:07:07.720
不同的设备

176
0:07:07.720 --> 0:07:08.960
包括笔记本的显示器

177
0:07:09.080 --> 0:07:10.440
还有平板智能屏

178
0:07:10.480 --> 0:07:11.600
还有可穿戴的设备

179
0:07:11.600 --> 0:07:13.680
包括现在还推出了一个打印机

180
0:07:14.760 --> 0:07:16.000
像这些设备

181
0:07:16.480 --> 0:07:18.800
都是用不同的操作系统的

182
0:07:19.160 --> 0:07:22.200
而且它都有不同的枪式的一个接口

183
0:07:22.200 --> 0:07:25.320
所以我们需要支持非常多的一个设备

184
0:07:25.320 --> 0:07:26.440
还有操作系统

185
0:07:26.440 --> 0:07:27.520
对于通用性来说

186
0:07:27.720 --> 0:07:29.440
挑战是非常大的

187
0:07:32.200 --> 0:07:34.480
最后我们看一下易用性

188
0:07:34.720 --> 0:07:36.560
易用性对于普通用户来说

189
0:07:36.560 --> 0:07:37.280
可能不感知

190
0:07:37.280 --> 0:07:39.280
但是对于开发者来说

191
0:07:39.280 --> 0:07:41.480
是非常重要的一个内容

192
0:07:41.480 --> 0:07:43.280
我们可能会使用很多

193
0:07:43.280 --> 0:07:44.000
lumpi的算子

194
0:07:44.000 --> 0:07:45.880
去做一些常用的一个计算

195
0:07:46.080 --> 0:07:46.560
这个时候

196
0:07:46.880 --> 0:07:48.800
AI推理框架怎幺跟它混用呢

197
0:07:50.600 --> 0:07:51.080
另外的话

198
0:07:51.080 --> 0:07:51.680
我们的任务

199
0:07:51.840 --> 0:07:54.480
可能更多的是聚焦CV或者NLP

200
0:07:54.480 --> 0:07:55.280
常用的任务

201
0:07:55.280 --> 0:07:57.080
我们希望不需要拧入大量的

202
0:07:57.080 --> 0:07:58.600
OpenCV这种三方的包

203
0:07:58.600 --> 0:08:01.360
而是怎幺样的快速的给用户

204
0:08:01.400 --> 0:08:03.840
给开发者提供相关的API

205
0:08:03.840 --> 0:08:06.720
最后可能会支持很多平台的模型训练

206
0:08:06.720 --> 0:08:07.960
还有丰富的API

207
0:08:07.960 --> 0:08:10.920
中米觉得第一点和第二点是比较重要的

208
0:08:10.920 --> 0:08:12.560
而我只有第一点和第二点

209
0:08:12.720 --> 0:08:14.760
其实提过了一些相关的想法

210
0:08:14.760 --> 0:08:17.280
自己当时候也手撸了一把

211
0:08:17.600 --> 0:08:19.400
确实做了非常多的相关的工作

212
0:08:19.400 --> 0:08:21.320
在这里面打了非常多的代码

213
0:08:24.480 --> 0:08:27.040
接着我们看一下技术的挑战

214
0:08:27.040 --> 0:08:28.240
我们的challenge

215
0:08:29.600 --> 0:08:30.480
技术的挑战

216
0:08:30.520 --> 0:08:32.320
其实了解完刚才的一些特点

217
0:08:32.440 --> 0:08:34.440
我们就可以总结了一些矛盾点

218
0:08:34.440 --> 0:08:36.840
第一个就是我们的AI的需求很复杂

219
0:08:36.840 --> 0:08:39.000
那进程的大小有限吗

220
0:08:40.120 --> 0:08:41.120
我们举几个例子

221
0:08:41.120 --> 0:08:43.520
就现在Pytorch有1200多个算子

222
0:08:43.520 --> 0:08:45.560
TensorFlow接近2000多个算子

223
0:08:45.560 --> 0:08:46.960
但是我们推理引擎

224
0:08:47.400 --> 0:08:50.240
针对每个后端都要提供这幺多算子吗

225
0:08:50.240 --> 0:08:51.400
那可不爆炸了

226
0:08:51.680 --> 0:08:52.680
针对马力的GPU

227
0:08:52.680 --> 0:08:54.120
我提供1200多个算子

228
0:08:54.560 --> 0:08:55.640
针对骁龙的芯片

229
0:08:55.640 --> 0:08:57.200
我们提供1200多个算子

230
0:08:57.200 --> 0:08:58.160
那还得了

231
0:08:58.160 --> 0:08:59.120
我们克隆开发算子

232
0:08:59.120 --> 0:09:00.680
开发的同时自己累死了

233
0:09:00.680 --> 0:09:01.760
天天加班三班

234
0:09:01.760 --> 0:09:02.880
倒都搞不定了

235
0:09:03.720 --> 0:09:05.040
所以说这个时候

236
0:09:05.280 --> 0:09:07.440
我们的需求是非常复杂的

237
0:09:07.440 --> 0:09:09.040
但是我们的进程大小有限

238
0:09:09.040 --> 0:09:11.480
我们的人员开发工作量也有限

239
0:09:11.760 --> 0:09:12.840
接着我们看一下

240
0:09:13.160 --> 0:09:14.200
我们的AI

241
0:09:14.400 --> 0:09:15.880
其实除了我们的模型推理

242
0:09:15.880 --> 0:09:18.760
我们还包括很多的千维域处理的问题

243
0:09:18.760 --> 0:09:20.800
不希望引入大量的三方依赖

244
0:09:20.800 --> 0:09:23.120
这个时候就需要有限的支持

245
0:09:23.480 --> 0:09:26.600
所以说这里面两个都是一个矛盾的点

246
0:09:26.920 --> 0:09:28.600
接着我们的算力需求

247
0:09:28.600 --> 0:09:30.160
跟我们的资源碎片化

248
0:09:30.440 --> 0:09:31.800
是一个矛盾的点

249
0:09:32.160 --> 0:09:33.520
我们的AI模型

250
0:09:33.760 --> 0:09:36.560
往往都是需要非常大量的计算量的

251
0:09:36.840 --> 0:09:37.800
但是我们推理引擎

252
0:09:37.880 --> 0:09:40.800
大部分都在一个IoT的设备

253
0:09:40.800 --> 0:09:42.960
或者一些边缘的设备进行一个推理的

254
0:09:43.280 --> 0:09:46.600
这个时候就会引入一个很大的矛盾

255
0:09:46.600 --> 0:09:48.720
我需要大量的一个计算资源

256
0:09:48.720 --> 0:09:51.600
但是我的硬件计算资源有限

257
0:09:51.880 --> 0:09:52.480
怎幺协调

258
0:09:52.840 --> 0:09:53.440
怎幺综合

259
0:09:54.040 --> 0:09:55.680
第二个就是我的计算资源

260
0:09:56.120 --> 0:09:58.080
我就以一个手机作为例子

261
0:09:58.240 --> 0:10:00.120
手机它是一款SoC

262
0:10:00.520 --> 0:10:03.120
SoC里面就包括用了ARM的CPU

263
0:10:03.400 --> 0:10:05.360
可能会用了马力的GPU

264
0:10:05.680 --> 0:10:07.800
可能会有自己的一个DSP

265
0:10:08.120 --> 0:10:09.800
麒麟就会有自己的DSP

266
0:10:10.600 --> 0:10:14.240
可能华为麒麟里面还有一个自己的NPU

267
0:10:16.640 --> 0:10:17.880
这个时候可以看到

268
0:10:17.880 --> 0:10:19.840
就算一款手指甲大小

269
0:10:20.120 --> 0:10:23.280
放在手机里面的一款芯片

270
0:10:23.640 --> 0:10:26.560
里面的计算资源都是极度的碎片化的

271
0:10:26.960 --> 0:10:28.840
每一个不同的硬件

272
0:10:28.840 --> 0:10:30.360
每一个不同的IP

273
0:10:30.520 --> 0:10:32.240
我们CPU可能当一个IP

274
0:10:32.240 --> 0:10:33.560
GPU当一个IP

275
0:10:33.800 --> 0:10:36.200
它都会有自己的一个编程的体系

276
0:10:36.680 --> 0:10:39.920
这个时候会使我们的进程极度的膨胀

277
0:10:40.240 --> 0:10:41.880
这是两个比较大的矛盾

278
0:10:45.120 --> 0:10:49.960
最后一个就是执行效率跟模型精度的一个矛盾

279
0:10:50.320 --> 0:10:53.800
我们希望我们的模型的效率跑得越快越好

280
0:10:54.040 --> 0:10:55.360
而且精度越高越好

281
0:10:55.680 --> 0:10:57.960
但是我们的模型的精度

282
0:10:57.960 --> 0:10:59.120
模型变小了

283
0:10:59.120 --> 0:11:01.240
模型的精度就会下降

284
0:11:01.640 --> 0:11:03.400
这是个不可调和的矛盾

285
0:11:03.880 --> 0:11:06.280
于是引入了很多这种量化的技术

286
0:11:06.280 --> 0:11:07.200
压缩的技术

287
0:11:07.200 --> 0:11:08.400
就希望模型小一点

288
0:11:08.400 --> 0:11:10.800
但是保持我们的原模型的状态

289
0:11:10.800 --> 0:11:14.920
还有很多做一个模型创新小模型的研究

290
0:11:15.360 --> 0:11:17.720
接着第二点就是云测的模型的精度

291
0:11:17.800 --> 0:11:20.360
我们希望在训练的时候精度越高越好

292
0:11:20.600 --> 0:11:22.920
转移到端测的时候越小越好

293
0:11:22.920 --> 0:11:24.440
但精度你不能给我掉

294
0:11:24.760 --> 0:11:28.600
例如我们现在在做一个大模型

295
0:11:29.200 --> 0:11:32.800
大模型它可能会有千亿的规模的参数量

296
0:11:32.800 --> 0:11:36.360
千亿规模可能动辄就上几百兆或者一个G

297
0:11:36.680 --> 0:11:39.520
一个G的一个模型你推到我们手机上面

298
0:11:39.520 --> 0:11:40.680
那不卡死了吗

299
0:11:40.680 --> 0:11:42.520
手机里面就两个G的内存

300
0:11:42.520 --> 0:11:45.040
你还希望它塞你一个模型进一个G吗

301
0:11:45.040 --> 0:11:46.000
这不可能的

302
0:11:46.840 --> 0:11:50.240
所以我们希望对我们的大模型进行一个100倍的压缩

303
0:11:50.440 --> 0:11:55.240
保持可能0.5%或者千分之五的一个进步的损失

304
0:11:55.720 --> 0:11:56.520
这个时候怎幺做

305
0:11:56.720 --> 0:11:58.720
这是一个很大的挑战

306
0:12:01.440 --> 0:12:03.920
好了今天的内容就到这里为止

307
0:12:03.920 --> 0:12:06.960
我们今天主要是讲了推进引擎的一个主要的特点

308
0:12:06.960 --> 0:12:10.920
四个特点效率通用性应用性轻量化

309
0:12:11.280 --> 0:12:15.800
最后我们了解了一下一些不可避免的一些技术的挑战

310
0:12:15.800 --> 0:12:18.120
主要是有几个矛盾所引起的

311
0:12:18.400 --> 0:12:20.120
自由的碎片化算定的需求大

312
0:12:20.440 --> 0:12:22.520
模型的精度高模型小

313
0:12:24.000 --> 0:12:26.520
那今天我们了解完这些基础的概念

314
0:12:26.520 --> 0:12:28.600
可能我的语速会稍微快一点没关系

315
0:12:28.600 --> 0:12:30.720
这是只是一些简单的concept

316
0:12:31.080 --> 0:12:32.840
再往下一节内容里面

317
0:12:32.960 --> 0:12:35.560
我们就会讲整体的架构和工作流程了

318
0:12:35.560 --> 0:12:36.760
好了谢谢各位

319
0:12:36.760 --> 0:12:37.640
拜拜

320
0:12:38.120 --> 0:12:39.680
卷的不行了卷的不行了

321
0:12:39.680 --> 0:12:41.120
记得一键三连加关注

322
0:12:41.560 --> 0:12:44.600
所有的内容都会开源在下面这条链接里面

323
0:12:45.200 --> 0:12:45.880
拜拜

