0
0:00:00.000 --> 0:00:10.600


1
0:00:10.600 --> 0:00:22.920
ZOMI每次跟着那群架构师吃饭呢他们吃完晚饭之后呢都要散步快半小时到一个小时的我实在受不了了所以以后的吃饭呢我就不跟着他们去散步了

2
0:00:22.920 --> 0:00:32.200
今天的我们主要是来到了推理系统这个系列里面的我个人觉得比较重要的一个章节就是推理引擎

3
0:00:32.200 --> 0:00:43.040
让我们会在后面的模型小心化了宁泄用化压缩护所和运行优化这些内容呢都是围绕着我们整个推理引擎去展开的

4
0:00:43.040 --> 0:00:49.800
而推理系统呢我们不会再深入地去介绍了因为推理系统更多的是平台相关的工作

5
0:00:49.800 --> 0:01:01.280
这在传统的云服务啊服务器啊还有相关的一些工作呢其实已经有非常多的书籍啊视频啊而我呢更多的是聚焦于AI系统

6
0:01:01.280 --> 0:01:05.840
那这个AI系统里面推理引擎呢是其中赞的比较大的一块

7
0:01:05.840 --> 0:01:13.480
那我们今天呢则是给大家分开两个内容来去汇报的第一个就是去看看推理引擎的一个主要的特点

8
0:01:13.480 --> 0:01:22.600
有了这些特点呢我们看看推理引擎的一个技术的挑战那这些呢纯粹是吹牛逼的工作让大家了解一下具体的概念就好了

9
0:01:22.600 --> 0:01:30.920
就你可能会比较系统地去了解一下接着呢我们会在下个内容里面去给大家汇报一个整体的架构

10
0:01:30.920 --> 0:01:36.720
因为我们知道推理引擎有哪些模块有哪些架构它应该注意哪些功能

11
0:01:36.720 --> 0:01:47.200
我们就可以站在一个比较宏观的概念去了解整个推理引擎应该长成什么样子面对我们自身自己的业务我应该设计一个怎么样的推理引擎

12
0:01:47.200 --> 0:01:57.800
最后呢我们去简单地去看看整个推理引擎的工作流程它到底是怎么样的那这两个工作应该比较重要的或内容呢我会去在下一节给大家汇报

13
0:01:57.800 --> 0:02:02.160
今天的我们来吹吹牛逼去看看推理引擎的特点和技术挑战

14
0:02:05.680 --> 0:02:27.320
下面的我们看看推理引擎我总结的有四个比较重要的特点第一个呢就是轻量化第二个呢你要做到通用第三个你除了通用你还要易用因为推理引擎的其实易用性啊很多人去忽略掉的也是我们一开始呢去追求极致的高效就是高性能的时候去忽略掉的

15
0:02:27.320 --> 0:02:34.040
所以说呢在这么多特点里面呢我觉得最重要的就是高效还有易用两个点

16
0:02:35.160 --> 0:02:38.480
接下来呢我们每一个点去打开

17
0:02:41.920 --> 0:02:45.720
首先第一个就是高性能就高效的问题嘛

18
0:02:47.120 --> 0:02:55.880
推理引擎呢其实要适配非常多的不同的架构和操作系统我们希望在单线层下面不管我们到时候跑的是一个并行还是什么

19
0:02:55.880 --> 0:03:08.600
希望尽可能地在单线层下面呢整个模型的运行效率是赞满我们整个设备的计算的峰值不管是理论峰值还是实际峰值我们希望跑得越快越好

20
0:03:09.440 --> 0:03:17.160
第二个点呢就是针对对应的一个加速芯片呢我们希望能够做到一个深度的调油做一个极致性能的优化

21
0:03:17.240 --> 0:03:37.960
那我们可能会在OpenCL或者温卡上面呢去做不同的一些写不同的算子写不同的科诺可能甚至我们会编写一些会议编的代码或者SIMT的一些代码充分地去发挥我们的算力说白了就是越快越好不管我用什么方式去实现反正你得给我实现得越快越好

22
0:03:39.080 --> 0:03:45.760
最后呢我们需要支持不同精度的一个计算在不同的架构上面呢去进行一个适配的

23
0:03:45.760 --> 0:03:52.560
这个就是高性能的一个具体的特点那可以看到呢其实我们不同不管是哪个框架啊

24
0:03:52.800 --> 0:04:00.760
嗯这些呢都是一些推理的框架大家可以简单的去了解一下有NCN呢腾讯的还有美司呢小米的

25
0:04:00.960 --> 0:04:08.000
电子赖的谷歌的还有宽幅平幅的阿里的丹娅呢还有那个华为的麦斯博赖特呢

26
0:04:08.800 --> 0:04:20.600
华为产品呢其实我也不知道为啥大家很少把它做一个对标的例子可以看到了不管是哪个情况我们都会在不同的设备上面去做一个比较这些比较呢就是比较性能

27
0:04:21.680 --> 0:04:32.000
性能才是第一位的性能才是我们推进型所聚焦的最重要的一个点那接着呢我们看一下轻量化

28
0:04:33.000 --> 0:04:39.800
因为呢我们的推进型呢要部署到不同的硬件设备上面这个呢就是华为

29
0:04:40.800 --> 0:04:57.600
从二零一二年呢到二零一九年的相关旗舰手机啊我们的推进型呢要部署在不同的一个手机上面不同的设备上面对我们的要求轻量化的要求是非常高的第二个呢可能我们还会部署在一些手表手机耳环上面

30
0:04:58.400 --> 0:05:13.400
那这个时候呢这些包括我们的手机啊包括我们的耳机啊降噪功能啊也好我们都会有对应的一些AI的系统AI的设备推理的引擎那这个时候呢就需要我们进一步的去降低我们整个包的大小

31
0:05:14.700 --> 0:05:23.500
简单的来说呢我们在手机上面去部署的推理引擎可能在我们手表还有耳机上面部署的推理引擎是不同的

32
0:05:23.500 --> 0:05:32.900
我们举个具体的例子就我们现在华为手机上面的部署的是马斯博莱特那可能在手表上面的我们部署的是马斯博的迈克的版本

33
0:05:32.900 --> 0:05:39.300
所以说呢它不同版本对轻量化的要求是不一样的不同的设备对轻量化的要求也不一样

34
0:05:41.300 --> 0:05:50.100
接着呢我们看一个一个通用性的问题做一个推理引擎呢我需要支持非常多不同的框架训练出来的一个格式

35
0:05:50.100 --> 0:06:09.600
而且呢我还要支持很多不同的主流的网络模型结构所以说为什么做系统的人要懂算法我们现在其实有很多系统的工程师呢是不懂算法的或者从其他产品线或者传统的一些优先过来的懂算法很重要懂业务也很重要

36
0:06:09.600 --> 0:06:19.900
那接着我们可以看到其实我们通用性会遇到很大的一个挑战这个呢也是会在后面去讲有什么解决方案的一个呢就是支持多输入的多输出

37
0:06:19.900 --> 0:06:31.400
还有任意维度的输出可能还会有动态的败去另外可能还会支持带控制流的模型这些都是非常大的一些挑战和特点

38
0:06:32.700 --> 0:06:49.800
而在而在综艺的眼中呢可能动态的输呢是比较大的一个挑战对我们的一个推理引擎来说因为我们在处理一些NLP的项目啊例如贝特的时候我们的输呢是一个变长的序列变长的序列就需要支持动态的输对我们的一个引擎来说

39
0:06:49.800 --> 0:07:19.800
推理引擎是一个很大的挑战接着呢我们看一下可能我们还要支持服务器啊跟电脑啊有不同的操作系统可以看到我假设举个例子现在就是华为的设备啊我都是来用华为作为例子啊虽然我不在终端产品线像华为它自己就有非常多的不同的设备包括笔记本的显示器了还有平板智慧屏还有可穿戴的设备包括现在还推出了一个打印机像这些设备呀都是用不同的操作系统的而且它

40
0:07:19.800 --> 0:07:49.800
都有不同的枪式的一个接口所以我们需要支持非常多的一个设备还有操作系统对于通用性来说呢挑战是非常大的最后呢我们看一下易用性易用性呢对于普通用户来说可能不感知但是对于开发者来说是非常重要的一个内容我们可能会使用很多能派的算子去做一些常用的一个计算那这个时候呢AI推理框架怎么跟它混用呢

41
0:07:49.800 --> 0:07:50.800
另外的话我们的任务呢可能更多的是聚焦CV啊或者NLP常用的任务我们希望不希望拟入大量的OpenCV这种三方的包而是怎么样的快速地给用户给开发者提供相关的API最后呢可能会支持很多平台的模型训练还有丰富的API中米呢觉得第一点呢和第二点呢是比较重要的而我只有第一点和第二点呢其实提过了一些相关的想法自己呢当时候也手撸了一把确实做了非常多的相关的工作在这里

42
0:08:19.800 --> 0:08:49.800
面打了非常多的代码接着呢我们看一下技术的挑战我们的challenge技术的挑战其实了解完刚才的一些特点呢我们就可以总结了一些矛盾点第一个呢就是我们的AI的需求很复杂那程序的大小有限吗我们举几个例子就现在的派套许有一千两百多个算子腾斯富几近两千多个算子但是我们推理引擎针对每个后端都要提供这么多算子

43
0:08:49.800 --> 0:09:19.800
吗那可不爆炸了吧针对马力的GPU我提供一千两百多个算子针对骁龙的芯片我们提供一千两百多个算子那还得了我们克隆开发算子开发的同事直接累死了天天加班三班倒都搞不定了吧所以说这个时候呢我们的需求是非常复杂的但是我们的程序大小有限我们的人员开发工作量也有限那接着呢我们看一下就是我们的AI呀其实除了我们的模型推理我们还包括很多的潜力处理的问题不希望引入大量的

44
0:09:19.800 --> 0:09:49.800
三方依赖这个时候呢就需要有限的支持所以说呢这里面两个都是一个矛盾的点那接着呢我们的算力需求跟我们的资源碎片化是一个矛盾的点我们的AI模型啊往往都是需要非常大量的计算量的但是我们推理引擎的大部分都在一个IoT的设备或者一些边缘的设备进行一个推理的这个时候呢就会引入一个很大的矛盾我需要大量的一个计算资源但是呢

45
0:09:49.800 --> 0:10:19.800
硬件计算资源有限怎么协调呢怎么综合呢第二个呢就是我的计算资源了我就以一个手机作为例子手机呢它是一款SOCSOC里面呢就包括用了ARM的CPU可能会用了马力的GPU可能会有自己的一个DSP那麒麟呢就会有自己的DSP可能华为麒麟里面呢还有一个自己的MPU这个时候呢可以看到就算一款手指甲大小

46
0:10:19.800 --> 0:10:49.800
放在手机里面的一款芯片里面的计算资源都是极度的碎片化的每一个不同的硬件每个不同的IP那我们CPU可能当一个IPGPU当一个IP它都会有自己的一个编程的体系这个时候会使我们的程序极度的膨胀这是两个比较大的矛盾最后一个呢就是执行效率跟模型精度的一个矛盾

47
0:10:49.800 --> 0:11:19.800
我们希望我们的模型的效率跑得越快越好而且精度越高越好但是我们的模型的精度啊模型变小了模型的精度就会下降这是个不可调和的矛盾于是呢引入了很多这种量化的技术压缩的技术就希望模型小一点但是保持我们的原模型的状态还有很多做一个模型创新小模型的研究那接着第二点就是云测的模型的精度呢我们希望在训练的时候精度越高

48
0:11:19.800 --> 0:11:49.800
越好转移到端侧的时候呢越小越好但精度你不能给我掉哦那我例如呢我们现在在做一个大模型嘛就大模型它可能会有千亿的规模的参数量千亿规模呢可能动辄就上几百兆或者一个G那一个G的一个模型你推到我们手机上面那不卡死了嘛而手机里面就两个G的内存你还希望它塞你一个模型就一个G吗这不可能的所以呢我们希望对我们的大模型进行一个一百倍的压力

49
0:11:49.800 --> 0:12:19.800
打缩保持可能一点五百分之五或者千分之五的一个精度的损失那这个时候怎么做呢这是一个很大的挑战好了今天的内容呢就到这里为止我们今天的主要是讲了推进引擎的一个主要的特点四个特点效率通用性应用性轻量化那最后呢我们了解了一下一些不可避免的一些技术的挑战主要是由几个矛盾所引起的资源的碎片化算计的需求

50
0:12:19.800 --> 0:12:46.440
大模型的精度高模型小那今天呢我们了解完这些基础的概念可能我的语速会稍微快一点没关系这是只是一些简单的概念在往下一节内容里面呢我们就会讲整体的架构和工作流程了

